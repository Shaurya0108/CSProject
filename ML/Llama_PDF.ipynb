{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**Llama 2+ Pinecone + LangChain**"
      ],
      "metadata": {
        "id": "7I2wTiMSu3PB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Step 1: Install All the Required Pakages**"
      ],
      "metadata": {
        "id": "udUBTNuQXOpZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_aEfiWhkWBHt",
        "outputId": "203cc6a7-a88e-4a70-ec1b-5441168b172a",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1702083316515,
          "user_tz": 360,
          "elapsed": 66628,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.0.348)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.23)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.8.6)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.6.3)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.33)\n",
            "Requirement already satisfied: langchain-core<0.1,>=0.0.12 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.0.12)\n",
            "Requirement already satisfied: langsmith<0.1.0,>=0.0.63 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.0.69)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.23.4)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.10.13)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (3.3.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.20.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)\n",
            "Requirement already satisfied: anyio<5,>=3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.1,>=0.0.12->langchain) (3.7.1)\n",
            "Requirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.1,>=0.0.12->langchain) (23.2)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.8.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2023.7.22)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.1,>=0.0.12->langchain) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.1,>=0.0.12->langchain) (1.1.3)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n",
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.10/dist-packages (3.17.1)\n",
            "Requirement already satisfied: unstructured in /usr/local/lib/python3.10/dist-packages (0.11.2)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.10/dist-packages (from unstructured) (5.2.0)\n",
            "Requirement already satisfied: filetype in /usr/local/lib/python3.10/dist-packages (from unstructured) (1.2.0)\n",
            "Requirement already satisfied: python-magic in /usr/local/lib/python3.10/dist-packages (from unstructured) (0.4.27)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from unstructured) (4.9.3)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from unstructured) (3.8.1)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from unstructured) (0.9.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from unstructured) (2.31.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from unstructured) (4.11.2)\n",
            "Requirement already satisfied: emoji in /usr/local/lib/python3.10/dist-packages (from unstructured) (2.9.0)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.10/dist-packages (from unstructured) (0.6.3)\n",
            "Requirement already satisfied: python-iso639 in /usr/local/lib/python3.10/dist-packages (from unstructured) (2023.6.15)\n",
            "Requirement already satisfied: langdetect in /usr/local/lib/python3.10/dist-packages (from unstructured) (1.0.9)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from unstructured) (1.23.4)\n",
            "Requirement already satisfied: rapidfuzz in /usr/local/lib/python3.10/dist-packages (from unstructured) (3.5.2)\n",
            "Requirement already satisfied: backoff in /usr/local/lib/python3.10/dist-packages (from unstructured) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from unstructured) (4.8.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from unstructured) (1.14.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->unstructured) (2.5)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json->unstructured) (3.20.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json->unstructured) (0.9.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from langdetect->unstructured) (1.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured) (4.66.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->unstructured) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->unstructured) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->unstructured) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->unstructured) (2023.7.22)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->unstructured) (23.2)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json->unstructured) (1.0.0)\n",
            "Requirement already satisfied: sentence_transformers in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.35.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.66.1)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (2.1.0+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (0.16.0+cu118)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.23.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.11.3)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (3.8.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (0.1.99)\n",
            "Requirement already satisfied: huggingface-hub>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (0.19.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2.31.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (4.8.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (23.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (3.1.2)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (2.1.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (2023.6.3)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (0.4.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->sentence_transformers) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->sentence_transformers) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (3.2.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->sentence_transformers) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->sentence_transformers) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2023.7.22)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->sentence_transformers) (1.3.0)\n",
            "Requirement already satisfied: pinecone-client in /usr/local/lib/python3.10/dist-packages (2.2.4)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (2.31.0)\n",
            "Requirement already satisfied: pyyaml>=5.4 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (6.0.1)\n",
            "Requirement already satisfied: loguru>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (0.7.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (4.8.0)\n",
            "Requirement already satisfied: dnspython>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (2.4.2)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (2.8.2)\n",
            "Requirement already satisfied: urllib3>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (2.0.7)\n",
            "Requirement already satisfied: tqdm>=4.64.1 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (4.66.1)\n",
            "Requirement already satisfied: numpy>=1.22.0 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (1.23.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.5.3->pinecone-client) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pinecone-client) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pinecone-client) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pinecone-client) (2023.7.22)\n",
            "Requirement already satisfied: llama-cpp-python in /usr/local/lib/python3.10/dist-packages (0.1.78)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (4.8.0)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (1.23.4)\n",
            "Requirement already satisfied: diskcache>=5.6.1 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (5.6.3)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.19.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.66.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.8.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (23.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2023.7.22)\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain\n",
        "!pip install pypdf\n",
        "!pip install unstructured\n",
        "!pip install sentence_transformers\n",
        "!pip install pinecone-client\n",
        "!pip install llama-cpp-python\n",
        "!pip install huggingface_hub"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Step 2: Import All the Required Libraries**"
      ],
      "metadata": {
        "id": "Logc48CDXirQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import PyPDFLoader, OnlinePDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import Pinecone\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "import pinecone\n",
        "import os"
      ],
      "metadata": {
        "id": "kmdLCsZPXqwF",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1702083342416,
          "user_tz": 360,
          "elapsed": 25905,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Step 3: Load the Data**"
      ],
      "metadata": {
        "id": "qBSwg7bOYCD8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown \"https://drive.google.com/uc?id=15hUEJQViQDxu_fnJeO_Og1hGqykCmJut&confirm=t\"\n",
        "\n",
        "# !gdown \"https://drive.google.com/file/d/1wh4tEM7BzcFhQQvQAuyr_f67TLbfrCXg/view\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hmrkINXzmBZj",
        "outputId": "01d5c165-7707-4e2a-d1cf-51cb294753c1",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1702083350499,
          "user_tz": 360,
          "elapsed": 8095,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=15hUEJQViQDxu_fnJeO_Og1hGqykCmJut&confirm=t\n",
            "To: /content/The-Field-Guide-to-Data-Science.pdf\n",
            "100% 30.3M/30.3M [00:00<00:00, 32.5MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# loader = OnlinePDFLoader(\"/content/cs4349.pdf\")\n",
        "# loader = PyPDFLoader(\"/content/cs4349.pdf\")\n",
        "loader = PyPDFLoader(\"/content/19908___Introduction to Algorithms.pdf\")"
      ],
      "metadata": {
        "id": "Wap-CQGCXah3",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1702083351056,
          "user_tz": 360,
          "elapsed": 559,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = loader.load()"
      ],
      "metadata": {
        "id": "Qv6XhgnPYOKn",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1702083387293,
          "user_tz": 360,
          "elapsed": 36239,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dOgHFZsvf0jV",
        "outputId": "ad533326-6a69-4c54-874e-dc8335988bbf",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1702083387670,
          "user_tz": 360,
          "elapsed": 391,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content='Instructor’s Manual\\nby ThomasH. Cormen\\ntoAccompany\\nIntroductiontoAlgorithms\\nThird Edition\\nby ThomasH. Cormen\\nCharles E. Leiserson\\nRonald L. Rivest\\nClifford Stein\\nTheMITPress\\nCambridge, Massachusetts London, England', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 0}),\n",
              " Document(page_content='Instructor’s Manual to Accompany Introduction to Algorithms , Third Edition\\nby Thomas H.Cormen, Charles E.Leiserson, Ronald L.Rivest, and Clifford Stein\\nPublished by the MIT Press. Copyright c\\r2009 by The Massachusetts Institute of Technology. All righ ts\\nreserved.\\nNopart ofthis publication may bereproduced ordistributed inany formor byany means,orstored in adatabase\\nor retrieval system, without the prior written consent of Th e MITPress, including, but not limited to, network or\\nother electronic storage or transmission, or broadcast for distance learning.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 1}),\n",
              " Document(page_content='Contents\\nRevision History R-1\\nPreface P-1\\nChapter2: GettingStarted\\nLecture Notes 2-1\\nSolutions 2-17\\nChapter3: Growth of Functions\\nLecture Notes 3-1\\nSolutions 3-7\\nChapter4: Divide-and-Conquer\\nLecture Notes 4-1\\nSolutions 4-17\\nChapter5: Probabilistic Analysis andRandomized Algorith ms\\nLecture Notes 5-1\\nSolutions 5-9\\nChapter6: Heapsort\\nLecture Notes 6-1\\nSolutions 6-10\\nChapter7: Quicksort\\nLecture Notes 7-1\\nSolutions 7-9\\nChapter8: Sortingin LinearTime\\nLecture Notes 8-1\\nSolutions 8-10\\nChapter9: MediansandOrderStatistics\\nLecture Notes 9-1\\nSolutions 9-10\\nChapter11: Hash Tables\\nLecture Notes 11-1\\nSolutions 11-16\\nChapter12: Binary Search Trees\\nLecture Notes 12-1\\nSolutions 12-15\\nChapter13: Red-Black Trees\\nLecture Notes 13-1\\nSolutions 13-13\\nChapter14: AugmentingData Structures\\nLecture Notes 14-1\\nSolutions 14-9', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 2}),\n",
              " Document(page_content='iv Contents\\nChapter15: DynamicProgramming\\nLecture Notes 15-1\\nSolutions 15-21\\nChapter16: Greedy Algorithms\\nLecture Notes 16-1\\nSolutions 16-9\\nChapter17: Amortized Analysis\\nLecture Notes 17-1\\nSolutions 17-14\\nChapter21: DataStructures for Disjoint Sets\\nLecture Notes 21-1\\nSolutions 21-6\\nChapter22: Elementary GraphAlgorithms\\nLecture Notes 22-1\\nSolutions 22-13\\nChapter23: MinimumSpanningTrees\\nLecture Notes 23-1\\nSolutions 23-8\\nChapter24: Single-Source Shortest Paths\\nLecture Notes 24-1\\nSolutions 24-13\\nChapter25: All-Pairs Shortest Paths\\nLecture Notes 25-1\\nSolutions 25-9\\nChapter26: Maximum Flow\\nLecture Notes 26-1\\nSolutions 26-12\\nChapter27: Multithreaded Algorithms\\nSolutions 27-1\\nIndex I-1', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 3}),\n",
              " Document(page_content='RevisionHistory\\nRevisions are listed by date rather than being numbered.\\n\\x0f22February2014. CorrectedanerrorinthesolutiontoExerc ise4.3-7,courtesy\\nof Dan Suthers. Corrected an error in the solution to Exercis e 23.1-6, courtesy\\nof Rachel Ginzberg. Updated the Preface.\\n\\x0f3 January 2012. Added solutions to Chapter 27. Added an alter native solution\\nto Exercise 2.3-7, courtesy of Viktor Korsun and Crystal Pen g. Corrected a\\nminor error inthe Chapter 15 notes inthe recurrence for T .n/for the recursive\\nCUT-RODprocedure. Updated the solution to Problem 24-3. Corrected an\\nerror in the proof about the Edmonds-Karp algorithm perform ingO.VE/ﬂow\\naugmentations. The bodies of all pseudocode procedures are indented slightly.\\n\\x0f28 January 2011. Corrected an error in the solution to Proble m 2-4(c), and\\nremoved unnecessary code in the solution to Problem 2-4(d). Added a missing\\nparameter to recursive calls of R EC-MAT-MULTon page 4-7. Changed the\\npseudocode for H EAP-EXTRACT-MAXon page 6-8 and M AX-HEAP-INSERT\\non page 6-9 to assume that the parameter nispassed by reference.\\n\\x0f7 May 2010. Changed the solutions to Exercises 22.2-3 and 22. 3-4 because\\nthese exercises changed.\\n\\x0f17 February 2010. Corrected aminor error inthe solution toE xercise 4.3-7.\\n\\x0f16 December 2009. Added an alternative solution to Exercise 6.3-3, courtesy\\nof Eyal Mashiach.\\n\\x0f7December 2009. Addedsolutions toExercises16.3-1, 26.1- 1, 26.1-3, 26.1-7,\\n26.2-1,26.2-8, 26.2-9, 26.2-12, 26.2-13, and 26.4-1 and to Problem 26-3. Cor-\\nrected spelling in the solution to Exercise 16.2-4. Several corrections to the\\nsolution to Exercise 16.4-3, courtesy of Zhixiang Zhu. Mino r changes to the\\nsolutions to Exercises 24.3-3 and 24.4-7 and Problem 24-1.\\n\\x0f7August 2009. Initial release.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 4}),\n",
              " Document(page_content='', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 5}),\n",
              " Document(page_content='Preface\\nThisdocument isaninstructor’s manual toaccompany Introduction toAlgorithms ,\\nThirdEdition,byThomasH.Cormen,CharlesE.Leiserson, Ro naldL.Rivest,and\\nCliffordStein. Itisintended foruseinacourse onalgorith ms. Youmight alsoﬁnd\\nsomeof the material herein to beuseful for aCS2-style cours e indata structures.\\nUnliketheinstructor’smanualfortheﬁrsteditionofthete xt—whichwasorganized\\naround the undergraduate algorithms course taught by Charl es Leiserson at MIT\\nin Spring 1991—but like the instructor’s manual for the seco nd edition, we have\\nchosen to organize the manual for the third edition accordin g to chapters of the\\ntext. That is, for most chapters wehave provided aset of lect ure notes and aset of\\nexercise and problem solutions pertaining to the chapter. T his organization allows\\nyou todecide how to best use the material in the manual inyour owncourse.\\nWe have not included lecture notes and solutions for every ch apter, nor have we\\nincluded solutions foreveryexerciseandproblem withinth echapters that wehave\\nselected. We felt that Chapter 1 is too nontechnical to inclu de here, and Chap-\\nter 10consists of background material that often falls outs ide algorithms and data-\\nstructures courses. We have also omitted the chapters that a re not covered in the\\ncourses that we teach: Chapters 18–20 and 27–35 (though we do include some\\nsolutions for Chapter 27), aswellasAppendices A–D;future editions ofthis man-\\nual may include some of these chapters. There are two reasons that we have not\\nincluded solutions to all exercises and problems in the sele cted chapters. First,\\nwritingupallthesesolutions wouldtakealongtime,andwef eltitmoreimportant\\nto release this manual in as timely a fashion as possible. Sec ond, if we were to\\ninclude all solutions, this manual would be muchlonger than thetext itself.\\nWe have numbered the pages in this manual using the format CC-PP, whereCC\\nis a chapter number of the text and PPis the page number within that chapter’s\\nlecturenotesandsolutions. The PPnumbersrestartfrom1atthebeginningofeach\\nchapter’s lecture notes. We chose this form of page numberin g so that if we add\\nor change solutions toexercises andproblems, theonly page s whose numbering is\\naffected are those for the solutions for that chapter. Moreo ver, if we add material\\nfor currently uncovered chapters, the numbers of the existi ng pages will remain\\nunchanged.\\nThelecture notes\\nThelecture notes arebased on three sources:', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 6}),\n",
              " Document(page_content='P-2 Preface\\n\\x0fSomearefromtheﬁrst-edition manual; theycorrespond toCh arles Leiserson’s\\nlectures in MIT’sundergraduate algorithms course, 6.046.\\n\\x0fSome are from Tom Cormen’s lectures in Dartmouth College’s u ndergraduate\\nalgorithms course, CS25.\\n\\x0fSomearewritten just for this manual.\\nYou will ﬁnd that the lecture notes are more informal than the text, as is appro-\\npriate for a lecture situation. In some places, we have simpl iﬁed the material for\\nlecture presentation or even omitted certain consideratio ns. Some sections of the\\ntext—usually starred—are omitted from the lecture notes. ( We have included lec-\\nture notes for one starred section: 12.4, on randomly built b inary search trees,\\nwhich wecovered inan optional CS25 lecture.)\\nIn several places in the lecture notes, we have included “asi des” to the instruc-\\ntor. The asides are typeset in a slanted font and are enclosed in square brack-\\nets.[Hereis anaside.] Some of the asides suggest leaving certain material on the\\nboard, since you will be coming back to it later. If you are pro jecting a presenta-\\ntionrather thanwritingonablackboard orwhiteboard, youm ightwanttoreplicate\\nslides containing this material so that you can easily repri se them later in the lec-\\nture.\\nWehavechosennottoindicatehowlongittakestocovermater ial,asthetimenec-\\nessary to cover a topic depends on the instructor, the studen ts, the class schedule,\\nand other variables.\\nThere are twodifferences in how wewrite pseudocode inthe le cture notes and the\\ntext:\\n\\x0fLines are not numbered in the lecture notes. We ﬁnd them incon venient to\\nnumber when writing pseudocode on the board.\\n\\x0fWe avoid using the lengthattribute of an array. Instead, we pass the array\\nlength as a parameter to the procedure. This change makes the pseudocode\\nmoreconcise, as well as matching better with the descriptio n of what it does.\\nWe have also minimized the use of shading in ﬁgures within lec ture notes, since\\ndrawing aﬁgurewith shading on ablackboard or whiteboard is difﬁcult.\\nThesolutions\\nThe solutions are based on the same sources as the lecture not es. They are written\\na bit more formally than the lecture notes, though a bit less f ormally than the text.\\nWe do not number lines of pseudocode, but we do use the lengthattribute (on the\\nassumption that you will want your students to write pseudoc ode as it appears in\\nthe text).\\nAsof the third edition, wehave publicly posted afew solutio ns on the book’s web-\\nsite. These solutions also appear in this manual, with the no tation “This solution\\nis also posted publicly” after the exercise or problem numbe r. The set of pub-\\nlicly posted solutions might increase over time, and so we en courage you to check\\nwhetheraparticularsolutionispostedonthewebsitebefor eyouassignanexercise\\nor problem to your students.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 7}),\n",
              " Document(page_content='Preface P-3\\nTheindexlistsall theexercises andproblems forwhichthis manual provides solu-\\ntions, along withthe number of the page on which each solutio n starts.\\nAsides appear in a handful of places throughout the solution s. Also, we are less\\nreluctant to use shading in ﬁgures within solutions, since t hese ﬁgures are more\\nlikely tobe reproduced than tobe drawnon aboard.\\nSourceﬁles\\nFor several reasons, weare unable to publish or transmit sou rce ﬁles for this man-\\nual. Weapologize for this inconvenience.\\nYoucanusetheclrscode3epackageforLATEX2\"totypeset pseudocode inthesame\\nway that we do. You can ﬁnd this package at http://www.cs.dar tmouth.edu/\\x18thc/\\nclrscode/. That site also includes documentation. Make sur e to use the clrscode3e\\npackage, not the clrscode package; clrscode isfor the secon d edition of thebook.\\nReportingerrors andsuggestions\\nUndoubtedly, instructors will ﬁnd errors in this manual. Pl ease report errors by\\nsending email toclrs-manual-bugs@mitpress.mit.edu.\\nIf you have a suggestion for an improvement to this manual, pl ease feel free to\\nsubmit it via email toclrs-manual-suggestions@mitpress. mit.edu.\\nAs usual, if you ﬁnd an error in the text itself, please verify that it has not already\\nbeen posted on the errata web page before you submit it. You ca n use the MIT\\nPress web site for the text, http://mitpress.mit.edu/algo rithms/, to locate the errata\\nwebpage and to submit an error report.\\nWethankyouinadvanceforyourassistanceincorrectingerr orsinboththismanual\\nand the text.\\nHowweproduced thismanual\\nLike the third edition of Introduction to Algorithms , this manual was produced in\\nLATEX2\". We used the Times font with mathematics typeset using the Ma thTime\\nPro 2 fonts. As in all three editions of the textbook, we compi led the index using\\nWindex, a C program that we wrote. We drew the illustrations u sing MacDraw\\nPro,1with some of the mathematical expressions in illustrations laid in with the\\npsfrag package for LATEX2\". We created the PDF ﬁles for this manual on a\\nMacBook Prorunning OS10.5.\\nAcknowledgments\\nThis manual borrows heavily from the manuals for the ﬁrst two editions. Julie\\nSussman, P.P.A.,wrote the ﬁrst-edition manual. Julie did s uch a superb job on the\\n1See our plea inthe preface for the thirdedition toApple, ask ing that they update MacDraw Profor\\nOSX.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 8}),\n",
              " Document(page_content='P-4 Preface\\nﬁrst-edition manual, ﬁndingnumerouserrorsintheﬁrst-ed ition textintheprocess,\\nthat we were thrilled to have her serve as technical copyedit or for both the second\\nand third editions of thebook. CharlesLeiserson also put in large amounts oftime\\nworking withJulie on theﬁrst-edition manual.\\nThe manual for the second edition was written by Tom Cormen, C lara Lee, and\\nErica Lin. Clara and Erica were undergraduate computer scie nce majors at Dart-\\nmouth at the time, and they dida superb job.\\nThe other three Introduction to Algorithms authors—Charles Leiserson, Ron\\nRivest, and Cliff Stein—provided helpful comments and sugg estions for solutions\\ntoexercisesandproblems. Someofthesolutionsaremodiﬁca tionsofthosewritten\\novertheyearsbyteachingassistantsforalgorithmscourse satMITandDartmouth.\\nAtthispoint, wedonot know whichTAswrotewhichsolutions, andsowesimply\\nthank them collectively. Several of the solutions to new exe rcises and problems\\nin the third edition were written by Sharath Gururaj of Colum bia University; we\\nthankSharathforhisﬁnework. ThesolutionsforChapter27w erewrittenbyPriya\\nNatarajan.\\nWe also thank the MIT Press and our editors—Ada Brunstein, Ji m DeWolf, and\\nMarie Lee— for moral and ﬁnancial support. Tim Tregubov and W ayne Cripps\\nprovided computer support at Dartmouth.\\nTHOMASH. CORMEN\\nHanover, NewHampshire\\nAugust 2009', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 9}),\n",
              " Document(page_content='Lecture Notes forChapter 2:\\nGettingStarted\\nChapter 2 overview\\nGoals\\n\\x0fStart using frameworks for describing and analyzing algori thms.\\n\\x0fExamine twoalgorithms for sorting: insertion sort and merg e sort.\\n\\x0fSeehow to describe algorithms inpseudocode.\\n\\x0fBegin using asymptotic notation toexpress running-time an alysis.\\n\\x0fLearn the technique of “divide and conquer” in the context of merge sort.\\nInsertionsort\\nThesorting problem\\nInput:Asequence of nnumbersha1; a2; : : : ; a ni.\\nOutput: A permutation (reordering) ha0\\n1; a0\\n2; : : : ; a0\\nniof the input sequence such\\nthata0\\n1\\x14a0\\n2\\x14\\x01\\x01\\x01\\x14 a0\\nn.\\nThesequences are typically stored inarrays.\\nWe also refer to the numbers as keys. Along with each key may be additional\\ninformation, known as satellite data .[You might want to clarify that “satellite\\ndata” does not necessarily come from asatellite.]\\nWewillseeseveral waystosolvethesorting problem. Eachwa ywillbeexpressed\\nasanalgorithm : awell-deﬁned computational procedure that takessomeval ue, or\\nset of values, as input and produces some value, or set of valu es, as output.\\nExpressing algorithms\\nWeexpress algorithms inwhatever wayisthe clearest and mos t concise.\\nEnglish is sometimes the best way.\\nWhen issues of control need tobe madeperfectly clear, weoft en usepseudocode .', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 10}),\n",
              " Document(page_content='2-2 Lecture Notes for Chapter 2: GettingStarted\\n\\x0fPseudocode is similar to C, C++, Pascal, and Java. If you know any of these\\nlanguages, you should be able tounderstand pseudocode.\\n\\x0fPseudocode is designed for expressing algorithms to humans . Software en-\\ngineering issues of data abstraction, modularity, and erro r handling are often\\nignored.\\n\\x0fWe sometimes embed English statements into pseudocode. The refore, unlike\\nfor “real” programming languages, we cannot create a compil er that translates\\npseudocode tomachine code.\\nInsertion sort\\nA good algorithm for sorting a small number of elements.\\nIt works theway you might sort ahand of playing cards:\\n\\x0fStart with anempty left hand and thecards face down onthe tab le.\\n\\x0fThen remove one card at a time from the table, and insert it int o the correct\\nposition inthe left hand.\\n\\x0fToﬁndthecorrect positionforacard,compareitwitheachof thecardsalready\\ninthe hand, from right to left.\\n\\x0fAt all times, the cards held in the left hand are sorted, and th ese cards were\\noriginally the top cards of the pile onthe table.\\nPseudocode\\nWeuse aprocedure I NSERTION -SORT.\\n\\x0fTakesas parameters an array AŒ1 : : n\\x8dand the length nof the array.\\n\\x0fAsinPascal, weuse “ : :”todenote a range within an array.\\n\\x0f[We usually use 1-origin indexing, as we do here. There are a f ew places in\\nlater chapters where we use 0-origin indexing instead. If yo u are translating\\npseudocode to C, C++, or Java, which use 0-origin indexing, y ou need to be\\ncareful to get the indices right. One option is to adjust all i ndex calculations in\\nthe C, C++, or Java code to compensate. An easier option is, wh en using an\\narray AŒ1 : : n\\x8d,toallocate thearraytobeoneentry longer— AŒ0 : : n\\x8d—andjust\\ndon’t use the entry at index 0.]\\n\\x0f[In the lecture notes, we indicate array lengths by paramete rs rather than by\\nusing the lengthattribute that is used in the book. That saves us a line of pseu -\\ndocode each time. Thesolutions continue touse the lengthattribute.]\\n\\x0fThe array Ais sorted in place: the numbers are rearranged within the array,\\nwithat most aconstant number outside the array at any time.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 11}),\n",
              " Document(page_content='Lecture Notes for Chapter 2: GettingStarted 2-3\\nINSERTION -SORT.A; n/ cost times\\nforjD2ton c1n\\nkeyDAŒj \\x8d c2n/NUL1\\n//Insert AŒj \\x8dinto the sorted sequence AŒ1 : : j/NUL1\\x8d. 0 n/NUL1\\niDj/NUL1 c4n/NUL1\\nwhile i > 0andAŒi\\x8d >key c5Pn\\njD2tj\\nAŒiC1\\x8dDAŒi\\x8d c 6Pn\\njD2.tj/NUL1/\\niDi/NUL1 c 7Pn\\njD2.tj/NUL1/\\nAŒiC1\\x8dDkey c8n/NUL1\\n[Leave this on the board, but show only the pseudocode for now . We’ll put in the\\n“cost” and “times” columns later.]\\nExample\\n1 2 3 4 5 6\\n5 2 4 6 1 31 2 3 4 5 6\\n2 5 4 6 1 31 2 3 4 5 6\\n2 4 5 6 1 3\\n1 2 3 4 5 6\\n2 4 5 6 1 31 2 3 4 5 6\\n2 4 5 61 31 2 3 4 5 6\\n2456 13j j j\\nj j\\n[Read this ﬁgure row by row. Each part shows what happens for a particular itera-\\ntion withthe value of jindicated. jindexes the “current card” being inserted into\\nthehand. Elementstotheleft of AŒj \\x8dthat aregreater than AŒj \\x8dmoveone position\\nto the right, and AŒj \\x8dmoves into the evacuated position. The heavy vertical lines\\nseparatethepartofthearrayinwhichaniterationworks— AŒ1 : : j \\x8d—fromthepart\\nof the array that is unaffected by this iteration— AŒjC1 : : n\\x8d. The last part of the\\nﬁgure showsthe ﬁnal sorted array.]\\nCorrectness\\nWe often use a loop invariant to help us understand why an algorithm gives the\\ncorrect answer. Here’s the loop invariant for I NSERTION -SORT:\\nLoop invariant: At the start of each iteration of the “outer” forloop—the\\nloopindexedby j—thesubarray AŒ1 : : j/NUL1\\x8dconsists oftheelementsorig-\\ninally in AŒ1 : : j/NUL1\\x8dbut in sorted order.\\nTouse aloop invariant toprove correctness, wemust show thr ee things about it:\\nInitialization: It istrue prior tothe ﬁrst iteration of the loop.\\nMaintenance: Ifitistruebeforeaniterationoftheloop,itremainstrueb eforethe\\nnext iteration.\\nTermination: When the loop terminates, the invariant—usually along with the\\nreasonthattheloopterminated—gives usauseful propertyt hathelpsshowthat\\nthe algorithm is correct.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 12}),\n",
              " Document(page_content='2-4 Lecture Notes for Chapter 2: GettingStarted\\nUsing loop invariants islike mathematical induction:\\n\\x0fToprove that aproperty holds, you prove abase case and an ind uctive step.\\n\\x0fShowing that theinvariant holds before the ﬁrst iteration i slike the base case.\\n\\x0fShowing that the invariant holds from iteration to iteratio n is like the inductive\\nstep.\\n\\x0fThe termination part differs from the usual use of mathemati cal induction, in\\nwhich the inductive step is used inﬁnitely. We stop the “indu ction” when the\\nloop terminates.\\n\\x0fWecan show the three parts in any order.\\nForinsertion sort\\nInitialization: Just before the ﬁrst iteration, jD2. The subarray AŒ1 : : j/NUL1\\x8d\\nis the single element AŒ1\\x8d, which is the element originally in AŒ1\\x8d, and it is\\ntrivially sorted.\\nMaintenance: To be precise, we would need to state and prove a loop invarian t\\nfor the “inner” whileloop. Rather than getting bogged down in another loop\\ninvariant,weinsteadnotethatthebodyoftheinner whileloopworksbymoving\\nAŒj/NUL1\\x8d,AŒj/NUL2\\x8d,AŒj/NUL3\\x8d, and so on, by one position to the right until the\\nproper position for key(which has the value that started out in AŒj \\x8d) is found.\\nAtthat point, the value of keyisplaced into this position.\\nTermination: Theouter forloopendswhen j > n,whichoccurswhen jDnC1.\\nTherefore, j/NUL1Dn. Plugging nin for j/NUL1in the loop invariant, the\\nsubarray AŒ1 : : n\\x8dconsists of the elements originally in AŒ1 : : n\\x8dbut in sorted\\norder. Inother words, the entire array is sorted.\\nPseudocode conventions\\n[Covering most, but not all, here. Seebook pages 20–22 for al l conventions.]\\n\\x0fIndentation indicates block structure. Saves space and wri ting time.\\n\\x0fLooping constructs are like in C, C++, Pascal, and Java. We as sume that the\\nloopvariableina forloopisstilldeﬁnedwhentheloopexits(unlikeinPascal).\\n\\x0f//indicates that the remainder of the line isacomment.\\n\\x0fVariables are local, unless otherwise speciﬁed.\\n\\x0fWeoftenuse objects,whichhave attributes . Foranattribute attrofobject x,we\\nwrite x:attr. (Thisnotation matches x:attrinJavaandisequivalent to x->attr\\nin C++.) Attributes can cascade, so that if x:yis an object and this object has\\nattributeattr, then x:y:attrindicates this object’s attribute. That is, x:y:attris\\nimplicitly parenthesized as .x:y/:attr.\\n\\x0fObjects are treated as references, like in Java. If xandydenote objects, then\\nthe assignment yDxmakes xandyreference the same object. It does not\\ncause attributes of one object to becopied to another.\\n\\x0fParametersarepassed byvalue, asinJavaandC(andthedefau lt mechanism in\\nPascal and C++). When an object is passed by value, it is actua lly a reference\\n(or pointer) that is passed; changes to the reference itself are not seen by the\\ncaller, but changes to theobject’s attributes are.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 13}),\n",
              " Document(page_content='Lecture Notes for Chapter 2: GettingStarted 2-5\\n\\x0fThe boolean operators “and” and “or” are short-circuiting : if after evaluating\\nthe left-hand operand, we know the result of the expression, then we don’t\\nevaluate the right-hand operand. (If xisFALSEin “xandy” then we don’t\\nevaluate y. IfxisTRUEin“xory”then wedon’t evaluate y.)\\nAnalyzingalgorithms\\nWewanttopredicttheresourcesthatthealgorithmrequires . Usually,runningtime.\\nInorder topredict resource requirements, weneed acomputa tional model.\\nRandom-access machine(RAM)model\\n\\x0fInstructions are executed one after another. Noconcurrent operations.\\n\\x0fIt’stootedioustodeﬁneeachoftheinstructions andtheira ssociatedtimecosts.\\n\\x0fInstead, werecognize that we’ll use instructions commonly found in real com-\\nputers:\\n\\x0fArithmetic: add, subtract, multiply, divide, remainder, ﬂ oor, ceiling). Also,\\nshift left/shift right (good for multiplying/dividing by 2k).\\n\\x0fDatamovement: load, store, copy.\\n\\x0fControl: conditional/unconditional branch, subroutine c all and return.\\nEach of these instructions takes aconstant amount of time.\\nTheRAMmodel uses integer and ﬂoating-point types.\\n\\x0fWe don’t worry about precision, although it is crucial in cer tain numerical ap-\\nplications.\\n\\x0fThere is a limit on the word size: when working with inputs of s izen, assume\\nthat integers are represented by clgnbits for some constant c\\x151. (lgnis a\\nvery frequently used shorthand for log2n.)\\n\\x0fc\\x151)wecanholdthevalueof n)wecanindextheindividualelements.\\n\\x0fcis aconstant)the wordsize cannot grow arbitrarily.\\nHowdoweanalyze an algorithm’s runningtime?\\nThetimetaken by analgorithm depends on theinput.\\n\\x0fSorting 1000 numbers takes longer than sorting 3numbers.\\n\\x0fA given sorting algorithm may even take differing amounts of time on two\\ninputs of the samesize.\\n\\x0fForexample,we’llseethatinsertionsorttakeslesstimeto sortnelementswhen\\nthey are already sorted than when they are inreverse sorted o rder.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 14}),\n",
              " Document(page_content='2-6 Lecture Notes for Chapter 2: GettingStarted\\nInput size\\nDepends on the problem being studied.\\n\\x0fUsually, the number of items in the input. Like the size nof the array being\\nsorted.\\n\\x0fBut could be something else. If multiplying two integers, co uld be the total\\nnumber of bits inthe twointegers.\\n\\x0fCould be described by more than one number. For example, grap h algorithm\\nrunning times are usually expressed in terms of the number of vertices and the\\nnumber of edges in the input graph.\\nRunningtime\\nOnaparticular input, it isthe number of primitive operatio ns (steps) executed.\\n\\x0fWant to deﬁne steps to bemachine-independent.\\n\\x0fFigure that each line of pseudocode requires aconstant amou nt of time.\\n\\x0fOne line may take a different amount of time than another, but each execution\\nof line itakes the sameamount of time ci.\\n\\x0fThisis assuming that the line consists only of primitive ope rations.\\n\\x0fIfthelineisasubroutine call,thentheactualcalltakesco nstant time,butthe\\nexecution of the subroutine being called might not.\\n\\x0fIf the line speciﬁes operations other than primitive ones, t hen it might take\\nmore than constant time. Example: “sort thepoints by x-coordinate.”\\nAnalysis of insertion sort\\n[Now add statement costs and number of times executed to INSERTION -SORT\\npseudocode.]\\n\\x0fAssumethat the ithlinetakes time ci,whichisaconstant. (Since thethird line\\nisacomment, it takes no time.)\\n\\x0fForjD2; 3; : : : ; n , lettjbe the number of times that the whileloop test is\\nexecuted for that value of j.\\n\\x0fNotethatwhena fororwhileloopexitsintheusual way—duetothetestinthe\\nloop header—the test is executed one timemore than the loop b ody.\\nTherunning timeof the algorithm is\\nX\\nall statements.cost of statement /\\x01.number of timesstatement isexecuted / :\\nLetT .n/Drunning time of I NSERTION -SORT.\\nT .n/Dc1nCc2.n/NUL1/Cc4.n/NUL1/Cc5nX\\njD2tjCc6nX\\njD2.tj/NUL1/\\nCc7nX\\njD2.tj/NUL1/Cc8.n/NUL1/ :\\nTherunning timedepends on thevalues of tj. Thesevary according tothe input.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 15}),\n",
              " Document(page_content='Lecture Notes for Chapter 2: GettingStarted 2-7\\nBest case\\nThearray is already sorted.\\n\\x0fAlwaysﬁndthat AŒi\\x8d\\x14keyupon theﬁrsttimethe whileloop test isrun(when\\niDj/NUL1).\\n\\x0fAlltjare1.\\n\\x0fRunning time is\\nT .n/Dc1nCc2.n/NUL1/Cc4.n/NUL1/Cc5.n/NUL1/Cc8.n/NUL1/\\nD.c1Cc2Cc4Cc5Cc8/n/NUL.c2Cc4Cc5Cc8/ :\\n\\x0fCanexpress T .n/asanCbforconstants aandb(thatdependonthestatement\\ncosts ci))T .n/isalinear function ofn.\\nWorst case\\nThearray is inreverse sorted order.\\n\\x0fAlways ﬁndthat AŒi\\x8d >keyinwhile loop test.\\n\\x0fHavetocompare keywithallelementstotheleftofthe jthposition)compare\\nwithj/NUL1elements.\\n\\x0fSince the while loop exits because ireaches 0, there’s one additional test after\\nthej/NUL1tests)tjDj.\\n\\x0fnX\\njD2tjDnX\\njD2jandnX\\njD2.tj/NUL1/DnX\\njD2.j/NUL1/.\\n\\x0fnX\\njD1jisknown asan arithmetic series , and equation (A.1)shows that it equals\\nn.nC1/\\n2.\\n\\x0fSincenX\\njD2jD nX\\njD1j!\\n/NUL1, it equalsn.nC1/\\n2/NUL1.\\n[The parentheses around the summation are not strictly nece ssary. They are\\nthere for clarity, but it might be a good idea to remind the stu dents that the\\nmeaning of theexpression would bethe sameeven without the p arentheses.]\\n\\x0fLetting kDj/NUL1, wesee thatnX\\njD2.j/NUL1/Dn/NUL1X\\nkD1kDn.n/NUL1/\\n2.\\n\\x0fRunning time is\\nT .n/Dc1nCc2.n/NUL1/Cc4.n/NUL1/Cc5\\x12n.nC1/\\n2/NUL1\\x13\\nCc6\\x12n.n/NUL1/\\n2\\x13\\nCc7\\x12n.n/NUL1/\\n2\\x13\\nCc8.n/NUL1/\\nD\\x10c5\\n2Cc6\\n2Cc7\\n2\\x11\\nn2C\\x10\\nc1Cc2Cc4Cc5\\n2/NULc6\\n2/NULc7\\n2Cc8\\x11\\nn\\n/NUL.c2Cc4Cc5Cc8/ :\\n\\x0fCan express T .n/asan2CbnCcfor constants a; b; c(that again depend on\\nstatement costs))T .n/isaquadratic function ofn.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 16}),\n",
              " Document(page_content='2-8 Lecture Notes for Chapter 2: GettingStarted\\nWorst-case and average-case analysis\\nWe usually concentrate on ﬁnding the worst-case running time : the longest run-\\nning timefor anyinput of size n.\\nReasons\\n\\x0fThe worst-case running time gives a guaranteed upper bound o n the running\\ntimefor any input.\\n\\x0fFor some algorithms, the worst case occurs often. For exampl e, when search-\\ning,theworstcaseoftenoccurswhentheitembeingsearched forisnotpresent,\\nand searches for absent items maybe frequent.\\n\\x0fWhynot analyze theaverage case? Because it’softenabout as badastheworst\\ncase.\\nExample: Suppose that we randomly choose nnumbers as the input to inser-\\ntion sort.\\nOn average, the key in AŒj \\x8dis less than half the elements in AŒ1 : : j/NUL1\\x8dand\\nit’s greater than the other half.\\n)Onaverage, the whileloop has tolook halfway through the sorted subarray\\nAŒ1 : : j/NUL1\\x8dto decide where todrop key.\\n)tj\\x19j=2.\\nAlthoughtheaverage-case runningtimeisapproximately ha lfoftheworst-case\\nrunning time, it’s still aquadratic function of n.\\nOrderof growth\\nAnother abstraction toease analysis and focus on the import ant features.\\nLook only at theleading term of theformula for running time.\\n\\x0fDroplower-order terms.\\n\\x0fIgnore the constant coefﬁcient inthe leading term.\\nExample: Forinsertionsort,wealreadyabstractedawaytheactualst atementcosts\\nto conclude that the worst-case running timeis an2CbnCc.\\nDrop lower-order terms )an2.\\nIgnore constant coefﬁcient )n2.\\nBut wecannot say that the worst-case running time T .n/equals n2.\\nItgrowslike n2. But it doesn’t equal n2.\\nWesaythattherunningtimeis ‚.n2/tocapturethenotionthatthe orderofgrowth\\nisn2.\\nWe usually consider one algorithm to be more efﬁcient than an other if its worst-\\ncase running timehas asmaller order of growth.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 17}),\n",
              " Document(page_content='Lecture Notes for Chapter 2: GettingStarted 2-9\\nDesigningalgorithms\\nThereare many waysto design algorithms.\\nForexample, insertion sort is incremental : having sorted AŒ1 : : j/NUL1\\x8d,place AŒj \\x8d\\ncorrectly, so that AŒ1 : : j \\x8dissorted.\\nDivideandconquer\\nAnother common approach.\\nDividetheproblem into anumber of subproblems that aresmaller ins tances ofthe\\nsame problem.\\nConquer the subproblems by solving them recursively.\\nBasecase: Ifthesubproblemsaresmallenough, justsolvethembybrute force.\\n[It would be a good idea to make sure that your students are com fortable with\\nrecursion. If they are not, then they will have a hard time und erstanding divide\\nand conquer.]\\nCombine the subproblem solutions togive asolution tothe original p roblem.\\nMerge sort\\nA sorting algorithm based on divide and conquer. Its worst-c ase running time has\\nalower order of growth than insertion sort.\\nBecause we are dealing with subproblems, we state each subpr oblem as sorting a\\nsubarray AŒp : : r\\x8d. Initially, pD1andrDn, but these values change as we\\nrecurse through subproblems.\\nTosort AŒp : : r\\x8d:\\nDivideby splitting into two subarrays AŒp : : q\\x8d andAŒqC1 : : r\\x8d, where qis the\\nhalfway point of AŒp : : r\\x8d.\\nConquer by recursively sorting the twosubarrays AŒp : : q\\x8d andAŒqC1 : : r\\x8d.\\nCombine by merging the two sorted subarrays AŒp : : q\\x8d andAŒqC1 : : r\\x8dto pro-\\nduce a single sorted subarray AŒp : : r\\x8d. To accomplish this step, we’ll deﬁne a\\nprocedure M ERGE .A; p; q; r/ .\\nTherecursionbottomsoutwhenthesubarrayhasjust 1element,sothatit’strivially\\nsorted.\\nMERGE-SORT.A; p; r/\\nifp < r //check for base case\\nqDb.pCr/=2c//divide\\nMERGE-SORT.A; p; q/ //conquer\\nMERGE-SORT.A; qC1; r///conquer\\nMERGE .A; p; q; r/ //combine', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 18}),\n",
              " Document(page_content='2-10 Lecture Notes for Chapter 2: GettingStarted\\nInitial call: MERGE-SORT.A; 1; n/\\n[It is astounding how often students forget how easy it is to c ompute the halfway\\npoint of pandras their average .pCr/=2. We of course have to take the ﬂoor\\nto ensure that we get an integer index q. But it is common to see students perform\\ncalculations like pC.r/NULp/=2,orevenmoreelaborate expressions, forgetting the\\neasy waytocompute an average.]\\nExample\\nBottom-up view for nD8:[Heavy lines demarcate subarrays used in subprob-\\nlems.]\\n1 2 3 4 5 6 7 85 2 4 7 1 3 2 62 5 4 7 1 3 2 6\\ninitial arraymerge2 4 5 7 1 2 3 6\\nmerge1 2 3 4 5 6 7\\nmergesorted array\\n21 2 3 4 5 6 7 8\\n[Examples when nis a power of 2are most straightforward, but students might\\nalso want an example when nis not a power of 2.]\\nBottom-up view for nD11:\\n1 2 3 4 5 6 7 84 7 2 6 1 4 7 3\\ninitial arraymergemergemergesorted array\\n526\\n9 10 114 7 2 1 6 4 3 7 5262 4 7 1 4 6 3 5 7261 2 4 4 6 7 2 3 5671 2 2 3 4 4 5 6 6771 2 3 4 5 6 7 8 9 10 11\\nmerge\\n[Here, at the next-to-last level of recursion, some of the su bproblems have only 1\\nelement. Therecursion bottoms out onthese single-element subproblems.]', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 19}),\n",
              " Document(page_content='Lecture Notes for Chapter 2: GettingStarted 2-11\\nMerging\\nWhat remains isthe M ERGEprocedure.\\nInput:Array Aand indices p; q; rsuch that\\n\\x0fp\\x14q < r.\\n\\x0fSubarray AŒp : : q\\x8d is sorted and subarray AŒqC1 : : r\\x8dis sorted. By the\\nrestrictions on p; q; r, neither subarray isempty.\\nOutput: Thetwo subarrays aremerged into asingle sorted subarray in AŒp : : r\\x8d.\\nWeimplement itsothat ittakes ‚.n/time, where nDr/NULpC1Dthenumber of\\nelements being merged.\\nWhat is n?Until now, nhas stood for the size of the original problem. But now\\nwe’re using it as the size of a subproblem. We will use this tec hnique when we\\nanalyze recursive algorithms. Although we may denote the or iginal problem size\\nbyn, ingeneral nwill be the size of agiven subproblem.\\nIdea behindlinear-time merging\\nThink of twopiles of cards.\\n\\x0fEachpileissortedandplaced face-up onatable withthesmal lest cardsontop.\\n\\x0fWewill merge these into asingle sorted pile, face-down on th etable.\\n\\x0fA basic step:\\n\\x0fChoose the smaller of the twotop cards.\\n\\x0fRemoveit from itspile, thereby exposing anew top card.\\n\\x0fPlace thechosen card face-down onto the output pile.\\n\\x0fRepeatedly perform basic steps until one input pile isempty .\\n\\x0fOnce one input pile empties, just take the remaining input pi le and place it\\nface-down onto the output pile.\\n\\x0fEachbasicstepshouldtakeconstanttime,sincewecheckjus tthetwotopcards.\\n\\x0fThere are\\x14nbasic steps, since each basic step removes one card from the\\ninput piles, and westarted with ncards in the input piles.\\n\\x0fTherefore, this procedure should take ‚.n/time.\\nWedon’t actually need to check whether apile is empty before each basic step.\\n\\x0fPut on the bottom of each input pile aspecial sentinelcard.\\n\\x0fIt contains aspecial value that weuse tosimplify the code.\\n\\x0fWeuse1,since that’s guaranteed to“lose” to anyother value.\\n\\x0fThe only way that1cannotlose is when both piles have 1exposed as their\\ntop cards.\\n\\x0fBut when that happens, all the nonsentinel cards have alread y been placed into\\nthe output pile.\\n\\x0fWeknow inadvance that there areexactly r/NULpC1nonsentinel cards)stop\\nonce we have performed r/NULpC1basic steps. Never a need to check for\\nsentinels, since they’ll always lose.\\n\\x0fRather thanevencounting basicsteps, justﬁlluptheoutput arrayfromindex p\\nup through and including index r.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 20}),\n",
              " Document(page_content='2-12 Lecture Notes for Chapter 2: GettingStarted\\nPseudocode\\nMERGE .A; p; q; r/\\nn1Dq/NULpC1\\nn2Dr/NULq\\nletLŒ1 : : n 1C1\\x8dandRŒ1 : : n 2C1\\x8dbenew arrays\\nforiD1ton1\\nLŒi\\x8dDAŒpCi/NUL1\\x8d\\nforjD1ton2\\nRŒj \\x8dDAŒqCj \\x8d\\nLŒn 1C1\\x8dD1\\nRŒn 2C1\\x8dD1\\niD1\\njD1\\nforkDptor\\nifLŒi\\x8d\\x14RŒj \\x8d\\nAŒk\\x8dDLŒi\\x8d\\niDiC1\\nelseAŒk\\x8dDRŒj \\x8d\\njDjC1\\n[The book uses a loop invariant to establish that MERGEworks correctly. In a\\nlecture situation, itisprobably better touseanexample to show that theprocedure\\nworks correctly.]\\nExample\\nA call of M ERGE .9; 12; 16/', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 21}),\n",
              " Document(page_content='Lecture Notes for Chapter 2: GettingStarted 2-13\\nA\\nL R1 2 3 4 1 2 3 4\\ni jk\\n2 4 5 7 1 2 3 6A\\nL R1 2 3 4 1 2 3 4\\ni jk\\n2 4 5 71\\n2 3 612 4 5 7 1 2 3 6 4 5 7 1 2 3 6\\nA\\nL R9 10 11 12 13 14 15 16\\n1 2 3 4 1 2 3 4\\ni jk\\n2 4 5 71\\n2 3 615 7 1 2 3 62 A\\nL R1 2 3 4 1 2 3 4\\ni jk\\n2 4 5 71\\n2 3 617 1 2 3 62 25\\n∞5\\n∞5\\n∞5\\n∞\\n5\\n∞5\\n∞5\\n∞5\\n∞9 10 11 12 13 14 15 16\\n9 10 11 12 13 14 15 169 10 11 12 13 14 15 16 8\\n…17\\n…\\n8\\n…17\\n…8\\n…17\\n…\\n8\\n…17\\n…\\nA\\nL R1 2 3 4 1 2 3 4\\ni jk\\n2 4 5 71\\n2 3 611 2 3 6 2 2 3 A\\nL R1 2 3 4 1 2 3 4\\ni jk\\n2 4 5 71\\n2 3 612 3 6 2 2 3 4\\nA\\nL R1 2 3 4 1 2 3 4\\ni jk\\n2 4 5 71\\n2 3 613 6 2 2 3 4 5 A\\nL R1 2 3 4 1 2 3 4\\ni jk\\n2 4 5 71\\n2 3 616 2 2 3 4 55\\n∞5\\n∞5\\n∞5\\n∞\\n5\\n∞5\\n∞5\\n∞5\\n∞6\\nA\\nL R1 2 3 4 1 2 3 4\\ni jk\\n2 4 5 71\\n2 3 617 2 2 3 4 5\\n5\\n∞5\\n∞69 10 11 12 13 14 15 16\\n9 10 11 12 13 14 15 16\\n9 10 11 12 13 14 15 169 10 11 12 13 14 15 16\\n9 10 11 12 13 14 15 168\\n…17\\n…\\n8\\n…17\\n…\\n8\\n…17\\n…8\\n…17\\n…\\n8\\n…17\\n…\\n[Read this ﬁgure row by row. The ﬁrst part shows the arrays at t he start of the\\n“forkDptor”loop,where AŒp : : q\\x8d iscopiedinto LŒ1 : : n 1\\x8dandAŒqC1 : : r\\x8dis\\ncopiedinto RŒ1 : : n 2\\x8d. Succeedingpartsshowthesituationatthestartofsuccess ive\\niterations. Entries in Awith slashes have had their values copied to either LorR\\nand have not had avalue copied back in yet. Entries in LandRwith slashes have\\nbeen copied back into A. The last part shows that the subarrays are merged back\\nintoAŒp : : r\\x8d, which is now sorted, and that only the sentinels ( 1) are exposed in\\nthe arrays LandR.]', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 22}),\n",
              " Document(page_content='2-14 Lecture Notes for Chapter 2: GettingStarted\\nRunningtime\\nThe ﬁrst two forloops take ‚.n 1Cn2/D‚.n/time. The last forloop makes n\\niterations, each taking constant time, for ‚.n/time.\\nTotal time: ‚.n/.\\nAnalyzing divide-and-conquer algorithms\\nUsearecurrenceequation (morecommonly,a recurrence )todescribetherunning\\ntime of adivide-and-conquer algorithm.\\nLetT .n/Drunning timeon aproblem of size n.\\n\\x0fIftheproblem sizeissmallenough (say, n\\x14cforsomeconstant c),wehavea\\nbase case. The brute-force solution takes constant time: ‚.1/.\\n\\x0fOtherwise, suppose that wedivide into asubproblems, each 1=bthesizeofthe\\noriginal. (Inmerge sort, aDbD2.)\\n\\x0fLet the timetodivide asize- nproblem be D.n/.\\n\\x0fHave asubproblems to solve, each of size n=b)each subproblem takes\\nT .n=b/timeto solve)wespend aT .n=b/ timesolving subproblems.\\n\\x0fLet the timetocombine solutions be C.n/.\\n\\x0fWeget the recurrence\\nT .n/D(\\n‚.1/ ifn\\x14c ;\\naT .n=b/CD.n/CC.n/otherwise :\\nAnalyzing merge sort\\nFor simplicity, assume that nis a power of 2)each divide step yields two sub-\\nproblems, both of size exactly n=2.\\nThebase case occurs when nD1.\\nWhen n\\x152, timefor merge sort steps:\\nDivide:Just compute qasthe average of pandr)D.n/D‚.1/.\\nConquer: Recursively solve 2subproblems, each of size n=2)2T .n=2/.\\nCombine: MERGEon an n-element subarray takes ‚.n/time)C.n/D‚.n/.\\nSince D.n/D‚.1/andC.n/D‚.n/,summedtogether theygive afunction that\\nis linear in n:‚.n/)recurrence for merge sort running timeis\\nT .n/D(\\n‚.1/ ifnD1 ;\\n2T .n=2/C‚.n/ifn > 1 :\\nSolving themerge-sort recurrence\\nBy the master theorem in Chapter 4, we can show that this recur rence has the\\nsolution T .n/D‚.nlgn/.[Reminder: lgnstands for log2n.]\\nCompared to insertion sort ( ‚.n2/worst-case time), merge sort is faster. Trading\\nafactor of nfor afactor of lg nisagood deal.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 23}),\n",
              " Document(page_content='Lecture Notes for Chapter 2: GettingStarted 2-15\\nOn small inputs, insertion sort may be faster. But for large e nough inputs, merge\\nsort will always be faster, because its running time grows mo re slowly than inser-\\ntion sort’s.\\nWecan understand how tosolve the merge-sort recurrence wit hout themaster the-\\norem.\\n\\x0fLetcbe a constant that describes the running time for the base cas e and also\\nis the time per array element for the divide and conquer steps .[Of course, we\\ncannot necessarily usethesameconstant forboth. It’snot w orthgoingintothis\\ndetail at this point.]\\n\\x0fWerewrite the recurrence as\\nT .n/D(\\nc ifnD1 ;\\n2T .n=2/Ccnifn > 1 :\\n\\x0fDraw arecursion tree , which showssuccessive expansions of the recurrence.\\n\\x0fForthe original problem, wehave acost of cn,plus the twosubproblems, each\\ncosting T .n=2/:\\ncn\\nT(n/2)T(n/2)\\n\\x0fFor each of the size- n=2subproblems, we have a cost of cn=2, plus two sub-\\nproblems, each costing T .n=4/:\\ncn\\ncn/2\\nT(n/4)T(n/4)cn/2\\nT(n/4)T(n/4)\\n\\x0fContinue expanding until the problem sizes get downto 1:', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 24}),\n",
              " Document(page_content='2-16 Lecture Notes for Chapter 2: GettingStarted\\ncn\\ncn\\n…\\nTotal: cn lg n + cncnlg ncn\\nnccccc cc\\n…cn\\ncn/2\\ncn/4 cn/4cn/2\\ncn/4 cn/4\\n\\x0fEachlevel has cost cn.\\n\\x0fThe toplevel has cost cn.\\n\\x0fThe next level down has 2subproblems, each contributing cost cn=2.\\n\\x0fThe next level has 4subproblems, each contributing cost cn=4.\\n\\x0fEachtimewegodownonelevel,thenumberofsubproblems doub lesbutthe\\ncost per subproblem halves )cost per level stays the same.\\n\\x0fThere arelg nC1levels (height islg n).\\n\\x0fUse induction.\\n\\x0fBase case: nD1)1level, and lg 1C1D0C1D1.\\n\\x0fInductivehypothesisisthatatreeforaproblemsizeof 2ihaslg 2iC1DiC1\\nlevels.\\n\\x0fBecause we assume that the problem size is a power of 2, the next problem\\nsize upafter 2iis2iC1.\\n\\x0fA tree for a problem size of 2iC1has one more level than the size- 2itree)\\niC2levels.\\n\\x0fSince lg 2iC1C1DiC2, we’re done with theinductive argument.\\n\\x0fTotal cost is sum of costs at each level. Have lg nC1levels, each costing cn\\n)total cost is cnlgnCcn.\\n\\x0fIgnore low-order term of cnand constant coefﬁcient c)‚.nlgn/.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 25}),\n",
              " Document(page_content='SolutionsforChapter 2:\\nGettingStarted\\nSolutionto Exercise 2.2-2\\nThissolutionisalsopostedpublicly\\nSELECTION -SORT.A/\\nnDA:length\\nforjD1ton/NUL1\\nsmallestDj\\nforiDjC1ton\\nifAŒi\\x8d < AŒ smallest \\x8d\\nsmallestDi\\nexchange AŒj \\x8dwithAŒsmallest \\x8d\\nThe algorithm maintains the loop invariant that at the start of each iteration of the\\nouterforloop, the subarray AŒ1 : : j/NUL1\\x8dconsists of the j/NUL1smallest elements\\nin the array AŒ1 : : n\\x8d, and this subarray is in sorted order. After the ﬁrst n/NUL1\\nelements, the subarray AŒ1 : : n/NUL1\\x8dcontains the smallest n/NUL1elements, sorted,\\nand therefore element AŒn\\x8dmust be the largest element.\\nTherunning time of the algorithm is ‚.n2/for all cases.\\nSolutionto Exercise 2.2-4\\nThissolutionisalsopostedpublicly\\nModify the algorithm so it tests whether the input satisﬁes s ome special-case con-\\ndition and, if it does, output apre-computed answer. Thebes t-case running timeis\\ngenerally not agood measure of an algorithm.\\nSolutionto Exercise 2.3-3\\nThebase case is when nD2, and wehave nlgnD2lg2D2\\x011D2.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 26}),\n",
              " Document(page_content='2-18 Solutions for Chapter 2: GettingStarted\\nFor the inductive step, our inductive hypothesis is that T .n=2/D.n=2/lg.n=2/.\\nThen\\nT .n/D2T .n=2/Cn\\nD2.n=2/lg.n=2/Cn\\nDn.lgn/NUL1/Cn\\nDnlgn/NULnCn\\nDnlgn ;\\nwhich completes theinductive proof for exact powers of 2.\\nSolution to Exercise2.3-4\\nSince it takes ‚.n/time in the worst case to insert AŒn\\x8dinto the sorted array\\nAŒ1 : : n/NUL1\\x8d, weget the recurrence\\nT .n/D(\\n‚.1/ ifnD1 ;\\nT .n/NUL1/C‚.n/ifn > 1 :\\nAlthough the exercise does not ask you to solve this recurren ce, its solution is\\nT .n/D‚.n2/.\\nSolution to Exercise2.3-5\\nThis solutionisalsopostedpublicly\\nProcedure B INARY-SEARCHtakes a sorted array A, a value \\x17, and a range\\nŒlow: :high\\x8dof the array, in which we search for the value \\x17. The procedure com-\\npares \\x17tothearrayentryatthemidpoint oftherange anddecides toe liminate half\\ntherangefromfurtherconsideration. Wegivebothiterativ eandrecursiveversions,\\neach of which returns either an index isuch that AŒi\\x8dD\\x17, orNILif no entry of\\nAŒlow: :high\\x8dcontains the value \\x17. The initial call to either version should have\\nthe parameters A; \\x17; 1; n.\\nITERATIVE -BINARY-SEARCH .A; \\x17;low;high/\\nwhilelow\\x14high\\nmidDb.lowChigh/=2c\\nif\\x17==AŒmid\\x8d\\nreturnmid\\nelseif \\x17 > AŒmid\\x8d\\nlowDmidC1\\nelsehighDmid/NUL1\\nreturn NIL', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 27}),\n",
              " Document(page_content='Solutions forChapter 2: GettingStarted 2-19\\nRECURSIVE -BINARY-SEARCH .A; \\x17;low;high/\\niflow>high\\nreturn NIL\\nmidDb.lowChigh/=2c\\nif\\x17==AŒmid\\x8d\\nreturnmid\\nelseif \\x17 > AŒmid\\x8d\\nreturnRECURSIVE -BINARY-SEARCH .A; \\x17;midC1;high/\\nelse return RECURSIVE -BINARY-SEARCH .A; \\x17;low;mid/NUL1/\\nBothprocedures terminatethesearchunsuccessfully whent herangeisempty(i.e.,\\nlow>high) and terminate it successfully if the value \\x17has been found. Based\\non the comparison of \\x17to the middle element in the searched range, the search\\ncontinues with the range halved. The recurrence for these pr ocedures is therefore\\nT .n/DT .n=2/C‚.1/,whose solution is T .n/D‚.lgn/.\\nSolutionto Exercise 2.3-6\\nThewhileloop of lines 5–7 of procedure I NSERTION -SORTscans backward\\nthrough the sorted array AŒ1 : : j/NUL1\\x8dto ﬁnd the appropriate place for AŒj \\x8d. The\\nhitchisthattheloopnotonlysearches fortheproper placef orAŒj \\x8d,butthat italso\\nmoveseachofthearrayelementsthatarebiggerthan AŒj \\x8doneposition totheright\\n(line 6). These movements can take as much as ‚.j /time, which occurs when all\\nthej/NUL1elements preceding AŒj \\x8dare larger than AŒj \\x8d. Wecan use binary search\\ntoimprovetherunningtimeofthesearchto ‚.lgj /,butbinarysearchwillhaveno\\neffect on the running time of moving the elements. Therefore , binary search alone\\ncannot improve the worst-case running timeof I NSERTION -SORTto‚.nlgn/.\\nSolutionto Exercise 2.3-7\\nThefollowing algorithm solves the problem:\\n1. Sort theelements in S.\\n2. Form theset S0Df´W´Dx/NULyfor some y2Sg.\\n3. Sort theelements in S0.\\n4. Merge thetwo sorted sets SandS0.\\n5. There exist two elements in Swhose sum is exactly xif and only if the same\\nvalue appears inconsecutive positions inthe merged output .\\nTo justify the claim in step 4, ﬁrst observe that if any value a ppears twice in the\\nmerged output, it must appear in consecutive positions. Thu s, we can restate the\\ncondition in step 5 as there exist two elements in Swhose sum is exactly xif and\\nonly if the same value appears twice in themerged output.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 28}),\n",
              " Document(page_content='2-20 Solutions for Chapter 2: GettingStarted\\nSuppose that some value wappears twice. Then wappeared once in Sand once\\ninS0. Because wappeared in S0,there exists some y2Ssuch that wDx/NULy,or\\nxDwCy. Since w2S, theelements wandyare in Sand sum to x.\\nConversely, suppose that there are values w; y2Ssuch that wCyDx. Then,\\nsince x/NULyDw,thevalue wappears in S0. Thus, wisinboth SandS0,and soit\\nwill appear twice in themerged output.\\nSteps1and3require ‚.nlgn/steps. Steps2,4,5,and6require O.n/steps. Thus\\nthe overall running timeis O.nlgn/.\\nA reader submitted a simpler solution that also runs in ‚.nlgn/time. First, sort\\ntheelementsin S,taking ‚.nlgn/time. Then,for eachelement yinS,perform a\\nbinary search in Sforx/NULy. Eachbinary search takes O.lgn/time, and there are\\nare most nof them, and so the time for all the binary searches is O.nlgn/. The\\noverall running time is ‚.nlgn/.\\nAnother reader pointed out that since Sis a set, if the value x=2appears in S, it\\nappears in Sjust once, and so x=2cannot beasolution.\\nSolution to Problem 2-1\\n[It may be better to assign this problem after covering asymp totic notation in Sec-\\ntion 3.1; otherwise part (c) maybe too difﬁcult.]\\na.Insertion sort takes ‚.k2/timeper k-element list in the worst case. Therefore,\\nsorting n=klists of kelements each takes ‚.k2n=k/D‚.nk/worst-case\\ntime.\\nb.Just extending the 2-list merge to merge all the lists at once would take\\n‚.n\\x01.n=k//D‚.n2=k/time ( nfrom copying each element once into the\\nresult list, n=kfrom examining n=klists at each step to select next item for\\nresult list).\\nToachieve ‚.nlg.n=k//-timemerging,wemergethelistspairwise,thenmerge\\nthe resulting lists pairwise, and so on, until there’s just o ne list. The pairwise\\nmerging requires ‚.n/work at each level, since we are still working on nel-\\nements, even if they are partitioned among sublists. The num ber of levels,\\nstarting with n=klists (with kelements each) and ﬁnishing with 1list (with n\\nelements), isdlg.n=k/e. Therefore, the total running time for the merging is\\n‚.nlg.n=k//.\\nc.The modiﬁed algorithm has the same asymptotic running time a s standard\\nmerge sort when ‚.nkCnlg.n=k//D‚.nlgn/. The largest asymptotic\\nvalue of kas afunction of nthat satisﬁes this condition is kD‚.lgn/.\\nTosee why, ﬁrst observe that kcannot be more than ‚.lgn/(i.e., it can’t have\\na higher-order term than lg n), for otherwise the left-hand expression wouldn’t\\nbe‚.nlgn/(because it would have a higher-order term than nlgn). Soall we\\nneed to do is verify that kD‚.lgn/works, which we can do by plugging\\nkDlgninto‚.nkCnlg.n=k//D‚.nkCnlgn/NULnlgk/to get', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 29}),\n",
              " Document(page_content='Solutions forChapter 2: GettingStarted 2-21\\n‚.nlgnCnlgn/NULnlglgn/D‚.2nlgn/NULnlglgn/ ;\\nwhich, by taking just the high-order term and ignoring the co nstant coefﬁcient,\\nequals ‚.nlgn/.\\nd.In practice, kshould be the largest list length on which insertion sort is f aster\\nthan merge sort.\\nSolutionto Problem 2-2\\na.We need to show that the elements of A0form a permutation of the elements\\nofA.\\nb.Loopinvariant: Atthestart ofeachiteration ofthe forloop oflines2–4,\\nAŒj \\x8dDminfAŒk\\x8dWj\\x14k\\x14ngand the subarray AŒj : : n\\x8d is a permuta-\\ntion of thevalues that werein AŒj : : n\\x8d at thetime that the loop started.\\nInitialization: Initially, jDn, and the subarray AŒj : : n\\x8d consists of single\\nelement AŒn\\x8d. Theloop invariant trivially holds.\\nMaintenance: Consider an iteration for a given value of j. By the loop in-\\nvariant, AŒj \\x8dis the smallest value in AŒj : : n\\x8d. Lines 3–4 exchange AŒj \\x8d\\nandAŒj/NUL1\\x8difAŒj \\x8dis less than AŒj/NUL1\\x8d, and so AŒj/NUL1\\x8dwill be the\\nsmallest value in AŒj/NUL1 : : n\\x8dafterward. Since the only change to the sub-\\narray AŒj/NUL1 : : n\\x8dis this possible exchange, and the subarray AŒj : : n\\x8d is\\na permutation of the values that were in AŒj : : n\\x8d at the time that the loop\\nstarted, we see that AŒj/NUL1 : : n\\x8dis a permutation of the values that were in\\nAŒj/NUL1 : : n\\x8dat the time that the loop started. Decrementing jfor the next\\niteration maintains the invariant.\\nTermination: The loop terminates when jreaches i. By the statement of the\\nloop invariant, AŒi\\x8dDminfAŒk\\x8dWi\\x14k\\x14ngandAŒi : : n\\x8disapermutation\\nof the values that werein AŒi : : n\\x8dat the timethat the loop started.\\nc.Loopinvariant: Atthestart ofeachiteration ofthe forloop oflines1–4,\\nthesubarray AŒ1 : : i/NUL1\\x8dconsists ofthe i/NUL1smallest valuesoriginally in\\nAŒ1 : : n\\x8d,insorted order, and AŒi : : n\\x8dconsists of the n/NULiC1remaining\\nvalues originally in AŒ1 : : n\\x8d.\\nInitialization: Before the ﬁrst iteration of the loop, iD1. The subarray\\nAŒ1 : : i/NUL1\\x8disempty, and sothe loop invariant vacuously holds.\\nMaintenance: Consider an iteration for a given value of i. By the loop invari-\\nant,AŒ1 : : i/NUL1\\x8dconsists ofthe ismallest valuesin AŒ1 : : n\\x8d,insortedorder.\\nPart (b) showed that after executing the forloop of lines 2–4, AŒi\\x8dis the\\nsmallest value in AŒi : : n\\x8d, and so AŒ1 : : i\\x8dis now the ismallest values orig-\\ninally in AŒ1 : : n\\x8d, in sorted order. Moreover, since the forloop of lines 2–4\\npermutes AŒi : : n\\x8d,thesubarray AŒiC1 : : n\\x8dconsists of the n/NULiremaining\\nvalues originally in AŒ1 : : n\\x8d.\\nTermination: Theforloopoflines1–4terminateswhen iDn,sothat i/NUL1D\\nn/NUL1. By the statement of the loop invariant, AŒ1 : : i/NUL1\\x8dis the subarray', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 30}),\n",
              " Document(page_content='2-22 Solutions for Chapter 2: GettingStarted\\nAŒ1 : : n/NUL1\\x8d,anditconsistsofthe n/NUL1smallestvaluesoriginallyin AŒ1 : : n\\x8d,\\ninsortedorder. Theremaining elementmustbethelargest va luein AŒ1 : : n\\x8d,\\nand it isin AŒn\\x8d. Therefore, the entire array AŒ1 : : n\\x8dis sorted.\\nNote:Tn the second edition, the forloop of lines 1–4 had an upper bound\\nofA:length. The last iteration of the outer forloop would then result in no\\niterationsoftheinner forloopoflines1–4,buttheterminationargument would\\nsimplify: AŒ1 : : i/NUL1\\x8dwould be the entire array AŒ1 : : n\\x8d, which, by the loop\\ninvariant, issorted.\\nd.The running time depends on the number of iterations of the forloop of\\nlines 2–4. For a given value of i, this loop makes n/NULiiterations, and itakes\\nonthe values 1; 2; : : : ; n/NUL1. Thetotal number of iterations, therefore, is\\nn/NUL1X\\niD1.n/NULi/Dn/NUL1X\\niD1n/NULn/NUL1X\\niD1i\\nDn.n/NUL1//NULn.n/NUL1/\\n2\\nDn.n/NUL1/\\n2\\nDn2\\n2/NULn\\n2:\\nThus, the running time of bubblesort is ‚.n2/in all cases. The worst-case\\nrunning time isthe sameas that of insertion sort.\\nSolution to Problem 2-4\\nThis solutionisalsopostedpublicly\\na.Theinversionsare .1; 5/; .2; 5/; .3; 4/; .3; 5/; .4; 5/ . (Rememberthatinversions\\nare speciﬁed by indices rather than by the values inthe array .)\\nb.The array with elements from f1; 2; : : : ; ngwith the most inversions is\\nhn; n/NUL1; n/NUL2; : : : ; 2; 1i. For all 1\\x14i < j\\x14n, there is an inversion .i; j /.\\nThenumber of such inversions is/NULn\\n2\\x01\\nDn.n/NUL1/=2.\\nc.Suppose that the array Astarts out with an inversion .k; j /. Then k < jand\\nAŒk\\x8d > AŒj \\x8d . At the time that the outer forloop of lines 1–8 sets keyDAŒj \\x8d,\\nthe value that started in AŒk\\x8dis still somewhere to the left of AŒj \\x8d. That is,\\nit’s in AŒi\\x8d, where 1\\x14i < j, and so the inversion has become .i; j /. Some\\niteration of the whileloop of lines 5–7 moves AŒi\\x8done position to the right.\\nLine 8 will eventually drop keyto the left of this element, thus eliminating\\nthe inversion. Because line 5 moves only elements that are gr eater than key,\\nit moves only elements that correspond to inversions. In oth er words, each\\niteration of the whileloop of lines 5–7 corresponds to the elimination of one\\ninversion.\\nd.Wefollow the hint and modify merge sort to count the number of inversions in\\n‚.nlgn/time.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 31}),\n",
              " Document(page_content='Solutions forChapter 2: GettingStarted 2-23\\nTo start, let us deﬁne a merge-inversion as a situation within the execution of\\nmerge sort in which the M ERGEprocedure, after copying AŒp : : q\\x8d toLand\\nAŒqC1 : : r\\x8dtoR, has values xinLandyinRsuch that x > y. Consider\\nan inversion .i; j /, and let xDAŒi\\x8dandyDAŒj \\x8d, so that i < jandx > y.\\nWeclaim that if we were to run merge sort, there would be exact ly one merge-\\ninversion involving xandy. To see why, observe that the only way in which\\narray elements change their positions is within the M ERGEprocedure. More-\\nover, since M ERGEkeeps elements within Lin the same relative order to each\\nother, and correspondingly for R, the only way in which two elements can\\nchange their ordering relative toeach other isfor thegreat er onetoappear in L\\nand the lesser one to appear in R. Thus, there is at least one merge-inversion\\ninvolving xandy. To see that there is exactly one such merge-inversion, ob-\\nserve that after any call of M ERGEthat involves both xandy, they are in the\\nsame sorted subarray and will therefore both appear in Lor both appear in R\\nin anygiven call thereafter. Thus, wehave proven the claim.\\nWe have shown that every inversion implies one merge-invers ion. In fact, the\\ncorrespondence between inversions and merge-inversions i s one-to-one. Sup-\\npose we have a merge-inversion involving values xandy, where xoriginally\\nwasAŒi\\x8dandywas originally AŒj \\x8d. Since we have a merge-inversion, x > y.\\nAnd since xis inLandyis inR,xmust be within a subarray preceding the\\nsubarray containing y. Therefore xstarted out in a position ipreceding y’s\\noriginal position j,and so .i; j /is aninversion.\\nHaving shown a one-to-one correspondence between inversio ns and merge-\\ninversions, it sufﬁces for usto count merge-inversions.\\nConsider a merge-inversion involving yinR. Let ´be the smallest value in L\\nthat is greater than y. At some point during the merging process, ´andywill\\nbe the “exposed” values in LandR,i.e., we will have ´DLŒi\\x8dandyDRŒj \\x8d\\nin line 13 of M ERGE. At that time, there will be merge-inversions involving y\\nandLŒi\\x8d; LŒiC1\\x8d; LŒiC2\\x8d; : : : ; LŒn 1\\x8d,andthese n1/NULiC1merge-inversions\\nwill be the only ones involving y. Therefore, we need to detect the ﬁrst time\\nthat´andybecome exposed during the M ERGEprocedure and add the value\\nofn1/NULiC1at that time toour total count of merge-inversions.\\nThe following pseudocode, modeled on merge sort, works as we have just de-\\nscribed. It also sorts thearray A.\\nCOUNT-INVERSIONS .A; p; r/\\nin\\x17ersionsD0\\nifp < r\\nqDb.pCr/=2c\\nin\\x17ersionsDin\\x17ersionsCCOUNT-INVERSIONS .A; p; q/\\nin\\x17ersionsDin\\x17ersionsCCOUNT-INVERSIONS .A; qC1; r/\\nin\\x17ersionsDin\\x17ersionsCMERGE-INVERSIONS .A; p; q; r/\\nreturnin\\x17ersions', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 32}),\n",
              " Document(page_content='2-24 Solutions for Chapter 2: GettingStarted\\nMERGE-INVERSIONS .A; p; q; r/\\nn1Dq/NULpC1\\nn2Dr/NULq\\nletLŒ1 : : n 1C1\\x8dandRŒ1 : : n 2C1\\x8dbe newarrays\\nforiD1ton1\\nLŒi\\x8dDAŒpCi/NUL1\\x8d\\nforjD1ton2\\nRŒj \\x8dDAŒqCj \\x8d\\nLŒn 1C1\\x8dD1\\nRŒn 2C1\\x8dD1\\niD1\\njD1\\nin\\x17ersionsD0\\nforkDptor\\nifRŒj \\x8d < LŒi\\x8d\\nin\\x17ersionsDin\\x17ersionsCn1/NULiC1\\nAŒk\\x8dDRŒj \\x8d\\njDjC1\\nelseAŒk\\x8dDLŒi\\x8d\\niDiC1\\nreturnin\\x17ersions\\nTheinitial call is C OUNT-INVERSIONS .A; 1; n/.\\nIn MERGE-INVERSIONS , whenever RŒj \\x8dis exposed and a value greater than\\nRŒj \\x8dbecomes exposed in the Larray, we increase in\\x17ersionsby the number\\nof remaining elements in L. Then because RŒjC1\\x8dbecomes exposed, RŒj \\x8d\\ncan never be exposed again. We don’t have to worry about merge -inversions\\ninvolving thesentinel 1inR,since novalue in Lwill be greater than 1.\\nSince we have added only a constant amount of additional work to each pro-\\ncedure call and to each iteration of the last forloop of the merging procedure,\\nthe total running time of the above pseudocode is the same as f or merge sort:\\n‚.nlgn/.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 33}),\n",
              " Document(page_content='Lecture Notes forChapter 3:\\nGrowthofFunctions\\nChapter 3 overview\\n\\x0fAwaytodescribebehavior offunctions inthelimit . We’restudying asymptotic\\nefﬁciency.\\n\\x0fDescribe growthof functions.\\n\\x0fFocus on what’s important by abstracting away low-order ter ms and constant\\nfactors.\\n\\x0fHowweindicate running times of algorithms.\\n\\x0fA waytocompare “sizes” of functions:\\nO\\x19 \\x14\\n\\x7f\\x19 \\x15\\n‚\\x19 D\\no\\x19<\\n!\\x19>\\nAsymptoticnotation\\nO-notation\\nO.g.n//Dff .n/Wthere exist positive constants candn0such that\\n0\\x14f .n/\\x14cg.n/for all n\\x15n0g:\\nn0nf(n)cg(n)\\ng.n/is anasymptotic upper bound forf .n/.\\nIff .n/2O.g.n//, wewrite f .n/DO.g.n//(will precisely explain this soon).', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 34}),\n",
              " Document(page_content='3-2 Lecture Notes for Chapter 3: Growthof Functions\\nExample\\n2n2DO.n3/,with cD1andn0D2.\\nExamples of functions in O.n2/:\\nn2\\nn2Cn\\nn2C1000n\\n1000n2C1000n\\nAlso,\\nn\\nn=1000\\nn1:99999\\nn2=lglglg n\\n\\x7f-notation\\n\\x7f.g.n//Dff .n/Wthere exist positive constants candn0such that\\n0\\x14cg.n/\\x14f .n/for all n\\x15n0g:\\nn0nf(n)\\ncg(n)\\ng.n/isanasymptotic lower bound forf .n/.\\nExample\\npnD\\x7f.lgn/,with cD1andn0D16.\\nExamples of functions in \\x7f.n2/:\\nn2\\nn2Cn\\nn2/NULn\\n1000n2C1000n\\n1000n2/NUL1000n\\nAlso,\\nn3\\nn2:00001\\nn2lglglg n\\n22n', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 35}),\n",
              " Document(page_content='Lecture Notes for Chapter 3: Growthof Functions 3-3\\n‚-notation\\n‚.g.n//Dff .n/Wthere exist positive constants c1,c2,and n0such that\\n0\\x14c1g.n/\\x14f .n/\\x14c2g.n/for all n\\x15n0g:\\nn0nf(n)\\nc1g(n)c2g(n)\\ng.n/is anasymptotically tight bound forf .n/.\\nExample\\nn2=2/NUL2nD‚.n2/, with c1D1=4,c2D1=2, and n0D8.\\nTheorem\\nf .n/D‚.g.n//if and only if fDO.g.n//andfD\\x7f.g.n// :\\nLeading constants and low-order terms don’t matter.\\nAsymptotic notation inequations\\nWhenonright-hand side\\nO.n2/stands for someanonymous function inthe set O.n2/.\\n2n2C3nC1D2n2C‚.n/means 2n2C3nC1D2n2Cf .n/for some\\nf .n/2‚.n/. Inparticular, f .n/D3nC1.\\nBytheway, weinterpret #of anonymous functions as D#of timesthe asymptotic\\nnotation appears:\\nnX\\niD1O.i/ OK:1anonymous function\\nO.1/CO.2/C\\x01\\x01\\x01C O.n/not OK: nhidden constants\\n)no clean interpretation\\nWhenonleft-hand side\\nNo matter how the anonymous functions are chosen on the left- hand side, there\\nis a way to choose the anonymous functions on the right-hand s ide to make the\\nequation valid.\\nInterpret 2n2C‚.n/D‚.n2/as meaning for allfunctions f .n/2‚.n/, there\\nexists afunction g.n/2‚.n2/such that 2n2Cf .n/Dg.n/.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 36}),\n",
              " Document(page_content='3-4 Lecture Notes for Chapter 3: Growthof Functions\\nCanchain together:\\n2n2C3nC1D2n2C‚.n/\\nD‚.n2/ :\\nInterpretation:\\n\\x0fFirstequation: Thereexists f .n/2‚.n/suchthat 2n2C3nC1D2n2Cf .n/.\\n\\x0fSecond equation: For all g.n/2‚.n/(such as the f .n/used to make the ﬁrst\\nequation hold), there exists h.n/2‚.n2/such that 2n2Cg.n/Dh.n/.\\no-notation\\no.g.n//Dff .n/Wfor all constants c > 0, there exists aconstant\\nn0> 0such that 0\\x14f .n/ < cg.n/ for all n\\x15n0g:\\nAnother view, probably easier touse: lim\\nn!1f .n/\\ng.n/D0.\\nn1:9999Do.n2/\\nn2=lgnDo.n2/\\nn2¤o.n2/(just like 26< 2)\\nn2=1000¤o.n2/\\n!-notation\\n!.g.n//Dff .n/Wfor all constants c > 0,there exists aconstant\\nn0> 0such that 0\\x14cg.n/ < f .n/ for all n\\x15n0g:\\nAnother view, again, probably easier to use: lim\\nn!1f .n/\\ng.n/D1.\\nn2:0001D!.n2/\\nn2lgnD!.n2/\\nn2¤!.n2/\\nComparisons of functions\\nRelational properties:\\nTransitivity:\\nf .n/D‚.g.n//andg.n/D‚.h.n//)f .n/D‚.h.n//.\\nSamefor O; \\x7f; o;and!.\\nReﬂexivity:\\nf .n/D‚.f .n//.\\nSamefor Oand\\x7f.\\nSymmetry:\\nf .n/D‚.g.n//if and only if g.n/D‚.f .n//.\\nTranspose symmetry:\\nf .n/DO.g.n//if and only if g.n/D\\x7f.f .n//.\\nf .n/Do.g.n//if and only if g.n/D!.f .n//.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 37}),\n",
              " Document(page_content='Lecture Notes for Chapter 3: Growthof Functions 3-5\\nComparisons:\\n\\x0ff .n/isasymptotically smaller thang.n/iff .n/Do.g.n//.\\n\\x0ff .n/isasymptotically larger thang.n/iff .n/D!.g.n//.\\nNo trichotomy. Although intuitively, we can liken Oto\\x14,\\x7fto\\x15, etc., unlike\\nreal numbers, where a < b,aDb, ora > b, we might not be able to compare\\nfunctions.\\nExample: n1Csinnandn, since 1Csinnoscillates between 0and 2.\\nStandard notations and commonfunctions\\n[You probably do not want to use lecture time going over all th e deﬁnitions and\\nproperties given in Section 3.2, but it might be worth spendi ng a few minutes of\\nlecture timeon someof the following.]\\nMonotonicity\\n\\x0ff .n/ismonotonically increasing ifm\\x14n)f .m/\\x14f .n/.\\n\\x0ff .n/ismonotonically decreasing ifm\\x15n)f .m/\\x15f .n/.\\n\\x0ff .n/isstrictly increasing ifm < n)f .m/ < f .n/ .\\n\\x0ff .n/isstrictly decreasing ifm > n)f .m/ > f .n/ .\\nExponentials\\nUseful identities:\\na/NUL1D1=a ;\\n.am/nDamn;\\namanDamCn:\\nCan relate rates of growth of polynomials and exponentials: for all real constants\\naandbsuch that a > 1,\\nlim\\nn!1nb\\nanD0 ;\\nwhich implies that nbDo.an/.\\nAsuprisingly useful inequality: for all real x,\\nex\\x151Cx :\\nAsxgets closer to 0,exgets closer to 1Cx.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 38}),\n",
              " Document(page_content='3-6 Lecture Notes for Chapter 3: Growthof Functions\\nLogarithms\\nNotations:\\nlgnDlog2n(binary logarithm) ,\\nlnnDlogen(natural logarithm) ,\\nlgknD.lgn/k(exponentiation) ,\\nlglgnDlg.lgn/(composition) .\\nLogarithm functions apply only to the next term in the formul a, so that lg nCk\\nmeans .lgn/Ck, andnotlg.nCk/.\\nIn the expression logba:\\n\\x0fIf wehold bconstant, then the expression is strictly increasing as aincreases.\\n\\x0fIf wehold aconstant, then the expression is strictly decreasing as bincreases.\\nUsefulidentitiesforallreal a > 0,b > 0,c > 0,and n,andwherelogarithmbases\\nare not 1:\\naDblogba;\\nlogc.ab/DlogcaClogcb ;\\nlogbanDnlogba ;\\nlogbaDlogca\\nlogcb;\\nlogb.1=a/D /NULlogba ;\\nlogbaD1\\nlogab;\\nalogbcDclogba:\\nChanging the base of a logarithm from one constant to another only changes the\\nvalue by a constant factor, so we usually don’t worry about lo garithm bases in\\nasymptotic notation. Convention istouselgwithinasympto tic notation, unlessthe\\nbase actually matters.\\nJust as polynomials grow more slowly than exponentials, log arithms grow more\\nslowly than polynomials. In lim\\nn!1nb\\nanD0, substitute lg nfornand2afora:\\nlim\\nn!1lgbn\\n.2a/lgnDlim\\nn!1lgbn\\nnaD0 ;\\nimplying that lgbnDo.na/.\\nFactorials\\nnŠD1\\x012\\x013\\x01n. Special case: 0ŠD1.\\nCanuseStirling’s approximation ,\\nnŠDp\\n2\\x19n\\x10n\\ne\\x11n\\x12\\n1C‚\\x121\\nn\\x13\\x13\\n;\\nto derive that lg .nŠ/D‚.nlgn/.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 39}),\n",
              " Document(page_content='SolutionsforChapter 3:\\nGrowthofFunctions\\nSolutionto Exercise 3.1-1\\nFirst, let’s clarify what the function max .f .n/; g.n// is. Let’s deﬁne the function\\nh.n/Dmax.f .n/; g.n// . Then\\nh.n/D(\\nf .n/iff .n/\\x15g.n/ ;\\ng.n/iff .n/ < g.n/ :\\nSince f .n/andg.n/are asymptotically nonnegative, there exists n0such that\\nf .n/\\x150andg.n/\\x150for all n\\x15n0. Thus for n\\x15n0,f .n/Cg.n/\\x15\\nf .n/\\x150andf .n/Cg.n/\\x15g.n/\\x150. Since for any particular n,h.n/\\nis either f .n/org.n/, we have f .n/Cg.n/\\x15h.n/\\x150, which shows that\\nh.n/Dmax.f .n/; g.n//\\x14c2.f .n/Cg.n//for all n\\x15n0(with c2D1in the\\ndeﬁnition of ‚).\\nSimilarly, since for any particular n,h.n/is the larger of f .n/andg.n/, we have\\nforall n\\x15n0,0\\x14f .n/\\x14h.n/and0\\x14g.n/\\x14h.n/. Addingthese twoinequal-\\nities yields 0\\x14f .n/Cg.n/\\x142h.n/, or equivalently 0\\x14.f .n/Cg.n//=2\\x14\\nh.n/,whichshowsthat h.n/Dmax.f .n/; g.n//\\x15c1.f .n/Cg.n//forall n\\x15n0\\n(with c1D1=2inthe deﬁnition of ‚).\\nSolutionto Exercise 3.1-2\\nThissolutionisalsopostedpublicly\\nToshow that .nCa/bD‚.nb/,wewant toﬁndconstants c1; c2; n0> 0suchthat\\n0\\x14c1nb\\x14.nCa/b\\x14c2nbfor all n\\x15n0.\\nNotethat\\nnCa\\x14nCjaj\\n\\x142n whenjaj\\x14n,\\nand\\nnCa\\x15n/NULjaj\\n\\x151\\n2nwhenjaj\\x141\\n2n.\\nThus, when n\\x152jaj,', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 40}),\n",
              " Document(page_content='3-8 Solutions for Chapter 3: Growthof Functions\\n0\\x141\\n2n\\x14nCa\\x142n :\\nSince b > 0, the inequality still holds when all parts areraised to thep ower b:\\n0\\x14\\x121\\n2n\\x13b\\n\\x14.nCa/b\\x14.2n/b;\\n0\\x14\\x121\\n2\\x13b\\nnb\\x14.nCa/b\\x142bnb:\\nThus, c1D.1=2/b,c2D2b, and n0D2jajsatisfy the deﬁnition.\\nSolution to Exercise3.1-3\\nThis solutionisalsopostedpublicly\\nLet the running time be T .n/.T .n/\\x15O.n2/means that T .n/\\x15f .n/for some\\nfunction f .n/in the set O.n2/. This statement holds for any running time T .n/,\\nsince the function g.n/D0for all nis inO.n2/, and running times are always\\nnonnegative. Thus, the statement tells usnothing about the running time.\\nSolution to Exercise3.1-4\\nThis solutionisalsopostedpublicly\\n2nC1DO.2n/,but22n¤O.2n/.\\nToshow that 2nC1DO.2n/,wemust ﬁndconstants c; n 0> 0such that\\n0\\x142nC1\\x14c\\x012nfor all n\\x15n0:\\nSince 2nC1D2\\x012nfor all n,wecan satisfy thedeﬁnition with cD2andn0D1.\\nToshow that 22n6DO.2n/,assume there exist constants c; n 0> 0such that\\n0\\x1422n\\x14c\\x012nfor all n\\x15n0:\\nThen 22nD2n\\x012n\\x14c\\x012n)2n\\x14c. But no constant is greater than all 2n, and\\nso the assumption leads to acontradiction.\\nSolution to Exercise3.1-8\\n\\x7f.g.n; m//Dff .n; m/Wthere exist positive constants c,n0, and m0\\nsuch that 0\\x14cg.n; m/\\x14f .n; m/\\nfor all n\\x15n0orm\\x15m0g:\\n‚.g.n; m//Dff .n; m/Wthere exist positive constants c1,c2,n0, and m0\\nsuch that 0\\x14c1g.n; m/\\x14f .n; m/\\x14c2g.n; m/\\nfor all n\\x15n0orm\\x15m0g:', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 41}),\n",
              " Document(page_content='Solutions forChapter 3: Growthof Functions 3-9\\nSolutionto Exercise 3.2-4\\nThissolutionisalsopostedpublicly\\ndlgneŠis not polynomially bounded, but dlglgneŠis.\\nProvingthatafunction f .n/ispolynomially bounded isequivalent toproving that\\nlg.f .n//DO.lgn/for the following reasons.\\n\\x0fIffis polynomially bounded, then there exist constants c,k,n0such that for\\nalln\\x15n0,f .n/\\x14cnk. Hence, lg .f .n//\\x14kclgn, which, since candkare\\nconstants, means that lg .f .n//DO.lgn/.\\n\\x0fSimilarly, if lg .f .n//DO.lgn/,then fis polynomially bounded.\\nInthe following proofs, wewill makeuse of the following two facts:\\n1. lg .nŠ/D‚.nlgn/(by equation (3.19)).\\n2.dlgneD‚.lgn/,because\\n\\x0fdlgne\\x15lgn\\n\\x0fdlgne<lgnC1\\x142lgnfor all n\\x152\\nlg.dlgneŠ/D‚.dlgnelgdlgne/\\nD‚.lgnlglgn/\\nD!.lgn/ :\\nTherefore, lg .dlgneŠ/¤O.lgn/,and sodlgneŠis not polynomially bounded.\\nlg.dlglgneŠ/D‚.dlglgnelgdlglgne/\\nD‚.lglgnlglglg n/\\nDo..lglgn/2/\\nDo.lg2.lgn//\\nDo.lgn/ :\\nThe last step above follows from the property that any polylo garithmic function\\ngrows more slowly than any positive polynomial function, i. e., that for constants\\na; b > 0, wehave lgbnDo.na/. Substitute lg nforn,2forb, and 1fora,giving\\nlg2.lgn/Do.lgn/.\\nTherefore, lg .dlglgneŠ/DO.lgn/,and sodlglgneŠis polynomially bounded.\\nSolutionto Exercise 3.2-5\\nlg\\x03.lgn/isasymptotically larger because lg\\x03.lgn/Dlg\\x03n/NUL1.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 42}),\n",
              " Document(page_content='3-10 Solutions for Chapter 3: Growthof Functions\\nSolution to Exercise3.2-6\\nBoth \\x1e2and\\x1eC1equal .3Cp\\n5/=2, and bothy\\x1e2andy\\x1eC1equal .3/NULp\\n5/=2.\\nSolution to Exercise3.2-7\\nWehave twobase cases: iD0andiD1. For iD0, wehave\\n\\x1e0/NULy\\x1e0\\np\\n5D1/NUL1p\\n5\\nD0\\nDF0;\\nand for iD1, wehave\\n\\x1e1/NULy\\x1e1\\np\\n5D.1Cp\\n5//NUL.1/NULp\\n5/\\n2p\\n5\\nD2p\\n5\\n2p\\n5\\nD1\\nDF1:\\nFor the inductive case, the inductive hypothesis is that Fi/NUL1D.\\x1ei/NUL1/NULy\\x1ei/NUL1/=p\\n5\\nandFi/NUL2D.\\x1ei/NUL2/NULy\\x1ei/NUL2/=p\\n5. Wehave\\nFiDFi/NUL1CFi/NUL2 (equation (3.22))\\nD\\x1ei/NUL1/NULy\\x1ei/NUL1\\np\\n5C\\x1ei/NUL2/NULy\\x1ei/NUL2\\np\\n5(inductive hypothesis)\\nD\\x1ei/NUL2.\\x1eC1//NULy\\x1ei/NUL2.y\\x1eC1/p\\n5\\nD\\x1ei/NUL2\\x1e2/NULy\\x1ei/NUL2y\\x1e2\\np\\n5(Exercise 3.2-6)\\nD\\x1ei/NULy\\x1ei\\np\\n5:\\nSolution to Problem 3-3\\na.Here is the ordering, where functions on the same line are in t he same equiva-\\nlence class, and those higher on the page are \\x7fof those below them:', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 43}),\n",
              " Document(page_content='Solutions forChapter 3: Growthof Functions 3-11\\n22nC1\\n22n\\n.nC1/Š\\nnŠ seejustiﬁcation 7\\nenseejustiﬁcation 1\\nn\\x012n\\n2n\\n.3=2/n\\n.lgn/lgnDnlglgnseeidentity 1\\n.lgn/Š seejustiﬁcations 2, 8\\nn3\\nn2D4lgnseeidentity 2\\nnlgnand lg .nŠ/seejustiﬁcation 6\\nnD2lgnseeidentity 3\\n.p\\n2/lgn.Dpn/seeidentity 6, justiﬁcation 3\\n2p2lgnseeidentity 5, justiﬁcation 4\\nlg2n\\nlnnp\\nlgn\\nlnlnn seejustiﬁcation 5\\n2lg\\x03n\\nlg\\x03nand lg\\x03.lgn/seeidentity 7\\nlg.lg\\x03/n\\nn1=lgn.D2/and1seeidentity 4\\nMuch of theranking isbased on thefollowing properties:\\n\\x0fExponential functions grow faster than polynomial functio ns, which grow\\nfaster than polylogarithmic functions.\\n\\x0fThe base of a logarithm doesn’t matter asymptotically, but t he base of an\\nexponential and the degree of apolynomial do matter.\\nWehave the following identities:\\n1..lgn/lgnDnlglgnbecause alogbcDclogba.\\n2.4lgnDn2because alogbcDclogba.\\n3.2lgnDn.\\n4.2Dn1=lgnbyraising identity 3to the power 1=lgn.\\n5.2p2lgnDnp\\n2=lgnbyraising identity 4to the powerp\\n2lgn.\\n6./NULp\\n2\\x01lgnDpnbecause/NULp\\n2\\x01lgnD2.1=2/lgnD2lgpnDpn.\\n7. lg\\x03.lgn/D.lg\\x03n//NUL1.\\nThefollowing justiﬁcations explain someof the rankings:\\n1.enD2n.e=2/nD!.n2n/,since .e=2/nD!.n/.\\n2..lgn/ŠD!.n3/by taking logs: lg .lgn/ŠD‚.lgnlglgn/by Stirling’s\\napproximation, lg .n3/D3lgn. lglg nD!.3/.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 44}),\n",
              " Document(page_content='3-12 Solutions for Chapter 3: Growthof Functions\\n3..p\\n2/lgnD!\\x10\\n2p2lgn\\x11\\nby taking logs: lg .p\\n2/lgnD.1=2/lgn, lg2p2lgnD\\np\\n2lgn..1=2/lgnD!.p\\n2lgn/.\\n4.2p2lgnD!.lg2n/by taking logs: lg 2p2lgnDp\\n2lgn, lglg2nD2lglgn.p\\n2lgnD!.2lglgn/.\\n5. lnln nD!.2lg\\x03n/by taking logs: lg 2lg\\x03nDlg\\x03n. lglnln nD!.lg\\x03n/.\\n6. lg.nŠ/D‚.nlgn/(equation (3.19)).\\n7.nŠD‚.nnC1=2e/NULn/by dropping constants and low-order terms in equa-\\ntion (3.18).\\n8..lgn/ŠD‚..lgn/lgnC1=2e/NULlgn/by substituting lg nfornin the previous\\njustiﬁcation. .lgn/ŠD‚..lgn/lgnC1=2n/NULlge/because alogbcDclogba.\\nb.Thefollowing f .n/isnonnegative, andfor allfunctions gi.n/inpart (a), f .n/\\nisneither O.g i.n//nor\\x7f.g i.n//.\\nf .n/D(\\n22nC2ifniseven ;\\n0ifnisodd :', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 45}),\n",
              " Document(page_content='Lecture Notes forChapter 4:\\nDivide-and-Conquer\\nChapter 4 overview\\nRecall the divide-and-conquer paradigm, which weused for m erge sort:\\nDividetheproblem into anumber of subproblems that aresmaller ins tances ofthe\\nsame problem.\\nConquer the subproblems by solving them recursively.\\nBasecase: Ifthesubproblemsaresmallenough, justsolvethembybrute force.\\nCombine the subproblem solutions togive asolution tothe original p roblem.\\nWelook at twomore algorithms based on divide-and-conquer.\\nAnalyzingdivide-and-conquer algorithms\\nUse a recurrence to characterize the running time of a divide -and-conquer algo-\\nrithm. Solving the recurrence gives us the asymptotic runni ng time.\\nArecurrence isafunction isdeﬁned interms of\\n\\x0fone or morebase cases, and\\n\\x0fitself, withsmaller arguments.\\nExamples\\n\\x0fT .n/D(\\n1 ifnD1 ;\\nT .n/NUL1/C1ifn > 1 :\\nSolution: T .n/Dn.\\n\\x0fT .n/D(\\n1 ifnD1 ;\\n2T .n=2/Cnifn\\x151 :\\nSolution: T .n/DnlgnCn.\\n\\x0fT .n/D(\\n0 ifnD2 ;\\nT .pn/C1ifn > 2 :\\nSolution: T .n/Dlglgn.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 46}),\n",
              " Document(page_content='4-2 Lecture Notes for Chapter 4: Divide-and-Conquer\\n\\x0fT .n/D(\\n1 ifnD1 ;\\nT .n=3/CT .2n=3/Cnifn > 1 :\\nSolution: T .n/D‚.nlgn/.\\n[The notes for this chapter are fairly brief because we teach recurrences in much\\ngreater detail in aseparate discrete mathcourse.]\\nMany technical issues:\\n\\x0fFloors and ceilings\\n[Floors and ceilings can easily be removed and don’t affect t he solution to the\\nrecurrence. Theyare better left toadiscrete mathcourse.]\\n\\x0fExact vs. asymptotic functions\\n\\x0fBoundary conditions\\nInalgorithm analysis, weusuallyexpressboththerecurren ce anditssolutionusing\\nasymptotic notation.\\n\\x0fExample: T .n/D2T .n=2/C‚.n/,withsolution T .n/D‚.nlgn/.\\n\\x0fThe boundary conditions are usually expressed as “ T .n/DO.1/for sufﬁ-\\nciently small n.”\\n\\x0fWhen we desire an exact, rather than an asymptotic, solution , we need to deal\\nwithboundary conditions.\\n\\x0fIn practice, we just use asymptotics most of the time, and we i gnore boundary\\nconditions.\\n[In my course, there are only two acceptable ways of solving r ecurrences: the\\nsubstitution method and the master method. Unless the recur sion tree is carefully\\naccounted for, I do not accept it as a proof of a solution, thou gh I certainly accept\\na recursion tree as a way to generate a guess for substitution method. You may\\nchoose toallow recursion trees asproofs inyour course, inw hichcasesomeofthe\\nsubstitution proofs in the solutions for this chapter becom e recursion trees.\\nI also never use the iteration method, which had appeared in t he ﬁrst edition of\\nIntroduction to Algorithms . I ﬁnd that it is too easy to make an error in paren-\\nthesization, and that recursion trees give a better intuiti ve idea than iterating the\\nrecurrence of how the recurrence progresses.]\\nMaximum-subarray problem\\nInput:An array AŒ1 : : n\\x8d of numbers. [Assume that some of the numbers are\\nnegative, because this problem istrivial when all numbers a re nonnegative.]\\nOutput: Indices iandjsuchthat AŒi : : j \\x8dhasthegreatest sumofanynonempty,\\ncontiguous subarray of A,along withthe sum of thevalues in AŒi : : j \\x8d.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 47}),\n",
              " Document(page_content='Lecture Notes for Chapter 4: Divide-and-Conquer 4-3\\nScenario\\n\\x0fYou have the prices that a stock traded at over aperiod of nconsecutive days.\\n\\x0fWhenshouldyouhaveboughtthestock? Whenshouldyouhaveso ldthestock?\\n\\x0fEven though it’s in retrospect, you can yell at your stockbro ker for not recom-\\nmending these buy and sell dates.\\nToconvert toamaximum-subarray problem, let\\nAŒi\\x8dD(price after day i)/NUL(price after day .i/NUL1/):\\n[Assuming that we start with a price after day 0, i.e., just be fore day 1.] Then the\\nnonempty, contiguous subarray with the greatest sum bracke ts the days that you\\nshould have held the stock.\\nIf the maximum subarray is AŒi : : j \\x8d, then should have bought just before day i\\n(i.e., just after day .i/NUL1/) and sold just after day j.\\nWhydoweneedtoﬁndthemaximumsubarray? Whynotjust“buylo w,sellhigh”?\\n\\x0fLowest price might occur afterthe highest price.\\n\\x0fBut wouldn’t the optimal strategy involve buying at the lowe st priceorselling\\nat thehighest price?\\n\\x0fNot necessarily:\\n0 1 2 3 411\\n10\\n9\\n8\\n7\\n6\\nMaximumproﬁtis$3pershare,frombuyingafterday2andsell ingafterday3.\\nYet lowest price occurs after day 4and highest occurs after d ay 1.\\nCan solve by brute force: check all/NULn\\n2\\x01\\nD‚.n2/subarrays. Can organize the\\ncomputation so that each subarray AŒi : : j \\x8d takes O.1/time, given that you’ve\\ncomputed AŒi : : j/NUL1\\x8d, sothat thebrute-force solution takes ‚.n2/time.\\nSolvingbydivide-and-conquer\\nUsedivide-and-conquer to solve in O.nlgn/time.\\n[Maximum subarray might not be unique, though its value is, s o we speak of a\\nmaximum subarray, rather than themaximum subarray.]\\nSubproblem: Find amaximum subarray of AŒlow: :high\\x8d.\\nInoriginal call, lowD1,highDn.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 48}),\n",
              " Document(page_content='4-4 Lecture Notes for Chapter 4: Divide-and-Conquer\\nDividethe subarray into two subarrays of as equal size as possible. Find the\\nmidpoint midof the subarrays, and consider the subarrays AŒlow: :mid\\x8dand\\nAŒmidC1 : :high\\x8d.\\nConquer byﬁndingamaximumsubarraysof AŒlow: :mid\\x8dandAŒmidC1 : :high\\x8d.\\nCombine byﬁnding amaximum subarray that crosses themidpoint, andu sing the\\nbest solution out of the three (the subarray crossing the mid point and the two\\nsolutions found inthe conquer step).\\nThisstrategyworksbecauseanysubarraymusteitherlieent irelyononesideofthe\\nmidpoint or cross the midpoint.\\nFinding themaximum subarray that crosses themidpoint\\nNota smaller instance of the original problem: has the added res triction that the\\nsubarray must cross the midpoint.\\nAgain,couldusebruteforce. Ifsizeof AŒlow: :high\\x8disn,wouldhave n=2choices\\nfor left endpoint and n=2choices right endpoint, so would have ‚.n2/combina-\\ntions altogether.\\nCansolve inlinear time.\\n\\x0fAnysubarraycrossingthemidpoint AŒmid\\x8dismadeoftwosubarrays AŒi : :mid\\x8d\\nandAŒmidC1 : : j \\x8d,wherelow\\x14i\\x14midandmid< j\\x14high.\\n\\x0fFind maximum subarrays of the form AŒi : :mid\\x8dandAŒmidC1 : : j \\x8dand then\\ncombine them.\\nProcedure to take array Aand indices low,mid,highand return a tuple giving\\nindices ofmaximum subarray that crosses themidpoint, alon g withthesuminthis\\nmaximum subarray:\\nFIND-MAX-CROSSING-SUBARRAY .A;low;mid;high/\\n//Findamaximum subarray of the form AŒi : :mid\\x8d.\\nleft-sumD/NUL1\\nsumD0\\nforiDmiddowntolow\\nsumDsumCAŒi\\x8d\\nifsum>left-sum\\nleft-sumDsum\\nmax-leftDi\\n//Findamaximum subarray of the form AŒmidC1 : : j \\x8d.\\nright-sumD/NUL1\\nsumD0\\nforjDmidC1tohigh\\nsumDsumCAŒj \\x8d\\nifsum>right-sum\\nright-sumDsum\\nmax-rightDj\\n//Return the indices and the sum of the twosubarrays.\\nreturn .max-left;max-right;left-sumCright-sum/', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 49}),\n",
              " Document(page_content='Lecture Notes for Chapter 4: Divide-and-Conquer 4-5\\nTime:The two loops together consider each index in the range low; : : : ;highex-\\nactly once, and each iteration takes ‚.1/time)procedure takes ‚.n/time.\\nDivide-and-conquer procedure for themaximum-subarray pr oblem\\nFIND-MAXIMUM-SUBARRAY .A;low;high/\\nifhigh==low\\nreturn .low;high; AŒlow\\x8d///base case: only one element\\nelsemidDb.lowChigh/=2c\\n.left-low;left-high;left-sum/D\\nFIND-MAXIMUM-SUBARRAY .A;low;mid/\\n.right-low;right-high;right-sum/D\\nFIND-MAXIMUM-SUBARRAY .A;midC1;high/\\n.cross-low;cross-high;cross-sum/D\\nFIND-MAX-CROSSING-SUBARRAY .A;low;mid;high/\\nifleft-sum\\x15right-sumandleft-sum\\x15cross-sum\\nreturn .left-low;left-high;left-sum/\\nelseifright-sum\\x15left-sumandright-sum\\x15cross-sum\\nreturn .right-low;right-high;right-sum/\\nelse return .cross-low;cross-high;cross-sum/\\nInitial call: FIND-MAXIMUM-SUBARRAY .A; 1; n/\\n\\x0fDivide by computing mid.\\n\\x0fConquer by the tworecursive calls to F IND-MAXIMUM-SUBARRAY .\\n\\x0fCombinebycalling F IND-MAX-CROSSING-SUBARRAY andthendetermining\\nwhich of thethree results gives the maximum sum.\\n\\x0fBase case iswhen the subarray has only 1element.\\nAnalysis\\nSimplifying assumption: Original problem size is a power of 2, so that all sub-\\nproblem sizes are integer. [We made the same simplifying assumption when we\\nanalyzed merge sort.]\\nLetT .n/denote the running time of F IND-MAXIMUM-SUBARRAY on a subarray\\nofnelements.\\nBase case: Occurs when highequalslow, so that nD1. The procedure just\\nreturns)T .n/D‚.1/.\\nRecursive case: Occurs when n > 1.\\n\\x0fDividing takes ‚.1/time.\\n\\x0fConqueringsolvestwosubproblems,eachonasubarrayof n=2elements. Takes\\nT .n=2/timefor each subproblem )2T .n=2/ timefor conquering.\\n\\x0fCombining consists of calling F IND-MAX-CROSSING-SUBARRAY , which\\ntakes ‚.n/time,andaconstant number ofconstant-time tests )‚.n/C‚.1/\\ntime for combining.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 50}),\n",
              " Document(page_content='4-6 Lecture Notes for Chapter 4: Divide-and-Conquer\\nRecurrence for recursive case becomes\\nT .n/D‚.1/C2T .n=2/C‚.n/C‚.1/\\nD2T .n=2/C‚.n/ (absorb ‚.1/terms into ‚.n/):\\nTherecurrence for all cases:\\nT .n/D(\\n‚.1/ ifnD1 ;\\n2T .n=2/C‚.n/ifn > 1 :\\nSame recurrence as for merge sort. Can use the master method t o show that it has\\nsolution T .n/D‚.nlgn/.\\nThus, withdivide-and-conquer, wehave developed a ‚.nlgn/-timesolution.\\nBetter than the ‚.n2/-time brute-force solution.\\n[Can actually solve this problem in ‚.n/time. SeeExercise 4.1-5.]\\nStrassen’s algorithm formatrix multiplication\\nInput:Twon\\x02n(square) matrices, AD.aij/andBD.bij/.\\nOutput: n\\x02nmatrix CD.cij/,where CDA\\x01B, i.e.,\\ncijDnX\\nkD1aikbkj\\nfori; jD1; 2; : : : ; n .\\nNeed tocompute n2entries of C. Eachentry isthe sum of nvalues.\\nObviousmethod\\n[Using a shorter procedure name than inthe book.]\\nSQUARE-MAT-MULT.A; B; n/\\nletCbea new n\\x02nmatrix\\nforiD1ton\\nforjD1ton\\ncijD0\\nforkD1ton\\ncijDcijCaik\\x01bkj\\nreturn C\\nAnalysis: Threenestedloops,eachiterates ntimes,andinnermostloopbodytakes\\nconstant time)‚.n3/.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 51}),\n",
              " Document(page_content='Lecture Notes for Chapter 4: Divide-and-Conquer 4-7\\nIs‚.n3/thebest wecan do? Can wemultiplymatrices in o.n3/time?\\nSeemslike any algorithm tomultiply matrices must take \\x7f.n3/time:\\n\\x0fMust compute n2entries.\\n\\x0fEach entry is thesum of nterms.\\nBut withStrassen’s method, wecan multiply matrices in o.n3/time.\\n\\x0fStrassen’s algorithm runs in ‚.nlg7/time.\\n\\x0f2:80\\x14lg7\\x142:81.\\n\\x0fHence, runs in O.n2:81/time.\\nSimpledivide-and-conquer method\\nAswith the other divide-and-conquer algorithms, assume th atnisapower of 2.\\nPartition each of A; B; Cinto four n=2\\x02n=2matrices:\\nAD\\x12A11A12\\nA21A22\\x13\\n; BD\\x12B11B12\\nB21B22\\x13\\n; CD\\x12C11C12\\nC21C22\\x13\\n:\\nRewrite CDA\\x01Bas\\x12C11C12\\nC21C22\\x13\\nD\\x12A11A12\\nA21A22\\x13\\n\\x01\\x12B11B12\\nB21B22\\x13\\n;\\ngiving thefour equations\\nC11DA11\\x01B11CA12\\x01B21;\\nC12DA11\\x01B12CA12\\x01B22;\\nC21DA21\\x01B11CA22\\x01B21;\\nC22DA21\\x01B12CA22\\x01B22:\\nEach of these equations multiplies two n=2\\x02n=2matrices and then adds their\\nn=2\\x02n=2products.\\nUse these equations to get a divide-and-conquer algorithm: [Using a shorter pro-\\ncedure namethan inthe book.]\\nREC-MAT-MULT.A; B; n/\\nletCbe anew n\\x02nmatrix\\nifn==1\\nc11Da11\\x01b11\\nelsepartition A,B, and Cinton=2\\x02n=2submatrices\\nC11DREC-MAT-MULT.A11; B11; n=2/CREC-MAT-MULT.A12; B21; n=2/\\nC12DREC-MAT-MULT.A11; B12; n=2/CREC-MAT-MULT.A12; B22; n=2/\\nC21DREC-MAT-MULT.A21; B11; n=2/CREC-MAT-MULT.A22; B21; n=2/\\nC22DREC-MAT-MULT.A21; B12; n=2/CREC-MAT-MULT.A22; B22; n=2/\\nreturn C\\n[Thebookbrieﬂydiscusses thequestion ofhowtoavoidcopyi ng entrieswhenpar-\\ntitioning matrices. Canpartition matrices without copyin g entries by instead using\\nindex calculations. Identify a submatrix by ranges of row an d column matrices', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 52}),\n",
              " Document(page_content='4-8 Lecture Notes for Chapter 4: Divide-and-Conquer\\nfrom the original matrix. End up representing a submatrix di fferently from how\\nwe represent the original matrix. The advantage of avoiding copying is that par-\\ntitioning would take only constant time, instead of ‚.n2/time. The result of the\\nasymptotic analysis won’t change, but using index calculat ions to avoid copying\\ngives better constant factors.]\\nAnalysis\\nLetT .n/be thetime tomultiply two n=2\\x02n=2matrices.\\nBase case: nD1. Perform one scalar multiplication: ‚.1/.\\nRecursive case: n > 1.\\n\\x0fDividing takes ‚.1/time, using index calculations. [Otherwise, ‚.n2/time.]\\n\\x0fConquering makes 8recursive calls, each multiplying n=2\\x02n=2matrices)\\n8T .n=2/.\\n\\x0fCombining takes ‚.n2/time to add n=2\\x02n=2matrices four times. [Doesn’t\\neven matter asymptotically whether we use index calculatio ns or copy: would\\nbe‚.n2/either way.]\\nRecurrence is\\nT .n/D(\\n‚.1/ ifnD1 ;\\n8T .n=2/C‚.n2/ifn > 1 :\\nCanuse master method toshow that it has solution T .n/D‚.n3/.\\nAsymptotically, no better than the obvious method.\\nConstant factors andrecurrences: When setting up recurrences, can absorb con-\\nstant factors intoasymptotic notation, but cannot absorb a constant number of sub-\\nprobems. Althoughweabsorbthe 4additionsof n=2\\x02n=2matricesintothe ‚.n2/\\ntime, we cannot lose the 8in front of the T .n=2/term. If we absorb the constant\\nnumber of subproblems, then the recursion tree would not be “ bushy” and would\\ninstead just bealinear chain.\\nStrassen’s method\\nIdea:Maketherecursiontreelessbushy. Performonly 7recursivemultiplications\\nofn=2\\x02n=2matrices, rather than 8. Will cost several additions of n=2\\x02n=2\\nmatrices, but just a constant number more )can still absorb the constant factor\\nfor matrix additions into the ‚.n=2/term.\\nThealgorithm:\\n1. As in the recursive method, partition each of the matrices into four n=2\\x02n=2\\nsubmatrices. Time: ‚.1/.\\n2. Create 10matrices S1; S2; : : : ; S 10. Each is n=2\\x02n=2and is the sum or dif-\\nference of two matrices created in previous step. Time: ‚.n2/to create all 10\\nmatrices.\\n3. Recursively compute 7matrix products P1; P2; : : : ; P 7, each n=2\\x02n=2.\\n4. Compute n=2\\x02n=2submatrices of Cbyadding and subtracting various com-\\nbinations of the Pi. Time: ‚.n2/.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 53}),\n",
              " Document(page_content='Lecture Notes for Chapter 4: Divide-and-Conquer 4-9\\nAnalysis\\nRecurrence will be\\nT .n/D(\\n‚.1/ ifnD1 ;\\n7T .n=2/C‚.n2/ifn > 1 :\\nBythemaster method, solution is T .n/D‚.nlg7/.\\nDetails\\nStep2:Create the 10 matrices\\nS1DB12/NULB22;\\nS2DA11CA12;\\nS3DA21CA22;\\nS4DB21/NULB11;\\nS5DA11CA22;\\nS6DB11CB22;\\nS7DA12/NULA22;\\nS8DB21CB22;\\nS9DA11/NULA21;\\nS10DB11CB12:\\nAddor subtract n=2\\x02n=2matrices 10times)timeis ‚.n=2/.\\nStep3:Create the 7matrices\\nP1DA11\\x01S1DA11\\x01B12/NULA11\\x01B22;\\nP2DS2\\x01B22DA11\\x01B22CA12\\x01B22;\\nP3DS3\\x01B11DA21\\x01B11CA22\\x01B11;\\nP4DA22\\x01S4DA22\\x01B21/NULA22\\x01B11;\\nP5DS5\\x01S6DA11\\x01B11CA11\\x01B22CA22\\x01B11CA22\\x01B22;\\nP6DS7\\x01S8DA12\\x01B21CA12\\x01B22/NULA22\\x01B21/NULA22\\x01B22;\\nP7DS9\\x01S10DA11\\x01B11CA11\\x01B12/NULA21\\x01B11/NULA21\\x01B12:\\nTheonly multiplications needed are in the middle column; ri ght-hand column just\\nshowsthe products in termsof the original submatrices of AandB.\\nStep4:Add and subtract the Pitoconstruct submatrices of C:\\nC11DP5CP4/NULP2CP6;\\nC12DP1CP2;\\nC21DP3CP4;\\nC22DP5CP1/NULP3/NULP7:\\nTo see how these computations work, expand each right-hand s ide, replacing\\neach Piwith the submatrices of AandBthat form it, and cancel terms: [We\\nexpand out all four right-hand sides here. You might want to d o just one or two of\\nthem, toconvince students that it works.]', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 54}),\n",
              " Document(page_content='4-10 Lecture Notes for Chapter 4: Divide-and-Conquer\\nA11\\x01B11CA11\\x01B22CA22\\x01B11CA22\\x01B22\\n/NULA22\\x01B11CA22\\x01B21\\n/NULA11\\x01B22 /NULA12\\x01B22\\n/NULA22\\x01B22/NULA22\\x01B21CA12\\x01B22CA12\\x01B21\\nA11\\x01B11 CA12\\x01B21\\nA11\\x01B12/NULA11\\x01B22\\nCA11\\x01B22CA12\\x01B22\\nA11\\x01B12CA12\\x01B22\\nA21\\x01B11CA22\\x01B11\\n/NULA22\\x01B11CA22\\x01B21\\nA21\\x01B11CA22\\x01B21\\nA11\\x01B11CA11\\x01B22CA22\\x01B11CA22\\x01B22\\n/NULA11\\x01B22 CA11\\x01B12\\n/NULA22\\x01B11 /NULA21\\x01B11\\n/NULA11\\x01B11 /NULA11\\x01B12CA21\\x01B11CA21\\x01B12\\nA22\\x01B22 CA21\\x01B12\\nTheoretical andpractical notes\\nStrassen’salgorithmwastheﬁrsttobeat ‚.n3/time,butit’snottheasymptotically\\nfastest known. A method by Coppersmith and Winograd runs in O.n2:376/time.\\nPractical issues against Strassen’s algorithm:\\n\\x0fHigher constant factor than the obvious ‚.n3/-timemethod.\\n\\x0fNot good for sparse matrices.\\n\\x0fNot numerically stable: larger errors accumulate than in th e obvious method.\\n\\x0fSubmatrices consume space, especially if copying.\\nNumericalstabilityproblemisnotasbadaspreviouslythou ght. Andcanuseindex\\ncalculations to reduce space requirement.\\nVarious researchers have tried to ﬁnd the crossover point, w here Strassen’s algo-\\nrthmrunsfasterthantheobvious ‚.n3/-timemethod. Analyses(thatignorecaches\\nand hardware pipelines) have produced crossover points as l ow as nD8, and ex-\\nperiments have found crossover points aslow as nD400.\\nSubstitution method\\n1. Guess the solution.\\n2. Useinduction to ﬁndthe constants and show that the soluti on works.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 55}),\n",
              " Document(page_content='Lecture Notes for Chapter 4: Divide-and-Conquer 4-11\\nExample\\nT .n/D(\\n1 ifnD1 ;\\n2T .n=2/Cnifn > 1 :\\n1.Guess: T .n/DnlgnCn.[Here, we have a recurrence with an exact func-\\ntion, rather than asymptotic notation, and the solution is a lso exact rather than\\nasymptotic. We’ll have tocheck boundary conditions and the base case.]\\n2.Induction:\\nBasis: nD1)nlgnCnD1DT .n/\\nInductive step: Inductive hypothesis is that T .k/DklgkCkfor all k < n.\\nWe’ll use this inductive hypothesis for T .n=2/.\\nT .n/D2T\\x10n\\n2\\x11\\nCn\\nD2\\x10n\\n2lgn\\n2Cn\\n2\\x11\\nCn(by inductive hypothesis)\\nDnlgn\\n2CnCn\\nDn.lgn/NULlg2/CnCn\\nDnlgn/NULnCnCn\\nDnlgnCn :\\nGenerally, weuse asymptotic notation:\\n\\x0fWewould write T .n/D2T .n=2/C‚.n/.\\n\\x0fWeassume T .n/DO.1/for sufﬁciently small n.\\n\\x0fWeexpress the solution by asymptotic notation: T .n/D‚.nlgn/.\\n\\x0fWedon’t worryabout boundary cases, nordoweshowbasecases inthesubsti-\\ntution proof.\\n\\x0fT .n/isalways constant for anyconstant n.\\n\\x0fSince we are ultimately interested in an asymptotic solutio n to a recurrence,\\nit will always bepossible tochoose base cases that work.\\n\\x0fWhenwewantanasymptotic solution toarecurrence, wedon’t worryabout\\nthe base cases in our proofs.\\n\\x0fWhen wewant anexact solution, then wehave to deal withbase c ases.\\nForthe substitution method:\\n\\x0fNamethe constant inthe additive term.\\n\\x0fShow the upper ( O) and lower ( \\x7f) bounds separately. Might need to use dif-\\nferent constants for each.\\nExample\\nT .n/D2T .n=2/C‚.n/. If we want to show an upper bound of T .n/D\\n2T .n=2/CO.n/, wewrite T .n/\\x142T .n=2/Ccnfor some positive constant c.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 56}),\n",
              " Document(page_content='4-12 Lecture Notes for Chapter 4: Divide-and-Conquer\\n1.Upper bound:\\nGuess: T .n/\\x14dnlgnfor some positive constant d. We are given cin the\\nrecurrence, and we get to choose das any positive constant. It’s OK for dto\\ndepend on c.\\nSubstitution:\\nT .n/\\x142T .n=2/Ccn\\nD2\\x10\\ndn\\n2lgn\\n2\\x11\\nCcn\\nDdnlgn\\n2Ccn\\nDdnlgn/NULdnCcn\\n\\x14dnlgnif/NULdnCcn\\x140 ;\\nd\\x15c\\nTherefore, T .n/DO.nlgn/.\\n2.Lowerbound: Write T .n/\\x152T .n=2/Ccnfor some positive constant c.\\nGuess: T .n/\\x15dnlgnfor some positive constant d.\\nSubstitution:\\nT .n/\\x152T .n=2/Ccn\\nD2\\x10\\ndn\\n2lgn\\n2\\x11\\nCcn\\nDdnlgn\\n2Ccn\\nDdnlgn/NULdnCcn\\n\\x15dnlgnif/NULdnCcn\\x150 ;\\nd\\x14c\\nTherefore, T .n/D\\x7f.nlgn/.\\nTherefore, T .n/D‚.nlgn/.[Forthisparticularrecurrence, wecanuse dDcfor\\nboththeupper-bound andlower-boundproofs. Thatwon’talw aysbethecase.]\\nMake sure you show the same exactform when doing asubstitution proof.\\nConsider the recurrence\\nT .n/D8T .n=2/C‚.n2/ :\\nFor anupper bound:\\nT .n/\\x148T .n=2/Ccn2:\\nGuess: T .n/\\x14dn3.\\nT .n/\\x148d.n=2/3Ccn2\\nD8d.n3=8/Ccn2\\nDdn3Ccn2\\n6\\x14dn3doesn’t work!\\nRemedy: Subtract off a lower-order term.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 57}),\n",
              " Document(page_content='Lecture Notes for Chapter 4: Divide-and-Conquer 4-13\\nGuess: T .n/\\x14dn3/NULd0n2.\\nT .n/\\x148.d.n=2/3/NULd0.n=2/2/Ccn2\\nD8d.n3=8//NUL8d0.n2=4/Ccn2\\nDdn3/NUL2d0n2Ccn2\\nDdn3/NULd0n2/NULd0n2Ccn2\\n\\x14dn3/NULd0n2if/NULd0n2Ccn2\\x140 ;\\nd0\\x15c\\nBecareful whenusing asymptotic notation.\\nThefalse proof for therecurrence T .n/D4T .n=4/Cn, that T .n/DO.n/:\\nT .n/\\x144.c.n=4//Cn\\n\\x14cnCn\\nDO.n/ wrong!\\nBecause we haven’t proven the exact form of our inductive hypothesis (which is\\nthatT .n/\\x14cn),this proof isfalse.\\nRecursiontrees\\nUsetogenerate aguess. Thenverify by substitution method.\\nExample\\nT .n/DT .n=3/CT .2n=3/C‚.n/.\\nForupper bound, rewriteas T .n/\\x14T .n=3/CT .2n=3/Ccn;for lowerbound, as\\nT .n/\\x15T .n=3/CT .2n=3/Ccn.\\nBy summing across each level, the recursion tree shows the co st at each level of\\nrecursion (minus the costs of recursive calls, which appear insubtrees):\\n…cn cn\\ncn\\ncnc(n/3) c(2n/3)\\nc(n/9)c(2n/9)c(2n/9)c(4n/9)\\nleftmost branch peters\\nout after log3 n levelsrightmost branch peters\\nout after log3/2 n levels\\n\\x0fThere are log3nfull levels, and after log3=2nlevels, the problem size is down\\nto1.\\n\\x0fEach level contributes \\x14cn.\\n\\x0fLower bound guess: \\x15dnlog3nD\\x7f.nlgn/for some positive constant d.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 58}),\n",
              " Document(page_content='4-14 Lecture Notes for Chapter 4: Divide-and-Conquer\\n\\x0fUpper bound guess: \\x14dnlog3=2nDO.nlgn/for somepositive constant d.\\n\\x0fThenproveby substitution.\\n1.Upper bound:\\nGuess: T .n/\\x14dnlgn.\\nSubstitution:\\nT .n/\\x14T .n=3/CT .2n=3/Ccn\\n\\x14d.n=3/lg.n=3/Cd.2n=3/lg.2n=3/Ccn\\nD.d.n=3/lgn/NULd.n=3/lg3/\\nC.d.2n=3/ lgn/NULd.2n=3/lg.3=2//Ccn\\nDdnlgn/NULd..n=3/lg3C.2n=3/lg.3=2//Ccn\\nDdnlgn/NULd..n=3/lg3C.2n=3/lg3/NUL.2n=3/lg2/Ccn\\nDdnlgn/NULdn.lg3/NUL2=3/Ccn\\n\\x14dnlgnif/NULdn.lg3/NUL2=3/Ccn\\x140 ;\\nd\\x15c\\nlg3/NUL2=3:\\nTherefore, T .n/DO.nlgn/.\\nNote:Makesurethatthesymbolicconstantsusedintherecurrence (e.g., c)and\\nthe guess (e.g., d) are different.\\n2.Lowerbound:\\nGuess: T .n/\\x15dnlgn.\\nSubstitution: Same as for the upper bound, but replacing \\x14by\\x15. End up\\nneeding\\n0 < d\\x14c\\nlg3/NUL2=3:\\nTherefore, T .n/D\\x7f.nlgn/.\\nSince T .n/DO.nlgn/andT .n/D\\x7f.nlgn/, we conclude that T .n/D\\n‚.nlgn/.\\nMaster method\\nUsed for manydivide-and-conquer recurrences of the form\\nT .n/DaT .n=b/Cf .n/ ;\\nwhere a\\x151,b > 1, and f .n/ > 0 .\\nBased on the master theorem (Theorem 4.1).\\nCompare nlogbavs.f .n/:\\nCase 1: f .n/DO.nlogba/NUL\\x0f/for someconstant \\x0f > 0.\\n(f .n/ispolynomially smaller than nlogba.)\\nSolution: T .n/D‚.nlogba/.\\n(Intuitively: cost isdominated byleaves.)', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 59}),\n",
              " Document(page_content='Lecture Notes for Chapter 4: Divide-and-Conquer 4-15\\nCase2: f .n/D‚.nlogbalgkn/,where k\\x150.\\n[ThisformulationofCase2ismoregeneral thaninTheorem4. 1,anditisgiven\\nin Exercise 4.6-2.]\\n(f .n/iswithin apolylog factor of nlogba, but not smaller.)\\nSolution: T .n/D‚.nlogbalgkC1n/.\\n(Intuitively: cost is nlogbalgknat each level, and there are ‚.lgn/levels.)\\nSimple case: kD0)f .n/D‚.nlogba/)T .n/D‚.nlogbalgn/.\\nCase3: f .n/D\\x7f.nlogbaC\\x0f/for someconstant \\x0f > 0andf .n/satisﬁes theregu-\\nlarity condition af .n=b/\\x14cf .n/for some constant c < 1and all sufﬁciently\\nlarge n.\\n(f .n/ispolynomially greater than nlogba.)\\nSolution: T .n/D‚.f .n//.\\n(Intuitively: cost is dominated by root.)\\nWhat’swith theCase3 regularity condition?\\n\\x0fGenerally not aproblem.\\n\\x0fIt always holds whenever f .n/Dnkandf .n/D\\x7f.nlogbaC\\x0f/for constant\\n\\x0f > 0.[Proving this makes a nice homework exercise. See below.] So you\\ndon’t need tocheck it when f .n/is apolynomial.\\n[Here’s a proof that the regularity condition holds when f .n/Dnkandf .n/D\\n\\x7f.nlogbaC\\x0f/for constant \\x0f > 0.\\nSince f .n/D\\x7f.nlogbaC\\x0f/andf .n/Dnk, we have that k >logba. Using a\\nbase of band treating both sides as exponents, we have bk> blogbaDa, and so\\na=bk< 1. Since a,b,and kare constants, if welet cDa=bk, then cisa constant\\nstrictly less than 1. We have that af .n=b/Da.n=b/kD.a=bk/nkDcf .n/, and\\nsothe regularity condition issatisﬁed.]\\nExamples\\n\\x0fT .n/D5T .n=2/C‚.n2/\\nnlog25vs.n2\\nSince log25/NUL\\x0fD2for some constant \\x0f > 0, use Case1)T .n/D‚.nlg5/\\n\\x0fT .n/D27T .n=3/C‚.n3lgn/\\nnlog327Dn3vs.n3lgn\\nUseCase 2with kD1)T .n/D‚.n3lg2n/\\n\\x0fT .n/D5T .n=2/C‚.n3/\\nnlog25vs.n3\\nNowlg 5C\\x0fD3for some constant \\x0f > 0\\nCheck regularity condition (don’t really need to since f .n/is a polynomial):\\naf .n=b/D5.n=2/3D5n3=8\\x14cn3forcD5=8 < 1\\nUseCase 3)T .n/D‚.n3/\\n\\x0fT .n/D27T .n=3/C‚.n3=lgn/\\nnlog327Dn3vs.n3=lgnDn3lg/NUL1n¤‚.n3lgkn/for any k\\x150.\\nCannot use themaster method.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 60}),\n",
              " Document(page_content='4-16 Lecture Notes for Chapter 4: Divide-and-Conquer\\n[Wedon’tprovethemastertheoreminouralgorithmscourse. Wesometimesprove\\nasimpliﬁedversionforrecurrencesoftheform T .n/DaT .n=b/Cnc. Section4.6\\nof the text has the full proof of the master theorem.]', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 61}),\n",
              " Document(page_content='SolutionsforChapter 4:\\nDivide-and-Conquer\\nSolutionto Exercise 4.1-1\\nIf the index of thegreatest element of Aisi,it returns .i; i; AŒi\\x8d/ .\\nSolutionto Exercise 4.1-2\\nMAX-SUBARRAY -BRUTE-FORCE .A/\\nnDA:length\\nmax-so-farD/NUL1\\nforlD1ton\\nsumD0\\nforhDlton\\nsumDsumCAŒh\\x8d\\nifsum>max-so-far\\nmax-so-farDsum\\nlowDl\\nhighDh\\nreturn .low;high/\\nSolutionto Exercise 4.1-4\\nIf the algorithm returns a negative sum, toss out the answer a nd use an empty\\nsubarray instead.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 62}),\n",
              " Document(page_content='4-18 Solutions for Chapter 4: Divide-and-Conquer\\nSolution to Exercise4.1-5\\nMAX-SUBARRAY -LINEAR .A/\\nnDA:length\\nmax-sumD/NUL1\\nending-here-sumD/NUL1\\nforjD1ton\\nending-here-highDj\\nifending-here-sum> 0\\nending-here-sumDending-here-sumCAŒj \\x8d\\nelseending-here-lowDj\\nending-here-sumDAŒj \\x8d\\nifending-here-sum>max-sum\\nmax-sumDending-here-sum\\nlowDending-here-low\\nhighDending-here-high\\nreturn .low;high;max-sum/\\nThevariables are intended asfollows:\\n\\x0flowandhighdemarcate amaximum subarray found sofar.\\n\\x0fmax-sumgives the sum of thevalues in amaximum subarray found sofar.\\n\\x0fending-here-lowandending-here-highdemarcate a maximum subarray ending\\nat index j. Since the high end of any subarray ending at index jmust be j,\\nevery iteration of the forloop automatically sets ending-here-highDj.\\n\\x0fending-here-sumgives the sum of the values in a maximum subarray ending at\\nindex j.\\nThe ﬁrst test within the forloop determines whether a maximum subarray\\nending at index jcontains just AŒj \\x8d. As we enter an iteration of the loop,\\nending-here-sumhasthesumofthevaluesinamaximumsubarrayendingat j/NUL1.\\nIfending-here-sumCAŒj \\x8d > AŒj \\x8d , then we extend the maximum subarray end-\\ning at index j/NUL1to include index j. (The test in the ifstatement just subtracts\\noutAŒj \\x8dfrom both sides.) Otherwise, we start a new subarray at index j, so both\\nits low and high ends have the value jand its sum is AŒj \\x8d. Once we know the\\nmaximum subarray ending at index j, we test to see whether it has a greater sum\\nthan the maximum subarray found so far, ending at any positio n less than or equal\\ntoj. If it does, then weupdate low,high,andmax-sumappropriately.\\nSince each iteration of the forloop takes constant time, and the loop makes n\\niterations, the running time of M AX-SUBARRAY -LINEARis‚.n/.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 63}),\n",
              " Document(page_content='Solutions forChapter 4: Divide-and-Conquer 4-19\\nSolutionto Exercise 4.2-2\\nSTRASSEN .A; B/\\nnDA:rows\\nletCbe anew n\\x02nmatrix\\nifn==1\\nc11Da11\\x01b11\\nelsepartition AandBin equations (4.9)\\nletC11,C12,C21, and C22ben=2\\x02n=2matrices\\ncreate n=2\\x02n=2matrices S1; S2; : : : ; S 10andP1; P2; : : : ; P 7\\nS1DB12/NULB22\\nS2DA11CA12\\nS3DA12CA22\\nS4DB21/NULB11\\nS5DA11CA22\\nS6DB11CB22\\nS7DA12/NULA22\\nS8DB21CB22\\nS9DA11/NULA21\\nS10DB11CB12\\nP1DSTRASSEN .A11; S1/\\nP2DSTRASSEN .S2; B22/\\nP3DSTRASSEN .S3; B11/\\nP4DSTRASSEN .A22; S4/\\nP5DSTRASSEN .S5; S6/\\nP6DSTRASSEN .S7; S8/\\nP7DSTRASSEN .S9; S10/\\nC11DP5CP4/NULP2CP6\\nC12DP1CP2\\nC21DP3CP4\\nC22DP5CP1/NULP3/NULP7\\ncombine C11,C12,C21, and C22intoC\\nreturn C\\nSolutionto Exercise 4.2-4\\nThissolutionisalsopostedpublicly\\nIf you can multiply 3\\x023matrices using kmultiplications, then you can multiply\\nn\\x02nmatrices by recursively multiplying n=3\\x02n=3matrices, in time T .n/D\\nkT .n=3/C‚.n2/.\\nUsing the master method to solve this recurrence, consider t he ratio of nlog3k\\nandn2:\\n\\x0fIf log3kD2, case 2 applies and T .n/D‚.n2lgn/. In this case, kD9and\\nT .n/Do.nlg7/.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 64}),\n",
              " Document(page_content='4-20 Solutions for Chapter 4: Divide-and-Conquer\\n\\x0fIf log3k < 2, case 3 applies and T .n/D‚.n2/. In this case, k < 9and\\nT .n/Do.nlg7/.\\n\\x0fIf log3k > 2, case 1 applies and T .n/D‚.nlog3k/. In this case, k > 9.\\nT .n/Do.nlg7/when log3k <lg7, i.e., when k < 3lg7\\x1921:85. The largest\\nsuch integer kis21.\\nThus, kD21and the running time is ‚.nlog3k/D‚.nlog321/DO.n2:80/(since\\nlog321\\x192:77).\\nSolution to Exercise4.3-1\\nWeguess that T .n/\\x14cn2for some constant c > 0. Wehave\\nT .n/DT .n/NUL1/Cn\\n\\x14c.n/NUL1/2Cn\\nDcn2/NUL2cnCcCn\\nDcn2Cc.1/NUL2n/Cn :\\nThislast quantity islessthanorequal to cn2ifc.1/NUL2n/Cn\\x140or,equivalently,\\nc\\x15n=.2n/NUL1/. Thislast condition holds for all n\\x151andc\\x151.\\nFor the boundary condition, weset T .1/D1, and so T .1/D1\\x14c\\x0112. Thus, we\\ncan choose n0D1andcD1.\\nSolution to Exercise4.3-7\\nIf we were to try a straight substitution proof, assuming tha tT .n/\\x14cnlog34, we\\nwould get stuck:\\nT .n/\\x144.c.n=3/log34/Cn\\nD4c\\x12nlog34\\n4\\x13\\nCn\\nDcnlog34Cn ;\\nwhich is greater than cnlog34. Instead, we subtract off a lower-order term and as-\\nsume that T .n/\\x14cnlog34/NULdn. Nowwehave\\nT .n/\\x144.c.n=3/log34/NULdn=3/Cn\\nD4\\x12cnlog34\\n4/NULdn\\n3\\x13\\nCn\\nDcnlog34/NUL4\\n3dnCn ;\\nwhich is less than or equal to cnlog34/NULdnifd\\x153.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 65}),\n",
              " Document(page_content='Solutions forChapter 4: Divide-and-Conquer 4-21\\nSolutionto Exercise 4.4-6\\nThissolutionisalsopostedpublicly\\nThe shortest path from the root to a leaf in the recursion tree isn!.1=3/n!\\n.1=3/2n!\\x01\\x01\\x01! 1. Since .1=3/knD1when kDlog3n, the height of the part\\nof thetree inwhich everynode hastwochildren islog3n. Sincethe values at each\\nof these levels of the tree add up to cn, the solution to the recurrence is at least\\ncnlog3nD\\x7f.nlgn/.\\nSolutionto Exercise 4.4-9\\nThissolutionisalsopostedpublicly\\nT .n/DT .˛n/CT ..1/NUL˛/n/Ccn\\nWesawthesolution totherecurrence T .n/DT .n=3/CT .2n=3/Ccninthetext.\\nThisrecurrence can be similarly solved.\\nWithoutlossofgenerality,let ˛\\x151/NUL˛,sothat 0 < 1/NUL˛\\x141=2and1=2\\x14˛ < 1.\\n……log1=.1/NUL˛/n log1=˛ncn\\ncncncn\\nTotal: O.nlgn/c˛n c.1 /NUL˛/n\\nc˛2n c˛.1 /NUL˛/n c˛.1/NUL˛/n c.1 /NUL˛/2n\\nThe recursion tree is full for log1=.1/NUL˛/nlevels, each contributing cn, so we guess\\n\\x7f.nlog1=.1/NUL˛/n/D\\x7f.nlgn/. It has log1=˛nlevels, each contributing \\x14cn, so\\nweguess O.nlog1=˛n/DO.nlgn/.\\nNow we show that T .n/D‚.nlgn/by substitution. To prove the upper bound,\\nweneed toshow that T .n/\\x14dnlgnfor asuitable constant d > 0.\\nT .n/DT .˛n/CT ..1/NUL˛/n/Ccn\\n\\x14d˛nlg.˛n/Cd.1/NUL˛/nlg..1/NUL˛/n/Ccn\\nDd˛nlg˛Cd˛nlgnCd.1/NUL˛/nlg.1/NUL˛/Cd.1/NUL˛/nlgnCcn\\nDdnlgnCdn.˛lg˛C.1/NUL˛/lg.1/NUL˛//Ccn\\n\\x14dnlgn ;\\nifdn.˛lg˛C.1/NUL˛/lg.1/NUL˛//Ccn\\x140. This condition isequivalent to\\nd.˛lg˛C.1/NUL˛/lg.1/NUL˛//\\x14/NULc :', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 66}),\n",
              " Document(page_content='4-22 Solutions for Chapter 4: Divide-and-Conquer\\nSince 1=2\\x14˛ < 1and0 < 1/NUL˛\\x141=2,wehavethatlg ˛ < 0andlg .1/NUL˛/ < 0.\\nThus, ˛lg˛C.1/NUL˛/lg.1/NUL˛/ < 0, so that when we multiply both sides of the\\ninequality bythis factor, weneed to reverse the inequality :\\nd\\x15/NULc\\n˛lg˛C.1/NUL˛/lg.1/NUL˛/\\nor\\nd\\x15c\\n/NUL˛lg˛C/NUL.1/NUL˛/lg.1/NUL˛/:\\nThe fraction on the right-hand side is a positive constant, a nd so it sufﬁces to pick\\nany value of dthat isgreater than or equal to this fraction.\\nTo prove the lower bound, we need to show that T .n/\\x15dnlgnfor a suitable\\nconstant d > 0. Wecan use the same proof as for the upper bound, substitutin g\\x15\\nfor\\x14,and weget therequirement that\\n0 < d\\x14c\\n/NUL˛lg˛/NUL.1/NUL˛/lg.1/NUL˛/:\\nTherefore, T .n/D‚.nlgn/.\\nSolution to Exercise4.5-2\\nWeneedtoﬁndthelargestinteger asuchthatlog4a <lg7. Theansweris aD48.\\nSolution to Problem 4-1\\nNote: Inparts(a),(b),and(d)below,weareapplyingcase3o fthemastertheorem,\\nwhich requires the regularity condition that af .n=b/\\x14cf .n/for some constant\\nc < 1. In each of these parts, f .n/has the form nk. The regularity condition is\\nsatisﬁed because af .n=b/Dank=bkD.a=bk/nkD.a=bk/f .n/, and in each of\\nthe cases below, a=bkisaconstant strictly less than 1.\\na.T .n/D2T .n=2/Cn3D‚.n3/. Thisisadivide-and-conquer recurrence with\\naD2,bD2,f .n/Dn3, and nlogbaDnlog22Dn. Since n3D\\x7f.nlog22C2/\\nanda=bkD2=23D1=4 < 1 , case 3 of the master theorem applies, and\\nT .n/D‚.n3/.\\nb.T .n/DT .9n=10/CnD‚.n/. This is a divide-and-conquer recurrence with\\naD1,bD10=9,f .n/Dn, and nlogbaDnlog10=9 1Dn0D1. Since\\nnD\\x7f.nlog10=9 1C1/anda=bkD1=.10=9/1D9=10 < 1 , case 3 of the master\\ntheorem applies, and T .n/D‚.n/.\\nc.T .n/D16T .n=4/Cn2D‚.n2lgn/. This is another divide-and-conquer\\nrecurrence with aD16,bD4,f .n/Dn2, and nlogbaDnlog416Dn2. Since\\nn2D‚.nlog416/,case2ofthemastertheorem applies, and T .n/D‚.n2lgn/.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 67}),\n",
              " Document(page_content='Solutions forChapter 4: Divide-and-Conquer 4-23\\nd.T .n/D7T .n=3/Cn2D‚.n2/. Thisisadivide-and-conquer recurrence with\\naD7,bD3,f .n/Dn2, and nlogbaDnlog37. Since 1 <log37 < 2, wehave\\nthatn2D\\x7f.nlog37C\\x0f/for some constant \\x0f > 0. We also have a=bkD7=32D\\n7=9 < 1, sothat case 3 of the master theorem applies, and T .n/D‚.n2/.\\ne.T .n/D7T .n=2/Cn2DO.nlg7/. This is a divide-and-conquer recurrence\\nwithaD7,bD2,f .n/Dn2, and nlogbaDnlog27. Since 2 <lg7 < 3, we\\nhavethat n2DO.nlog27/NUL\\x0f/forsomeconstant \\x0f > 0. Thus,case1ofthemaster\\ntheorem applies, and T .n/D‚.nlg7/.\\nf.T .n/D2T .n=4/CpnD‚.pnlgn/. This is another divide-and-conquer\\nrecurrence with aD2,bD4,f .n/Dpn, and nlogbaDnlog42Dpn.\\nSincepnD‚.nlog42/, case 2 of the master theorem applies, and T .n/D\\n‚.pnlgn/.\\ng.T .n/DT .n/NUL1/Cn\\nUsing the recursion tree shown below, weget aguess of T .n/D‚.n2/.\\nn-1\\nn-2n\\n1n\\nn-1\\nn-2\\n12n\\n2:::\\n‚.n2/\\nFirst, weprove the T .n/D\\x7f.n2/part by induction. The inductive hypothesis\\nisT .n/\\x15cn2for someconstant c > 0.\\nT .n/DT .n/NUL1/Cn\\n\\x15c.n/NUL1/2Cn\\nDcn2/NUL2cnCcCn\\n\\x15cn2\\nif/NUL2cnCnCc\\x150or, equivalently, n.1/NUL2c/Cc\\x150. Thiscondition holds\\nwhen n\\x150and0 < c\\x141=2.\\nFor the upper bound, T .n/DO.n2/, we use the inductive hypothesis that\\nT .n/\\x14cn2for some constant c > 0. By a similar derivation, we get that\\nT .n/\\x14cn2if/NUL2cnCnCc\\x140or, equivalently, n.1/NUL2c/Cc\\x140. This\\ncondition holds for cD1andn\\x151.\\nThus, T .n/D\\x7f.n2/andT .n/DO.n2/,soweconclude that T .n/D‚.n2/.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 68}),\n",
              " Document(page_content='4-24 Solutions for Chapter 4: Divide-and-Conquer\\nh.T .n/DT .pn/C1\\nThe easy way to do this is with a change of variables, as on page 86 of\\nthe text. Let mDlgnandS.m/DT .2m/.T .2m/DT .2m=2/C1, so\\nS.m/DS.m=2/C1. Using the master theorem, nlogbaDnlog21Dn0D1\\nandf .n/D1. Since 1D‚.1/, case 2 applies and S.m/D‚.lgm/. There-\\nfore, T .n/D‚.lglgn/.\\nSolution to Problem 4-3\\n[This problem issolved only for parts a, c, e, f,g, h, and i.]\\na.T .n/D3T .n=2/Cnlgn\\nWe have f .n/DnlgnandnlogbaDnlg3\\x19n1:585. Since nlgnDO.nlg3/NUL\\x0f/\\nfor any 0 < \\x0f\\x140:58, by case 1 of the master theorem, we have T .n/D\\n‚.nlg3/.\\nc.T .n/D4T .n=2/Cn2pn\\nWe have f .n/Dn2pnDn5=2andnlogbaDnlog24Dn2. Since n5=2D\\n\\x7f.n2C\\x0f/for\\x0fD1=2, we look at the regularity condition in case 3 of the\\nmaster theorem. We have af .n=b/D4.n=2/2p\\nn=2Dn5=2=p\\n2\\x14cn5=2for\\n1=p\\n2\\x14c < 1. Case 3applies, and wehave T .n/D‚.n2pn/.\\ne.T .n/D2T .n=2/Cn=lgn\\nWecan get aguess by means of arecursion tree:……lgnn\\nlgnn\\nlgn\\nn=2\\nlg.n=2/n=2\\nlg.n=2/\\nn=4\\nlg.n=4/n=4\\nlg.n=4/n=4\\nlg.n=4/n=4\\nlg.n=4/n\\nlgn/NUL1\\nn\\nlgn/NUL2\\nlgn/NUL1X\\niD0n\\nlgn/NULiD‚.nlglgn/\\nWe get the sum on each level by observing that at depth i, we have 2inodes,\\neach with a numerator of n=2iand a denominator of lg .n=2i/Dlgn/NULi, so\\nthat the cost at depth iis\\n2i\\x01n=2i\\nlgn/NULiDn\\nlgn/NULi:', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 69}),\n",
              " Document(page_content='Solutions forChapter 4: Divide-and-Conquer 4-25\\nThesum for all levels is\\nlgn/NUL1X\\niD0n\\nlgn/NULiDnlgnX\\niD1n\\ni\\nDnlgnX\\niD11=i\\nDn\\x01‚.lglgn/(by equation (A.7), the harmonic series)\\nD‚.nlglgn/ :\\nWe can use this analysis as a guess that T .n/D‚.nlglgn/. If we were to do\\nastraight substitution proof, it wouldbe rather involved. Instead, wewill show\\nby substitution that T .n/\\x14n.1CHblgnc/andT .n/\\x15n\\x01Hdlgne, where Hk\\nis the kth harmonic number: HkD1=1C1=2C1=3C\\x01\\x01\\x01C 1=k. We also\\ndeﬁne H0D0. Since HkD‚.lgk/, we have that HblgncD‚.lgblgnc/D\\n‚.lglgn/andHdlgneD‚.lgdlgne/D‚.lglgn/. Thus, we will have that\\nT .n/D‚.nlglgn/.\\nThebasecase for theproof isfor nD1, andweuse T .1/D1. Here, lg nD0,\\nsothat lg nDblgncDdlgne. Since H0D0,wehave T .1/D1\\x141.1CH0/\\nandT .1/D1\\x150D1\\x01H0.\\nFor the upper bound of T .n/\\x14n.1CHblgnc/, wehave\\nT .n/D2T .n=2/Cn=lgn\\n\\x142..n=2/.1CHblg.n=2/ c//Cn=lgn\\nDn.1CHblgn/NUL1c/Cn=lgn\\nDn.1CHblgnc/NUL1C1=lgn/\\n\\x14n.1CHblgnc/NUL1C1=blgnc/\\nDn.1CHblgnc/ ;\\nwhere the last line follows from theidentity HkDHk/NUL1C1=k.\\nTheupper bound of T .n/\\x15n\\x01Hdlgneissimilar:\\nT .n/D2T .n=2/Cn=lgn\\n\\x152..n=2/\\x01Hdlg.n=2/ e/Cn=lgn\\nDn\\x01Hdlgn/NUL1eCn=lgn\\nDn\\x01.Hdlgne/NUL1C1=lgn/\\n\\x15n\\x01.Hdlgne/NUL1C1=dlgne/\\nDn\\x01Hdlgne:\\nThus, T .n/D‚.nlglgn/.\\nf.T .n/DT .n=2/CT .n=4/CT .n=8/Cn\\nUsing the recursion tree shown below, weget aguess of T .n/D‚.n/.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 70}),\n",
              " Document(page_content='4-26 Solutions for Chapter 4: Divide-and-Conquer\\nn n\\nlog4nn\\n2\\nn\\n4n\\n4\\nn\\n8n\\n8n\\n8\\nn\\n16n\\n16n\\n16n\\n32n\\n32n\\n64log8n\\n:::n.4C2C1\\n8/D7\\n8n\\nn.1\\n4C2\\n8C3\\n16C2\\n32C1\\n64/\\nDn16C16C12C4C1\\n64\\nDn49\\n64D7\\n82n\\nlognX\\niD1\\x127\\n8\\x13i\\nnD‚.n/\\nWe use the substitution method to prove that T .n/DO.n/. Our inductive\\nhypothesis isthat T .n/\\x14cnfor some constant c > 0. Wehave\\nT .n/DT .n=2/CT .n=4/CT .n=8/Cn\\n\\x14cn=2Ccn=4Ccn=8Cn\\nD7cn=8Cn\\nD.1C7c=8/n\\n\\x14cnifc\\x158 :\\nTherefore, T .n/DO.n/.\\nShowing that T .n/D\\x7f.n/is easy:\\nT .n/DT .n=2/CT .n=4/CT .n=8/Cn\\x15n :\\nSince T .n/DO.n/andT .n/D\\x7f.n/, wehave that T .n/D‚.n/.\\ng.T .n/DT .n/NUL1/C1=n\\nThis recurrence corresponds to the harmonic series, so that T .n/DHn, where\\nHnD1=1C1=2C1=3C\\x01\\x01\\x01C 1=n. Forthebasecase,wehave T .1/D1DH1.\\nForthe inductive step, weassume that T .n/NUL1/DHn/NUL1, and wehave\\nT .n/DT .n/NUL1/C1=n\\nDHn/NUL1C1=n\\nDHn:\\nSince HnD‚.lgn/by equation (A.7), wehave that T .n/D‚.lgn/.\\nh.T .n/DT .n/NUL1/Clgn\\nWeguess that T .n/D‚.nlgn/. Toprove the upper bound, wewill show that\\nT .n/DO.nlgn/. Our inductive hypothesis is that T .n/\\x14cnlgnfor some\\nconstant c. Wehave', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 71}),\n",
              " Document(page_content='Solutions forChapter 4: Divide-and-Conquer 4-27\\nT .n/DT .n/NUL1/Clgn\\n\\x14c.n/NUL1/lg.n/NUL1/Clgn\\nDcnlg.n/NUL1//NULclg.n/NUL1/Clgn\\n\\x14cnlg.n/NUL1//NULclg.n=2/Clgn\\n(since lg .n/NUL1/\\x15lg.n=2/forn\\x152)\\nDcnlg.n/NUL1//NULclgnCcClgn\\n< cnlgn/NULclgnCcClgn\\n\\x14cnlgn ;\\nif/NULclgnCcClgn\\x140. Equivalently,\\n/NULclgnCcClgn\\x140\\nc\\x14.c/NUL1/lgn\\nlgn\\x15c=.c/NUL1/ :\\nThis works for cD2and all n\\x154.\\nTo prove the lower bound, we will show that T .n/D\\x7f.nlgn/. Our inductive\\nhypothesis is that T .n/\\x15cnlgnCdnfor constants candd. Wehave\\nT .n/DT .n/NUL1/Clgn\\n\\x15c.n/NUL1/lg.n/NUL1/Cd.n/NUL1/Clgn\\nDcnlg.n/NUL1//NULclg.n/NUL1/Cdn/NULdClgn\\n\\x15cnlg.n=2//NULclg.n/NUL1/Cdn/NULdClgn\\n(since lg .n/NUL1/\\x15lg.n=2/forn\\x152)\\nDcnlgn/NULcn/NULclg.n/NUL1/Cdn/NULdClgn\\n\\x15cnlgn ;\\nif/NULcn/NULclg.n/NUL1/Cdn/NULdClgn\\x150. Since\\n/NULcn/NULclg.n/NUL1/Cdn/NULdClgn >\\n/NULcn/NULclg.n/NUL1/Cdn/NULdClg.n/NUL1/ ;\\nitsufﬁcestoﬁndconditionsinwhich /NULcn/NULclg.n/NUL1/Cdn/NULdClg.n/NUL1/\\x150.\\nEquivalently,\\n/NULcn/NULclg.n/NUL1/Cdn/NULdClg.n/NUL1/\\x150\\n.d/NULc/n\\x15.c/NUL1/lg.n/NUL1/Cd :\\nThis works for cD1,dD2, and all n\\x152.\\nSince T .n/DO.nlgn/andT .n/D\\x7f.nlgn/, we conclude that T .n/D\\n‚.nlgn/.\\ni.T .n/DT .n/NUL2/C2lgn\\nWe guess that T .n/D‚.nlgn/. We show the upper bound of T .n/D\\nO.nlgn/by means of the inductive hypothesis T .n/\\x14cnlgnfor some con-\\nstant c > 0. Wehave\\nT .n/DT .n/NUL2/C2lgn\\n\\x14c.n/NUL2/lg.n/NUL2/C2lgn\\n\\x14c.n/NUL2/lgnC2lgn\\nD.cn/NUL2cC2/lgn', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 72}),\n",
              " Document(page_content='4-28 Solutions for Chapter 4: Divide-and-Conquer\\nDcnlgnC.2/NUL2c/lgn\\n\\x14cnlgnifc > 1 :\\nTherefore, T .n/DO.nlgn/.\\nForthelowerboundof T .n/D\\x7f.nlgn/,we’llshowthat T .n/\\x15cnlgnCdn,\\nfor constants c; d > 0tobe chosen. Weassume that n\\x154, which implies that\\n1. lg.n/NUL2/\\x15lg.n=2/,\\n2.n=2\\x15lgn,and\\n3.n=2\\x152.\\n(We’ll use these inequalities aswego along.) Wehave\\nT .n/\\x15c.n/NUL2/lg.n/NUL2/Cd.n/NUL2/C2lgn\\nDcnlg.n/NUL2//NUL2clg.n/NUL2/Cdn/NUL2dC2lgn\\n> cnlg.n/NUL2//NUL2clgnCdn/NUL2dC2lgn\\n(since/NULlgn </NULlg.n/NUL2/)\\nDcnlg.n/NUL2//NUL2.c/NUL1/lgnCdn/NUL2d\\n\\x15cnlg.n=2//NUL2.c/NUL1/lgnCdn/NUL2d(by inequality (1) above)\\nDcnlgn/NULcn/NUL2.c/NUL1/lgnCdn/NUL2d\\n\\x15cnlgn ;\\nif/NULcn/NUL2.c/NUL1/lgnCdn/NUL2d\\x150or, equivalently, dn\\x15cnC2.c/NUL\\n1/lgnC2d. Pick any constant c > 1=2, and then pick any constant dsuch\\nthat\\nd\\x152.2c/NUL1/ :\\n(Therequirement that c > 1=2means that dispositive.) Then\\nd=2\\x152c/NUL1DcC.c/NUL1/ ;\\nand adding d=2toboth sides, wehave\\nd\\x15cC.c/NUL1/Cd=2 :\\nMultiplying by nyields\\ndn\\x15cnC.c/NUL1/nCdn=2 ;\\nand then both multiplying and dividing the middle term by 2gives\\ndn\\x15cnC2.c/NUL1/n=2Cdn=2 :\\nUsing inequalities (2) and (3) above, weget\\ndn\\x15cnC2.c/NUL1/lgnC2d ;\\nwhich is what we needed to show. Thus T .n/D\\x7f.nlgn/. Since T .n/D\\nO.nlgn/andT .n/D\\x7f.nlgn/,weconclude that T .n/D‚.nlgn/.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 73}),\n",
              " Document(page_content='Lecture Notes forChapter 5:\\nProbabilisticAnalysisandRandomized\\nAlgorithms\\n[This chapter introduces probabilistic analysis and rando mized algorithms. It as-\\nsumesthatthestudentisfamiliarwiththebasicprobabilit ymaterialinAppendixC.\\nTheprimary goals of these notes areto\\n\\x0fexplain the difference between probabilistic analysis and randomized algo-\\nrithms,\\n\\x0fpresent thetechnique of indicator random variables, and\\n\\x0fgive another example of the analysis of a randomized algorit hm (permuting an\\narray inplace).\\nThesenotesomitthetechnique ofpermuting anarraybysorti ng, andtheyomitthe\\nstarred Section 5.4.]\\nThe hiring problem\\nScenario\\n\\x0fYou areusing an employment agency tohire anew ofﬁce assista nt.\\n\\x0fTheagency sends you one candidate each day.\\n\\x0fYou interview the candidate and must immediately decide whe ther or not to\\nhire that person. But if you hire, you must also ﬁre your curre nt ofﬁce assis-\\ntant—even if it’ssomeone you have recently hired.\\n\\x0fCost tointerview is ciper candidate (interview fee paid to agency).\\n\\x0fCost to hire is chper candidate (includes cost to ﬁre current ofﬁce assistant +\\nhiring fee paid to agency).\\n\\x0fAssume that ch> c i.\\n\\x0fYou are committed to having hired, at all times, the best cand idate seen so\\nfar. Meaning that whenever you interview a candidate who is b etter than your\\ncurrent ofﬁce assistant, you must ﬁre the current ofﬁce assi stant and hire the\\ncandidate. Since you must have someone hired at all times, yo u will always\\nhire the ﬁrst candidate that you interview.\\nGoal\\nDetermine what theprice of this strategy will be.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 74}),\n",
              " Document(page_content='5-2 Lecture Notes for Chapter 5: Probabilistic Analysis and Randomized Algorithms\\nPseudocode to model thisscenario\\nAssumes that the candidates are numbered 1tonand that after interviewing each\\ncandidate, we can determine if it’s better than the current o fﬁce assistant. Uses a\\ndummycandidate 0thatisworsethanallothers,sothattheﬁrstcandidateisal ways\\nhired.\\nHIRE-ASSISTANT .n/\\nbestD0//candidate 0 isaleast-qualiﬁed dummycandidate\\nforiD1ton\\ninterview candidate i\\nifcandidate iisbetter than candidate best\\nbestDi\\nhire candidate i\\nCost\\nIfncandidates, and wehire mof them, the cost is O.nc iCmch/.\\n\\x0fHaveto pay ncito interview, no matter how manywehire.\\n\\x0fSowefocus on analyzing the hiring cost mch.\\n\\x0fmchvaries with each run—it depends on the order in which we inter view the\\ncandidates.\\n\\x0fThis is a model of a common paradigm: we need to ﬁnd the maximum or\\nminimum in a sequence by examining each element and maintain ing a current\\n“winner.” The variable mdenotes how many times we change our notion of\\nwhich element is currently winning.\\nWorst-case analysis\\nIn the worst case, wehire all ncandidates.\\nThis happens if each one is better than all who came before. In other words, if the\\ncandidates appear inincreasing order of quality.\\nIf wehire all n,then thecost is O.nc iCnch/DO.nc h/(since ch> c i).\\nProbabilistic analysis\\nIn general, wehave nocontrol over the order in which candida tes appear.\\nWecould assume that they come inarandom order:\\n\\x0fAssignarank toeachcandidate: rank.i/isaunique integer intherange 1ton.\\n\\x0fThe ordered listhrank.1/;rank.2/; : : : ;rank.n/iis a permutation of the candi-\\ndate numbersh1; 2; : : : ; ni.\\n\\x0fThelist of ranks isequally likely tobe anyone of the nŠpermutations.\\n\\x0fEquivalently, the ranks form a uniform random permutation : each of the pos-\\nsible nŠpermutations appears with equal probability.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 75}),\n",
              " Document(page_content='Lecture Notes for Chapter 5: ProbabilisticAnalysis and Ran domized Algorithms 5-3\\nEssential idea of probabilistic analysis\\nWemust use knowledge of, or make assumptions about, the dist ribution of inputs.\\n\\x0fTheexpectation isover this distribution.\\n\\x0fThis technique requires that we can make a reasonable charac terization of the\\ninput distribution.\\nRandomizedalgorithms\\nWe might not know the distribution of inputs, or we might not b e able to model it\\ncomputationally.\\nInstead, we use randomization within the algorithm in order to impose a distribu-\\ntion on theinputs.\\nForthehiring problem\\nChange the scenario:\\n\\x0fTheemployment agency sends us alist of all ncandidates inadvance.\\n\\x0fOn each day, we randomly choose a candidate from the list to in terview (but\\nconsidering only those wehave not yet interviewed).\\n\\x0fInstead of relying on the candidates being presented to us in a random order,\\nwetake control of the process and enforce arandom order.\\nWhat makes an algorithm randomized\\nAn algorithm is randomized if its behavior is determined in part by values pro-\\nduced bya random-number generator .\\n\\x0fRANDOM .a; b/returnsaninteger r,where a\\x14r\\x14bandeachofthe b/NULaC1\\npossible values of risequally likely.\\n\\x0fIn practice, R ANDOMisimplemented by a pseudorandom-number generator ,\\nwhichisadeterministicmethodreturningnumbersthat“loo k”randomandpass\\nstatistical tests.\\nIndicatorrandom variables\\nA simple yet powerful technique for computing the expected v alue of a random\\nvariable.\\nHelpful in situations inwhich there maybe dependence.\\nGivenasample space and an event A,wedeﬁne the indicator random variable\\nIfAgD(\\n1ifAoccurs ;\\n0ifAdoes not occur :\\nLemma\\nForan event A,letXADIfAg. Then E ŒXA\\x8dDPrfAg.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 76}),\n",
              " Document(page_content='5-4 Lecture Notes for Chapter 5: Probabilistic Analysis and Randomized Algorithms\\nProofLetting Abethe complement of A,wehave\\nEŒXA\\x8dDEŒIfAg\\x8d\\nD1\\x01PrfAgC0\\x01Pr˚\\nA/TAB\\n(deﬁnition of expected value)\\nDPrfAg: (lemma)\\nSimple example\\nDetermine the expected number of heads whenweﬂipafair coin one time.\\n\\x0fSamplespace isfH; Tg.\\n\\x0fPrfHgDPrfTgD1=2.\\n\\x0fDeﬁneindicator randomvariable XHDIfHg.XHcountsthenumberofheads\\ninone ﬂip.\\n\\x0fSince PrfHgD1=2, lemmasays that E ŒXH\\x8dD1=2.\\nSlightly more complicated example\\nDetermine the expected number of heads in ncoin ﬂips.\\n\\x0fLetXbe arandom variable for the number of heads in nﬂips.\\n\\x0fCould compute E ŒX\\x8dDPn\\nkD0k\\x01PrfXDkg. In fact, this is what the book\\ndoes in equation (C.37).\\n\\x0fInstead, we’ll use indicator random variables.\\n\\x0fForiD1; 2; : : : ; n , deﬁne XiDIftheithﬂipresults in event Hg.\\n\\x0fThen XDPn\\niD1Xi.\\n\\x0fLemmasays that E ŒXi\\x8dDPrfHgD1=2foriD1; 2; : : : ; n .\\n\\x0fExpected number of heads is E ŒX\\x8dDEŒPn\\niD1Xi\\x8d.\\n\\x0fProblem: We want E ŒPn\\niD1Xi\\x8d. We have only the individual expectations\\nEŒX1\\x8d ;EŒX2\\x8d ; : : : ;EŒXn\\x8d.\\n\\x0fSolution: Linearity of expectation says that the expectation of the su m equals\\nthe sum of the expectations. Thus,\\nEŒX\\x8dDE\"nX\\niD1Xi#\\nDnX\\niD1EŒXi\\x8d\\nDnX\\niD11=2\\nDn=2 :\\n\\x0fLinearity ofexpectation applies evenwhenthere isdepende nce amongtheran-\\ndom variables. [Not an issue in this example, but it can be a great help. The\\nhat-check problem of Exercise 5.2-4 isaproblem withlots of dependence. See\\nthe solution on page 5-11 of this manual.]', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 77}),\n",
              " Document(page_content='Lecture Notes for Chapter 5: ProbabilisticAnalysis and Ran domized Algorithms 5-5\\nAnalysis of thehiringproblem\\nAssumethat the candidates arrive inarandom order.\\nLetXbe a random variable that equals the number of times we hire a n ew ofﬁce\\nassistant.\\nDeﬁneindicator random variables X1; X2; : : : ; X n,where\\nXiDIfcandidate iis hiredg:\\nUseful properties:\\n\\x0fXDX1CX2C\\x01\\x01\\x01C Xn.\\n\\x0fLemma)EŒXi\\x8dDPrfcandidate iishiredg.\\nWeneed tocompute Pr fcandidate iishiredg.\\n\\x0fCandidate iis hired if and only if candidate iis better than each of candidates\\n1; 2; : : : ; i/NUL1.\\n\\x0fAssumptionthatthecandidatesarriveinrandomorder )candidates 1; 2; : : : ; i\\narriveinrandom order )anyoneof theseﬁrst icandidates isequally likely to\\nbe the best one sofar.\\n\\x0fThus, Prfcandidate iisthe best so fargD1=i.\\n\\x0fWhich implies E ŒXi\\x8dD1=i.\\nNowcompute E ŒX\\x8d:\\nEŒX\\x8dDE\"nX\\niD1Xi#\\nDnX\\niD1EŒXi\\x8d\\nDnX\\niD11=i\\nDlnnCO.1/(equation (A.7): thesum isaharmonic series) .\\nThus, the expected hiring cost is O.c hlnn/, which is much better than the worst-\\ncase cost of O.nc h/.\\nRandomized algorithms\\nInstead of assuming adistribution of the inputs, weimpose a distribution.\\nThehiringproblem\\nForthe hiring problem, thealgorithm isdeterministic:\\n\\x0fFor any given input, the number of times we hire a new ofﬁce ass istant will\\nalways bethe same.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 78}),\n",
              " Document(page_content='5-6 Lecture Notes for Chapter 5: Probabilistic Analysis and Randomized Algorithms\\n\\x0fThenumber of times wehire anew ofﬁce assistant depends only onthe input.\\n\\x0fInfact, it depends only on the ordering of the candidates’ ra nks that it isgiven.\\n\\x0fSomerankorderings willalways produce ahigh hiring cost. E xample:h1; 2; 3;\\n4; 5; 6i, where each candidate is hired.\\n\\x0fSome will always produce a low hiring cost. Example: any orde ring in which\\nthe best candidate is the ﬁrst one interviewed. Then only the best candidate is\\nhired.\\n\\x0fSomemaybein between.\\nInstead of always interviewing the candidates in the order p resented, what if we\\nﬁrst randomly permuted this order?\\n\\x0fTherandomization isnow in thealgorithm, not in the input di stribution.\\n\\x0fGivenaparticular input, wecannolongersaywhatitshiring costwillbe. Each\\ntimewerun the algorithm, wecan get adifferent hiring cost.\\n\\x0fIn other words, each time we run the algorithm, the execution depends on the\\nrandom choices made.\\n\\x0fNoparticular input always elicits worst-case behavior.\\n\\x0fBad behavior occurs only if we get “unlucky” numbers from the random-\\nnumber generator.\\nPseudocode for randomized hiring problem\\nRANDOMIZED -HIRE-ASSISTANT .n/\\nrandomly permute the list of candidates\\nHIRE-ASSISTANT .n/\\nLemma\\nTheexpected hiring cost of R ANDOMIZED -HIRE-ASSISTANT isO.c hlnn/.\\nProofAfter permuting the input array, we have a situation identic al to the proba-\\nbilistic analysis of deterministic H IRE-ASSISTANT .\\nRandomlypermutingan array\\n[Thebook considers twomethods of randomly permuting an n-element array. The\\nﬁrstmethodassignsarandompriorityintherange1to n3toeachpositionandthen\\nreorders the array elements into increasing priority order . We omit this method\\nfrom these notes. The second method is better: it works in pla ce (unlike the\\npriority-based method),itrunsinlineartimewithoutrequ iringsorting,anditneeds\\nfewer random bits ( nrandom numbers in the range 1 to nrather than the range 1\\nton3). Wepresent and analyze the second method inthese notes.]\\nGoal\\nProduce a uniform random permutation (each of the nŠpermutations is equally\\nlikely to beproduced).', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 79}),\n",
              " Document(page_content='Lecture Notes for Chapter 5: ProbabilisticAnalysis and Ran domized Algorithms 5-7\\nNon-goal: Show that for each element AŒi\\x8d, the probability that AŒi\\x8dmoves to\\nposition jis1=n. (See Exercise 5.3-4, whose solution is on page 5-14 of this\\nmanual.)\\nThe following procedure permutes the array AŒ1 : : n\\x8din place (i.e., no auxiliary\\narray is required).\\nRANDOMIZE -IN-PLACE .A; n/\\nforiD1ton\\nswap AŒi\\x8dwithAŒRANDOM .i; n/\\x8d\\nIdea\\n\\x0fIn iteration i, choose AŒi\\x8drandomly from AŒi : : n\\x8d.\\n\\x0fWill never alter AŒi\\x8dafter iteration i.\\nTime\\nO.1/per iteration)O.n/total.\\nCorrectness\\nGiven a set of nelements, a k-permutation is a sequence containing kof the n\\nelements. There are nŠ=.n/NULk/Špossible k-permutations.\\nLemma\\nRANDOMIZE -IN-PLACEcomputes auniform random permutation.\\nProofUsealoop invariant:\\nLoop invariant: Just prior to the ith iteration of the forloop, for each\\npossible .i/NUL1/-permutation, subarray AŒ1 : : i/NUL1\\x8dcontains this .i/NUL1/-\\npermutation with probability .n/NULiC1/Š=nŠ.\\nInitialization: Just before ﬁrst iteration, iD1. Loop invariant says that for each\\npossible 0-permutation, subarray AŒ1 : : 0\\x8d contains this 0-permutation with\\nprobability nŠ=nŠD1.AŒ1 : : 0\\x8dis an empty subarray, and a 0-permutation\\nhas no elements. So, AŒ1 : : 0\\x8dcontains any 0-permutation withprobability 1.\\nMaintenance: Assume that just prior to the ith iteration, each possible .i/NUL1/-\\npermutation appearsin AŒ1 : : i/NUL1\\x8dwithprobability .n/NULiC1/Š=nŠ. Willshow\\nthat after the ithiteration, each possible i-permutation appears in AŒ1 : : i\\x8dwith\\nprobability .n/NULi/Š=nŠ. Incrementing iforthenextiteration thenmaintainsthe\\ninvariant.\\nConsider a particular i-permutation \\x19Dhx1; x2; : : : ; x ii. It consists of an\\n.i/NUL1/-permutation \\x190Dhx1; x2; : : : ; x i/NUL1i,followed by xi.\\nLetE1betheevent that thealgorithm actually puts \\x190intoAŒ1 : : i/NUL1\\x8d. Bythe\\nloop invariant, PrfE1gD.n/NULiC1/Š=nŠ.\\nLetE2bethe event that the ith iteration puts xiintoAŒi\\x8d.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 80}),\n",
              " Document(page_content='5-8 Lecture Notes for Chapter 5: Probabilistic Analysis and Randomized Algorithms\\nWeget the i-permutation \\x19inAŒ1 : : i\\x8dif and only if both E1andE2occur)\\nthe probability that the algorithm produces \\x19inAŒ1 : : i\\x8disPrfE2\\\\E1g.\\nEquation (C.14))PrfE2\\\\E1gDPrfE2jE1gPrfE1g.\\nThealgorithm chooses xirandomly from the n/NULiC1possibilities in AŒi : : n\\x8d\\n)PrfE2jE1gD1=.n/NULiC1/. Thus,\\nPrfE2\\\\E1gDPrfE2jE1gPrfE1g\\nD1\\nn/NULiC1\\x01.n/NULiC1/Š\\nnŠ\\nD.n/NULi/Š\\nnŠ:\\nTermination: At termination, iDnC1, so we conclude that AŒ1 : : n\\x8dis a given\\nn-permutation withprobability .n/NULn/Š=nŠD1=nŠ. (lemma)', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 81}),\n",
              " Document(page_content='SolutionsforChapter 5:\\nProbabilisticAnalysisandRandomized\\nAlgorithms\\nSolutionto Exercise 5.1-3\\nTo get an unbiased random bit, given only calls to B IASED-RANDOM, call\\nBIASED-RANDOMtwice. Repeatedly do so until the two calls return different\\nvalues, and when this occurs, return the ﬁrst of the twobits:\\nUNBIASED -RANDOM\\nwhileTRUE\\nxDBIASED-RANDOM\\nyDBIASED-RANDOM\\nifx¤y\\nreturn x\\nTo see that U NBIASED -RANDOMreturns 0and1each with probability 1=2, ob-\\nserve that the probability that agiven iteration returns 0is\\nPrfxD0andyD1gD.1/NULp/p ;\\nand the probability that agiven iteration returns 1is\\nPrfxD1andyD0gDp.1/NULp/ :\\n(Werely on the bits returned by B IASED-RANDOMbeing independent.) Thus, the\\nprobability that a given iteration returns 0equals the probability that it returns 1.\\nSincethere isnoother wayfor U NBIASED -RANDOMtoreturn avalue, itreturns 0\\nand1each with probability 1=2.\\nAssuming that each iteration takes O.1/time, the expected running time of\\nUNBIASED -RANDOMis linear in the expected number of iterations. We can view\\neach iteration as a Bernoulli trial, where “success” means t hat the iteration returns\\navalue. Theprobability ofsuccessequalstheprobability t hat0isreturned plusthe\\nprobability that 1is returned, or 2p.1/NULp/. The number of trials until a success\\noccurs isgivenbythegeometric distribution, andbyequati on (C.32), theexpected\\nnumber of trials for this scenario is 1=.2p.1/NULp//. Thus, the expected running\\ntimeof U NBIASED -RANDOMis‚.1=.2p.1/NULp//.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 82}),\n",
              " Document(page_content='5-10 Solutions for Chapter 5: Probabilistic Analysis andRa ndomized Algorithms\\nSolution to Exercise5.2-1\\nThis solutionisalsopostedpublicly\\nSince H IRE-ASSISTANT alwayshires candidate 1,ithiresexactly onceif andonly\\nif no candidates other than candidate 1are hired. This event occurs when candi-\\ndate1isthe best candidate of the n, which occurs withprobability 1=n.\\nHIRE-ASSISTANT hires ntimesifeachcandidate isbetter thanallthosewhowere\\ninterviewed (and hired) before. This event occurs precisel y when the list of ranks\\ngiven to the algorithm is h1; 2; : : : ; ni,which occurs with probability 1=nŠ.\\nSolution to Exercise5.2-2\\nWemake three observations:\\n1. Candidate 1is always hired.\\n2. Thebest candidate, i.e.,the one whose rank is n,is always hired.\\n3. If the best candidate is candidate 1, then that isthe only candidate hired.\\nTherefore, in order for H IRE-ASSISTANT to hire exactly twice, candidate 1must\\nhaverank i\\x14n/NUL1andallcandidates whoseranksare iC1; iC2; : : : ; n/NUL1must\\nbe interviewed after the candidate whose rank is n. (When iDn/NUL1, this second\\ncondition vacuously holds.)\\nLetEibetheeventinwhichcandidate 1hasrank i;clearly, PrfEigD1=nforany\\ngiven value of i.\\nLetting jdenote the position in the interview order of the best candid ate, let Fbe\\nthe event in which candidates 2; 3; : : : ; j/NUL1have ranks strictly less than the rank\\nof candidate 1. Given that event Eihas occurred, event Foccurs when the best\\ncandidate is the ﬁrst one interviewed out of the n/NULicandidates whose ranks are\\niC1; iC2; : : : ; n. Thus, PrfFjEigD1=.n/NULi/.\\nOur ﬁnal event is A, which occurs when H IRE-ASSISTANT hires exactly twice.\\nNoting that theevents E1; E2; : : : ; E nare disjoint, wehave\\nADF\\\\.E1[E2[\\x01\\x01\\x01[ En/NUL1/\\nD.F\\\\E1/[.F\\\\E2/[\\x01\\x01\\x01[ .F\\\\En/NUL1/ :\\nand\\nPrfAgDn/NUL1X\\niD1PrfF\\\\Eig:\\nByequation (C.14),\\nPrfF\\\\EigDPrfFjEigPrfEig\\nD1\\nn/NULi\\x011\\nn;', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 83}),\n",
              " Document(page_content='Solutions forChapter 5: Probabilistic Analysis and Random ized Algorithms 5-11\\nand so\\nPrfAgDn/NUL1X\\niD11\\nn/NULi\\x011\\nn\\nD1\\nnn/NUL1X\\niD11\\nn/NULi\\nD1\\nn\\x121\\nn/NUL1C1\\nn/NUL2C\\x01\\x01\\x01C1\\n1\\x13\\nD1\\nn\\x01Hn/NUL1;\\nwhere Hn/NUL1isthe nthharmonic number.\\nSolutionto Exercise 5.2-4\\nThissolutionisalsopostedpublicly\\nAnother way to think of the hat-check problem is that we want t o determine the\\nexpected number of ﬁxed points in a random permutation. (A ﬁxed point of a\\npermutation \\x19is a value ifor which \\x19.i/Di.) We could enumerate all nŠper-\\nmutations, count the total number of ﬁxed points, and divide bynŠto determine\\nthe average number of ﬁxed points per permutation. This woul d be a painstak-\\ning process, and the answer would turn out to be 1. We can use indicator random\\nvariables, however, toarrive at the sameanswer much more ea sily.\\nDeﬁnearandomvariable Xthatequalsthenumberofcustomersthatgetbacktheir\\nownhat, so that wewant to compute E ŒX\\x8d.\\nForiD1; 2; : : : ; n , deﬁne the indicator random variable\\nXiDIfcustomer igets back his ownhat g:\\nThen XDX1CX2C\\x01\\x01\\x01C Xn.\\nSince the ordering of hats is random, each customer has a prob ability of 1=nof\\ngetting back his or her own hat. In other words, Pr fXiD1gD1=n, which, by\\nLemma5.1, implies that E ŒXi\\x8dD1=n.\\nThus,\\nEŒX\\x8dDE\"nX\\niD1Xi#\\nDnX\\niD1EŒXi\\x8d(linearity of expectation)\\nDnX\\niD11=n\\nD1 ;\\nand soweexpect that exactly 1customer gets back his ownhat.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 84}),\n",
              " Document(page_content='5-12 Solutions for Chapter 5: Probabilistic Analysis andRa ndomized Algorithms\\nNote that this is a situation in which the indicator random va riables are notinde-\\npendent. For example, if nD2andX1D1, then X2must also equal 1. Con-\\nversely, if nD2andX1D0, then X2must also equal 0. Despite the dependence,\\nPrfXiD1gD1=nfor all i, and linearity of expectation holds. Thus, we can use\\nthe technique of indicator random variables evenin the pres ence of dependence.\\nSolution to Exercise5.2-5\\nThis solutionisalsopostedpublicly\\nLetXijbe an indicator random variable for the event where the pair AŒi\\x8d; AŒj \\x8d\\nfori < jis inverted, i.e., AŒi\\x8d > AŒj \\x8d . More precisely, we deﬁne XijD\\nIfAŒi\\x8d > AŒj \\x8dgfor1\\x14i < j\\x14n. We have PrfXijD1gD1=2, because\\ngiven two distinct random numbers, the probability that the ﬁrst is bigger than the\\nsecond is 1=2. ByLemma5.1, E ŒXij\\x8dD1=2.\\nLetXbethetherandom variable denoting thetotal number ofinver ted pairsinthe\\narray, sothat\\nXDn/NUL1X\\niD1nX\\njDiC1Xij:\\nWewant theexpected number of inverted pairs, sowetake thee xpectation of both\\nsides of the above equation toobtain\\nEŒX\\x8dDE\"n/NUL1X\\niD1nX\\njDiC1Xij#\\n:\\nWeuse linearity of expectation to get\\nEŒX\\x8dDE\"n/NUL1X\\niD1nX\\njDiC1Xij#\\nDn/NUL1X\\niD1nX\\njDiC1EŒXij\\x8d\\nDn/NUL1X\\niD1nX\\njDiC11=2\\nD \\nn\\n2!\\n1\\n2\\nDn.n/NUL1/\\n2\\x011\\n2\\nDn.n/NUL1/\\n4:\\nThus the expected number of inverted pairs is n.n/NUL1/=4.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 85}),\n",
              " Document(page_content='Solutions forChapter 5: Probabilistic Analysis and Random ized Algorithms 5-13\\nSolutionto Exercise 5.3-1\\nHere’s the rewritten procedure:\\nRANDOMIZE -IN-PLACE .A/\\nnDA:length\\nswap AŒ1\\x8dwithAŒRANDOM .1; n/\\x8d\\nforiD2ton\\nswap AŒi\\x8dwithAŒRANDOM .i; n/\\x8d\\nTheloop invariant becomes\\nLoop invariant: Just prior to the iteration of the forloop for each value of\\niD2; : : : ; n,foreachpossible .i/NUL1/-permutation,thesubarray AŒ1 : : i/NUL1\\x8d\\ncontains this .i/NUL1/-permutation withprobability .n/NULiC1/Š=nŠ.\\nThe maintenance and termination parts remain the same. The i nitialization part\\nis for the subarray AŒ1 : : 1\\x8d, which contains any 1-permutation with probability\\n.n/NUL1/Š=nŠD1=n.\\nSolutionto Exercise 5.3-2\\nThissolutionisalsopostedpublicly\\nAlthough P ERMUTE-WITHOUT-IDENTITY will not produce the identity permuta-\\ntion, there are other permutations that it fails to produce. For example, consider\\nits operation when nD3, when it should be able to produce the nŠ/NUL1D5non-\\nidentity permutations. The forloop iterates for iD1andiD2. When iD1,\\nthe call to R ANDOMreturns one of two possible values (either 2or3), and when\\niD2,thecalltoR ANDOMreturnsjustonevalue( 3). Thus,P ERMUTE-WITHOUT-\\nIDENTITY can produce only 2\\x011D2possible permutations, rather than the 5that\\narerequired.\\nSolutionto Exercise 5.3-3\\nThe PERMUTE-WITH-ALLprocedure does not produce a uniform random per-\\nmutation. Consider the permutations it produces when nD3. The procedure\\nmakes 3calls to R ANDOM, each of which returns one of 3values, and so calling\\nPERMUTE-WITH-ALLhas27possible outcomes. Sincethereare 3ŠD6permuta-\\ntions, if P ERMUTE-WITH-ALLdid produce a uniform random permutation, then\\neachpermutation wouldoccur 1=6of thetime. That wouldmeanthat each permu-\\ntation would have to occur an integer number mtimes, where m=27D1=6. No\\ninteger msatisﬁes this condition.\\nInfact, ifweweretoworkoutthepossible permutations of h1; 2; 3iandhowoften\\nthey occur with P ERMUTE-WITH-ALL, wewould get the following probabilities:', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 86}),\n",
              " Document(page_content='5-14 Solutions for Chapter 5: Probabilistic Analysis andRa ndomized Algorithms\\npermutation probability\\nh1; 2; 3i 4=27\\nh1; 3; 2i 5=27\\nh2; 1; 3i 5=27\\nh2; 3; 1i 5=27\\nh3; 1; 2i 4=27\\nh3; 2; 1i 4=27\\nAlthough these probabilities sum to 1, none are equal to 1=6.\\nSolution to Exercise5.3-4\\nThis solutionisalsopostedpublicly\\nPERMUTE-BY-CYCLICchoosesoffsetas a random integer in the range 1\\x14\\noffset\\x14n, and then it performs a cyclic rotation of the array. That is,\\nBŒ..iCoffset/NUL1/modn/C1\\x8dDAŒi\\x8dforiD1; 2; : : : ; n . (The subtraction\\nand addition of 1in the index calculation is due to the 1-origin indexing. If we\\nhad used 0-origin indexing instead, the index calculation would have simplied to\\nBŒ.iCoffset /modn\\x8dDAŒi\\x8dforiD0; 1; : : : ; n/NUL1.)\\nThus, once offsetis determined, so is the entire permutation. Since each valu e of\\noffsetoccurs with probability 1=n, each element AŒi\\x8dhas a probability of ending\\nup inposition BŒj \\x8dwith probability 1=n.\\nThis procedure does not produce a uniform random permutatio n, however, since\\nit can produce only ndifferent permutations. Thus, npermutations occur with\\nprobability 1=n, and the remaining nŠ/NULnpermutations occur withprobability 0.\\nSolution to Exercise5.3-7\\nSince each recursive call reduces mby1and makes only one call to R ANDOM,\\nit’s easy to see that there are a total of mcalls to R ANDOM. Moreover, since each\\nrecursive call adds exactly one element to the set, it’s easy to see that the resulting\\nsetScontains exactly melements.\\nBecause the elements of set Sare chosen independently of each other, it sufﬁces\\nto show that each of the nvalues appears in Swith probability m=n. We use an\\ninductiveproof. TheinductivehypothesisisthatacalltoR ANDOM-SUBSET .m; n/\\nreturns a set Sofmelements, each appearing with probability m=n. The base\\ncases are for mD0andmD1. When mD0, the returned set is empty, and so\\nit contains each element withprobability 0. When mD1, the returned set has one\\nelement, and it isequally likely to beany number in f1; 2; 3; : : : ; ng.\\nFor the inductive step, we assume that the call R ANDOM-SUBSET .m/NUL1; n/NUL1/\\nreturnsaset S0ofm/NUL1elementsinwhicheachvaluein f1; 2; 3; : : : ; n/NUL1goccurs\\nwith probability .m/NUL1/=.n/NUL1/. After the line iDRANDOM .1; n/,iis equally\\nlikely to be any value in f1; 2; 3; : : : ; ng. We consider separately the probabilities', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 87}),\n",
              " Document(page_content='Solutions forChapter 5: Probabilistic Analysis and Random ized Algorithms 5-15\\nthatScontains j < nand that Scontains n. Let Rjbe the event that the call\\nRANDOM .1; n/returns j,so that PrfRjgD1=n.\\nForj < n, theevent that j2Sisthe union of twodisjoint events:\\n\\x0fj2S0,and\\n\\x0fj62S0andRj(these events areindependent),\\nThus,\\nPrfj2Sg\\nDPrfj2S0gCPrfj62S0andRjg(the events are disjoint)\\nDm/NUL1\\nn/NUL1C\\x12\\n1/NULm/NUL1\\nn/NUL1\\x13\\n\\x011\\nn(by theinductive hypothesis)\\nDm/NUL1\\nn/NUL1C\\x12n/NUL1\\nn/NUL1/NULm/NUL1\\nn/NUL1\\x13\\n\\x011\\nn\\nDm/NUL1\\nn/NUL1\\x01n\\nnCn/NULm\\nn/NUL1\\x011\\nn\\nD.m/NUL1/nC.n/NULm/\\n.n/NUL1/n\\nDmn/NULnCn/NULm\\n.n/NUL1/n\\nDm.n/NUL1/\\n.n/NUL1/n\\nDm\\nn:\\nTheevent that n2Sisalso the union of twodisjoint events:\\n\\x0fRn, and\\n\\x0fRjandj2S0for some j < n(these events areindependent).\\nThus,\\nPrfn2Sg\\nDPrfRngCPrfRjandj2S0for some j < ng(the events are disjoint)\\nD1\\nnCn/NUL1\\nn\\x01m/NUL1\\nn/NUL1(by the inductive hypothesis)\\nD1\\nn\\x01n/NUL1\\nn/NUL1Cn/NUL1\\nn\\x01m/NUL1\\nn/NUL1\\nDn/NUL1Cnm/NULn/NULmC1\\nn.n/NUL1/\\nDnm/NULm\\nn.n/NUL1/\\nDm.n/NUL1/\\nn.n/NUL1/\\nDm\\nn:', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 88}),\n",
              " Document(page_content='5-16 Solutions for Chapter 5: Probabilistic Analysis andRa ndomized Algorithms\\nSolution to Exercise5.4-6\\nFirst we determine the expected number of empty bins. We deﬁn e a random vari-\\nableXtobethenumberofemptybins,sothatwewanttocomputeE ŒX\\x8d. Next,for\\niD1; 2; : : : ; n , we deﬁne the indicator random variable YiDIfbiniisemptyg.\\nThus,\\nXDnX\\niD1Yi;\\nand so\\nEŒX\\x8dDE\"nX\\niD1Yi#\\nDnX\\niD1EŒYi\\x8d (by linearity of expectation)\\nDnX\\niD1Prfbiniis emptyg(by Lemma5.1) .\\nLet us focus on a speciﬁc bin, say bin i. We view a toss as a success if it misses\\nbiniand as a failure if it lands in bin i. We have nindependent Bernoulli trials,\\neach with probability of success 1/NUL1=n. In order for bin ito be empty, we need\\nnsuccesses in ntrials. Using abinomial distribution, therefore, wehave t hat\\nPrfbiniisemptygD \\nn\\nn!\\x12\\n1/NUL1\\nn\\x13n\\x121\\nn\\x130\\nD\\x12\\n1/NUL1\\nn\\x13n\\n:\\nThus,\\nEŒX\\x8dDnX\\niD1\\x12\\n1/NUL1\\nn\\x13n\\nDn\\x12\\n1/NUL1\\nn\\x13n\\n:\\nBy equation (3.14), as napproaches1, the quantity .1/NUL1=n/napproaches 1=e,\\nand so E ŒX\\x8dapproaches n=e.\\nNow we determine the expected number of bins with exactly one ball. We re-\\ndeﬁne Xto be number of bins with exactly one ball, and we redeﬁne Yito be\\nIfbinigets exactly one ball g. Asbefore, weﬁnd that\\nEŒX\\x8dDnX\\niD1Prfbinigets exactly one ball g:\\nAgainfocusingonbin i,weneedexactly n/NUL1successesin nindependentBernoulli\\ntrials, and so', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 89}),\n",
              " Document(page_content='Solutions forChapter 5: Probabilistic Analysis and Random ized Algorithms 5-17\\nPrfbinigets exactly one ball gD \\nn\\nn/NUL1!\\x12\\n1/NUL1\\nn\\x13n/NUL1\\x121\\nn\\x131\\nDn\\x01\\x12\\n1/NUL1\\nn\\x13n/NUL11\\nn\\nD\\x12\\n1/NUL1\\nn\\x13n/NUL1\\n;\\nand so\\nEŒX\\x8dDnX\\niD1\\x12\\n1/NUL1\\nn\\x13n/NUL1\\nDn\\x12\\n1/NUL1\\nn\\x13n/NUL1\\n:\\nBecause\\nn\\x12\\n1/NUL1\\nn\\x13n/NUL1\\nDn/NUL\\n1/NUL1\\nn\\x01n\\n1/NUL1\\nn;\\nasnapproaches1, weﬁnd that E ŒX\\x8dapproaches\\nn=e\\n1/NUL1=nDn2\\ne.n/NUL1/:\\nSolutionto Problem 5-1\\na.Todeterminetheexpectedvaluerepresentedbythecountera fternINCREMENT\\noperations, wedeﬁne some random variables:\\n\\x0fForjD1; 2; : : : ; n , letXjdenote the increase in the value represented by\\nthe counter due tothe jth INCREMENT operation.\\n\\x0fLetVnbe the value represented by the counter after nINCREMENT opera-\\ntions.\\nThen VnDX1CX2C\\x01\\x01\\x01C Xn. We want to compute E ŒVn\\x8d. By linearity of\\nexpectation,\\nEŒVn\\x8dDEŒX1CX2C\\x01\\x01\\x01C Xn\\x8dDEŒX1\\x8dCEŒX2\\x8dC\\x01\\x01\\x01CEŒXn\\x8d :\\nWe shall show that E ŒXj\\x8dD1forjD1; 2; : : : ; n , which will prove that\\nEŒVn\\x8dDn.\\nWe actually show that E ŒXj\\x8dD1in two ways, the second more rigorous than\\nthe ﬁrst:\\n1. Supposethat atthestart ofthe jth INCREMENT operation, thecounter holds\\nthe value i, which represents ni. If the counter increases due to this I NCRE-\\nMENToperation, then the value it represents increases by niC1/NULni. The\\ncounter increases withprobability 1=.n iC1/NULni/,and so', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 90}),\n",
              " Document(page_content='5-18 Solutions for Chapter 5: Probabilistic Analysis andRa ndomized Algorithms\\nEŒXj\\x8dD.0\\x01Prfcounter does not increase g/\\nC..niC1/NULni/\\x01Prfcounter increasesg/\\nD\\x12\\n0\\x01\\x12\\n1/NUL1\\nniC1/NULni\\x13\\x13\\nC\\x12\\n.niC1/NULni/\\x011\\nniC1/NULni\\x13\\nD1 ;\\nand so E ŒXj\\x8dD1regardless of the value held by the counter.\\n2. Let Cjbe the random variable denoting the value held in the counter at the\\nstart of the jth INCREMENT operation. Since we can ignore values of Cj\\ngreater than 2b/NUL1, weuse aformula for conditional expectation:\\nEŒXj\\x8dDEŒEŒXjjCj\\x8d\\x8d\\nD2b/NUL1X\\niD0EŒXjjCjDi\\x8d\\x01PrfCjDig:\\nTocompute E ŒXjjCjDi\\x8d,wenote that\\n\\x0fPrfXjD0jCjDigD1/NUL1=.n iC1/NULni/,\\n\\x0fPrfXjDniC1/NULnijCjDigD1=.n iC1/NULni/,and\\n\\x0fPrfXjDkjCjDigD0for all other k.\\nThus,\\nEŒXjjCjDi\\x8dDX\\nkk\\x01PrfXjDkjCjDig\\nD\\x12\\n0\\x01\\x12\\n1/NUL1\\nniC1/NULni\\x13\\x13\\nC\\x12\\n.niC1/NULni/\\x011\\nniC1/NULni\\x13\\nD1 :\\nTherefore, noting that\\n2b/NUL1X\\niD0PrfCjDigD1 ;\\nwehave\\nEŒXj\\x8dD2b/NUL1X\\niD01\\x01PrfCjDig\\nD1 :\\nWhyisthesecondwaymorerigorousthantheﬁrst? Bothwaysco nditiononthe\\nvalueheldinthecounter, butonlythesecondwayincorporat estheconditioning\\ninto the expression for E ŒXj\\x8d.\\nb.Deﬁning VnandXjas in part (a), we want to compute Var ŒVn\\x8d, where niD\\n100i. The Xjare pairwise independent, and so by equation (C.29), Var ŒVn\\x8dD\\nVarŒX1\\x8dCVarŒX2\\x8dC\\x01\\x01\\x01CVarŒXn\\x8d.\\nSince niD100i,weseethat niC1/NULniD100.iC1//NUL100iD100. Therefore,\\nwith probability 99=100, the increase in the value represented by the counter\\ndue to the jth INCREMENT operation is 0, and with probability 1=100, the', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 91}),\n",
              " Document(page_content='Solutions forChapter 5: Probabilistic Analysis and Random ized Algorithms 5-19\\nvalue represented increases by 100. Thus, by equation (C.27),\\nVarŒXj\\x8dDE\\x02\\nX2\\nj\\x03\\n/NULE2ŒXj\\x8d\\nD\\x12\\x12\\n02\\x0199\\n100\\x13\\nC\\x12\\n1002\\x011\\n100\\x13\\x13\\n/NUL12\\nD100/NUL1\\nD99 :\\nSumming upthe variances of the Xjgives Var ŒVn\\x8dD99n.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 92}),\n",
              " Document(page_content='Lecture Notes forChapter 6:\\nHeapsort\\nChapter 6 overview\\nHeapsort\\n\\x0fO.nlgn/worst case—like merge sort.\\n\\x0fSorts inplace—like insertion sort.\\n\\x0fCombines the best of both algorithms.\\nTounderstand heapsort, we’llcoverheapsandheapoperatio ns, andthenwe’lltake\\nalook at priority queues.\\nHeaps\\nHeap datastructure\\n\\x0fHeap A(notgarbage-collected storage) isanearly complete binary tre e.\\n\\x0fHeightof node=#of edgesonalongest simple pathfromthenodedownt o\\naleaf.\\n\\x0fHeightof heapDheight of rootD‚.lgn/.\\n\\x0fA heap can bestored as anarray A.\\n\\x0fRoot of tree is AŒ1\\x8d.\\n\\x0fParent of AŒi\\x8dDAŒbi=2c\\x8d.\\n\\x0fLeft child of AŒi\\x8dDAŒ2i\\x8d.\\n\\x0fRight child of AŒi\\x8dDAŒ2iC1\\x8d.\\n\\x0fComputing isfast with binary representation implementati on.\\n[Inbook,have lengthandheap-sizeattributes. Here,webypasstheseattributesand\\nuse parameter values instead.]', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 93}),\n",
              " Document(page_content='6-2 Lecture Notes for Chapter 6: Heapsort\\nExample\\nOf a max-heap. [Arcs above and below the array on the right go between parent s\\nand children. There is no signiﬁcance to whether an arc is dra wn above or below\\nthe array.]\\n16 14 10 8 7 9 3 2 4 11 2 3 4 5 6 7 8 9 101\\n2 3\\n4 5 6 7\\n8 9 1016\\n14 10\\n8 7 9 3\\n2 4 1\\nHeap property\\n\\x0fFor max-heaps (largest element at root), max-heap property: for all nodes i,\\nexcluding the root, AŒPARENT .i/\\x8d\\x15AŒi\\x8d.\\n\\x0fFor min-heaps (smallest element at root), min-heap property: for all nodes i,\\nexcluding the root, AŒPARENT .i/\\x8d\\x14AŒi\\x8d.\\nByinduction andtransitivity of \\x14,themax-heapproperty guarantees thatthemax-\\nimum element of amax-heap is at the root. Similar argument fo r min-heaps.\\nTheheapsort algorithm we’ll show uses max-heaps.\\nNote: In general, heaps can be k-ary tree instead of binary.\\nMaintaining the heap property\\nMAX-HEAPIFYis important for manipulating max-heaps. It is used to maint ain\\nthe max-heap property.\\n\\x0fBefore M AX-HEAPIFY,AŒi\\x8dmaybe smaller than itschildren.\\n\\x0fAssumeleft and right subtrees of iaremax-heaps.\\n\\x0fAfter M AX-HEAPIFY, subtree rooted at iisa max-heap.\\nMAX-HEAPIFY .A; i; n/\\nlDLEFT.i/\\nrDRIGHT.i/\\nifl\\x14nandAŒl\\x8d > AŒi\\x8d\\nlargestDl\\nelselargestDi\\nifr\\x14nandAŒr\\x8d > AŒ largest \\x8d\\nlargestDr\\niflargest¤i\\nexchange AŒi\\x8dwithAŒlargest \\x8d\\nMAX-HEAPIFY .A;largest ; n/', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 94}),\n",
              " Document(page_content='Lecture Notes for Chapter 6: Heapsort 6-3\\n[Parameter nreplaces attribute A:heap-size.]\\nTheway M AX-HEAPIFYworks:\\n\\x0fCompare AŒi\\x8d,AŒLEFT.i/\\x8d,and AŒRIGHT.i/\\x8d.\\n\\x0fIf necessary, swap AŒi\\x8dwith the larger of the two children to preserve heap\\nproperty.\\n\\x0fContinue this process of comparing and swapping down the hea p, until subtree\\nrooted at iis max-heap. If we hit a leaf, then the subtree rooted at the le af is\\ntrivially amax-heap.\\nRun MAX-HEAPIFYonthe following heap example.\\n16\\n4 10\\n14 7 9\\n2 8 1\\n(a)16\\n14 10\\n4 7 9 3\\n2 8 1\\n(b)\\n16\\n14 10\\n8 7 9 3\\n2 4 1\\n(c)31\\n3\\n4 5 6 7\\n9 102\\n81\\n3\\n4 5 6 7\\n9 102\\n8\\n1\\n3\\n4 5 6 7\\n9 102\\n8i\\ni\\ni\\n\\x0fNode 2violates themax-heap property.\\n\\x0fCompare node 2 with its children, and then swap it with the lar ger of the two\\nchildren.\\n\\x0fContinue down the tree, swapping until the value is properly placed at the root\\nof asubtree that is amax-heap. Inthis case, the max-heap is a leaf.\\nTime\\nO.lgn/.\\nAnalysis\\n[Insteadofbook’sformalanalysiswithrecurrence, justco meupwith O.lgn/intu-\\nitively.]Heap is almost-complete binary tree, hence must process O.lgn/levels,\\nwithconstant workat each level (comparing 3items and maybe swapping 2).', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 95}),\n",
              " Document(page_content='6-4 Lecture Notes for Chapter 6: Heapsort\\nBuilding aheap\\nThefollowing procedure, given an unordered array, will pro duce amax-heap.\\nBUILD-MAX-HEAP.A; n/\\nforiDbn=2cdownto 1\\nMAX-HEAPIFY .A; i; n/\\n[Parameter nreplaces both attributes A:lengthandA:heap-size.]\\nExample\\nBuilding a max-heap from the following unsorted array resul ts in the ﬁrst heap\\nexample.\\n\\x0fistarts off as5.\\n\\x0fMAX-HEAPIFYis applied to subtrees rooted at nodes (in order): 16, 2, 3, 1, 4.\\n1\\n2 3\\n4 5 6 7\\n8 9 101\\n2 3\\n4 5 6 7\\n8 9 104\\n1 3\\n2 9 10\\n14 8 7164 1 23 16 9 10 14 8 7\\n16\\n14 10\\n8 9 3\\n2 4 17A\\ni2 3 4 5 6 7 8 9 101\\nCorrectness\\nLoop invariant: At start of every iteration of forloop, each node iC1,\\niC2, ..., nis root of amax-heap.\\nInitialization: ByExercise6.1-7, weknowthat eachnode bn=2cC1,bn=2cC2,\\n...,nisa leaf, which is theroot of atrivial max-heap. Since iDbn=2cbefore\\nthe ﬁrst iteration of the forloop, the invariant is initially true.\\nMaintenance: Childrenofnode iareindexedhigher than i,sobytheloopinvari-\\nant,theyarebothrootsofmax-heaps. Correctlyassumingth atiC1; iC2; : : : ; n\\nare all roots of max-heaps, M AX-HEAPIFYmakes node ia max-heap root.\\nDecrementing ireestablishes theloop invariant at each iteration.\\nTermination: When iD0,theloopterminates. Bytheloopinvariant, each node,\\nnotably node 1, isthe root of a max-heap.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 96}),\n",
              " Document(page_content='Lecture Notes for Chapter 6: Heapsort 6-5\\nAnalysis\\n\\x0fSimple bound: O.n/calls to M AX-HEAPIFY, each of which takes O.lgn/\\ntime)O.nlgn/. (Note: A good approach to analysis in general is to start by\\nproving easy bound, then tryto tighten it.)\\n\\x0fTighter analysis: Observation: Time to run M AX-HEAPIFYis linear in the\\nheight of the node it’s run on, and most nodes have small heigh ts. Have\\n\\x14˙\\nn=2hC1\\x07\\nnodesofheight h(seeExercise6.3-3),andheightofheapis blgnc\\n(Exercise 6.1-2).\\nThe time required by M AX-HEAPIFYwhen called on a node of height h\\nisO.h/, so thetotal cost of B UILD-MAX-HEAPis\\nblgncX\\nhD0ln\\n2hC1m\\nO.h/DO \\nnblgncX\\nhD0h\\n2h!\\n:\\nEvaluate the last summation by substituting xD1=2in the formula (A.8)/NULP1\\nkD0kxk\\x01\\n, which yields\\n1X\\nhD0h\\n2hD1=2\\n.1/NUL1=2/2\\nD2 :\\nThus, therunning timeof B UILD-MAX-HEAPisO.n/.\\nBuilding a min-heap from an unordered array can be done by cal ling MIN-\\nHEAPIFYinstead of M AX-HEAPIFY, also taking linear time.\\nThe heapsortalgorithm\\nGivenan input array, the heapsort algorithm acts as follows :\\n\\x0fBuilds a max-heap from the array.\\n\\x0fStarting with the root (the maximum element), the algorithm places the maxi-\\nmumelementintothecorrectplaceinthearraybyswappingit withtheelement\\nin the last position in the array.\\n\\x0f“Discard”thislastnode(knowingthatitisinitscorrectpl ace)bydecreasingthe\\nheapsize,andcalling M AX-HEAPIFYonthenew(possibly incorrectly-placed)\\nroot.\\n\\x0fRepeat this “discarding” process until only one node (the sm allest element)\\nremains, and therefore is inthe correct place inthe array.\\nHEAPSORT .A; n/\\nBUILD-MAX-HEAP.A; n/\\nforiDndownto2\\nexchange AŒ1\\x8dwithAŒi\\x8d\\nMAX-HEAPIFY .A; 1; i/NUL1/\\n[Parameter nreplaces A:length, andparameter value i/NUL1inMAX-HEAPIFYcall\\nreplaces decrementing of A:heap-size.]', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 97}),\n",
              " Document(page_content='6-6 Lecture Notes for Chapter 6: Heapsort\\nExample\\nSort anexample heapontheboard. [Nodes withheavy outline arenolonger inthe\\nheap.]\\n(a) (b)\\n(c) (d)\\n(e)1 2 3 4 72\\n1 3\\n4 7\\n1\\n2 3\\n4 73\\n2 1\\n7 44\\n2 3\\n7 17\\n4 3\\n2 1\\nAi\\nii\\ni\\nAnalysis\\n\\x0fBUILD-MAX-HEAP:O.n/\\n\\x0fforloop: n/NUL1times\\n\\x0fexchange elements: O.1/\\n\\x0fMAX-HEAPIFY:O.lgn/\\nTotal time: O.nlgn/.\\nThough heapsort is a great algorithm, a well-implemented qu icksort usually beats\\nit inpractice.\\nHeapimplementation ofpriority queue\\nHeaps efﬁciently implement priority queues. These notes wi ll deal with max-\\npriority queues implemented with max-heaps. Min-priority queues are imple-\\nmented withmin-heaps similarly.\\nA heap gives a good compromise between fast insertion but slo w extraction and\\nvice versa. Bothoperations take O.lgn/time.\\nPriority queue\\n\\x0fMaintains adynamic set Sof elements.\\n\\x0fEachset element hasa key—an associated value.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 98}),\n",
              " Document(page_content='Lecture Notes for Chapter 6: Heapsort 6-7\\n\\x0fMax-priority queue supports dynamic-set operations:\\n\\x0fINSERT .S; x/: inserts element xinto set S.\\n\\x0fMAXIMUM .S/: returns element of Swith largest key.\\n\\x0fEXTRACT-MAX.S/: removes and returns element of Swith largest key.\\n\\x0fINCREASE -KEY.S; x; k/: increases value of element x’s key to k. Assume\\nk\\x15x’scurrent keyvalue.\\n\\x0fExample max-priority queue application: schedule jobs ons hared computer.\\n\\x0fMin-priority queue supports similar operations:\\n\\x0fINSERT .S; x/: inserts element xinto set S.\\n\\x0fMINIMUM .S/: returns element of Swithsmallest key.\\n\\x0fEXTRACT-MIN.S/: removes and returns element of Swith smallest key.\\n\\x0fDECREASE -KEY.S; x; k/: decreases valueofelement x’skeyto k. Assume\\nk\\x14x’scurrent keyvalue.\\n\\x0fExample min-priority queue application: event-driven sim ulator.\\nNote: Actualimplementationsoftenhavea handleineachheapelementthatallows\\naccess to an object in the application, and objects in the app lication often have a\\nhandle (likely an array index) toaccess the heap element.\\nWill examine how toimplement max-priority queue operation s.\\nFindingthemaximumelement\\nGetting the maximum element iseasy: it’s the root.\\nHEAP-MAXIMUM .A/\\nreturn AŒ1\\x8d\\nTime\\n‚.1/.\\nExtracting maxelement\\nGiventhe array A:\\n\\x0fMake sure heap isnot empty.\\n\\x0fMake acopy of the maximum element (the root).\\n\\x0fMake the last node in the tree thenew root.\\n\\x0fRe-heapify the heap, withone fewer node.\\n\\x0fReturn the copy of the maximum element.\\nNote: Because weneed to decrement theheap size ninthe following pseudocode,\\nassume that it is passed by reference, not by value.\\n[This issue does not come up in the pseudocode in the book, bec ause it uses the\\nattribute A:heap-sizeinstead of passing in the heap size as aparameter.]', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 99}),\n",
              " Document(page_content='6-8 Lecture Notes for Chapter 6: Heapsort\\nHEAP-EXTRACT-MAX.A; n/\\nifn < 1\\nerror“heap underﬂow”\\nmaxDAŒ1\\x8d\\nAŒ1\\x8dDAŒn\\x8d\\nnDn/NUL1\\nMAX-HEAPIFY .A; 1; n///remakes heap\\nreturnmax\\nAnalysis\\nConstant-time assignments plus timefor M AX-HEAPIFY.\\nTime\\nO.lgn/.\\nExample\\nRun HEAP-EXTRACT-MAXonﬁrst heap example.\\n\\x0fTake16 out of node 1.\\n\\x0fMove1from node 10 tonode 1.\\n\\x0fErasenode 10.\\n\\x0fMAX-HEAPIFYfrom the root topreserve max-heap property.\\n\\x0fNotethat successive extractions will removeitems inrever se sorted order.\\nIncreasing key value\\nGiven set S, element x,and new key value k:\\n\\x0fMake sure k\\x15x’scurrent key.\\n\\x0fUpdate x’skeyvalue to k.\\n\\x0fTraversethetreeupwardcomparing xtoitsparent andswapping keysifneces-\\nsary, until x’skey issmaller than itsparent’s key.\\nHEAP-INCREASE -KEY.A; i;key/\\nifkey< AŒi\\x8d\\nerror“new keyis smaller than current key”\\nAŒi\\x8dDkey\\nwhile i > 1andAŒPARENT .i/\\x8d < AŒi\\x8d\\nexchange AŒi\\x8dwithAŒPARENT .i/\\x8d\\niDPARENT .i/\\nAnalysis\\nUpward path from node ihas length O.lgn/inan n-element heap.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 100}),\n",
              " Document(page_content='Lecture Notes for Chapter 6: Heapsort 6-9\\nTime\\nO.lgn/.\\nExample\\nIncrease key of node 9 in ﬁrst heap example to have value 15. Ex change keys of\\nnodes 4and 9, then of nodes 2and 4.\\nInserting intotheheap\\nGivenakey kto insert into the heap:\\n\\x0fIncrement theheap size.\\n\\x0fInsert anew node in the last position in the heap, withkey /NUL1.\\n\\x0fIncrease the/NUL1key to kusing the H EAP-INCREASE -KEYprocedure deﬁned\\nabove.\\nNote: Again, the parameter nispassed by reference, not by value.\\nMAX-HEAP-INSERT .A;key; n/\\nnDnC1\\nAŒn\\x8dD/NUL1\\nHEAP-INCREASE -KEY.A; n;key/\\nAnalysis\\nConstant timeassignments Ctimefor H EAP-INCREASE -KEY.\\nTime\\nO.lgn/.\\nMin-priority queue operations are implemented similarly w ith min-heaps.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 101}),\n",
              " Document(page_content='Solutionsfor Chapter6:\\nHeapsort\\nSolution to Exercise6.1-1\\nThis solutionisalsopostedpublicly\\nSince a heap is an almost-complete binary tree (complete at a ll levels except pos-\\nsibly the lowest), it has at most 2hC1/NUL1elements (if it is complete) and at least\\n2h/NUL1C1D2helements(ifthelowestlevelhasjust1elementandtheother levels\\nare complete).\\nSolution to Exercise6.1-2\\nThis solutionisalsopostedpublicly\\nGiven an n-element heap of height h, weknow from Exercise 6.1-1 that\\n2h\\x14n\\x142hC1/NUL1 < 2hC1:\\nThus, h\\x14lgn < hC1. Since hisan integer, hDblgnc(by deﬁnition ofbc).\\nSolution to Exercise6.1-3\\nAssume theclaim isfalse—i.e., that there isasubtree whose root isnot thelargest\\nelement in the subtree. Then the maximum element is somewher e else in the sub-\\ntree, possibly even at more than one location. Let mbe the index at which the\\nmaximumappears(thelowestsuchindexifthemaximumappear smorethanonce).\\nSince the maximum is not at the root of the subtree, node mhas a parent. Since\\nthe parent of a node has a lower index than the node, and mwas chosen to be the\\nsmallest index of the maximum value, AŒPARENT .m/\\x8d < AŒm\\x8d . But by the max-\\nheap property, we must have AŒPARENT .m/\\x8d\\x15AŒm\\x8d. So our assumption is false,\\nand the claim istrue.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 102}),\n",
              " Document(page_content='Solutions forChapter 6: Heapsort 6-11\\nSolutionto Exercise 6.2-6\\nThissolutionisalsopostedpublicly\\nIf you put a value at the root that is less than every value in th e left and right\\nsubtrees, then M AX-HEAPIFYwill be called recursively until aleaf isreached. To\\nmaketherecursivecallstraversethelongest pathtoaleaf, choosevaluesthatmake\\nMAX-HEAPIFYalways recurse on the left child. It follows the left branch w hen\\nthe left child is greater than or equal to the right child, so p utting 0 at the root\\nand 1 at all the other nodes, for example, will accomplish tha t. With such values,\\nMAX-HEAPIFYwill be called htimes (where his the heap height, which is the\\nnumber of edges in the longest path from the root to a leaf), so its running time\\nwill be ‚.h/(since each call does ‚.1/work), which is ‚.lgn/. Since we have\\na case in which M AX-HEAPIFY’s running time is ‚.lgn/, its worst-case running\\ntimeis \\x7f.lgn/.\\nSolutionto Exercise 6.3-3\\nLetHbe the height of the heap.\\nTwosubtleties to beware of:\\n\\x0fBe careful not to confuse the height of a node (longest distan ce from a leaf)\\nwith itsdepth (distance from the root).\\n\\x0fIftheheapisnotacompletebinarytree(bottomlevelisnotf ull),thenthenodes\\natagivenlevel(depth)don’tallhavethesameheight. Forex ample,althoughall\\nnodes at depth Hhave height 0, nodes at depth H/NUL1can have either height 0\\nor height 1.\\nFor a complete binary tree, it’s easy to show that there are˙\\nn=2hC1\\x07\\nnodes of\\nheight h. But the proof for an incomplete tree is tricky and is not deri ved from the\\nproof for acomplete tree.\\nProofByinduction on h.\\nBasis:Showthat it’s true for hD0(i.e., that #of leaves \\x14˙\\nn=2hC1\\x07\\nDdn=2e).\\nInfact, we’ll show that the #of leaves Ddn=2e.\\nThetree leaves (nodes at height 0) are at depths HandH/NUL1. Theyconsist of\\n\\x0fall nodes at depth H,and\\n\\x0fthe nodes at depth H/NUL1that are not parents of depth- Hnodes.\\nLetxbe the number of nodes at depth H—that is, the number of nodes in the\\nbottom (possibly incomplete) level.\\nNote that n/NULxis odd, because the n/NULxnodes above the bottom level form a\\ncomplete binary tree, and a complete binary tree has an odd nu mber of nodes (1\\nless than apower of 2). Thusif nis odd, xiseven, and if nis even, xisodd.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 103}),\n",
              " Document(page_content='6-12 Solutions for Chapter 6: Heapsort\\nTo prove the base case, we must consider separately the case i n which nis even\\n(xis odd) and the case in which nis odd ( xis even). Here are two ways to do\\nthis: The ﬁrst requires more cleverness, and the second requ ires more algebraic\\nmanipulation.\\n1. First method of proving the base case:\\n\\x0fIfnis odd, then xis even, so all nodes have siblings—i.e., all internal\\nnodes have 2 children. Thus (see Exercise B.5-3), #of intern al nodesD\\n# of leaves/NUL1.\\nSo,nD#of nodesD#of leavesC#of internal nodesD2\\x01# of leaves/NUL1.\\nThus,# of leavesD.nC1/=2Ddn=2e. (Thelatterequalityholdsbecause n\\nis odd.)\\n\\x0fIfnis even, then xis odd, and some leaf doesn’t have a sibling. If we gave\\nit a sibling, we would have nC1nodes, where nC1is odd, so the case\\nwe analyzed above would apply. Observe that we would also inc rease the\\nnumber of leaves by 1, since we added a node to a parent that alr eady had\\na child. By the odd-node case above, #of leaves C1Dd.nC1/=2eD\\ndn=2eC1. (Thelatter equality holds because niseven.)\\nIneither case, #of leaves Ddn=2e.\\n2. Second method of proving the base case:\\nNote that at any depth d < Hthere are 2dnodes, because all such tree levels\\nare complete.\\n\\x0fIfxiseven, there are x=2nodes at depth H/NUL1that are parents of depth H\\nnodes, hence 2H/NUL1/NULx=2nodesatdepth H/NUL1thatarenotparentsofdepth-\\nHnodes. Thus,\\ntotal #of height-0 nodes DxC2H/NUL1/NULx=2\\nD2H/NUL1Cx=2\\nD.2HCx/=2\\nD˙\\n.2HCx/NUL1/=2\\x07\\n(because xiseven)\\nDdn=2e:\\n(nD2HCx/NUL1because thecompletetreedowntodepth H/NUL1has2H/NUL1\\nnodes and depth Hhasxnodes.)\\n\\x0fIfxisodd, by anargument similar tothe even case, wesee that\\n# of height-0 nodes DxC2H/NUL1/NUL.xC1/=2\\nD2H/NUL1C.x/NUL1/=2\\nD.2HCx/NUL1/=2\\nDn=2\\nDdn=2e(because xodd)neven) :\\nInductive step: Show that if it’s true for height h/NUL1, it’s true for h.\\nLetnhbethe number of nodes at height hin the n-node tree T.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 104}),\n",
              " Document(page_content='Solutions forChapter 6: Heapsort 6-13\\nConsiderthetree T0formedbyremovingtheleavesof T. Ithas n0Dn/NULn0nodes.\\nWe know from the base case that n0Ddn=2e, son0Dn/NULn0Dn/NULdn=2eD\\nbn=2c.\\nNote that the nodes at height hinTwould be at height h/NUL1if the leaves of the\\ntreewereremoved—that is, they areat height h/NUL1inT0. Letting n0\\nh/NUL1denote the\\nnumber of nodes at height h/NUL1inT0,wehave\\nnhDn0\\nh/NUL1:\\nByinduction, wecan bound n0\\nh/NUL1:\\nnhDn0\\nh/NUL1\\x14˙\\nn0=2h\\x07\\nD˙\\nbn=2c=2h\\x07\\n\\x14˙\\n.n=2/=2h\\x07\\nD˙\\nn=2hC1\\x07\\n:\\nAlternative solution\\nAnalternative solution relies onfour facts:\\n1. Every node noton the unique simple path from the last leaf to the root is the\\nroot of acomplete binary subtree.\\n2. A node that is the root of a complete binary subtree and has h eight his the\\nancestor of 2hleaves.\\n3. ByExercise 6.1-7, an n-element heap hasdn=2eleaves.\\n4. For nonnegative reals aandb,wehavedae\\x01b\\x15dabe.\\nThe proof is by contradiction. Assume that an n-element heap contains at least˙\\nn=2hC1\\x07\\nC1nodes of height h. Exactly one node of height his on the unique\\nsimple path from the last leaf to the root, and the subtree roo ted at this node has\\nat least one leaf (that being the last leaf). All other nodes o f height h, of which\\ntheheap contains at least˙\\nn=2hC1\\x07\\n,arethe roots ofcomplete binary subtrees, and\\neach such node is the root of a subtree with 2hleaves. Moreover, each subtree\\nwhose root is at height his disjoint. Therefore, the number of leaves in the entire\\nheap isat leastln\\n2hC1m\\n\\x012hC1\\x15ln\\n2hC1\\x012hm\\nC1\\nDln\\n2m\\nC1 ;\\nwhich contradicts the property that an n-element heap hasdn=2eleaves.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 105}),\n",
              " Document(page_content='6-14 Solutions for Chapter 6: Heapsort\\nSolution to Exercise6.4-1\\nThis solutionisalsopostedpublicly\\n(b) (c)\\n(d) (e) (f)\\n(g) (h) (i)\\n2 4 5 7 8 13 17 20 25204\\n2 5\\n7 8 13 17\\n252\\n4 5\\n7 8 13 17\\n25205\\n4 2\\n17 138 7\\n20 257\\n4 5\\n17 138 2\\n20 2513\\n5 8\\n2 7 4 17\\n25208\\n7 5\\n17 134 2\\n20 2517\\n13 5\\n2 47 8\\n252020\\n13 17\\n2 47 8\\n255\\nAi\\ni\\ni i i\\nii i(a)25\\n13 20\\n2 177 8\\n45', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 106}),\n",
              " Document(page_content='Solutions forChapter 6: Heapsort 6-15\\nSolutionto Exercise 6.5-2\\nThissolutionisalsopostedpublicly\\n2 22 2\\n81 818\\n1 10i8\\n1 -∞15\\n13 9\\n5 12 8 7\\n4 0 6\\n(a)15\\n13 9\\n5 12 8 7\\n406\\n(b)\\n15\\n13 9\\n012 10 7\\n45\\n6\\n(c)i15\\n510\\n012 9 7\\n413\\n6\\n(d)i\\nSolutionto Exercise 6.5-6\\nChange the procedure to thefollowing:\\nHEAP-INCREASE -KEY.A; i;key/\\nifkey< AŒi\\x8d\\nerror“new key issmaller than current key”\\nAŒi\\x8dDkey\\nwhile i > 1andAŒPARENT .i/\\x8d < AŒi\\x8d\\nAŒi\\x8dDAŒPARENT .i/\\x8d\\niDPARENT .i/\\nAŒi\\x8dDkey\\nSolutionto Problem 6-1\\nThissolutionisalsopostedpublicly\\na.The procedures B UILD-MAX-HEAPand BUILD-MAX-HEAP0do not always\\ncreate thesameheapwhenrunonthesameinput array. Conside r thefollowing\\ncounterexample.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 107}),\n",
              " Document(page_content='6-16 Solutions for Chapter 6: Heapsort\\nInput array A:\\n1 2 3A\\nBUILD-MAX-HEAP.A/:\\n1\\n3 23\\n1 23 2 1A\\nBUILD-MAX-HEAP0.A/:\\n1\\n-∞2\\n-∞ 13\\n2 13 1 2A\\nb.Anupper bound of O.nlgn/timefollows immediately from there being n/NUL1\\ncalls to M AX-HEAP-INSERT, each taking O.lgn/time. For a lower bound\\nof\\x7f.nlgn/, consider the case in which the input array is given in strict ly in-\\ncreasing order. Each call to M AX-HEAP-INSERTcauses H EAP-INCREASE -\\nKEYto go all the way up to the root. Since the depth of node iisblgic, the\\ntotal time is\\nnX\\niD1‚.blgic/\\x15nX\\niDdn=2e‚.blgdn=2ec/\\n\\x15nX\\niDdn=2e‚.blg.n=2/c/\\nDnX\\niDdn=2e‚.blgn/NUL1c/\\n\\x15n=2\\x01‚.lgn/\\nD\\x7f.nlgn/ :\\nIn the worst case, therefore, B UILD-MAX-HEAP0requires ‚.nlgn/time to\\nbuild an n-element heap.\\nSolution to Problem 6-2\\na.We can represent a d-ary heap in a 1-dimensional array as follows. The root\\nresides in AŒ1\\x8d, itsdchildren reside in order in AŒ2\\x8dthrough AŒdC1\\x8d, their\\nchildren reside in order in AŒdC2\\x8dthrough AŒd2CdC1\\x8d, and so on. The\\nfollowing two procedures map a node with index ito its parent and to its jth\\nchild (for 1\\x14j\\x14d),respectively.\\nD-ARY-PARENT .i/\\nreturnb.i/NUL2/=dC1c\\nD-ARY-CHILD .i; j /\\nreturn d.i/NUL1/CjC1', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 108}),\n",
              " Document(page_content='Solutions forChapter 6: Heapsort 6-17\\nToconvince yourself that these procedures really work, ver ify that\\nD-ARY-PARENT .D-ARY-CHILD .i; j //Di ;\\nfor any 1\\x14j\\x14d. Notice that the binary heap procedures are a special case\\nof the above procedures when dD2.\\nb.Since each node has dchildren, the height of a d-ary heap with nnodes is\\n‚.logdn/D‚.lgn=lgd/.\\nc.Theprocedure H EAP-EXTRACT-MAXgiveninthetext for binary heaps works\\nﬁnefor d-aryheapstoo. Thechangeneededtosupport d-aryheapsisinM AX-\\nHEAPIFY, which must compare the argument node to all dchildren instead of\\njust 2 children. Therunning timeof H EAP-EXTRACT-MAXisstill the running\\ntimefor M AX-HEAPIFY, butthatnowtakesworst-casetimeproportional tothe\\nproduct of the height of the heap by the number of children exa mined at each\\nnode (at most d), namely ‚.dlogdn/D‚.dlgn=lgd/.\\nd.The procedure M AX-HEAP-INSERTgiven in the text for binary heaps works\\nﬁnefor d-aryheapstoo,assumingthatH EAP-INCREASE -KEYworksfor d-ary\\nheaps. The worst-case running time is still ‚.h/, where his the height of the\\nheap. (Since only parent pointers are followed, the number o f children a node\\nhas is irrelevant.) Fora d-ary heap, this is ‚.logdn/D‚.lgn=lgd/.\\ne.TheHEAP-INCREASE -KEYprocedurewithtwosmallchangesworksfor d-ary\\nheaps. First, because the problem speciﬁes that the new key i s given by the\\nparameter k,change instances of the variable keytok. Second, change calls of\\nPARENTto calls of D-ARY-PARENTfrom part (a).\\nIn the worst case, the entire height of the tree must be traver sed, so the worst-\\ncase running timeis ‚.h/D‚.logdn/D‚.lgn=lgd/.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 109}),\n",
              " Document(page_content='Lecture Notes forChapter 7:\\nQuicksort\\nChapter 7 overview\\n[Thetreatment inthesecond andthirdeditions differsfrom thatoftheﬁrstedition.\\nWe use a different partitioning method—known as “Lomuto par titioning”—in the\\nsecond andthird editions, rather than the“Hoarepartition ing” usedintheﬁrstedi-\\ntion. Using Lomuto partitioning helps simplify the analysi s, which uses indicator\\nrandom variables inthe second edition.]\\nQuicksort\\n\\x0fWorst-case running time: ‚.n2/.\\n\\x0fExpected running time: ‚.nlgn/.\\n\\x0fConstants hidden in ‚.nlgn/aresmall.\\n\\x0fSorts inplace.\\nDescriptionofquicksort\\nQuicksort isbased on the three-step process of divide-and- conquer.\\n\\x0fTosort the subarray AŒp : : r\\x8d:\\nDivide:Partition AŒp : : r\\x8d, into two (possibly empty) subarrays AŒp : : q/NUL1\\x8d\\nandAŒqC1 : : r\\x8d,suchthat eachelement intheﬁrstsubarray AŒp : : q/NUL1\\x8dis\\n\\x14AŒq\\x8dandAŒq\\x8dis\\x14each element inthe second subarray AŒqC1 : : r\\x8d.\\nConquer: Sort the twosubarrays by recursive calls to Q UICKSORT .\\nCombine: Noworkisneededtocombinethesubarrays,becausetheyares orted\\ninplace.\\n\\x0fPerform the divide step by a procedure P ARTITION , which returns the index q\\nthat marks the position separating the subarrays.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 110}),\n",
              " Document(page_content='7-2 Lecture Notes for Chapter 7: Quicksort\\nQUICKSORT .A; p; r/\\nifp < r\\nqDPARTITION .A; p; r/\\nQUICKSORT .A; p; q/NUL1/\\nQUICKSORT .A; qC1; r/\\nInitial call isQ UICKSORT .A; 1; n/.\\nPartitioning\\nPartition subarray AŒp : : r\\x8dbythe following procedure:\\nPARTITION .A; p; r/\\nxDAŒr\\x8d\\niDp/NUL1\\nforjDptor/NUL1\\nifAŒj \\x8d\\x14x\\niDiC1\\nexchange AŒi\\x8dwithAŒj \\x8d\\nexchange AŒiC1\\x8dwithAŒr\\x8d\\nreturn iC1\\n\\x0fPARTITION always selects thelast element AŒr\\x8dinthesubarray AŒp : : r\\x8dasthe\\npivot—the element around which topartition.\\n\\x0fAs the procedure executes, the array is partitioned into fou r regions, some of\\nwhich maybe empty:\\nLoop invariant:\\n1. All entries in AŒp : : i\\x8dare\\x14pivot.\\n2. All entries in AŒiC1 : : j/NUL1\\x8dare>pivot.\\n3.AŒr\\x8dDpivot.\\nIt’snotneededaspartoftheloopinvariant, butthefourthr egionis AŒj : : r/NUL1\\x8d,\\nwhose entries have not yet been examined, and so we don’t know how they\\ncompare tothe pivot.\\nExample\\nOnan 8-element subarray.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 111}),\n",
              " Document(page_content='Lecture Notes for Chapter 7: Quicksort 7-3\\n8 1 6 4 0 3 9 5p,j ri\\n8 1 6 4 0 3 9 5p rj\\n1 8 6 4 0 3 9 5p,i rj\\n1 8 6 4 0 3 9 5p,i r j\\n1 864 0 3 9 5p r j i\\n1 8 64 0 3 9 5p r j i\\n1 3 64 0 8 9 5p r j i\\n1 3 64 0 8 9 5p r i\\n1 6 5 4 0 8 93p r ii\\nA[r]: pivot\\nA[j .. r–1]: not yet examined\\nA[i+1 .. j–1]: known to be > pivot\\nA[p .. i]: known to be ≤ pivot\\n[Theindex jdisappearsbecauseitisnolongerneededoncethe forloopisexited.]\\nCorrectness\\nUsethe loop invariant toprove correctness of P ARTITION :\\nInitialization: Before the loop starts, all the conditions of the loop invari ant are\\nsatisﬁed, because risthepivot andthesubarrays AŒp : : i\\x8dandAŒiC1 : : j/NUL1\\x8d\\nare empty.\\nMaintenance: While theloopisrunning, if AŒj \\x8d\\x14pivot,then AŒj \\x8dandAŒiC1\\x8d\\nareswapped andthen iandjareincremented. If AŒj \\x8d >pivot,thenincrement\\nonlyj.\\nTermination: When the loop terminates, jDr, so all elements in Aare parti-\\ntioned into one of the three cases: AŒp : : i\\x8d\\x14pivot, AŒiC1 : : r/NUL1\\x8d >pivot,\\nandAŒr\\x8dDpivot.\\nThelast twolines of P ARTITION movethe pivot element from the end of the array\\nto between the two subarrays. This is done by swapping the piv ot and the ﬁrst\\nelement of the second subarray, i.e., byswapping AŒiC1\\x8dandAŒr\\x8d.\\nTimefor partitioning\\n‚.n/to partition an n-element subarray.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 112}),\n",
              " Document(page_content='7-4 Lecture Notes for Chapter 7: Quicksort\\nPerformance ofquicksort\\nTherunning timeof quicksort depends on the partitioning of the subarrays:\\n\\x0fIf the subarrays are balanced, then quicksort can runas fast asmergesort.\\n\\x0fIf they are unbalanced, then quicksort canrun as slowly asin sertion sort.\\nWorst case\\n\\x0fOccurs when thesubarrays arecompletely unbalanced.\\n\\x0fHave0elements in one subarray and n/NUL1elements inthe other subarray.\\n\\x0fGet the recurrence\\nT .n/DT .n/NUL1/CT .0/C‚.n/\\nDT .n/NUL1/C‚.n/\\nD‚.n2/ :\\n\\x0fSamerunning timeas insertion sort.\\n\\x0fIn fact, the worst-case running time occurs when quicksort t akes a sorted array\\nasinput, but insertion sort runs in O.n/timeinthis case.\\nBest case\\n\\x0fOccurs when thesubarrays arecompletely balanced every tim e.\\n\\x0fEachsubarray has\\x14n=2elements.\\n\\x0fGet the recurrence\\nT .n/D2T .n=2/C‚.n/\\nD‚.nlgn/ :\\nBalanced partitioning\\n\\x0fQuicksort’s average running time is much closer to the best c ase than to the\\nworst case.\\n\\x0fImagine that P ARTITION always produces a9-to-1 split.\\n\\x0fGet the recurrence\\nT .n/\\x14T .9n=10/CT .n=10/C‚.n/\\nDO.nlgn/ :\\n\\x0fIntuition: look at the recursion tree.\\n\\x0fIt’s like theone for T .n/DT .n=3/CT .2n=3/CO.n/inSection 4.4.\\n\\x0fExcept that here the constants are different; we get log10nfull levels and\\nlog10=9nlevels that arenonempty.\\n\\x0fAs long as it’s a constant, the base of the log doesn’t matter i n asymptotic\\nnotation.\\n\\x0fAny split of constant proportionality will yield a recursio n tree of depth\\n‚.lgn/.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 113}),\n",
              " Document(page_content='Lecture Notes for Chapter 7: Quicksort 7-5\\nIntuition for theaverage case\\n\\x0fSplits in therecursion tree will not always beconstant.\\n\\x0fThere will usually be a mix of good and bad splits throughout t he recursion\\ntree.\\n\\x0fToseethat thisdoesn’t affect theasymptotic running timeo fquicksort, assume\\nthat levels alternate between best-case and worst-case spl its.\\nn\\n0 n–1n\\n(n–1)/2 ( n–1)/2Θ(n) Θ(n)\\n(n–1)/2 (n–1)/2 – 1\\n\\x0fThe extra level in the left-hand ﬁgure only adds to the consta nt hidden in the\\n‚-notation.\\n\\x0fThere are still the same number of subarrays to sort, and only twice as much\\nwork wasdone toget tothat point.\\n\\x0fBoth ﬁgures result in O.nlgn/time, though the constant for the ﬁgure on the\\nleft is higher than that of the ﬁgure on theright.\\nRandomized versionofquicksort\\n\\x0fWehave assumed that all input permutations areequally like ly.\\n\\x0fThis isnot always true.\\n\\x0fTocorrect this, weadd randomization to quicksort.\\n\\x0fWecould randomly permute the input array.\\n\\x0fInstead, weuse random sampling , or picking one element at random.\\n\\x0fDon’talwaysuse AŒr\\x8dasthepivot. Instead, randomlypickanelementfromthe\\nsubarray that isbeing sorted.\\nRANDOMIZED -PARTITION .A; p; r/\\niDRANDOM .p; r/\\nexchange AŒr\\x8dwithAŒi\\x8d\\nreturnPARTITION .A; p; r/\\nRandomly selecting the pivot element will, on average, caus e the split of the input\\narray to bereasonably well balanced.\\nRANDOMIZED -QUICKSORT .A; p; r/\\nifp < r\\nqDRANDOMIZED -PARTITION .A; p; r/\\nRANDOMIZED -QUICKSORT .A; p; q/NUL1/\\nRANDOMIZED -QUICKSORT .A; qC1; r/', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 114}),\n",
              " Document(page_content='7-6 Lecture Notes for Chapter 7: Quicksort\\nRandomization of quicksort stops any speciﬁc type of array f rom causing worst-\\ncase behavior. For example, an already-sorted array causes worst-case behavior in\\nnon-randomized Q UICKSORT , but not in R ANDOMIZED -QUICKSORT .\\nAnalysisofquicksort\\nWewill analyze\\n\\x0fthe worst-case running time of Q UICKSORT and RANDOMIZED -QUICKSORT\\n(the same), and\\n\\x0fthe expected (average-case) running timeof R ANDOMIZED -QUICKSORT .\\nWorst-case analysis\\nWe will prove that a worst-case split at every level produces a worst-case running\\ntime of O.n2/.\\n\\x0fRecurrence for the worst-case running timeof Q UICKSORT :\\nT .n/Dmax\\n0\\x14q\\x14n/NUL1.T .q/CT .n/NULq/NUL1//C‚.n/ :\\n\\x0fBecause P ARTITION produces two subproblems, totaling size n/NUL1,qranges\\nfrom 0to n/NUL1.\\n\\x0fGuess: T .n/\\x14cn2, for some c.\\n\\x0fSubstituting our guess into the above recurrence:\\nT .n/\\x14max\\n0\\x14q\\x14n/NUL1.cq2Cc.n/NULq/NUL1/2/C‚.n/\\nDc\\x01max\\n0\\x14q\\x14n/NUL1.q2C.n/NULq/NUL1/2/C‚.n/ :\\n\\x0fThemaximum value of .q2C.n/NULq/NUL1/2/occurs when qiseither 0orn/NUL1.\\n(Second derivative withrespect to qispositive.) Therefore,\\nmax\\n0\\x14q\\x14n/NUL1.q2C.n/NULq/NUL1/2/\\x14.n/NUL1/2\\nDn2/NUL2nC1 :\\n\\x0fAndthus,\\nT .n/\\x14cn2/NULc.2n/NUL1/C‚.n/\\n\\x14cn2ifc.2n/NUL1/\\x15‚.n/ :\\n\\x0fPickcso that c.2n/NUL1/dominates ‚.n/.\\n\\x0fTherefore, theworst-case running timeof quicksort is O.n2/.\\n\\x0fCan also show that the recurrence’s solution is \\x7f.n2/. Thus, the worst-case\\nrunning time is ‚.n2/.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 115}),\n",
              " Document(page_content='Lecture Notes for Chapter 7: Quicksort 7-7\\nAverage-case analysis\\n\\x0fThedominant cost of the algorithm is partitioning.\\n\\x0fPARTITION removes thepivot element from future consideration each ti me.\\n\\x0fThus, P ARTITION iscalled at most ntimes.\\n\\x0fQUICKSORT recurses on the partitions.\\n\\x0fThe amount of work that each call to P ARTITION does is a constant plus the\\nnumber of comparisons that areperformed inits forloop.\\n\\x0fLetXDthetotal number ofcomparisons performed inallcallsto P ARTITION .\\n\\x0fTherefore, the total work done over the entire execution is O.nCX/.\\nWewill now compute abound on the overall number of compariso ns.\\nForease of analysis:\\n\\x0fRename the elements of Aas´1; ´2; : : : ; ´ n, with ´ibeing the ith smallest\\nelement.\\n\\x0fDeﬁne the set ZijDf´i; ´iC1; : : : ; ´ jgto be the set of elements between ´i\\nand´j,inclusive.\\nEach pair of elements is compared at most once, because eleme nts are compared\\nonly to the pivot element, and then the pivot element is never in any later call to\\nPARTITION .\\nLetXijDIf´iis compared to ´jg.\\n(Considering whether ´iis compared to ´jat any time during the entire quicksort\\nalgorithm, not just during one call of P ARTITION .)\\nSince each pair is compared at most once, the total number of c omparisons per-\\nformed by thealgorithm is\\nXDn/NUL1X\\niD1nX\\njDiC1Xij:\\nTakeexpectations of both sides, use Lemma5.1 and linearity of expectation:\\nEŒX\\x8dDE\"n/NUL1X\\niD1nX\\njDiC1Xij#\\nDn/NUL1X\\niD1nX\\njDiC1EŒXij\\x8d\\nDn/NUL1X\\niD1nX\\njDiC1Prf´iiscompared to ´jg:\\nNowall wehave to do isﬁndthe probability that twoelements a recompared.\\n\\x0fThink about when twoelements are notcompared.\\n\\x0fFor example, numbers in separate partitions will not becomp ared.\\n\\x0fInthe previous example, h8; 1; 6; 4; 0; 3; 9; 5iand thepivot is5, sothat none\\nof the setf1; 4; 0; 3gwill ever be compared toany of the set f8; 6; 9g.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 116}),\n",
              " Document(page_content='7-8 Lecture Notes for Chapter 7: Quicksort\\n\\x0fOnce a pivot xis chosen such that ´i< x < ´ j, then ´iand´jwill never be\\ncompared at any later time.\\n\\x0fIf either ´ior´jis chosen before any other element of Zij, then it will be\\ncompared toall the elements of Zij,except itself.\\n\\x0fThe probability that ´iis compared to ´jis the probability that either ´ior´j\\nisthe ﬁrst element chosen.\\n\\x0fThereare j/NULiC1elements,andpivotsarechosenrandomlyandindependently .\\nThus, the probability that any particular one of them is the ﬁ rst one chosen is\\n1=.j/NULiC1/.\\nTherefore,\\nPrf´iis compared to ´jgDPrf´ior´jisthe ﬁrst pivot chosen from Zijg\\nDPrf´iisthe ﬁrst pivot chosen from Zijg\\nCPrf´jisthe ﬁrst pivot chosen from Zijg\\nD1\\nj/NULiC1C1\\nj/NULiC1\\nD2\\nj/NULiC1:\\n[The second line follows because thetwo events are mutually exclusive.]\\nSubstituting into the equation for E ŒX\\x8d:\\nEŒX\\x8dDn/NUL1X\\niD1nX\\njDiC12\\nj/NULiC1:\\nEvaluatebyusingachangeinvariables( kDj/NULi)andtheboundontheharmonic\\nseries inequation (A.7):\\nEŒX\\x8dDn/NUL1X\\niD1nX\\njDiC12\\nj/NULiC1\\nDn/NUL1X\\niD1n/NULiX\\nkD12\\nkC1\\n<n/NUL1X\\niD1nX\\nkD12\\nk\\nDn/NUL1X\\niD1O.lgn/\\nDO.nlgn/ :\\nSo the expected running time of quicksort, using R ANDOMIZED -PARTITION , is\\nO.nlgn/.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 117}),\n",
              " Document(page_content='SolutionsforChapter 7:\\nQuicksort\\nSolutionto Exercise 7.2-3\\nThissolutionisalsopostedpublicly\\nPARTITION does a “worst-case partitioning” when the elements are in de creasing\\norder. Itreducesthesizeofthesubarrayunderconsiderati on byonly 1ateachstep,\\nwhich we’ve seen has running time ‚.n2/.\\nIn particular, P ARTITION , given a subarray AŒp : : r\\x8d of distinct elements in de-\\ncreasing order, produces an empty partition in AŒp : : q/NUL1\\x8d, puts the pivot (orig-\\ninally in AŒr\\x8d) into AŒp\\x8d, and produces a partition AŒpC1 : : r\\x8dwith only one\\nfewer element than AŒp : : r\\x8d. The recurrence for Q UICKSORT becomes T .n/D\\nT .n/NUL1/C‚.n/,which has the solution T .n/D‚.n2/.\\nSolutionto Exercise 7.2-5\\nThissolutionisalsopostedpublicly\\nThe minimum depth follows a path that always takes the smalle r part of the parti-\\ntion—i.e., that multiplies the number of elements by ˛. One iteration reduces the\\nnumber of elements from nto˛n,and iiterations reduces thenumber ofelements\\nto˛in. At a leaf, there is just one remaining element, and so at a min imum-depth\\nleaf of depth m, we have ˛mnD1. Thus, ˛mD1=n. Taking logs, we get\\nmlg˛D/NULlgn, ormD/NULlgn=lg˛.\\nSimilarly, maximum depth corresponds to always taking the l arger part of the par-\\ntition, i.e., keeping a fraction 1/NUL˛of the elements each time. The maximum\\ndepth Mis reached when there is one element left, that is, when .1/NUL˛/MnD1.\\nThus, MD/NULlgn=lg.1/NUL˛/.\\nAll these equations are approximate because weare ignoring ﬂoors and ceilings.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 118}),\n",
              " Document(page_content='7-10 Solutions for Chapter 7: Quicksort\\nSolution to Exercise7.3-1\\nWemay be interested in the worst-case performance, but in th at case, the random-\\nization is irrelevant: it won’t improve the worst case. What randomization can do\\nis makethe chance of encountering aworst-case scenario sma ll.\\nSolution to Exercise7.4-2\\nTo show that quicksort’s best-case running time is \\x7f.nlgn/, we use a technique\\nsimilar to the one used in Section 7.4.1 to show that its worst -case running time\\nisO.n2/.\\nLetT .n/bethebest-casetimefortheprocedure Q UICKSORT onaninputofsize n.\\nWehave the recurrence\\nT .n/Dmin\\n1\\x14q\\x14n/NUL1.T .q/CT .n/NULq/NUL1//C‚.n/ :\\nWeguess that T .n/\\x15cnlgnfor some constant c. Substituting this guess into the\\nrecurrence, weobtain\\nT .n/\\x15min\\n1\\x14q\\x14n/NUL1.cqlgqCc.n/NULq/NUL1/lg.n/NULq/NUL1//C‚.n/\\nDc\\x01min\\n1\\x14q\\x14n/NUL1.qlgqC.n/NULq/NUL1/lg.n/NULq/NUL1//C‚.n/ :\\nAswe’ll show below, the expression qlgqC.n/NULq/NUL1/lg.n/NULq/NUL1/achieves a\\nminimumovertherange 1\\x14q\\x14n/NUL1when qDn/NULq/NUL1,orqD.n/NUL1/=2,since\\ntheﬁrstderivative oftheexpression withrespect to qis0when qD.n/NUL1/=2and\\nthe second derivative of the expression is positive. (It doe sn’t matter that qis not\\nan integer when niseven, since we’re just trying todetermine the minimum val ue\\nof a function, knowing that when we constrain qto integer values, the function’s\\nvalue will beno lower.)\\nChoosing qD.n/NUL1/=2gives us the bound\\nmin\\n1\\x14q\\x14n/NUL1.qlgqC.n/NULq/NUL1/lg.n/NULq/NUL1/\\n\\x15n/NUL1\\n2lgn/NUL1\\n2C\\x12\\nn/NULn/NUL1\\n2/NUL1\\x13\\nlg\\x12\\nn/NULn/NUL1\\n2/NUL1\\x13\\nD.n/NUL1/lgn/NUL1\\n2:\\nContinuing with our bounding of T .n/, weobtain, for n\\x152,\\nT .n/\\x15c.n/NUL1/lgn/NUL1\\n2C‚.n/\\nDc.n/NUL1/lg.n/NUL1//NULc.n/NUL1/C‚.n/\\nDcnlg.n/NUL1//NULclg.n/NUL1//NULc.n/NUL1/C‚.n/\\n\\x15cnlg.n=2//NULclg.n/NUL1//NULc.n/NUL1/C‚.n/(since n\\x152)\\nDcnlgn/NULcn/NULclg.n/NUL1//NULcnCcC‚.n/\\nDcnlgn/NUL.2cnCclg.n/NUL1//NULc/C‚.n/\\n\\x15cnlgn ;', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 119}),\n",
              " Document(page_content='Solutions forChapter 7: Quicksort 7-11\\nsincewecanpicktheconstant csmallenough sothat the ‚.n/termdominatesthe\\nquantity 2cnCclg.n/NUL1//NULc. Thus, the best-case running time of quicksort is\\n\\x7f.nlgn/.\\nLetting f .q/DqlgqC.n/NULq/NUL1/lg.n/NULq/NUL1/, we now show how to ﬁnd the\\nminimum value of this function in the range 1\\x14q\\x14n/NUL1. We need to ﬁnd the\\nvalue of qfor which the derivative of fwith respect to qis 0. We rewrite this\\nfunction as\\nf .q/DqlnqC.n/NULq/NUL1/ln.n/NULq/NUL1/\\nln2;\\nand so\\nf0.q/Dd\\ndq\\x12qlnqC.n/NULq/NUL1/ln.n/NULq/NUL1/\\nln2\\x13\\nDlnqC1/NULln.n/NULq/NUL1//NUL1\\nln2\\nDlnq/NULln.n/NULq/NUL1/\\nln2:\\nThe derivative f0.q/is0when qDn/NULq/NUL1, or when qD.n/NUL1/=2. Toverify\\nthatqD.n/NUL1/=2is indeed a minimum (not a maximum or an inﬂection point),\\nweneed tocheck that the second derivative of fispositive at qD.n/NUL1/=2:\\nf00.q/Dd\\ndq\\x12lnq/NULln.n/NULq/NUL1/\\nln2\\x13\\nD1\\nln2\\x121\\nqC1\\nn/NULq/NUL1\\x13\\nf00\\x12n/NUL1\\n2\\x13\\nD1\\nln2\\x122\\nn/NUL1C2\\nn/NUL1\\x13\\nD1\\nln2\\x014\\nn/NUL1\\n> 0 (since n\\x152):\\nSolutionto Problem 7-2\\na.Ifallelementsareequal,thenwhenP ARTITION returns, qDrandallelements\\ninAŒp : : q/NUL1\\x8dareequal. Wegettherecurrence T .n/DT .n/NUL1/CT .0/C‚.n/\\nfor the running time, and so T .n/D‚.n2/.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 120}),\n",
              " Document(page_content='7-12 Solutions for Chapter 7: Quicksort\\nb.The PARTITION0procedure:\\nPARTITION0.A; p; r/\\nxDAŒp\\x8d\\niDhDp\\nforjDpC1tor\\n//Invariant: AŒp : : i/NUL1\\x8d < x,AŒi : : h\\x8dDx,\\nAŒhC1 : : j/NUL1\\x8d > x,AŒj : : r\\x8dunknown.\\nifAŒj \\x8d < x\\nyDAŒj \\x8d\\nAŒj \\x8dDAŒhC1\\x8d\\nAŒhC1\\x8dDAŒi\\x8d\\nAŒi\\x8dDy\\niDiC1\\nhDhC1\\nelseif AŒj \\x8d==x\\nexchange AŒhC1\\x8dwithAŒj \\x8d\\nhDhC1\\nreturn .i; h/\\nc.RANDOMIZED -PARTITION0is the same as R ANDOMIZED -PARTITION , but\\nwiththe call to P ARTITION replaced by acall to P ARTITION0.\\nQUICKSORT0.A; p; r/\\nifp < r\\n.q; t/DRANDOMIZED -PARTITION0.A; p; r/\\nQUICKSORT0.A; p; q/NUL1/\\nQUICKSORT0.A; tC1; r/\\nd.Putting elements equal to the pivot in the same partition as t he pivot can only\\nhelp us, because we do not recurse on elements equal to the piv ot. Thus, the\\nsubproblem sizes with Q UICKSORT0, even with equal elements, are no larger\\nthan the subproblem sizes with Q UICKSORT when all elements are distinct.\\nSolution to Problem 7-4\\na.QUICKSORT0does exactly what Q UICKSORT does; hence it sorts correctly.\\nQUICKSORT and QUICKSORT0do the same partitioning, and then each calls\\nitself with arguments A; p; q/NUL1. QUICKSORT then calls itself again, with\\narguments A; qC1; r. QUICKSORT0instead sets pDqC1and performs\\nanother iteration of its whileloop. Thisexecutesthesameoperations ascalling\\nitself with A; qC1; r, because in both cases, the ﬁrst and third arguments ( A\\nandr) have the samevalues as before, and phas the old value of qC1.\\nb.The stack depth of Q UICKSORT0will be ‚.n/on an n-element input array if\\nthere are ‚.n/recursive calls to Q UICKSORT0. This happens if every call to\\nPARTITION .A; p; r/ returns qDr. The sequence of recursive calls in this\\nscenario is', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 121}),\n",
              " Document(page_content='Solutions forChapter 7: Quicksort 7-13\\nQUICKSORT0.A; 1; n/ ;\\nQUICKSORT0.A; 1; n/NUL1/ ;\\nQUICKSORT0.A; 1; n/NUL2/ ;\\n:::\\nQUICKSORT0.A; 1; 1/ :\\nAny array that is already sorted in increasing order will cau se QUICKSORT0to\\nbehave this way.\\nc.Theproblem demonstrated bythe scenario inpart (b) isthat e ach invocation of\\nQUICKSORT0calls QUICKSORT0again with almost the same range. To avoid\\nsuch behavior, we must change Q UICKSORT0so that the recursive call is on a\\nsmaller interval of the array. The following variation of Q UICKSORT0checks\\nwhich of the two subarrays returned from P ARTITION is smaller and recurses\\nonthesmallersubarray, whichisatmosthalfthesizeofthec urrentarray. Since\\nthe array size is reduced by at least half on each recursive ca ll, the number of\\nrecursive calls, and hence the stack depth, is ‚.lgn/in the worst case. Note\\nthat this method works no matter how partitioning is perform ed (as long as\\nthe PARTITION procedure has the same functionality as the procedure given in\\nSection 7.1).\\nQUICKSORT00.A; p; r/\\nwhile p < r\\n//Partition and sort thesmall subarray ﬁrst.\\nqDPARTITION .A; p; r/\\nifq/NULp < r/NULq\\nQUICKSORT00.A; p; q/NUL1/\\npDqC1\\nelseQUICKSORT00.A; qC1; r/\\nrDq/NUL1\\nThe expected running time is not affected, because exactly t he same work is\\ndone as before: the same partitions are produced, and the sam e subarrays are\\nsorted.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 122}),\n",
              " Document(page_content='Lecture Notes forChapter 8:\\nSortinginLinearTime\\nChapter 8 overview\\nHowfast can wesort?\\nWewill prove alower bound, then beat it byplaying adifferen t game.\\nComparison sorting\\n\\x0fTheonlyoperationthatmaybeusedtogainorderinformation aboutasequence\\nis comparison of pairs of elements.\\n\\x0fAll sorts seen so far are comparison sorts: insertion sort, s election sort, merge\\nsort, quicksort, heapsort, treesort.\\nLowerbounds forsorting\\nLowerbounds\\n\\x0f\\x7f.n/to examine all the input.\\n\\x0fAll sorts seen sofar are \\x7f.nlgn/.\\n\\x0fWe’ll show that \\x7f.nlgn/is alower bound for comparison sorts.\\nDecision tree\\n\\x0fAbstraction of anycomparison sort.\\n\\x0fRepresents comparisons made by\\n\\x0faspeciﬁc sorting algorithm\\n\\x0fon inputs of agiven size.\\n\\x0fAbstracts away everything else: control and data movement.\\n\\x0fWe’re counting onlycomparisons.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 123}),\n",
              " Document(page_content='8-2 Lecture Notes for Chapter 8: Sorting inLinear Time\\nFor insertion sort on 3elements:\\n≤ >\\n≤ >1:2\\n2:3 1:3\\n〈1,2,3〉 1:3 〈2,1,3〉 2:3\\n〈1,3,2〉〈3,1,2〉 〈3,2,1〉≤ >\\n≤ >\\n≤ >\\n〈2,3,1〉A[1] ≤ A[2] A[1] > A[2] (swap in array)\\nA[1] ≤ A[2]\\nA[2] > A[3]A[1] > A[2]\\nA[1] > A[3]\\nA[1] ≤ A[2] ≤ A[3]compare A[1] to A[2]\\n[Each internal node is labeled by indices of array elements from their original\\npositions . Each leaf is labeled by the permutation of orders that the al gorithm\\ndetermines.]\\nHow many leaves on the decision tree? There are \\x15nŠleaves, because every\\npermutation appears at least once.\\nFor any comparison sort,\\n\\x0f1tree for each n.\\n\\x0fView the tree as if the algorithm splits in two at each node, ba sed on the infor-\\nmation it has determined up tothat point.\\n\\x0fThetree models all possible execution traces.\\nWhat isthe length of thelongest path from root toleaf?\\n\\x0fDepends onthe algorithm\\n\\x0fInsertion sort: ‚.n2/\\n\\x0fMerge sort: ‚.nlgn/\\nLemma\\nAnybinary tree of height hhas\\x142hleaves.\\nIn other words:\\n\\x0flD#of leaves,\\n\\x0fhDheight,\\n\\x0fThen l\\x142h.\\n(We’ll prove this lemmalater.)\\nWhyis this useful?\\nTheorem\\nAnydecision tree that sorts nelements has height \\x7f.nlgn/.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 124}),\n",
              " Document(page_content='Lecture Notes for Chapter 8: SortinginLinear Time 8-3\\nProof\\n\\x0fl\\x15nŠ\\n\\x0fBylemma, nŠ\\x14l\\x142hor2h\\x15nŠ\\n\\x0fTake logs: h\\x15lg.nŠ/\\n\\x0fUseStirling’s approximation: nŠ > .n=e/n(byequation (3.17))\\nh\\x15lg.n=e/n\\nDnlg.n=e/\\nDnlgn/NULnlge\\nD\\x7f.nlgn/ : (theorem)\\nNowtoprove the lemma:\\nProofByinduction on h.\\nBasis: hD0. Treeis just one node, which isaleaf. 2hD1.\\nInductive step: Assume true for height Dh/NUL1. Extend tree of height h/NUL1\\nby making as many new leaves as possible. Each leaf becomes pa rent to two new\\nleaves.\\n#of leaves for height hD2\\x01(#of leaves for height h/NUL1)\\nD2\\x012h/NUL1(ind. hypothesis)\\nD2h: (lemma)\\nCorollary\\nHeapsort and merge sort are asymptotically optimal compari son sorts.\\nSorting inlineartime\\nNon-comparison sorts.\\nCountingsort\\nDepends ona key assumption : numbers tobe sorted are integers in f0; 1; : : : ; kg.\\nInput: AŒ1 : : n\\x8d, where AŒj \\x8d2f0; 1; : : : ; kgforjD1; 2; : : : ; n . Array Aand\\nvalues nandkare given as parameters.\\nOutput: BŒ1 : : n\\x8d, sorted. Bis assumed to be already allocated and is given as a\\nparameter.\\nAuxiliary storage: C Œ0 : : k\\x8d', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 125}),\n",
              " Document(page_content='8-4 Lecture Notes for Chapter 8: Sorting inLinear Time\\nCOUNTING -SORT.A; B; n; k/\\nletC Œ0 : : k\\x8d be anew array\\nforiD0tok\\nC Œi\\x8dD0\\nforjD1ton\\nC ŒAŒj \\x8d\\x8dDC ŒAŒj \\x8d\\x8dC1\\nforiD1tok\\nC Œi\\x8dDC Œi\\x8dCC Œi/NUL1\\x8d\\nforjDndownto1\\nBŒC ŒAŒj \\x8d\\x8d\\x8dDAŒj \\x8d\\nC ŒAŒj \\x8d\\x8dDC ŒAŒj \\x8d\\x8d/NUL1\\nDoan example for AD21; 51; 31; 01; 22; 32; 02; 33\\nCounting sort is stable(keys with same value appear in same order in output as\\nthey did in input) because of how the last loop works.\\nAnalysis\\n‚.nCk/, which is ‚.n/ifkDO.n/.\\nHowbig a kispractical?\\n\\x0fGood for sorting 32-bit values? No.\\n\\x0f16-bit? Probably not.\\n\\x0f8-bit? Maybe, depending on n.\\n\\x0f4-bit? Probably (unless nisreally small).\\nCounting sort will beused inradix sort.\\nRadixsort\\nHow IBM made its money. Punch card readers for census tabulat ion in early\\n1900’s. Card sorters, worked on one column at a time. It’s the algorithm for\\nusing the machine that extends the technique to multi-colum n sorting. The human\\noperator waspart of thealgorithm!\\nKeyidea: Sortleastsigniﬁcant digits ﬁrst.\\nTosort ddigits:\\nRADIX-SORT.A; d/\\nforiD1tod\\nuse astable sort to sort array Aondigit i', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 126}),\n",
              " Document(page_content='Lecture Notes for Chapter 8: SortinginLinear Time 8-5\\nExample\\n326\\n453\\n608\\n835\\n751\\n435\\n704\\n690326453\\n608835751\\n435704690\\n326\\n453608\\n835\\n751435704\\n690326\\n453\\n608\\n835751435\\n704690sorted\\nCorrectness\\n\\x0fInduction on number of passes ( iin pseudocode).\\n\\x0fAssume digits 1; 2; : : : ; i/NUL1are sorted.\\n\\x0fShow that astable sort on digit ileaves digits 1; : : : ; isorted:\\n\\x0fIf 2 digits in position iare different, ordering by position iis correct, and\\npositions 1; : : : ; i/NUL1are irrelevant.\\n\\x0fIf 2 digits in position iare equal, numbers are already in the right order\\n(by inductive hypothesis). The stable sort on digit ileaves them in the right\\norder.\\nThis argument shows why it’s so important to use a stable sort for intermediate\\nsort.\\nAnalysis\\nAssumethat weuse counting sort as theintermediate sort.\\n\\x0f‚.nCk/per pass (digits in range 0; : : : ; k)\\n\\x0fdpasses\\n\\x0f‚.d.nCk//total\\n\\x0fIfkDO.n/, timeD‚.dn/.\\nHowtobreak each key into digits?\\n\\x0fnwords.\\n\\x0fbbits/word.\\n\\x0fBreak into r-bit digits. Have dDdb=re.\\n\\x0fUsecounting sort, kD2r/NUL1.\\nExample: 32-bit words, 8-bit digits. bD32,rD8,dDd32=8eD4,\\nkD28/NUL1D255.\\n\\x0fTimeD‚(b\\nr.nC2r/).\\nHow to choose r? Balance b=randnC2r. Choosing r\\x19lgngives us\\n‚/NULb\\nlgn.nCn/\\x01\\nD‚.bn=lgn/.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 127}),\n",
              " Document(page_content='8-6 Lecture Notes for Chapter 8: Sorting inLinear Time\\n\\x0fIf wechoose r <lgn,then b=r > b= lgn, and nC2rterm doesn’t improve.\\n\\x0fIf we choose r >lgn, then nC2rterm gets big. Example: rD2lgn)\\n2rD22lgnD.2lgn/2Dn2.\\nSo, tosort 21632-bit numbers, use rDlg216D16bits.db=reD2passes.\\nCompare radix sort tomerge sort and quicksort:\\n\\x0f1million .220/32-bit integers.\\n\\x0fRadix sort:d32=20eD2passes.\\n\\x0fMerge sort/quicksort: lg nD20passes.\\n\\x0fRemember, though, that each radix sort “pass” is really 2 pas ses—one to take\\ncensus, and one tomovedata.\\nHowdoes radix sort violate theground rules for acomparison sort?\\n\\x0fUsing counting sort allows us to gain information about keys by means other\\nthan directly comparing 2keys.\\n\\x0fUsedkeys as array indices.\\nBucket sort\\nAssumes the input is generated by a random process that distr ibutes elements uni-\\nformly over Œ0; 1/.\\nIdea\\n\\x0fDivide Œ0; 1/intonequal-sized buckets.\\n\\x0fDistribute the ninput values into the buckets.\\n\\x0fSort each bucket.\\n\\x0fThengo through buckets in order, listing elements in each on e.\\nInput: AŒ1 : : n\\x8d,where 0\\x14AŒi\\x8d < 1 for all i.\\nAuxiliary array: BŒ0 : : n/NUL1\\x8dof linked lists, each list initially empty.\\nBUCKET-SORT.A; n/\\nletBŒ0 : : n/NUL1\\x8dbe anew array\\nforiD1ton/NUL1\\nmake BŒi\\x8danempty list\\nforiD1ton\\ninsert AŒi\\x8dinto list BŒbn\\x01AŒi\\x8dc\\x8d\\nforiD0ton/NUL1\\nsort list BŒi\\x8dwithinsertion sort\\nconcatenate lists BŒ0\\x8d; BŒ1\\x8d; : : : ; BŒn /NUL1\\x8dtogether in order\\nreturntheconcatenated lists', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 128}),\n",
              " Document(page_content='Lecture Notes for Chapter 8: SortinginLinear Time 8-7\\nCorrectness\\nConsider AŒi\\x8d,AŒj \\x8d. Assume without loss of generality that AŒi\\x8d\\x14AŒj \\x8d. Then\\nbn\\x01AŒi\\x8dc\\x14bn\\x01AŒj \\x8dc. SoAŒi\\x8dis placed into the same bucket as AŒj \\x8dor into a\\nbucket witha lower index.\\n\\x0fIf samebucket, insertion sort ﬁxesup.\\n\\x0fIf earlier bucket, concatenation of lists ﬁxesup.\\nAnalysis\\n\\x0fRelies on no bucket getting too manyvalues.\\n\\x0fAll lines of algorithm except insertion sorting take ‚.n/altogether.\\n\\x0fIntuitively, if each bucket gets a constant number of elemen ts, it takes O.1/\\ntime tosort each bucket )O.n/sort timefor all buckets.\\n\\x0fWe “expect” each bucket to have few elements, since the avera ge is 1 element\\nper bucket.\\n\\x0fBut weneed to do acareful analysis.\\nDeﬁnearandom variable:\\n\\x0fniDthenumber of elements placed in bucket BŒi\\x8d.\\nBecause insertion sort runs inquadratic time, bucket sort t imeis\\nT .n/D‚.n/Cn/NUL1X\\niD0O.n2\\ni/ :\\nTakeexpectations of both sides:\\nEŒT .n/\\x8dDE\"\\n‚.n/Cn/NUL1X\\niD0O.n2\\ni/#\\nD‚.n/Cn/NUL1X\\niD0E\\x02\\nO.n2\\ni/\\x03\\n(linearity of expectation)\\nD‚.n/Cn/NUL1X\\niD0O.E\\x02\\nn2\\ni\\x03\\n/(EŒaX\\x8dDaEŒX\\x8d)\\nClaim\\nEŒn2\\ni\\x8dD2/NUL.1=n/foriD0; : : : ; n/NUL1.\\nProofof claim\\nDeﬁneindicator random variables:\\n\\x0fXijDIfAŒj \\x8dfalls inbucket ig\\n\\x0fPrfAŒj \\x8dfalls inbucket igD1=n\\n\\x0fniDnX\\njD1Xij', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 129}),\n",
              " Document(page_content='8-8 Lecture Notes for Chapter 8: Sorting inLinear Time\\nThen\\nE\\x02\\nn2\\ni\\x03\\nDE\" nX\\njD1Xij!2#\\nDE\"nX\\njD1X2\\nijC2n/NUL1X\\njD1nX\\nkDjC1XijXik#\\nDnX\\njD1E\\x02\\nX2\\nij\\x03\\nC2n/NUL1X\\njD1nX\\nkDjC1EŒXijXik\\x8d(linearity of expectation)\\nE\\x02\\nX2\\nij\\x03\\nD02\\x01PrfAŒj \\x8ddoesn’t fall in bucket igC12\\x01PrfAŒj \\x8dfalls in bucket ig\\nD0\\x01\\x12\\n1/NUL1\\nn\\x13\\nC1\\x011\\nn\\nD1\\nn\\nEŒXijXik\\x8dforj¤k: Since j¤k,XijandXikareindependent randomvariables\\n)EŒXijXik\\x8dDEŒXij\\x8dEŒXik\\x8d\\nD1\\nn\\x011\\nn\\nD1\\nn2\\nTherefore:\\nE\\x02\\nn2\\ni\\x03\\nDnX\\njD11\\nnC2n/NUL1X\\njD1nX\\nkDjC11\\nn2\\nDn\\x011\\nnC2 \\nn\\n2!\\n1\\nn2\\nD1C2\\x01n.n/NUL1/\\n2\\x011\\nn2\\nD1Cn/NUL1\\nn\\nD1C1/NUL1\\nn\\nD2/NUL1\\nn(claim)\\nTherefore:\\nEŒT .n/\\x8dD‚.n/Cn/NUL1X\\niD0O.2/NUL1=n/\\nD‚.n/CO.n/\\nD‚.n/\\n\\x0fAgain, not a comparison sort. Used a function of key values to index into an\\narray.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 130}),\n",
              " Document(page_content='Lecture Notes for Chapter 8: SortinginLinear Time 8-9\\n\\x0fThis is a probabilistic analysis —we used probability to analyze an algorithm\\nwhose running timedepends onthe distribution of inputs.\\n\\x0fDifferentfroma randomizedalgorithm ,whereweuserandomizationto impose\\nadistribution.\\n\\x0fWith bucket sort, if the input isn’t drawn from a uniform dist ribution on Œ0; 1/,\\nall bets are off (performance-wise, but the algorithm is sti ll correct).', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 131}),\n",
              " Document(page_content='Solutionsfor Chapter8:\\nSortinginLinearTime\\nSolution to Exercise8.1-3\\nThis solutionisalsopostedpublicly\\nIf the sort runs in linear time for minput permutations, then the height hof the\\nportion of the decision tree consisting of the mcorresponding leaves and their\\nancestors is linear.\\nUse the same argument as in the proof of Theorem 8.1 to show tha t this is impos-\\nsible for mDnŠ=2,nŠ=n, ornŠ=2n.\\nWe have 2h\\x15m, which gives us h\\x15lgm. For all the possible m’s given here,\\nlgmD\\x7f.nlgn/,hence hD\\x7f.nlgn/.\\nIn particular,\\nlgnŠ\\n2DlgnŠ/NUL1\\x15nlgn/NULnlge/NUL1 ;\\nlgnŠ\\nnDlgnŠ/NULlgn\\x15nlgn/NULnlge/NULlgn ;\\nlgnŠ\\n2nDlgnŠ/NULn\\x15nlgn/NULnlge/NULn :\\nSolution to Exercise8.1-4\\nLetSbeasequence of nelementsdividedinto n=ksubsequences eachoflength k\\nwhere all of the elements in any subsequence are larger than a ll of the elements\\nof a preceding subsequence and smaller than all of the elemen ts of a succeeding\\nsubsequence.\\nClaim\\nAny comparison-based sorting algorithm to sort smust take \\x7f.nlgk/time in the\\nworst case.\\nProofFirst notice that, as pointed out in the hint, we cannot prove the lower\\nbound by multiplying together the lower bounds for sorting e ach subsequence.\\nThat wouldonlyprovethat thereisnofaster algorithm that sorts thesubsequences', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 132}),\n",
              " Document(page_content='Solutions forChapter 8: Sorting inLinear Time 8-11\\nindependently . This was not what weare asked to prove; wecannot introduce any\\nextra assumptions.\\nNow,considerthedecisiontreeofheight hforanycomparisonsortfor S. Sincethe\\nelements of each subsequence can be in any order, any of the kŠpermutations cor-\\nrespond to the ﬁnal sorted order of a subsequence. And, since there are n=ksuch\\nsubsequences, each of which can be in any order, there are .kŠ/n=kpermutations\\nofSthat could correspond to the sorting of some input order. Thu s, any decision\\ntree for sorting Smust have at least .kŠ/n=kleaves. Since abinary tree of height h\\nhas no more than 2hleaves, we must have 2h\\x15.kŠ/n=korh\\x15lg..kŠ/n=k/. We\\ntherefore obtain\\nh\\x15lg..kŠ/n=k\\nD.n=k/lg.kŠ/\\n\\x15.n=k/lg..k=2/k=2/\\nD.n=2/lg.k=2/ :\\nThe third line comes from kŠhaving its k=2largest terms being at least k=2each.\\n(Weimplicitlyassumeherethat kiseven. Wecouldadjust withﬂoorsandceilings\\nifkwereodd.)\\nSincethereexistsatleastonepathinanydecision treefors orting Sthathaslength\\nat least .n=2/lg.k=2/, the worst-case running time of any comparison-based sort-\\ning algorithm for Sis\\x7f.nlgk/.\\nSolutionto Exercise 8.2-3\\nThissolutionisalsopostedpublicly\\n[Thefollowing solution also answers Exercise 8.2-2.]\\nNotice that the correctness argument in the text does not dep end on the order in\\nwhich Aisprocessed. Thealgorithm is correct nomatter what order i s used!\\nButthemodiﬁedalgorithmisnotstable. Asbefore,intheﬁna lforloopanelement\\nequal to one taken from Aearlier is placed before the earlier one (i.e., at a lower\\nindex position) in the output arrray B. The original algorithm was stable because\\nanelementtakenfrom Alater startedoutwithalowerindexthanonetakenearlier.\\nBut in the modiﬁed algorithm, an element taken from Alater started out with a\\nhigher index than one taken earlier.\\nIn particular, the algorithm still places the elements with value kin positions\\nC Œk/NUL1\\x8dC1through C Œk\\x8d,but in thereverse order of their appearance in A.\\nSolutionto Exercise 8.2-4\\nCompute the Carray as is done in counting sort. The number of integers in th e\\nrange Œa : : b\\x8disC Œb\\x8d/NULC Œa/NUL1\\x8d, where weinterpret C Œ/NUL1\\x8das0.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 133}),\n",
              " Document(page_content='8-12 Solutions for Chapter 8: Sorting inLinear Time\\nSolution to Exercise8.3-2\\nInsertionsortisstable. Wheninserting AŒj \\x8dintothesortedsequence AŒ1 : : : j/NUL1\\x8d,\\nwe do it the following way: compare AŒj \\x8dtoAŒi\\x8d, starting with iDj/NUL1and\\ngoing down to iD1. Continue at long as AŒj \\x8d < AŒi\\x8d .\\nMergesortasdeﬁnedisstable,becausewhentwoelementscom paredareequal,the\\ntie is broken by taking the element from array Lwhich keeps them in the original\\norder.\\nHeapsort and quicksort are not stable.\\nOne scheme that makes a sorting algorithm stable is to store t he index of each\\nelement (the element’s place in the original ordering) with the element. When\\ncomparing two elements, compare them by their values and bre ak ties by their\\nindices.\\nAdditional space requirements: For nelements, their indices are 1 : : : n. Each can\\nbe written in lg nbits, sotogether they take O.nlgn/additional space.\\nAdditional time requirements: The worst case is when all ele ments are equal. The\\nasymptotic time does not change because we add a constant amo unt of work to\\neach comparison.\\nSolution to Exercise8.3-3\\nThis solutionisalsopostedpublicly\\nBasis:IfdD1, there’s only one digit, sosorting onthat digit sorts the ar ray.\\nInductivestep: Assuming that radix sort worksfor d/NUL1digits, we’ll showthat it\\nworks for ddigits.\\nRadix sort sorts separately on each digit, starting from dig it1. Thus, radix sort of\\nddigits, which sorts on digits 1; : : : ; dis equivalent to radix sort of the low-order\\nd/NUL1digits followed byasort on digit d. Byour induction hypothesis, the sort of\\nthe low-order d/NUL1digits works, so just before the sort on digit d, the elements\\nare inorder according to their low-order d/NUL1digits.\\nThe sort on digit dwill order the elements by their dth digit. Consider two ele-\\nments, aandb,with dthdigits adandbdrespectively.\\n\\x0fIfad< b d,thesortwillput abefore b,whichiscorrect,since a < bregardless\\nof the low-order digits.\\n\\x0fIfad> b d, the sort will put aafterb, which is correct, since a > bregardless\\nof the low-order digits.\\n\\x0fIfadDbd, the sort will leave aandbin the same order they were in, because\\nit is stable. But that order is already correct, since the cor rect order of aandb\\nisdetermined bythelow-order d/NUL1digitswhentheir dthdigits areequal, and\\nthe elements arealready sorted by their low-order d/NUL1digits.\\nIf the intermediate sort were not stable, it might rearrange elements whose dth\\ndigits were equal—elements that werein the right order after the sort on their\\nlower-order digits.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 134}),\n",
              " Document(page_content='Solutions forChapter 8: Sorting inLinear Time 8-13\\nSolutionto Exercise 8.3-4\\nThissolutionisalsopostedpublicly\\nTreatthenumbersas 3-digit numbersinradix n. Eachdigitranges from 0ton/NUL1.\\nSort these 3-digit numbers withradix sort.\\nThere are 3calls to counting sort, each taking ‚.nCn/D‚.n/time, so that the\\ntotal timeis ‚.n/.\\nSolutionto Exercise 8.4-2\\nTheworst-caserunningtimeforthebucket-sortalgorithmo ccurswhentheassump-\\ntionofuniformlydistributed inputdoesnothold. If,forex ample,alltheinputends\\nup in the ﬁrst bucket, then in the insertion sort phase it need s to sort all the input,\\nwhich takes O.n2/time.\\nA simple change that will preserve the linear expected runni ng time and make the\\nworst-case running time O.nlgn/istouseaworst-case O.nlgn/-timealgorithm,\\nsuch as merge sort, instead of insertion sort when sorting th e buckets.\\nSolutionto Problem 8-1\\nThissolutionisalsopostedpublicly\\na.For a comparison algorithm Ato sort, no twoinput permutations can reach the\\nsame leaf of the decision tree, sothere must beat least nŠleaves reached in TA,\\noneforeachpossibleinputpermutation. Since Aisadeterministicalgorithm, it\\nmust always reach the same leaf when given a particular permu tation as input,\\nso at most nŠleaves are reached (one for each permutation). Therefore ex actly\\nnŠleaves are reached, one for each input permutation.\\nThese nŠleaves will each have probability 1=nŠ, since each of the nŠpossible\\npermutations is the input with the probability 1=nŠ. Any remaining leaves will\\nhave probability 0, since they arenot reached for any input.\\nWithout lossofgenerality, wecanassumefortherestofthis problem thatpaths\\nleading only to 0-probability leaves aren’t in the tree, since they cannot af fect\\ntherunning timeofthesort. Thatis,wecanassumethat TAconsists ofonlythe\\nnŠleaves labeled 1=nŠand their ancestors.\\nb.Ifk > 1, then the root of Tis not a leaf. This implies that all of T’s leaves\\nare leaves in LTandRT. Since every leaf at depth hinLTorRThas depth\\nhC1inT,D.T /mustbethesumof D.LT /,D.RT /,and k,thetotal number\\nof leaves. To prove this last assertion, let dT.x/Ddepth of node xin tree T.\\nThen,', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 135}),\n",
              " Document(page_content='8-14 Solutions for Chapter 8: Sorting inLinear Time\\nD.T /DX\\nx2leaves .T /dT.x/\\nDX\\nx2leaves .LT /dT.x/CX\\nx2leaves .RT /dT.x/\\nDX\\nx2leaves .LT /.dLT.x/C1/CX\\nx2leaves .RT /.dRT.x/C1/\\nDX\\nx2leaves .LT /dLT.x/CX\\nx2leaves .RT /dRT.x/CX\\nx2leaves .T /1\\nDD.LT /CD.RT /Ck :\\nc.To show that d.k/Dmin 1\\x14i\\x14k/NUL1fd.i/Cd.k/NULi/Ckgwe will show sepa-\\nrately that\\nd.k/\\x14min\\n1\\x14i\\x14k/NUL1fd.i/Cd.k/NULi/Ckg\\nand\\nd.k/\\x15min\\n1\\x14i\\x14k/NUL1fd.i/Cd.k/NULi/Ckg:\\n\\x0fToshowthat d.k/\\x14min 1\\x14i\\x14k/NUL1fd.i/Cd.k/NULi/Ckg,weneedonlyshow\\nthatd.k/\\x14d.i/Cd.k/NULi/Ck, foriD1; 2; : : : ; k/NUL1. For any ifrom 1\\ntok/NUL1we can ﬁnd trees RTwithileaves and LTwithk/NULileaves such\\nthatD.RT /Dd.i/andD.LT /Dd.k/NULi/. Construct Tsuchthat RTand\\nLTare the right and left subtrees of T’s root respectively. Then\\nd.k/\\x14D.T / (bydeﬁnition of dasmin D.T /value)\\nDD.RT /CD.LT /Ck(bypart (b))\\nDd.i/Cd.k/NULi/Ck(bychoice of RTandLT) .\\n\\x0fToshowthat d.k/\\x15min 1\\x14i\\x14k/NUL1fd.i/Cd.k/NULi/Ckg,weneedonlyshow\\nthatd.k/\\x15d.i/Cd.k/NULi/Ck, for some iinf1; 2; : : : ; k/NUL1g. Take the\\ntreeTwithkleaves such that D.T /Dd.k/, letRTandLTbeT’s right\\nand left subtree, respecitvely, andlet ibethenumber of leaves in RT. Then\\nk/NULiis the number of leaves in LTand\\nd.k/DD.T / (by choice of T)\\nDD.RT /CD.LT /Ck(by part (b))\\n\\x15d.i/Cd.k/NULi/Ck(by deﬁntion of dasmin D.T /value) .\\nNeither inork/NULicanbe 0(andhence 1\\x14i\\x14k/NUL1), sinceif oneofthese\\nwere 0, either RTorLTwould contain all kleaves of T, and that k-leaf\\nsubtree would have a Dequal to D.T //NULk(by part (b)), contradicting the\\nchoice of Tasthe k-leaf tree with theminimum D.\\nd.Letfk.i/DilgiC.k/NULi/lg.k/NULi/. Toﬁndthevalue of ithat minimizes fk,\\nﬁndthe ifor which the derivative of fkwith respect to iis0:\\nf0\\nk.i/Dd\\ndi\\x12ilniC.k/NULi/ln.k/NULi/\\nln2\\x13\\nDlniC1/NULln.k/NULi//NUL1\\nln2\\nDlni/NULln.k/NULi/\\nln2', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 136}),\n",
              " Document(page_content='Solutions forChapter 8: Sorting inLinear Time 8-15\\nis0atiDk=2. To verify this is indeed a minimum (not a maximum), check\\nthat the second derivative of fkis positive at iDk=2:\\nf00\\nk.i/Dd\\ndi\\x12lni/NULln.k/NULi/\\nln2\\x13\\nD1\\nln2\\x121\\niC1\\nk/NULi\\x13\\n:\\nf00\\nk.k=2/D1\\nln2\\x122\\nkC2\\nk\\x13\\nD1\\nln2\\x014\\nk\\n> 0 since k > 1.\\nNow we use substitution to prove d.k/D\\x7f.klgk/. The base case of the\\ninduction is satisﬁed because d.1/\\x150Dc\\x011\\x01lg1for any constant c. For\\nthe inductive step we assume that d.i/\\x15cilgifor1\\x14i\\x14k/NUL1, where cis\\nsome constant tobe determined.\\nd.k/Dmin\\n1\\x14i\\x14k/NUL1fd.i/Cd.k/NULi/Ckg\\n\\x15min\\n1\\x14i\\x14k/NUL1fc.ilgiC.k/NULi/lg.k/NULi//Ckg\\nDmin\\n1\\x14i\\x14k/NUL1fcfk.i/Ckg\\nDc\\x12k\\n2lgk\\n2\\x12\\nk/NULk\\n2\\x13\\nlg\\x12\\nk/NULk\\n2\\x13\\x13\\nCk\\nDcklg\\x12k\\n2\\x13\\nCk\\nDc.klgk/NULk/Ck\\nDcklgkC.k/NULck/\\n\\x15cklgkifc\\x141 ;\\nand so d.k/D\\x7f.klgk/.\\ne.Using the result of part (d) and the fact that TA(as modiﬁed in our solution to\\npart (a)) has nŠleaves, wecan conclude that\\nD.T A/\\x15d.nŠ/D\\x7f.nŠlg.nŠ// :\\nD.T A/is the sum of the decision-tree path lengths for sorting all i nput per-\\nmutations, and the path lengths are proportional to the run t ime. Since the nŠ\\npermutations have equal probability 1=nŠ, the expected time to sort nrandom\\nelements ( 1input permutation) is the total time for all permutations di vided\\nbynŠ:\\n\\x7f.nŠlg.nŠ//\\nnŠD\\x7f.lg.nŠ//D\\x7f.nlgn/ :\\nf.Wewillshowhowtomodifyarandomized decision tree(algori thm) todeﬁnea\\ndeterministicdecisiontree(algorithm)thatisatleastas goodastherandomized\\none in termsof the average number of comparisons.\\nAt each randomized node, pick the child with the smallest sub tree (the subtree\\nwiththesmallestaveragenumberofcomparisonsonapathtoa leaf). Deleteall', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 137}),\n",
              " Document(page_content='8-16 Solutions for Chapter 8: Sorting inLinear Time\\nthe other children of the randomized node and splice out the r andomized node\\nitself.\\nThedeterministic algorithm corresponding tothismodiﬁed treestill works, be-\\ncause the randomized algorithm worked no matter which path w as taken from\\neach randomized node.\\nThe average number of comparisons for the modiﬁed algorithm is no larger\\nthan the average number for the original randomized tree, si nce we discarded\\nthe higher-average subtrees in each case. In particular, ea ch time we splice out\\na randomized node, we leave the overall average less than or e qual to what it\\nwas, because\\n\\x0fthesamesetofinputpermutationsreachesthemodiﬁedsubtr eeasbefore,but\\nthoseinputsarehandledinlessthanorequaltoaveragetime thanbefore,and\\n\\x0fthe rest of the tree isunmodiﬁed.\\nThe randomized algorithm thus takes at least as much time on a verage as the\\ncorresponding deterministic one. (We’veshown that theexp ected running time\\nfor a deterministic comparison sort is \\x7f.nlgn/, hence the expected time for a\\nrandomized comparison sort isalso \\x7f.nlgn/.)\\nSolution to Problem 8-3\\na.The usual, unadorned radix sort algorithm will not solve thi s problem in the\\nrequired time bound. The number of passes, d, would have to be the number\\nof digits in the largest integer. Suppose that there are mintegers; we always\\nhave m\\x14n. In the worst case, we would have one integer with n=2digits and\\nn=2integers with one digit each. We assume that the range of a sin gle digit is\\nconstant. Therefore, we would have dDn=2andmDn=2C1, and so the\\nrunning time would be ‚.dm/D‚.n2/.\\nLet us assume without loss of generality that all the integer s are positive and\\nhavenoleadingzeros. (Iftherearenegativeintegers or0,d eal withthepositive\\nnumbers, negative numbers, and 0 separately.) Under this as sumption, we can\\nobserve that integers with more digits are always greater th an integers with\\nfewer digits. Thus, we can ﬁrst sort the integers by number of digits (using\\ncounting sort), and then use radix sort to sort each group of i ntegers with the\\nsame length. Noting that each integer has between 1 and ndigits, let mibe the\\nnumber of integers with idigits, for iD1; 2; : : : ; n . Since there are ndigits\\naltogether, wehavePn\\niD1i\\x01miDn.\\nIt takes O.n/time to compute how many digits all the integers have and, onc e\\nthe numbers of digits have been computed, it takes O.mCn/DO.n/time\\nto group the integers by number of digits. To sort the group wi thmidigits by\\nradix sort takes ‚.i\\x01mi/time. Thetime tosort all groups, therefore, is\\nnX\\niD1‚.i\\x01mi/D‚ nX\\niD1i\\x01mi!\\nD‚.n/ :', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 138}),\n",
              " Document(page_content=\"Solutions forChapter 8: Sorting inLinear Time 8-17\\nb.One way to solve this problem is by a radix sort from right to le ft. Since the\\nstrings have varying lengths, however, we have to pad out all strings that are\\nshorter than the longest string. The padding is on the right e nd of the string,\\nand it’s with a special character that is lexicographically less than any other\\ncharacter (e.g., in C, the character '\\\\0'with ASCII value 0). Of course, we\\ndon’thavetoactuallychangeanystring;ifwewanttoknowth ejthcharacterof\\nastring whoselength is k,then if j > k,thejthcharacter isthepad character.\\nUnfortunately, this scheme does not always run in the requir ed time bound.\\nSuppose that there are mstrings and that the longest string has dcharacters.\\nIn the worst case, one string has n=2characters and, before padding, n=2\\nstrings have one character each. As in part (a), we would have dDn=2and\\nmDn=2C1. Westillhavetoexaminethepadcharacters ineachpassofra dix\\nsort, even if we don’t actually create them in the strings. As suming that the\\nrange of a single character is constant, the running time of r adix sort would be\\n‚.dm/D‚.n2/.\\nTo solve the problem in O.n/time, we use the property that, if the ﬁrst letter\\nof string xis lexicographically less that the ﬁrst letter of string y, then xis\\nlexicographically less than y, regardless of the lengths of the two strings. We\\ntake advantage of this property by sorting the strings on the ﬁrst letter, using\\ncounting sort. We take an empty string as a special case and pu t it ﬁrst. We\\ngathertogether allstringswiththesameﬁrstletterasagro up. Thenwerecurse,\\nwithin each group , based oneach string with theﬁrst letter removed.\\nThe correctness of this algorithm is straightforward. Anal yzing the running\\ntimeis abit trickier. Letus count the number of timesthat ea ch string issorted\\nby a call of counting sort. Suppose that the ith string, si, has length li. Then\\nsiis sorted by at most liC1counting sorts. (The“ C1” is because it may have\\nto be sorted as an empty string at some point; for example, abandaend up in\\nthe same group in the ﬁrst pass and are then ordered based on band the empty\\nstringinthesecondpass. Thestring aissorteditslength,1,timeplusonemore\\ntime.) A call of counting sort on tstrings takes ‚.t/time (remembering that\\nthenumber ofdifferent characters onwhichwearesortingis aconstant.) Thus,\\nthe total time for all calls of counting sort is\\nO mX\\niD1.liC1/!\\nDO mX\\niD1liCm!\\nDO.nCm/\\nDO.n/ ;\\nwhere the second line follows fromPm\\niD1liDn, and the last line is because\\nm\\x14n.\\nSolutionto Problem 8-4\\na.Compareeach redjugwitheachbluejug. Sincethere are nredjugsand nblue\\njugs, that will take ‚.n2/comparisons inthe worst case.\", metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 139}),\n",
              " Document(page_content='8-18 Solutions for Chapter 8: Sorting inLinear Time\\nb.To solve the problem, an algorithm has to perform a series of c omparisons\\nuntil it has enough information to determine the matching. W e can view the\\ncomputation of the algorithm in terms of a decision tree. Eve ry internal node\\nis labeled with two jugs (one red, one blue) which we compare, and has three\\noutgoing edges (red jug smaller, same size, or larger than th e blue jug). The\\nleaves arelabeled withaunique matching of jugs.\\nTheheightofthedecisiontreeisequaltotheworst-casenum berofcomparisons\\nthealgorithm hastomaketodetermine thematching. Tobound that size, letus\\nﬁrst compute the number of possible matchings for nred and nblue jugs.\\nIf we label the red jugs from 1tonand we also label the blue jugs from 1\\ntonbefore starting the comparisons, every outcome of the algor ithm can be\\nrepresented asaset\\nf.i; \\x19.i//W1\\x14i\\x14nand\\x19isapermutation on f1; : : : ; ngg;\\nwhich contains the pairs of red jugs (ﬁrst component) and blu e jugs (second\\ncomponent) that are matched up. Since every permutation \\x19corresponds to a\\ndifferent outcome, there must beexactly nŠdifferent results.\\nNowwecan bound the height hof our decision tree. Every tree with abranch-\\ning factor of 3(every inner node has at most three children) has at most 3h\\nleaves. Sincethe decison tree must have at least nŠchildren, it follows that\\n3h\\x15nŠ\\x15.n=e/n)h\\x15nlog3n/NULnlog3eD\\x7f.nlgn/ :\\nSoanyalgorithm solving the problem must use \\x7f.nlgn/comparisons.\\nc.Assume that the red jugs are labeled with numbers 1; 2; : : : ; n and so are the\\nblue jugs. The numbers are arbitrary and do not correspond to the volumes of\\njugs,butarejustusedtorefertothejugsinthealgorithmde scription. Moreover,\\nthe output of the algorithm will consist of ndistinct pairs .i; j /, where the red\\njugiand theblue jug jhave the same volume.\\nThe procedure M ATCH-JUGStakes as input two sets representing jugs to be\\nmatched: R\\x12f1; : : : ; ng, representing red jugs, and B\\x12f1; : : : ; ng, rep-\\nresenting blue jugs. We will call the procedure only with inp uts that can be\\nmatched; one necessary condition isthat jRjDjBj.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 140}),\n",
              " Document(page_content='Solutions forChapter 8: Sorting inLinear Time 8-19\\nMATCH-JUGS.R; B/\\nifjRj==0//sets are empty\\nreturn\\nifjRj==1//sets contain just one jug each\\nletRDfrgandBDfbg\\noutput “ .r; b/”\\nreturn\\nelserDarandomly chosen jug in R\\ncompare rtoevery jug of B\\nB<Dthe set of jugs in Bthat are smaller than r\\nB>Dthe set of jugs in Bthat are larger than r\\nbDthe one jug in Bwith the samesize as r\\ncompare btoevery jug of R/NULfrg\\nR<Dthe set of jugs in Rthat aresmaller than b\\nR>Dthe set of jugs in Rthat arelarger than b\\noutput “ .r; b/”\\nMATCH-JUGS.R<; B</\\nMATCH-JUGS.R>; B>/\\nCorrectness can be seen as follows (remember that jRjDjBjin each call).\\nOnce we pick rrandomly from R, there will be a matching among the jugs in\\nvolumesmallerthan r(whichareinthesets R<andB<),andlikewisebetween\\nthejugslargerthan r(whicharein R>andB>). Terminationisalsoeasytosee:\\nsincejR<jCjR>j<jRjin every recursive step, the size of the ﬁrst parameter\\nreduces witheveryrecursivecall. Iteventually mustreach 0or1,inwhichcase\\nthe recursion terminates.\\nWhat about the running time? The analysis of the expected num ber of com-\\nparisons is similar to that of the quicksort algorithm in Sec tion 7.4.2. Let us\\norder the jugs as r1; : : : ; r nandb1; : : : ; b nwhere ri< r iC1andbi< b iC1for\\niD1; : : : ; n, and riDbi. Our analysis uses indicator random variables\\nXijDIfredjug riis compared toblue jug bjg:\\nAs in quicksort, a given pair riandbjis compared at most once. When we\\ncompare rito every jug in B, jugriwill not be put in either R<orR>. When\\nwe compare bito every jug in R/NULfrig, jugbiis not put into either B<orB>.\\nThetotal number of comparisons is\\nXDn/NUL1X\\niD1nX\\njDiC1Xij:\\nTocalculate theexpected value of X,wefollow thequicksort analysis toarrive\\nat\\nEŒX\\x8dDn/NUL1X\\niD1nX\\njDiC1Prfriiscompared to bjg:\\nAs in the quicksort analysis, once we choose a jug rksuch that ri< r k< b j,\\nwe will put riinR<andbjinR>, and so riandbjwill never be compared', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 141}),\n",
              " Document(page_content='8-20 Solutions for Chapter 8: Sorting inLinear Time\\nagain. Let us denote RijDfri; : : : ; r jg. Then jugs riandbjwill be compared\\nif and only if the ﬁrst jug in Rijtobe chosen iseither riorrj.\\nStill following the quicksort analysis, until a jug from Rijis chosen, the entire\\nsetRijistogether. Anyjugin Rijisequally likely tobeﬁrstonechosen. Since\\njRijjDj/NULiC1, the probability of any given jug being the ﬁrst one chosen\\ninRijis1=.j/NULiC1/. Theremainderoftheanalysisisthesameasthequicksort\\nanalysis, and wearrive at the solution of O.nlgn/comparisons.\\nJust like inquicksort, inthe worst case wealways choose the largest (or small-\\nest) jugtopartition thesets, whichreduces theset sizes by only 1. Therunning\\ntime then obeys the recurrence T .n/DT .n/NUL1/C‚.n/, and the number of\\ncomparisons wemake inthe worst case is T .n/D‚.n2/.\\nSolution to Problem 8-7\\na.AŒq\\x8dmust go the wrong place, because it goes where AŒp\\x8dshould go. Since\\nAŒp\\x8disthesmallest valueinarray Athatgoestothewrongarraylocation, AŒp\\x8d\\nmust be smaller than AŒq\\x8d.\\nb.From how we have deﬁned the array B, we have that if AŒi\\x8d\\x14AŒj \\x8dthen\\nBŒi\\x8d\\x14BŒj \\x8d. Therefore,algorithmXperformsthesamesequenceofexcha nges\\non array Bas it does on array A. The output produced on array Ais of the\\nform : : : AŒq\\x8d : : : AŒp\\x8d : : : ,andsotheoutput produced onarray Bisoftheform\\n: : : BŒq\\x8d : : : BŒp\\x8d : : : , or: : : 1 : : : 0 : : : . Hence algorithm X fails to sort array B\\ncorrectly.\\nc.The even steps perform ﬁxed permutations. The odd steps sort each column\\nbysomesorting algorithm, whichmightnotbeanoblivious co mpare-exchange\\nalgorithm. Buttheresultofsortingeachcolumnwouldbethe sameasifwedid\\nuse anoblivious compare-exchange algorithm.\\nd.After step 1,each column has 0sontop and1son thebottom, wit hat most one\\ntransition between0sand1s,anditisa 0!1transition. (Aswereadthearray\\nincolumn-major order, all 1!0transitions occur between adjacent columns.)\\nAfter step 2, therefore, each consecutive group of r=srows, read in row-major\\norder, has at most one transition, and again it is a 0!1transition. All 1!0\\ntransitions occur at the end of a group of r=srows. Since there are sgroups\\nofr=srows, there are at most sdirty rows, and the rest of the rows are clean.\\nStep 3 moves the 0s to the top rows and the 1s to the bottom rows. Thesdirty\\nrowsare somewhere in themiddle.\\ne.Thedirtyareaafter step3isatmost srowshighand scolumnswide,andsoits\\narea is at most s2. Step 4 turns the clean 0s in the top rowsinto a clean area on\\nthe left, the clean 1s in the bottom rows into a clean area on th e right, and the\\ndirty area of size s2isbetween thetwo clean areas.\\nf.First, we argue that if the dirty area after step 4 has size at m ostr=2, then\\nsteps 5–8 complete the sorting. If the dirty area has size at m ostr=2(half a\\ncolumn), theniteither resides entirely inonecolumnor itr esides inthebottom', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 142}),\n",
              " Document(page_content='Solutions forChapter 8: Sorting inLinear Time 8-21\\nhalf of one column and the top half of the next column. In the fo rmer case,\\nstep 5 sorts the column containing the dirty area, and steps 6 –8 maintain that\\nthe array issorted. Inthe latter case, step 5cannot increas e thesize of the dirty\\narea,step6movestheentiredirtyareaintothesamecolumn, step7sortsit,and\\nstep 8 movesit back.\\nSecond, we argue that the dirty area after step 4 has size at mo str=2. But that\\nfollows immediately from the requirement that r\\x152s2and the property that\\nafter step 4, thedirty area has size at most s2.\\ng.Ifsdoesnotdivide r,thenafterstep2,wecanseeupto s 0!1transitionsand\\ns/NUL1 1!0transitions in the rows. After step 3, we would have up to 2s/NUL1\\ndirtyrows,foradirtyareasizeofatmost 2s2/NULs. Topushthecorrectness proof\\nthrough, weneed 2s2/NULs\\x14r=2,orr\\x154s2/NUL2s.\\nh.We can reduce the number of transitions in the rows after step 2 back down to\\nat most sby sorting every other column in reverse order in step 1. Now i f we\\nhave a transition (either 1!0or0!1) between columns after step 1, then\\neither one of the columns had all 1s or the other had all 0s, in w hich case we\\nwould not have a transition within one of the columns.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 143}),\n",
              " Document(page_content='Lecture Notes forChapter 9:\\nMediansandOrder Statistics\\nChapter 9 overview\\n\\x0fithorder statistic is the ithsmallest element of aset of nelements.\\n\\x0fTheminimum isthe ﬁrst order statistic ( iD1).\\n\\x0fThemaximum isthe nth order statistic ( iDn).\\n\\x0fAmedianis the“halfway point” of the set.\\n\\x0fWhen nisodd, the median isunique, at iD.nC1/=2.\\n\\x0fWhen niseven, there are twomedians:\\n\\x0fThelower median , atiDn=2, and\\n\\x0fTheupper median , atiDn=2C1.\\n\\x0fWemeanlower median when weuse the phrase “the median.”\\nTheselection problem :\\nInput:Aset Aofndistinct numbers and anumber i, with 1\\x14i\\x14n.\\nOutput: The element x2Athat is larger than exactly i/NUL1other elements in A.\\nIn other words, the ithsmallest element of A.\\nWecan easily solve the selection problem in O.nlgn/time:\\n\\x0fSortthenumbers using an O.nlgn/-timealgorithm, suchasheapsort ormerge\\nsort.\\n\\x0fThen return the ith element inthe sorted array.\\nThereare faster algorithms, however.\\n\\x0fFirst, we’ll look at the problem of selecting the minimum and maximum of a\\nset of elements.\\n\\x0fThen, we’ll look at a simple general selection algorithm wit h a time bound of\\nO.n/in the average case.\\n\\x0fFinally, we’ll look at a more complicated general selection algorithm with a\\ntime bound of O.n/in theworst case.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 144}),\n",
              " Document(page_content='9-2 Lecture Notes for Chapter 9: Medians and Order Statistic s\\nMinimum and maximum\\nWecaneasilyobtainanupperboundof n/NUL1comparisonsforﬁndingtheminimum\\nof aset of nelements.\\n\\x0fExamine each element inturn and keep track of thesmallest on e.\\n\\x0fThisisthebestwecando,becauseeachelement, exceptthemi nimum,mustbe\\ncompared toa smaller element at least once.\\nThefollowing pseudocode ﬁnds the minimum element inarray AŒ1 : : n\\x8d:\\nMINIMUM .A; n/\\nminDAŒ1\\x8d\\nforiD2ton\\nifmin> AŒi\\x8d\\nminDAŒi\\x8d\\nreturnmin\\nThemaximum can be found in exactly the sameway byreplacing t he>with<in\\nthe above algorithm.\\nSimultaneousminimumandmaximum\\nSomeapplications need both the minimum and maximum of aset o f elements.\\n\\x0fFor example, a graphics program may need to scale a set of .x; y/data to ﬁt\\nonto a rectangular display. To do so, the program must ﬁrst ﬁn d the minimum\\nand maximum of each coordinate.\\nAsimplealgorithmtoﬁndtheminimumandmaximumistoﬁndeac honeindepen-\\ndently. There will be n/NUL1comparisons for the minimum and n/NUL1comparisons\\nfor the maximum,for atotal of 2n/NUL2comparisons. Thiswillresult in ‚.n/time.\\nIn fact, at most 3bn=2ccomparisons sufﬁce to ﬁnd both the minimum and maxi-\\nmum:\\n\\x0fMaintain the minimum and maximum of elements seen so far.\\n\\x0fDon’t compare each element tothe minimum and maximum separa tely.\\n\\x0fProcess elements in pairs.\\n\\x0fCompare the elements of apair to each other.\\n\\x0fThen compare the larger element to the maximum so far, and com pare the\\nsmaller element tothe minimum so far.\\nThis leads to only 3comparisons for every 2elements.\\nSetting up the initial values for the min and max depends on wh ether nis odd or\\neven.\\n\\x0fIfniseven,comparetheﬁrsttwoelementsandassignthelarger t omaxandthe\\nsmaller to min. Then process the rest of the elements inpairs .\\n\\x0fIfnis odd, set both min and max to the ﬁrst element. Then process t he rest of\\nthe elements inpairs.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 145}),\n",
              " Document(page_content='Lecture Notes for Chapter 9: Medians andOrder Statistics 9- 3\\nAnalysis of thetotal numberof comparisons\\n\\x0fIfniseven,wedo1initial comparison andthen 3.n/NUL2/=2morecomparisons.\\n#of comparisonsD3.n/NUL2/\\n2C1\\nD3n/NUL6\\n2C1\\nD3n\\n2/NUL3C1\\nD3n\\n2/NUL2 :\\n\\x0fIfnis odd, wedo 3.n/NUL1/=2D3bn=2ccomparisons.\\nIneither case, the maximum number of comparisons is \\x143bn=2c.\\nSelection inexpected lineartime\\nSelection of the ith smallest element of the array Acan bedone in ‚.n/time.\\nThe function R ANDOMIZED -SELECTuses RANDOMIZED -PARTITION from the\\nquicksort algorithm in Chapter 7. R ANDOMIZED -SELECTdiffers from quicksort\\nbecause it recurses on one side of the partition only.\\nRANDOMIZED -SELECT .A; p; r; i/\\nifp==r\\nreturn AŒp\\x8d\\nqDRANDOMIZED -PARTITION .A; p; r/\\nkDq/NULpC1\\nifi==k//pivot value isthe answer\\nreturn AŒq\\x8d\\nelseif i < k\\nreturnRANDOMIZED -SELECT .A; p; q/NUL1; i/\\nelse return RANDOMIZED -SELECT .A; qC1; r; i/NULk/\\nAfter the call to R ANDOMIZED -PARTITION , the array is partitioned into two sub-\\narrays AŒp : : q/NUL1\\x8dandAŒqC1 : : r\\x8d,along witha pivotelement AŒq\\x8d.\\n\\x0fTheelements of subarray AŒp : : q/NUL1\\x8dareall\\x14AŒq\\x8d.\\n\\x0fTheelements of subarray AŒqC1 : : r\\x8dare all > AŒq\\x8d.\\n\\x0fThe pivot element is the kth element of the subarray AŒp : : r\\x8d, where kD\\nq/NULpC1.\\n\\x0fIf thepivot element is the ithsmallest element (i.e., iDk),return AŒq\\x8d.\\n\\x0fOtherwise, recurse onthe subarray containing the ith smallest element.\\n\\x0fIfi < k,thissubarray is AŒp : : q/NUL1\\x8d,andwewantthe ithsmallestelement.\\n\\x0fIfi > k, this subarray is AŒqC1 : : r\\x8dand, since there are kelements in\\nAŒp : : r\\x8d that precede AŒqC1 : : r\\x8d, we want the .i/NULk/th smallest element\\nof this subarray.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 146}),\n",
              " Document(page_content='9-4 Lecture Notes for Chapter 9: Medians and Order Statistic s\\nAnalysis\\nWorst-case runningtime\\n‚.n2/, because we could be extremely unlucky and always recurse on a subarray\\nthat is only 1element smaller than the previous subarray.\\nExpected runningtime\\nRANDOMIZED -SELECTworks well on average. Because it israndomized, no par-\\nticular input brings out the worst-case behavior consisten tly.\\nThe running time of R ANDOMIZED -SELECTis a random variable that we denote\\nbyT .n/. Weobtain anupper bound onE ŒT .n/\\x8das follows:\\n\\x0fRANDOMIZED -PARTITION is equally likely to return any element of Aas the\\npivot.\\n\\x0fFor each ksuch that 1\\x14k\\x14n, the subarray AŒp : : q\\x8d haskelements (all\\x14\\npivot) with probability 1=n.[Note that we’re now considering a subarray that\\nincludes thepivot, along with elements less than the pivot. ]\\n\\x0fForkD1; 2; : : : ; n , deﬁne indicator random variable\\nXkDIfsubarray AŒp : : q\\x8d hasexactly kelementsg:\\n\\x0fSince Prfsubarray AŒp : : q\\x8d has exactly kelementsgD1=n, Lemma 5.1 says\\nthat E ŒXk\\x8dD1=n.\\n\\x0fWhen we call R ANDOMIZED -SELECT, we don’t know if it will terminate im-\\nmediately with the correct answer, recurse on AŒp : : q/NUL1\\x8d, or recurse on\\nAŒqC1 : : r\\x8d. Itdepends onwhether the ithsmallest element islessthan, equal\\nto, or greater than the pivot element AŒq\\x8d.\\n\\x0fTo obtain an upper bound, we assume that T .n/is monotonically increasing\\nand that the ithsmallest element isalways in the larger subarray.\\n\\x0fForagivencall of R ANDOMIZED -SELECT,XkD1forexactly onevalueof k,\\nandXkD0for all other k.\\n\\x0fWhen XkD1, the twosubarrays have sizes k/NUL1andn/NULk.\\n\\x0fFor a subproblem of size n, RANDOMIZED -PARTITION takes O.n/time.[Ac-\\ntually,ittakes ‚.n/time,but O.n/sufﬁces,sincewe’reobtainingonlyanupper\\nbound onthe expected running time.]\\n\\x0fTherefore, wehave therecurrence\\nT .n/\\x14nX\\nkD1Xk\\x01.T .max.k/NUL1; n/NULk//CO.n//\\nDnX\\nkD1Xk\\x01T .max.k/NUL1; n/NULk//CO.n/ :\\n\\x0fTaking expected values gives\\nEŒT .n/\\x8d\\n\\x14E\"nX\\nkD1Xk\\x01T .max.k/NUL1; n/NULk//CO.n/#', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 147}),\n",
              " Document(page_content='Lecture Notes for Chapter 9: Medians andOrder Statistics 9- 5\\nDnX\\nkD1EŒXk\\x01T .max.k/NUL1; n/NULk//\\x8dCO.n/ (linearity of expectation)\\nDnX\\nkD1EŒXk\\x8d\\x01EŒT .max.k/NUL1; n/NULk//\\x8dCO.n/(equation (C.24))\\nDnX\\nkD11\\nn\\x01EŒT .max.k/NUL1; n/NULk//\\x8dCO.n/ :\\n\\x0fWerely on XkandT .max.k/NUL1; n/NULk//being independent random variables\\nin order to apply equation (C.24).\\n\\x0fLooking at the expression max .k/NUL1; n/NULk/,wehave\\nmax.k/NUL1; n/NULk/D(\\nk/NUL1ifk >dn=2e;\\nn/NULkifk\\x14dn=2e:\\n\\x0fIfnis even, each term from T .dn=2e/up to T .n/NUL1/appears exactly twice\\ninthe summation.\\n\\x0fIfnisodd, these terms appear twice and T .bn=2c/appears once.\\n\\x0fEither way,\\nEŒT .n/\\x8d\\x142\\nnn/NUL1X\\nkDbn=2cEŒT .k/\\x8dCO.n/ :\\n\\x0fSolve this recurrence by substitution:\\n\\x0fGuessthat T .n/\\x14cnfor someconstant cthatsatisﬁestheinitial conditions\\nof the recurrence.\\n\\x0fAssume that T .n/DO.1/forn <some constant. We’ll pick this constant\\nlater.\\n\\x0fAlso pick a constant asuch that the function described by the O.n/term is\\nbounded from above by anfor all n > 0.\\n\\x0fUsing this guess and constants canda,wehave\\nEŒT .n/\\x8d\\x142\\nnn/NUL1X\\nkDbn=2cckCan\\nD2c\\nn n/NUL1X\\nkD1k/NULbn=2c/NUL1X\\nkD1k!\\nCan\\nD2c\\nn\\x12.n/NUL1/n\\n2/NUL.bn=2c/NUL1/bn=2c\\n2\\x13\\nCan\\n\\x142c\\nn\\x12.n/NUL1/n\\n2/NUL.n=2/NUL2/.n=2/NUL1/\\n2\\x13\\nCan\\nD2c\\nn\\x12n2/NULn\\n2/NULn2=4/NUL3n=2C2\\n2\\x13\\nCan\\nDc\\nn\\x123n2\\n4Cn\\n2/NUL2\\x13\\nCan', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 148}),\n",
              " Document(page_content='9-6 Lecture Notes for Chapter 9: Medians and Order Statistic s\\nDc\\x123n\\n4C1\\n2/NUL2\\nn\\x13\\nCan\\n\\x143cn\\n4Cc\\n2Can\\nDcn/NUL\\x10cn\\n4/NULc\\n2/NULan\\x11\\n:\\n\\x0fTocomplete this proof, wechoose csuch that\\ncn=4/NULc=2/NULan\\x150\\ncn=4/NULan\\x15c=2\\nn.c=4/NULa/\\x15c=2\\nn\\x15c=2\\nc=4/NULa\\nn\\x152c\\nc/NUL4a:\\n\\x0fThus,aslongasweassumethat T .n/DO.1/forn < 2c=.c/NUL4a/,wehave\\nEŒT .n/\\x8dDO.n/.\\nTherefore, wecan determine any order statistic inlinear ti meon average.\\nSelection inworst-caselineartime\\nWecanﬁndthe ithsmallestelementin O.n/timeintheworstcase . We’lldescribe\\naprocedure S ELECTthat does so.\\nSELECTrecursively partitions the input array.\\n\\x0fIdea:Guarantee agood split when the array ispartitioned.\\n\\x0fWill use the deterministic procedure P ARTITION , but with a small modiﬁca-\\ntion. Instead of assuming that the last element of the subarr ay is the pivot, the\\nmodiﬁed P ARTITION procedure is told which element to use asthe pivot.\\nSELECTworks onan array of n > 1elements. It executes the following steps:\\n1. Divide the nelements into groups of 5. Getdn=5egroups:bn=5cgroups with\\nexactly 5elements and, if 5does not divide n, one group with the remaining\\nnmod5elements.\\n2. Findthe median of each of the dn=5egroups:\\n\\x0fRun insertion sort on each group. Takes O.1/time per group since each\\ngroup has\\x145elements.\\n\\x0fThen just pick the median from each group, in O.1/time.\\n3. Find the median xof thedn=5emedians by a recursive call to S ELECT. (If\\ndn=5eiseven, then follow our convention and ﬁndthe lower median. )\\n4. Usingthemodiﬁedversion of P ARTITION that takes thepivot element asinput,\\npartition the input array around x. Let xbe the kth element of the array after\\npartitioning, sothatthereare k/NUL1elementsonthelowsideofthepartition and\\nn/NULkelements on the high side.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 149}),\n",
              " Document(page_content='Lecture Notes for Chapter 9: Medians andOrder Statistics 9- 7\\n5. Nowthere are three possibilities:\\n\\x0fIfiDk,just return x.\\n\\x0fIfi < k, return the ith smallest element on the low side of the partition by\\nmaking arecursive call to S ELECT.\\n\\x0fIfi > k,returnthe .i/NULk/thsmallestelementonthehighsideofthepartition\\nby making arecursive call to S ELECT.\\nAnalysis\\nStart by getting a lower bound on the number of elements that a re greater than the\\npartitioning element x:\\nx\\n[Each group is a column. Each white circle is the median of a gr oup, as found\\nin step 2. Arrows go from larger elements to smaller elements , based on what we\\nknowafterstep4. Elementsintheregiononthelowerrightar eknowntobegreater\\nthanx.]\\n\\x0fAt least half of the medians found in step 2are \\x15x.\\n\\x0fLook at the groups containing these medians that are \\x15x. All of them con-\\ntribute 3elements that are > x(the median of the group and the 2elements\\nin the group greater than the group’s median), except for 2of the groups: the\\ngroup containing x(which has only 2elements > x) and the group with < 5\\nelements.\\n\\x0fForget about these 2groups. That leaves \\x15\\x181\\n2ln\\n5m\\x19\\n/NUL2groups with 3ele-\\nments known to be > x.\\n\\x0fThus, weknow that at least\\n3\\x12\\x181\\n2ln\\n5m\\x19\\n/NUL2\\x13\\n\\x153n\\n10/NUL6\\nelements are > x.\\nSymmetrically, thenumber of elements that are < xisat least 3n=10/NUL6.\\nTherefore, when we call S ELECTrecursively in step 5, it’s on \\x147n=10C6ele-\\nments.\\nDevelop arecurrence for the worst-case running timeof S ELECT:\\n\\x0fSteps 1, 2, and 4each take O.n/time:', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 150}),\n",
              " Document(page_content='9-8 Lecture Notes for Chapter 9: Medians and Order Statistic s\\n\\x0fStep 1: making groups of 5elements takes O.n/time.\\n\\x0fStep 2: sortingdn=5egroups in O.1/timeeach.\\n\\x0fStep 4: partitioning the n-element array around xtakes O.n/time.\\n\\x0fStep3takes time T .dn=5e/.\\n\\x0fStep 5 takes time\\x14T .7n=10C6/, assuming that T .n/is monotonically in-\\ncreasing.\\n\\x0fAssume that T .n/DO.1/for small enough n. We’ll use n < 140 as “small\\nenough.” Why140? We’ll see whylater.\\n\\x0fThus, weget therecurrence\\nT .n/\\x14(\\nO.1/ ifn < 140 ;\\nT .dn=5e/CT .7n=10C6/CO.n/ifn\\x15140 :\\nSolve this recurrence by substitution:\\n\\x0fInductive hypothesis: T .n/\\x14cnfor some constant cand all n > 0.\\n\\x0fAssume that cis large enough that T .n/\\x14cnfor all n < 140. So we are\\nconcerned only with the case n\\x15140.\\n\\x0fPick a constant asuch that the function described by the O.n/term in the\\nrecurrence is\\x14anfor all n > 0.\\n\\x0fSubstitute the inductive hypothesis inthe right-hand side of the recurrence:\\nT .n/\\x14cdn=5eCc.7n=10C6/Can\\n\\x14cn=5CcC7cn=10C6cCan\\nD9cn=10C7cCan\\nDcnC./NULcn=10C7cCan/ :\\n\\x0fThislast quantity is \\x14cnif\\n/NULcn=10C7cCan\\x140\\ncn=10/NUL7c\\x15an\\ncn/NUL70c\\x1510an\\nc.n/NUL70/\\x1510an\\nc\\x1510a.n=.n/NUL70// :\\n\\x0fBecause weassumed that n\\x15140, wehave n=.n/NUL70/\\x142.\\n\\x0fThus, 20a\\x1510a.n=.n/NUL70//,sochoosing c\\x1520agives c\\x1510a.n=.n/NUL70//,\\nwhich inturn gives us the condition weneed toshow that T .n/\\x14cn.\\n\\x0fWeconcludethat T .n/DO.n/,sothat S ELECTrunsinlineartimeinallcases.\\n\\x0fWhy140? Wecould have used anyinteger strictly greater than 70.\\n\\x0fObserve that for n > 70, the fraction n=.n/NUL70/decreases as nincreases.\\n\\x0fWe picked n\\x15140so that the fraction would be \\x142, which is an easy\\nconstant to workwith.\\n\\x0fWecouldhavepicked, say, n\\x1571,sothatforall n\\x1571,thefraction would\\nbe\\x1471=.71/NUL70/D71. Then we would have had 20a\\x15710a, so we’d\\nhave needed tochoose c\\x15710a.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 151}),\n",
              " Document(page_content='Lecture Notes for Chapter 9: Medians andOrder Statistics 9- 9\\nNotice that S ELECTand RANDOMIZED -SELECTdetermine information about the\\nrelative order of elements only bycomparing elements.\\n\\x0fSorting requires \\x7f.nlgn/time inthe comparison model.\\n\\x0fSortingalgorithms that runinlinear timeneedtomakeassum ptions about their\\ninput.\\n\\x0fLinear-time selection algorithms do not require any assumptions about their\\ninput.\\n\\x0fLinear-time selection algorithms solve the selection prob lem without sorting\\nand therefore are not subject tothe \\x7f.nlgn/lower bound.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 152}),\n",
              " Document(page_content='Solutionsfor Chapter9:\\nMedians andOrder Statistics\\nSolution to Exercise9.1-1\\nThe smallest of nnumbers can be found with n/NUL1comparisons by conducting a\\ntournament asfollows: Compareallthenumbersinpairs. Onl ythesmallerofeach\\npaircouldpossibly bethesmallestofall n,sotheproblemhasbeenreducedtothat\\nof ﬁndingthesmallest of dn=2enumbers. Compare thosenumbers inpairs, andso\\non, until there’s just one number left, which is theanswer.\\nToseethat thisalgorithm doesexactly n/NUL1comparisons, noticethat eachnumber\\nexcept the smallest loses exactly once. Toshow this more for mally, draw a binary\\ntreeofthecomparisonsthealgorithmdoes. The nnumbersaretheleaves,andeach\\nnumberthatcameoutsmallerinacomparisonistheparentoft hetwonumbersthat\\nwere compared. Eachnon-leaf node of the tree represents aco mparison, and there\\naren/NUL1internal nodes in an n-leaf full binary tree (see Exercise (B.5-3)), so\\nexactly n/NUL1comparisons are made.\\nInthesearchforthesmallest number, thesecondsmallest nu mber musthavecome\\nout smallest in every comparison made with it until it was eve ntually compared\\nwith the smallest. So the second smallest is among the elemen ts that were com-\\npared with the smallest during the tournament. To ﬁnd it, con duct another tourna-\\nment (as above) to ﬁnd the smallest of these numbers. At most dlgne(the height\\nof the tree of comparisons) elements were compared with the s mallest, so ﬁnding\\nthe smallest of these takes dlgne/NUL1comparisons inthe worst case.\\nThetotal number of comparisons made inthe twotournaments w as\\nn/NUL1Cdlgne/NUL1DnCdlgne/NUL2\\nin the worst case.\\nSolution to Exercise9.3-1\\nThis solutionisalsopostedpublicly\\nFor groups of 7, the algorithm still works in linear time. The number of elements\\ngreater than x(and similarly, the number less than x) isat least\\n4\\x12\\x181\\n2ln\\n7m\\x19\\n/NUL2\\x13\\n\\x152n\\n7/NUL8 ;', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 153}),\n",
              " Document(page_content='Solutions forChapter 9: Medians and Order Statistics 9-11\\nand the recurrence becomes\\nT .n/\\x14T .dn=7e/CT .5n=7C8/CO.n/ ;\\nwhich can be shown to be O.n/by substitution, as for the groups of 5 case in the\\ntext.\\nForgroupsof3,however,thealgorithmnolongerworksinlin eartime. Thenumber\\nof elements greater than x,and the number of elements less than x,isat least\\n2\\x12\\x181\\n2ln\\n3m\\x19\\n/NUL2\\x13\\n\\x15n\\n3/NUL4 ;\\nand the recurrence becomes\\nT .n/\\x14T .dn=3e/CT .2n=3C4/CO.n/ ;\\nwhich does not have alinear solution.\\nWe can prove that the worst-case time for groups of 3 is \\x7f.nlgn/. We do so by\\nderiving arecurrence for aparticular case that takes \\x7f.nlgn/time.\\nIn counting up the number of elements greater than x(and similarly, the num-\\nber less than x), consider the particular case in which there are exactlyl\\n1\\n2l\\nn\\n3mm\\ngroups with medians \\x15xand in which the “leftover” group does contribute 2\\nelements greater than x. Then the number of elements greater than xis exactly\\n2\\x10l\\n1\\n2l\\nn\\n3mm\\n/NUL1\\x11\\nC1(the/NUL1discounts x’s group, as usual, and the C1is con-\\ntributed by x’s group)D2dn=6e/NUL1, and the recursive step for elements \\x14xhas\\nn/NUL.2dn=6e/NUL1/\\x15n/NUL.2.n=6C1//NUL1/D2n=3/NUL1elements. Observe also\\nthat the O.n/term in the recurrence is really ‚.n/, since the partitioning instep 4\\ntakes ‚.n/(not just O.n/) time. Thus, weget the recurrence\\nT .n/\\x15T .dn=3e/CT .2n=3/NUL1/C‚.n/\\x15T .n=3/CT .2n=3/NUL1/C‚.n/ ;\\nfrom which you can show that T .n/\\x15cnlgnby substitution. You can also see\\nthatT .n/isnonlinear by noticing that each level of the recursion tre e sumsto n.\\n[Infact, any odd group size \\x155works inlinear time.]\\nSolutionto Exercise 9.3-3\\nThissolutionisalsopostedpublicly\\nAmodiﬁcation toquicksort that allowsittorunin O.nlgn/timeintheworstcase\\nuses the deterministic P ARTITION algorithm that was modiﬁed to take an element\\ntopartition around as aninput parameter.\\nSELECTtakes an array A, the bounds pandrof the subarray in A, and the rank i\\nofanorderstatistic, andintimelinearinthesizeofthesub array AŒp : : r\\x8ditreturns\\ntheithsmallest element in AŒp : : r\\x8d.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 154}),\n",
              " Document(page_content='9-12 Solutions for Chapter 9: Medians and Order Statistics\\nBEST-CASE-QUICKSORT .A; p; r/\\nifp < r\\niDb.r/NULpC1/=2c\\nxDSELECT .A; p; r; i/\\nqDPARTITION .x/\\nBEST-CASE-QUICKSORT .A; p; q/NUL1/\\nBEST-CASE-QUICKSORT .A; qC1; r/\\nFor an n-element array, the largest subarray that B EST-CASE-QUICKSORT re-\\ncurses on has n=2elements. This situation occurs when nDr/NULpC1is even;\\nthen the subarray AŒqC1 : : r\\x8dhasn=2elements, and the subarray AŒp : : q/NUL1\\x8d\\nhasn=2/NUL1elements.\\nBecause B EST-CASE-QUICKSORT always recurses on subarrays that are at most\\nhalf the size of the original array, the recurrence for the wo rst-case running timeis\\nT .n/\\x142T .n=2/C‚.n/DO.nlgn/.\\nSolution to Exercise9.3-5\\nThis solutionisalsopostedpublicly\\nWe assume that are given a procedure M EDIANthat takes as parameters an ar-\\nrayAandsubarray indices pandr,andreturns thevalueofthemedianelement of\\nAŒp : : r\\x8dinO.n/timein the worst case.\\nGiven M EDIAN, here isalinear-time algorithm S ELECT0for ﬁnding the ith small-\\nest element in AŒp : : r\\x8d. This algorithm uses the deterministic P ARTITION algo-\\nrithm that was modiﬁed totake an element topartition around as an input parame-\\nter.\\nSELECT0.A; p; r; i/\\nifp==r\\nreturn AŒp\\x8d\\nxDMEDIAN .A; p; r/\\nqDPARTITION .x/\\nkDq/NULpC1\\nifi==k\\nreturn AŒq\\x8d\\nelseif i < k\\nreturnSELECT0.A; p; q/NUL1; i/\\nelse return SELECT0.A; qC1; r; i/NULk/\\nBecause xis the median of AŒp : : r\\x8d, each of the subarrays AŒp : : q/NUL1\\x8dand\\nAŒqC1 : : r\\x8dhas at most half the number of elements of AŒp : : r\\x8d. The recurrence\\nfor the worst-case running timeof S ELECT0isT .n/\\x14T .n=2/CO.n/DO.n/.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 155}),\n",
              " Document(page_content='Solutions forChapter 9: Medians and Order Statistics 9-13\\nSolutionto Exercise 9.3-8\\nLet’s start out by supposing that the median (the lower media n, since we know we\\nhavean evennumber of elements) isin X. Let’scall themedian value m,and let’s\\nsuppose that it’s in XŒk\\x8d. Then kelements of Xare less than or equal to mand\\nn/NULkelementsof Xaregreater thanorequal to m. Weknowthatinthetwoarrays\\ncombined, theremustbe nelementslessthanor equal to mandnelements greater\\nthan or equal to m, and so there must be n/NULkelements of Ythat are less than or\\nequal to mandn/NUL.n/NULk/Dkelements of Ythat are greater than or equal to m.\\nThus,wecancheckthat XŒk\\x8disthelowermedianbycheckingwhether Y Œn/NULk\\x8d\\x14\\nXŒk\\x8d\\x14Y Œn/NULkC1\\x8d. A boundary case occurs for kDn. Then n/NULkD0, and\\nthere is noarray entry Y Œ0\\x8d;weonly need tocheck that XŒn\\x8d\\x14Y Œ1\\x8d.\\nNow, if the median is in Xbut is not in XŒk\\x8d, then the above condition will not\\nhold. If the median is in XŒk0\\x8d, where k0< k, then XŒk\\x8dis above the median, and\\nY Œn/NULkC1\\x8d < XŒk\\x8d . Conversely, if the median is in XŒk00\\x8d, where k00> k, then\\nXŒk\\x8dis below the median, and XŒk\\x8d < Y Œn/NULk\\x8d.\\nThus, we can use a binary search to determine whether there is anXŒk\\x8dsuch that\\neither k < nandY Œn/NULk\\x8d\\x14XŒk\\x8d\\x14Y Œn/NULkC1\\x8dorkDnandXŒk\\x8d\\x14Y Œn/NULkC1\\x8d;\\nifweﬁndsuchan XŒk\\x8d,thenitisthemedian. Otherwise, weknowthatthemedian\\nis in Y, and we use a binary search to ﬁnd a Y Œk\\x8dsuch that either k < nand\\nXŒn/NULk\\x8d\\x14Y Œk\\x8d\\x14XŒn/NULkC1\\x8dorkDnandY Œk\\x8d\\x14XŒn/NULkC1\\x8d; such a\\nY Œk\\x8dis the median. Since each binary search takes O.lgn/time, we spend a total\\nofO.lgn/time.\\nHere’s how wewrite the algorithm in pseudocode:\\nTWO-ARRAY-MEDIAN .X; Y /\\nnDX:length//nalso equals Y:length\\nmedianDFIND-MEDIAN .X; Y; n; 1; n/\\nifmedian==NOT-FOUND\\nmedianDFIND-MEDIAN .Y; X; n; 1; n/\\nreturnmedian\\nFIND-MEDIAN .A; B; n; low;high/\\niflow>high\\nreturn NOT-FOUND\\nelsekDb.lowChigh/=2c\\nifk==nandAŒn\\x8d\\x14BŒ1\\x8d\\nreturn AŒn\\x8d\\nelseif k < nandBŒn/NULk\\x8d\\x14AŒk\\x8d\\x14BŒn/NULkC1\\x8d\\nreturn AŒk\\x8d\\nelseif AŒk\\x8d > BŒn/NULkC1\\x8d\\nreturnFIND-MEDIAN .A; B; n; low; k/NUL1/\\nelse return FIND-MEDIAN .A; B; n; kC1;high/', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 156}),\n",
              " Document(page_content='9-14 Solutions for Chapter 9: Medians and Order Statistics\\nSolution to Exercise9.3-9\\nIn order to ﬁnd the optimal placement for Professor Olay’s pi peline, weneed only\\nﬁnd the median(s) of the y-coordinates of his oil wells, as the following proof\\nexplains.\\nClaim\\nTheoptimal y-coordinate for Professor Olay’s east-west oil pipeline is as follows:\\n\\x0fIfniseven, then oneither the oil well whose y-coordinate isthelower median\\northeonewhose y-coordinate istheuppermedian,oranywherebetweenthem.\\n\\x0fIfnisodd, then on the oil well whose y-coordinate is the median.\\nProofWe examine various cases. In each case, we will start out with the pipeline\\nat a particular y-coordinate and see what happens when we move it. We’ll denot e\\nbysthe sum of the north-south spurs with the pipeline at the star ting location,\\nands0will denote the sum after moving the pipeline.\\nWestart withthecaseinwhich niseven. Letusstart withthepipeline somewhere\\non or between the two oil wells whose y-coordinates are the lower and upper me-\\ndians. If we move the pipeline by a vertical distance dwithout crossing either of\\nthe median wells, then n=2of the wells become dfarther from the pipeline and\\nn=2become dcloser, and so s0DsCdn=2/NULdn=2Ds; thus, all locations onor\\nbetween the twomedians areequally good.\\nNowsuppose that the pipeline goes through theoil well whose y-coordinate isthe\\nupper median. What happens when we increase the y-coordinate of the pipeline\\nbyd > 0units, sothat itmovesabovetheoilwellthat achievestheup per median?\\nAlloilwellswhose y-coordinates areatorbelowtheuppermedianbecome dunits\\nfarther from the pipeline, and there are at least n=2C1such oil wells (the upper\\nmedian, and every well at or below the lower median). There ar e at most n=2/NUL1\\noil wells whose y-coordinates are above the upper median, and each of these oi l\\nwells becomes at most dunits closer to the pipeline when it moves up. Thus, we\\nhave a lower bound on s0ofs0\\x15sCd.n=2C1//NULd.n=2/NUL1/DsC2d > s.\\nWe conclude that moving the pipeline up from the oil well at th e upper median\\nincreases the total spur length. A symmetric argument shows that if we start with\\nthepipelinegoingthroughtheoilwellwhose y-coordinate isthelowermedianand\\nmoveit down, then the total spur length increases.\\nWe see, therefore, that when nis even, an optimal placement of the pipeline is\\nanywhere on or between the twomedians.\\nNowweconsider thecase when nisodd. Westart withthepipeline going through\\ntheoilwellwhose y-coordinateisthemedian,andweconsiderwhathappenswhen\\nwemoveitupby d > 0units. Alloilwellsatorbelowthemedianbecome dunits\\nfarther fromthepipeline, andthereareatleast .nC1/=2suchwells(theoneatthe\\nmedian and the .n/NUL1/=2at or below the median. There are at most .n/NUL1/=2oil\\nwells above the median, and each of these becomes at most dunits closer to the\\npipeline. We get a lower bound on s0ofs0\\x15sCd.nC1/=2/NULd.n/NUL1/=2D\\nsCd > s, and we conclude that moving the pipeline up from the oil well at the', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 157}),\n",
              " Document(page_content='Solutions forChapter 9: Medians and Order Statistics 9-15\\nmedian increases the total spur length. A symmetric argumen t shows that moving\\nthe pipeline down from the median also increases the total sp ur length, and so the\\noptimal placement of the pipeline ison themedian. (claim)\\nSince weknow wearelooking for the median, wecan use the line ar-time median-\\nﬁnding algorithm.\\nSolutionto Problem 9-1\\nThissolutionisalsopostedpublicly\\nWeassume that the numbers start out inan array.\\na.Sortthenumbersusingmergesortorheapsort, whichtake ‚.nlgn/worst-case\\ntime. (Don’t use quicksort or insertion sort, which can take ‚.n2/time.) Put\\ntheilargest elements (directly accessible in the sorted array) into the output\\narray, taking ‚.i/time.\\nTotal worst-case running time: ‚.nlgnCi/D‚.nlgn/(because i\\x14n).\\nb.Implement the priority queue as a heap. Build the heap using B UILD-HEAP,\\nwhich takes ‚.n/time, then call H EAP-EXTRACT-MAXitimes to get the i\\nlargest elements, in ‚.ilgn/worst-case time, and store them in reverse order\\nof extraction in the output array. The worst-case extractio n time is ‚.ilgn/\\nbecause\\n\\x0fiextractions from a heap with O.n/elements takes i\\x01O.lgn/DO.ilgn/\\ntime, and\\n\\x0fhalf of the iextractions are from a heap with \\x15n=2elements, so those i=2\\nextractions take .i=2/\\x7f.lg.n=2//D\\x7f.ilgn/timeinthe worst case.\\nTotal worst-case running time: ‚.nCilgn/.\\nc.UsetheS ELECTalgorithmofSection9.3toﬁndthe ithlargestnumberin ‚.n/\\ntime. Partition around that number in ‚.n/time. Sort the ilargest numbers in\\n‚.ilgi/worst-case time (withmerge sort or heapsort).\\nTotal worst-case running time: ‚.nCilgi/.\\nNote that method (c) is always asymptotically at least as goo d as the other two\\nmethods, and that method (b) is asymptotically at least as go od as (a). (Com-\\nparing (c) to (b) is easy, but it is less obvious how to compare (c) and (b) to (a).\\n(c)and(b) areasymptotically atleast asgoodas(a)because n,ilgi,and ilgnare\\nallO.nlgn/. Thesum of two things that are O.nlgn/is also O.nlgn/.)', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 158}),\n",
              " Document(page_content='9-16 Solutions for Chapter 9: Medians and Order Statistics\\nSolution to Problem 9-2\\na.The median xof the elements x1; x2; : : : ; x n, is an element xDxksatisfy-\\ningjfxiW1\\x14i\\x14nandxi< xgj\\x14n=2andjfxiW1\\x14i\\x14nandxi> xgj\\x14\\nn=2. If each element xiis assigned aweight wiD1=n, then weget\\nX\\nxi<xwiDX\\nxi<x1\\nn\\nD1\\nn\\x01X\\nxi<x1\\nD1\\nn\\x01jfxiW1\\x14i\\x14nandxi< xgj\\n\\x141\\nn\\x01n\\n2\\nD1\\n2;\\nand\\nX\\nxi>xwiDX\\nxi>x1\\nn\\nD1\\nn\\x01X\\nxi>x1\\nD1\\nn\\x01jfxiW1\\x14i\\x14nandxi> xgj\\n\\x141\\nn\\x01n\\n2\\nD1\\n2;\\nwhichproves that xisalso theweighted median of x1; x2; : : : ; x nwithweights\\nwiD1=n, foriD1; 2; : : : ; n .\\nb.We ﬁrst sort the nelements into increasing order by xivalues. Then we scan\\nthe array of sorted xi’s, starting with the smallest element and accumulating\\nweightsaswescan,untilthetotalexceeds 1=2. Thelastelement,say xk,whose\\nweight caused the total to exceed 1=2, is the weighted median. Notice that the\\ntotal weight of all elements smaller than xkis less than 1=2, because xkwas\\nthe ﬁrst element that caused the total weight to exceed 1=2. Similarly, the total\\nweight of all elements larger than xkis also less than 1=2, because the total\\nweight of all the other elements exceeds 1=2.\\nThe sorting phase can be done in O.nlgn/worst-case time (using merge sort\\nor heapsort), and the scanning phase takes O.n/time. The total running time\\ninthe worst case, therefore, is O.nlgn/.\\nc.We ﬁnd the weighted median in ‚.n/worst-case time using the ‚.n/worst-\\ncase median algorithm in Section 9.3. (Although the ﬁrst par agraph of the\\nsection onlyclaimsan O.n/upper bound, itiseasytoseethatthemoreprecise', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 159}),\n",
              " Document(page_content='Solutions forChapter 9: Medians and Order Statistics 9-17\\nrunningtimeof ‚.n/appliesaswell,sincesteps1,2,and4ofS ELECTactually\\ntake‚.n/time.)\\nThe weighted-median algorithm works as follows. If n\\x142, we just return\\nthe brute-force solution. Otherwise, we proceed as follows . We ﬁnd the actual\\nmedian xkof the nelements and then partition around it. Wethen compute the\\ntotalweightsofthetwohalves. Iftheweightsofthetwohalv esareeachstrictly\\nlessthan 1=2,thentheweightedmedianis xk. Otherwise, theweightedmedian\\nshould be in the half with total weight exceeding 1=2. The total weight of the\\n“light”halfislumpedintotheweightof xk,andthesearchcontinues withinthe\\nhalf that weighs more than 1=2. Here’s pseudocode, which takes as input a set\\nXDfx1; x2; : : : ; x ng:\\nWEIGHTED -MEDIAN .X/\\nifn==1\\nreturn x1\\nelseif n==2\\nifw1\\x15w2\\nreturn x1\\nelse return x2\\nelseﬁnd themedian xkofXDfx1; x2; : : : ; x ng\\npartition the set Xaround xk\\ncompute WLDP\\nxi<xkwiandWGDP\\nxi>xkwi\\nifWL< 1=2andWG< 1=2\\nreturn xk\\nelseif WL> 1=2\\nwkDwkCWG\\nX0Dfxi2XWxi\\x14xkg\\nreturnWEIGHTED -MEDIAN .X0/\\nelsewkDwkCWL\\nX0Dfxi2XWxi\\x15xkg\\nreturnWEIGHTED -MEDIAN .X0/\\nThe recurrence for the worst-case running time of W EIGHTED -MEDIANis\\nT .n/DT .n=2C1/C‚.n/,sincethereisatmostonerecursivecallonhalfthe\\nnumberofelements,plusthemedianelement xk,andalltheworkprecedingthe\\nrecursive call takes ‚.n/time. Thesolution of therecurrence is T .n/D‚.n/.\\nd.Let the npoints be denoted by their coordinates x1; x2; : : : ; x n, let the corre-\\nsponding weights be w1; w2; : : : ; w n, and let xDxkbe the weighted median.\\nFor any point p, letf .p/DPn\\niD1wijp/NULxij; we want to ﬁnd a point psuch\\nthatf .p/isminimum. Let ybeanypoint(realnumber)otherthan x. Weshow\\ntheoptimalityoftheweightedmedian xbyshowingthat f .y//NULf .x/\\x150. We\\nexamine separately the cases in which y > xandx > y. For any xandy, we\\nhave', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 160}),\n",
              " Document(page_content='9-18 Solutions for Chapter 9: Medians and Order Statistics\\nf .y//NULf .x/DnX\\niD1wijy/NULxij/NULnX\\niD1wijx/NULxij\\nDnX\\niD1wi.jy/NULxij/NULjx/NULxij/ :\\nWhen y > x, we bound the quantity jy/NULxij/NULjx/NULxijfrom below by exam-\\nining three cases:\\n1.x < y\\x14xi: Here,jx/NULyjCjy/NULxijDjx/NULxijandjx/NULyjDy/NULx,\\nwhich imply thatjy/NULxij/NULjx/NULxijD/NULjx/NULyjDx/NULy.\\n2.x < x i\\x14y: Here,jy/NULxij\\x150andjxi/NULxj\\x14y/NULx, which imply that\\njy/NULxij/NULjx/NULxij\\x15/NUL.y/NULx/Dx/NULy.\\n3.xi\\x14x < y: Here,jx/NULxijCjy/NULxjDjy/NULxijandjy/NULxjDy/NULx,\\nwhich imply thatjy/NULxij/NULjx/NULxijDjy/NULxjDy/NULx.\\nSeparating out the ﬁrst two cases, in which x < x i, from the third case, in\\nwhich x\\x15xi, weget\\nf .y//NULf .x/DnX\\niD1wi.jy/NULxij/NULjx/NULxij/\\n\\x15X\\nx<x iwi.x/NULy/CX\\nx\\x15xiwi.y/NULx/\\nD.y/NULx/ X\\nx\\x15xiwi/NULX\\nx<x iwi!\\n:\\nThe property thatP\\nxi<xwi< 1=2implies thatP\\nx\\x15xiwi\\x151=2. This fact,\\ncombined with y/NULx > 0andP\\nx<x iwi\\x141=2, yields that f .y//NULf .x/\\x150.\\nWhen x > y, we again bound the quantity jy/NULxij/NULjx/NULxijfrom below by\\nexamining three cases:\\n1.xi\\x14y < x: Here,jy/NULxijCjx/NULyjDjx/NULxijandjx/NULyjDx/NULy,\\nwhich imply thatjy/NULxij/NULjx/NULxijD/NULjx/NULyjDy/NULx.\\n2.y\\x14xi< x: Here,jy/NULxij\\x150andjx/NULxij\\x14x/NULy, which imply that\\njy/NULxij/NULjx/NULxij\\x15/NUL.x/NULy/Dy/NULx.\\n3.y < x\\x14xi. Here,jx/NULyjCjx/NULxijDjy/NULxijandjx/NULyjDx/NULy,\\nwhich imply thatjy/NULxij/NULjx/NULxijDjx/NULyjDx/NULy.\\nSeparating out the ﬁrst two cases, in which x > x i, from the third case, in\\nwhich x\\x14xi, weget\\nf .y//NULf .x/DnX\\niD1wi.jy/NULxij/NULjx/NULxij/\\n\\x15X\\nx>x iwi.y/NULx/CX\\nx\\x14xiwi.x/NULy/\\nD.x/NULy/ X\\nx\\x14xiwi/NULX\\nx>x iwi!\\n:\\nThe property thatP\\nxi>xwi\\x141=2implies thatP\\nx\\x14xiwi> 1=2. This fact,\\ncombined with x/NULy > 0andP\\nx>x iwi< 1=2, yields that f .y//NULf .x/ > 0 .', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 161}),\n",
              " Document(page_content='Solutions forChapter 9: Medians and Order Statistics 9-19\\ne.Weare given n2-dimensional points p1; p2; : : : ; p n, where each piisa pair of\\nreal numbers piD.xi; yi/, and positive weights w1; w2; : : : ; w n. The goal is\\nto ﬁndapoint pD.x; y/that minimizes the sum\\nf .x; y/DnX\\niD1wi.jx/NULxijCjy/NULyij/ :\\nWe can express the cost function of the two variables, f .x; y/, as the sum of\\ntwo functions of one variable each: f .x; y/Dg.x/Ch.y/, where g.x/DPn\\niD1wijx/NULxij, and h.y/DPn\\niD1wijy/NULyij. The goal of ﬁnding a point\\npD.x; y/that minimizes the value of f .x; y/can be achieved by treating\\neach dimension independently, because gdoes not depend on yandhdoesnot\\ndepend on x. Thus,\\nmin\\nx;yf .x; y/Dmin\\nx;y.g.x/Ch.y//\\nDmin\\nx\\x10\\nmin\\ny.g.x/Ch.y//\\x11\\nDmin\\nx\\x10\\ng.x/Cmin\\nyh.y/\\x11\\nDmin\\nxg.x/Cmin\\nyh.y/ :\\nConsequently, ﬁnding the best location in 2dimensions canb e done by ﬁnding\\nthe weighted median xkof the x-coordinates and then ﬁnding the weighted\\nmedian yjof the y-coordinates. The point .xk; yj/is an optimal solution for\\nthe 2-dimensional post-ofﬁce location problem.\\nSolutionto Problem 9-3\\na.Our algorithm relies on a particular property of S ELECT: that not only does it\\nreturn the ith smallest element, but that it also partitions the input ar ray so that\\nthe ﬁrst ipositions contain the ismallest elements (though not necessarily in\\nsorted order). Tosee that S ELECThasthis property, observe that there are only\\ntwo ways in which returns a value: when nD1, and when immediately after\\npartitioning in step 4, it ﬁnds that there are exactly ielements on the low side\\nof the partition.\\nTaking the hint from the book, here is our modiﬁed algorithm t o select the ith\\nsmallest elementof nelements. Whenever itiscalledwith i\\x15n=2,itjustcalls\\nSELECTand returns its result; in this case, Ui.n/DT .n/.\\nWhen i < n=2, our modiﬁed algorithm works as follows. Assume that the\\ninput is in asubarray AŒpC1 : : pCn\\x8d, and let mDbn=2c. In the initial call,\\npD1.\\n1. Divide the input as follows. If nis even, divide the input into two parts:\\nAŒpC1 : : pCm\\x8dandAŒpCmC1 : : pCn\\x8d. Ifnis odd, divide the input\\nintothreeparts: AŒpC1 : : pCm\\x8d,AŒpCmC1 : : pCn/NUL1\\x8d,and AŒpCn\\x8d\\nasaleftover piece.\\n2. Compare AŒpCi\\x8dandAŒpCiCm\\x8dforiD1; 2; : : : ; m ,puttingthesmaller\\nof the the twoelements into AŒpCiCm\\x8dand the larger into AŒpCi\\x8d.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 162}),\n",
              " Document(page_content='9-20 Solutions for Chapter 9: Medians and Order Statistics\\n3. Recursively ﬁnd the ith smallest element in AŒpCmC1 : : pCn\\x8d, but\\nwithanadditional actionperformedbythepartitioning pro cedure: whenever\\nit exchanges AŒj \\x8dandAŒk\\x8d(where pCmC1\\x14j; k\\x14pC2m), it\\nalso exchanges AŒj/NULm\\x8dandAŒk/NULm\\x8d. The idea is that after recursively\\nﬁnding the ith smallest element in AŒpCmC1 : : pCn\\x8d, the subarray\\nAŒpCmC1 : : pCmCi\\x8dcontains the ismallestelementsthathadbeenin\\nAŒpCmC1 : : pCn\\x8dandthesubarray AŒpC1 : : pCi\\x8dcontainstheirlarger\\ncounterparts, asfoundinstep1. The ithsmallestelementof AŒpC1 : : pCn\\x8d\\nmustbeeitheroneofthe ismallest,asplacedinto AŒpCmC1 : : pCmCi\\x8d,\\nor itmust beoneof thelarger counterparts, asplaced into AŒpC1 : : pCi\\x8d.\\n4. Collect thesubarrays AŒpC1 : : pCi\\x8dandAŒpCmC1 : : pCmCi\\x8dinto\\na single array BŒ1 : : 2i\\x8d , call SELECTto ﬁnd the ith smallest element of B,\\nand return the result of this call to S ELECT.\\nThenumber of comparisons in each step isas follows:\\n1. Nocomparisons.\\n2.mDbn=2ccomparisons.\\n3. Since werecurse on AŒpCmC1 : : pCn\\x8d,which hasdn=2eelements, the\\nnumber of comparisons is Ui.dn=2e/.\\n4. Since wecall S ELECTon an array with 2ielements, the number of compar-\\nisons is T .2i/.\\nThus,when i < n=2,thetotal numberofcomparisons is bn=2cCUi.dn=2e/C\\nT .2i/.\\nb.Weshow bysubstitution that if i < n=2, then Ui.n/DnCO.T .2i/ lg.n=i//.\\nIn particular, we show that Ui.n/\\x14nCcT .2i/lg.n=i//NULd.lglgn/T .2i/D\\nnCcT .2i/lgn/NULcT .2i/lgi/NULd.lglgn/T .2i/for some positive constant c,\\nsomepositive constant dtobe chosen later, and n\\x154. Wehave\\nUi.n/Dbn=2cCUi.dn=2e/CT .2i/\\n\\x14bn=2cCdn=2eCcT .2i/lgdn=2e/NULcT .2i/lgi\\n/NULd.lglgdn=2e/T .2i/\\nDnCcT .2i/lgdn=2e/NULcT .2i/lgi/NULd.lglgdn=2e/T .2i/\\n\\x14nCcT .2i/lg.n=2C1//NULcT .2i/lgi/NULd.lglg.n=2//T .2i/\\nDnCcT .2i/lg.n=2C1//NULcT .2i/lgi/NULd.lg.lgn/NUL1//T .2i/\\n\\x14nCcT .2i/lgn/NULcT .2i/lgi/NULd.lglgn/T .2i/\\nifcT .2i/lg.n=2C1//NULd.lg.lgn/NUL1//T .2i/\\x14cT .2i/lgn/NULd.lglgn/T .2i/.\\nSimplealgebraicmanipulationsgivesthefollowingsequen ceofequivalentcon-\\nditions:\\ncT .2i/lg.n=2C1//NULd.lg.lgn/NUL1//T .2i/\\x14cT .2i/lgn/NULd.lglgn/T .2i/\\nclg.n=2C1//NULd.lg.lgn/NUL1//\\x14clgn/NULd.lglgn/\\nc.lg.n=2C1//NULlgn/\\x14d.lg.lgn/NUL1//NULlglgn/\\nc\\x12\\nlgn=2C1\\nn\\x13\\n\\x14dlglgn/NUL1\\nlgn\\nc\\x12\\nlg\\x121\\n2C1\\nn\\x13\\x13\\n\\x14dlglgn/NUL1\\nlgn', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 163}),\n",
              " Document(page_content='Solutions forChapter 9: Medians and Order Statistics 9-21\\nObservethat 1=2C1=ndecreasesas nincreases,but .lgn/NUL1/=lgnincreasesas\\nnincreases. When nD4,wehave 1=2C1=nD3=4and.lgn/NUL1/=lgnD1=2.\\nThus,wejustneedtochoose dsuchthat clg.3=4/\\x14dlg.1=2/or,equivalently,\\nclg.3=4/\\x14/NULd. Multiplying both sides by /NUL1, we get d\\x14/NULclg.3=4/D\\nclg.4=3/. Thus, anyvalue of dthat isat most clg.4=3/sufﬁces.\\nc.When iis a constant, T .2i/DO.1/and lg .n=i/Dlgn/NULlgiDO.lgn/.\\nThus, when iisaconstant less than n=2, wehave that\\nUi.n/DnCO.T .2i/ lg.n=i//\\nDnCO.O.1/\\x01O.lgn//\\nDnCO.lgn/ :\\nd.Suppose that iDn=kfork\\x152. Then i\\x14n=2. Ifk > 2, then i < n=2, and\\nwehave\\nUi.n/DnCO.T .2i/ lg.n=i//\\nDnCO.T .2n=k/ lg.n=.n=k//\\nDnCO.T .2n=k/ lgk/ :\\nIfkD2, then nD2iand lg kD1. Wehave\\nUi.n/DT .n/\\nDnC.T .n//NULn/\\n\\x14nC.T .2i//NULn/\\nDnC.T .2n=k//NULn/\\nDnC.T .2n=k/ lgk/NULn/\\nDnCO.T .2n=k/ lgk/ :\\nSolutionto Problem 9-4\\na.As in the quicksort analysis, elements ´iand´jwill not be compared with\\neachother ifanyelement in f´iC1; ´iC2; : : : ; ´ j/NUL1gischosen asapivot element\\nbefore either ´ior´j, because ´iand´jwould then lie in separate partitions.\\nThere can be another reason that ´iand´jmight not be compared, however.\\nSuppose that k < i, so that ´k< ´ i, and suppose further that the element\\nchosen as the pivot is ´l, where k\\x14l < i. In this case, because k\\x14l,\\nthe recursion won’t consider elements indexed higher than l. Therefore, the\\nrecursionwillneverlookat ´ior´j,andtheywillneverbecomparedwitheach\\nother. Similarly, if j < kand the pivot element ´lissuch that j < l\\x14k,then\\nthe recursion won’t consider elements indexed less than l, and again ´iand´j\\nwill never be compared with each other. The ﬁnal case is when i\\x14k\\x14j\\n(but disallowing iDj), so that ´i\\x14´k\\x14´j; in this case, we have the same\\nanalysis asforquicksort: ´iand´jarecompared witheachother onlyifoneof\\nthem ischosen as the pivot element.\\nGetting back to the case in which k < i, it is again true that ´iand´jare\\ncompared with each other only if one of them is chosen as the pi vot element.\\nAs we know, they won’t be compared with each other if the pivot element is', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 164}),\n",
              " Document(page_content='9-22 Solutions for Chapter 9: Medians and Order Statistics\\nbetween them, and we argued above that they won’t be compared with each\\nother if the pivot element is ´lforl < i. Similarly, when j < k, elements ´i\\nand´jare compared with each other only if one of them is chosen as th e pivot\\nelement.\\nNow we need to compute the probability that ´iand´jare compared with\\neach other. Let Zijkbe the set of elements that includes ´i; : : : ; ´ j, along with\\n´k; : : : ; ´ i/NUL1ifk < ior´jC1; : : : ; ´ kifj < k. In other words,\\nZijkD\\x80\\nf´i; ´iC1; : : : ; ´ jgifi\\x14k\\x14j ;\\nf´k; ´kC1; : : : ; ´ jgifk < i ;\\nf´i; ´iC1; : : : ; ´ kgifj < k :\\nWith this deﬁnition of Zijk,wehave that\\njZijkjDmax.j/NULiC1; j/NULkC1; k/NULiC1/ :\\nAs in the quicksort analysis, we observe that until an elemen t from Zijkis\\nchosen as the pivot, the whole set Zijkis together in the same partition, and so\\neach element of Zijkis equally likely tobe the ﬁrst one chosen asthe pivot.\\nLetting Cbe the event that ´iis compared with ´jwhen ﬁnding ´ksometime\\nduring the execution of thealgorithm, wehave that\\nEŒXijk\\x8dDPrfCg\\nDPrf´ior´jisthe ﬁrst pivot chosen from Zijkg\\nDPrf´iisthe ﬁrst pivot chosen from Zijkg\\nCPrf´jisthe ﬁrst pivot chosen from Zijkg\\nD1\\njZijkjC1\\njZijkj\\nD2\\nmax.j/NULiC1; j/NULkC1; k/NULiC1/:\\nb.Adding up all thepossible pairs that might becompared gives\\nXkDn/NUL1X\\niD1nX\\njDiC1Xijk;\\nand so, by linearity of expectation, wehave\\nEŒXk\\x8dDE\"n/NUL1X\\niD1nX\\njDiC1Xijk#\\nDn/NUL1X\\niD1nX\\njDiC1EŒXijk\\x8d\\nDn/NUL1X\\niD1nX\\njDiC12\\nmax.j/NULiC1; j/NULkC1; k/NULiC1/:\\nWe break this sum into the same three cases as before: i\\x14k\\x14j,k < i, and\\nj < k. With kﬁxed,wevary iandj. Weget an inequality because wecannot', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 165}),\n",
              " Document(page_content='Solutions forChapter 9: Medians and Order Statistics 9-23\\nhave iDkDj, but our summation will allow it:\\nEŒXk\\x8d\\x142 kX\\niD1nX\\njDk1\\nj/NULiC1CnX\\njDkC1j/NUL1X\\niDkC11\\nj/NULkC1\\nCk/NUL2X\\niD1k/NUL1X\\njDiC11\\nk/NULiC1!\\nD2 kX\\niD1nX\\njDk1\\nj/NULiC1CnX\\njDkC1j/NULk/NUL1\\nj/NULkC1Ck/NUL2X\\niD1k/NULi/NUL1\\nk/NULiC1!\\n:\\nc.First,let’sfocusonthelatter twosummations. Eachonesum sfractions thatare\\nstrictly less than 1. Themiddle summation has n/NULkterms, and the right-hand\\nsummationhas k/NUL2terms,andsothelattertwosummationssumtolessthan n.\\nNow we look at the ﬁrst summation. Let mDj/NULi. There is only one way\\nformto equal 0: ifiDkDj. There are only two ways for mto equal 1: if\\niDk/NUL1andjDk,orif iDkandjDkC1. Thereareonlythreewaysfor\\nmtoequal 2: ifiDk/NUL2andjDk,ifiDk/NUL1andjDkC1, or if iDk\\nandjDkC2. Continuing on, we see that there are at most mC1ways for\\nj/NULito equal m. Since j/NULi\\x14n/NUL1, wecanrewrite the ﬁrst summation as\\nn/NUL1X\\nmD0mC1\\nmC1Dn :\\nThus, wehave\\nEŒXk\\x8d < 2.nCn/\\nD4n :\\nd.To show that R ANDOMIZED -SELECTruns in expected time O.n/, we adapt\\nLemma 7.1 for R ANDOMIZED -SELECT. The adaptation is trivial: just re-\\nplace the variable Xin the lemma statement by the random variable Xkthat\\nwe just analyzed. Thus, the expected running time of R ANDOMIZED -SELECT\\nisO.nCXk/DO.n/.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 166}),\n",
              " Document(page_content='Lecture Notes forChapter 11:\\nHashTables\\nChapter 11overview\\nMany applications require a dynamic set that supports only t hedictionary opera-\\ntionsINSERT, SEARCH, and DELETE. Example: asymbol table inacompiler.\\nAhash table iseffective for implementing adictionary.\\n\\x0fTheexpected timetosearch for anelement inahashtable is O.1/, under some\\nreasonable assumptions.\\n\\x0fWorst-case search timeis ‚.n/,however.\\nAhash table isageneralization of anordinary array.\\n\\x0fWithanordinary array,westoretheelementwhosekeyis kinposition kofthe\\narray.\\n\\x0fGiven a key k, we ﬁnd the element whose key is kby just looking in the kth\\nposition of the array. Thisis called direct addressing .\\n\\x0fDirect addressing is applicable when we can afford to alloca te an array with\\none position for every possible key.\\nWeuse ahash table whenwedonot want to(or cannot) allocate a narray withone\\nposition per possible key.\\n\\x0fUse a hash table when the number of keys actually stored is sma ll relative to\\nthe number of possible keys.\\n\\x0fA hash table is an array, but it typically uses a size proporti onal to the number\\nof keys tobe stored (rather than the number of possible keys) .\\n\\x0fGiven akey k, don’t just use kas the index into the array.\\n\\x0fInstead, compute afunction of k,anduse that value toindex into thearray. We\\ncall this function a hash function .\\nIssues that we’ll explore in hash tables:\\n\\x0fHow to compute hash functions. We’ll look at the multiplicat ion and division\\nmethods.\\n\\x0fWhat to dowhen the hash function maps multiple keys to the sam etable entry.\\nWe’ll look at chaining and open addressing.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 167}),\n",
              " Document(page_content='11-2 Lecture Notes for Chapter 11: HashTables\\nDirect-address tables\\nScenario\\n\\x0fMaintain adynamic set.\\n\\x0fEachelement hasakeydrawnfromauniverse UDf0; 1; : : : ; m/NUL1gwhere m\\nisn’t toolarge.\\n\\x0fNotwoelements have the samekey.\\nRepresent bya direct-address table , or array, T Œ0 : : : m/NUL1\\x8d:\\n\\x0fEachslot, or position, corresponds toa keyin U.\\n\\x0fIf there’s an element xwith key k, then T Œk\\x8dcontains apointer to x.\\n\\x0fOtherwise, T Œk\\x8disempty, represented by NIL.\\nT\\nU\\n(universe of keys)\\nK\\n(actual\\nkeys)  2\\n3\\n581940\\n762\\n3\\n5\\n8key satellite data\\n20\\n1\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\nDictionary operations are trivial and take O.1/timeeach:\\nDIRECT-ADDRESS-SEARCH .T; k/\\nreturn T Œk\\x8d\\nDIRECT-ADDRESS-INSERT .T; x/\\nT ŒkeyŒx\\x8d\\x8dDx\\nDIRECT-ADDRESS-DELETE .T; x/\\nT ŒkeyŒx\\x8d\\x8dDNIL\\nHashtables\\nThe problem with direct addressing is if the universe Uis large, storing a table of\\nsizejUjmaybe impractical or impossible.\\nOften, the set Kof keys actually stored is small, compared to U, so that most of\\nthe space allocated for Tis wasted.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 168}),\n",
              " Document(page_content='Lecture Notes for Chapter 11: HashTables 11-3\\n\\x0fWhen Kis much smaller than U, a hash table requires much less space than a\\ndirect-address table.\\n\\x0fCanreduce storage requirements to ‚.jKj/.\\n\\x0fCanstill get O.1/search time, but in the average case , not theworst case .\\nIdea\\nInstead of storing an element with key kin slot k, use a function hand store the\\nelement in slot h.k/.\\n\\x0fWecall hahash function .\\n\\x0fhWU!f0; 1; : : : ; m/NUL1g,so that h.k/isalegal slot number in T.\\n\\x0fWesay that khashestoslot h.k/.\\nCollisions\\nWhen twoor more keys hash to thesame slot.\\n\\x0fCanhappen whenthere are morepossible keys than slots ( jUj> m).\\n\\x0fFor a given set Kof keys withjKj\\x14m, may or may not happen. Deﬁnitely\\nhappens ifjKj> m.\\n\\x0fTherefore, must beprepared tohandle collisions inall case s.\\n\\x0fUsetwo methods: chaining and open addressing.\\n\\x0fChaining is usually better than open addressing. We’ll exam ine both.\\nCollision resolution bychaining\\nPut all elements that hash tothe same slot into alinked list.\\nT\\nU\\n(universe of keys)\\nK\\n(actual\\nkeys)   \\n k1\\nk2k3k4k5\\nk6k7\\nk8k1\\nk2\\nk3k4\\nk5\\nk6k7\\nk8\\n[This ﬁgure shows singly linked lists. If we want to delete el ements, it’s better to\\nuse doubly linked lists.]\\n\\x0fSlotjcontains a pointer to the head of the list of all stored elemen ts that hash\\ntoj[or tothe sentinel if using acircular, doubly linked list wi thasentinel] ,\\n\\x0fIf there are nosuch elements, slot jcontains NIL.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 169}),\n",
              " Document(page_content='11-4 Lecture Notes for Chapter 11: HashTables\\nHowto implement dictionary operations with chaining:\\n\\x0fInsertion:\\nCHAINED-HASH-INSERT .T; x/\\ninsert xat the head of list T Œh.keyŒx\\x8d/\\x8d\\n\\x0fWorst-case running time is O.1/.\\n\\x0fAssumes that theelement being inserted isn’t already in the list.\\n\\x0fIt would take an additional search tocheck if it wasalready i nserted.\\n\\x0fSearch:\\nCHAINED-HASH-SEARCH .T; k/\\nsearch for anelement withkey kin list T Œh.k/\\x8d\\nRunning timeisproportional tothe length of the list of elem ents in slot h.k/.\\n\\x0fDeletion:\\nCHAINED-HASH-DELETE .T; x/\\ndelete xfrom the list T Œh.keyŒx\\x8d/\\x8d\\n\\x0fGiven pointer xto the element to delete, so no search is needed to ﬁnd this\\nelement.\\n\\x0fWorst-case running time is O.1/timeif the lists are doubly linked.\\n\\x0fIf the lists are singly linked, then deletion takes as long as searching, be-\\ncause we must ﬁnd x’s predecessor in its list in order to correctly update\\nnextpointers.\\nAnalysis of hashingwithchaining\\nGivenakey,howlongdoesittaketoﬁndanelement withthatke y, ortodetermine\\nthat there isno element with that key?\\n\\x0fAnalysis isin termsof the load factor ˛Dn=m:\\n\\x0fnD# of elements inthe table.\\n\\x0fmD#of slots in the table D#of (possibly empty) linked lists.\\n\\x0fLoad factor isaverage number of elements per linked list.\\n\\x0fCan have ˛ < 1,˛D1, or˛ > 1.\\n\\x0fWorstcaseiswhenall nkeyshashtothesameslot )getasinglelistoflength n\\n)worst-case timeto search is ‚.n/,plus time tocompute hash function.\\n\\x0fAveragecasedependsonhowwellthehashfunctiondistribut esthekeysamong\\nthe slots.\\nWefocus onaverage-case performance of hashing with chaini ng.\\n\\x0fAssumesimple uniform hashing : any given element is equally likely to hash\\ninto any of the mslots.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 170}),\n",
              " Document(page_content='Lecture Notes for Chapter 11: HashTables 11-5\\n\\x0fForjD0; 1; : : : ; m/NUL1, denote the length of list T Œj \\x8dbynj. Then\\nnDn0Cn1C\\x01\\x01\\x01C nm/NUL1.\\n\\x0fAverage value of njisEŒnj\\x8dD˛Dn=m.\\n\\x0fAssume that we can compute the hash function in O.1/time, so that the time\\nrequired tosearch fortheelement withkey kdepends onthelength nh.k/ofthe\\nlistT Œh.k/\\x8d.\\nWeconsider twocases:\\n\\x0fIfthehashtablecontainsnoelementwithkey k,thenthesearchisunsuccessful.\\n\\x0fIfthehashtabledoescontainanelementwithkey k,thenthesearchissuccess-\\nful.\\n[In the theorem statements that follow, we omit the assumpti ons that we’re resolv-\\ning collisions by chaining and that simple uniform hashing a pplies.]\\nUnsuccessfulsearch\\nTheorem\\nAnunsuccessful search takes expected time ‚.1C˛/.\\nProofSimpleuniformhashing )anykeynotalreadyinthetableisequallylikely\\ntohash toany of the mslots.\\nTosearchunsuccessfully foranykey k,needtosearchtotheendofthelist T Œh.k/\\x8d.\\nThis list has expected length E Œnh.k/\\x8dD˛. Therefore, the expected number of\\nelements examined in anunsuccessful search is ˛.\\nAdding in the time to compute the hash function, the total tim e required is\\n‚.1C˛/.\\nSuccessfulsearch\\n\\x0fTheexpected timefor asuccessful search is also ‚.1C˛/.\\n\\x0fThecircumstances are slightly different from an unsuccess ful search.\\n\\x0fThe probability that each list is searched is proportional t o the number of ele-\\nments it contains.\\nTheorem\\nAsuccessful search takes expected time ‚.1C˛/.\\nProofAssume that the element xbeing searched for is equally likely to be any of\\nthenelements stored inthe table.\\nThenumber of elements examined during asuccessful search f orxis 1 more than\\nthe number of elements that appear before xinx’s list. These are the elements\\ninsertedafter xwasinserted (because weinsert at the head of thelist).\\nSo we need to ﬁnd the average, over the nelements xin the table, of how many\\nelements wereinserted into x’slist after xwasinserted.\\nForiD1; 2; : : : ; n , let xibe the ith element inserted into the table, and let\\nkiDkeyŒxi\\x8d.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 171}),\n",
              " Document(page_content='11-6 Lecture Notes for Chapter 11: HashTables\\nFor all iandj, deﬁne indicator random variable XijDIfh.k i/Dh.k j/g.\\nSimple uniform hashing )Prfh.k i/Dh.k j/gD1=m)EŒXij\\x8dD1=m(by\\nLemma5.1).\\nExpected number of elements examined inasuccessful search is\\nE\"\\n1\\nnnX\\niD1 \\n1CnX\\njDiC1Xij!#\\nD1\\nnnX\\niD1 \\n1CnX\\njDiC1EŒXij\\x8d!\\n(linearity of expectation)\\nD1\\nnnX\\niD1 \\n1CnX\\njDiC11\\nm!\\nD1C1\\nnmnX\\niD1.n/NULi/\\nD1C1\\nnm nX\\niD1n/NULnX\\niD1i!\\nD1C1\\nnm\\x12\\nn2/NULn.nC1/\\n2\\x13\\n(equation (A.1))\\nD1Cn/NUL1\\n2m\\nD1C˛\\n2/NUL˛\\n2n:\\nAdding in the time for computing the hash function, weget tha t the expected total\\ntime for asuccessful search is ‚.2C˛=2/NUL˛=2n/D‚.1C˛/.\\nAlternative analysis, usingindicator random variables ev en more\\nFor each slot land for each pair of keys kiandkj, deﬁne the indicator random\\nvariable XijlDIfthe search is for xi,h.k i/Dl, and h.k j/Dlg.XijlD1when\\nkeyskiandkjcollide at slot land when weare searching for xi.\\nSimple uniform hashing )Prfh.k i/DlgD1=mand Prfh.k j/DlgD1=m.\\nAlso have Prfthe search is for xigD1=n. These events are all independent )\\nPrfXijlD1gD1=nm2)EŒXijl\\x8dD1=nm2(by Lemma5.1).\\nDeﬁne, for each element xj, theindicator random variable\\nYjDIfxjappears in alist prior to the element being searched for g:\\nYjD1ifandonlyifthereissomeslot lthat hasbothelements xiandxjinitslist,\\nand also i < j(so that xiappears after xjinthe list). Therefore,\\nYjDj/NUL1X\\niD1m/NUL1X\\nlD0Xijl:', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 172}),\n",
              " Document(page_content='Lecture Notes for Chapter 11: HashTables 11-7\\nOne ﬁnal random variable: Z, which counts how many elements appear in the list\\npriortotheelementbeingsearchedfor: ZDPn\\njD1Yj. Wemustcounttheelement\\nbeingsearchedforaswellasallthoseprecedingitinitslis t)computeE ŒZC1\\x8d:\\nEŒZC1\\x8dDE\"\\n1CnX\\njD1Yj#\\nD1CE\"nX\\njD1j/NUL1X\\niD1m/NUL1X\\nlD0Xijl#\\n(linearity of expectation)\\nD1CnX\\njD1j/NUL1X\\niD1m/NUL1X\\nlD0EŒXijl\\x8d(linearity of expectation)\\nD1CnX\\njD1j/NUL1X\\niD1m/NUL1X\\nlD01\\nnm2\\nD1C \\nn\\n2!\\n\\x01m\\x011\\nnm2\\nD1Cn.n/NUL1/\\n2\\x011\\nnm\\nD1Cn/NUL1\\n2m\\nD1Cn\\n2m/NUL1\\n2m\\nD1C˛\\n2/NUL˛\\n2n:\\nAdding in the time for computing the hash function, weget tha t the expected total\\ntimefor asuccessful search is ‚.2C˛=2/NUL˛=2n/D‚.1C˛/.\\nInterpretation\\nIfnDO.m/, then ˛Dn=mDO.m/=mDO.1/, which means that searching\\ntakes constant timeon average.\\nSince insertion takes O.1/worst-case time and deletion takes O.1/worst-case\\ntime when the lists are doubly linked, all dictionary operat ions take O.1/time on\\naverage.\\nHashfunctions\\nWe discuss some issues regarding hash-function design and p resent schemes for\\nhash function creation.\\nWhat makesagood hashfunction?\\n\\x0fIdeally, the hash function satisﬁes the assumption of simpl e uniform hashing.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 173}),\n",
              " Document(page_content='11-8 Lecture Notes for Chapter 11: HashTables\\n\\x0fIn practice, it’s not possible to satisfy this assumption, s ince we don’t know in\\nadvancetheprobability distribution thatkeysaredrawnfr om,andthekeysmay\\nnot bedrawn independently.\\n\\x0fOftenuseheuristics, based onthe domain of thekeys, tocrea te ahash function\\nthat performs well.\\nKeys asnatural numbers\\n\\x0fHashfunctions assume that the keys are natural numbers.\\n\\x0fWhen they’re not, have tointerpret them as natural numbers.\\n\\x0fExample: Interpret a character string as an integer expressed in some radix\\nnotation. Suppose thestring is CLRS:\\n\\x0fASCIIvalues: CD67,LD76,RD82,SD83.\\n\\x0fThere are 128 basic ASCIIvalues.\\n\\x0fSointerpret CLRSas.67\\x011283/C.76\\x011282/C.82\\x011281/C.83\\x011280/D\\n141,764,947.\\nDivision method\\nh.k/Dkmodm :\\nExample: mD20andkD91)h.k/D11.\\nAdvantage: Fast, since requires just one division operation.\\nDisadvantage: Haveto avoid certain values of m:\\n\\x0fPowers of 2are bad. If mD2pfor integer p, then h.k/is just the least\\nsigniﬁcant pbits of k.\\n\\x0fIfkis a character string interpreted in radix 2p(as inCLRSexample), then\\nmD2p/NUL1is bad: permuting characters in a string does not change its h ash\\nvalue (Exercise 11.3-3).\\nGood choice for m:A primenot too close toan exact power of 2.\\nMultiplication method\\n1. Choose constant Ain the range 0 < A < 1 .\\n2. Multiply key kbyA.\\n3. Extract the fractional part of kA.\\n4. Multiply the fractional part by m.\\n5. Takethe ﬂoor of the result.\\nPut another way, h.k/Dbm .k Amod1/c, where k Amod1DkA/NULbkAcD\\nfractional part of kA.\\nDisadvantage: Slower than division method.\\nAdvantage: Value of misnot critical.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 174}),\n",
              " Document(page_content='Lecture Notes for Chapter 11: HashTables 11-9\\n(Relatively) easy implementation:\\n\\x0fChoose mD2pfor some integer p.\\n\\x0fLet the word size of the machine be wbits.\\n\\x0fAssume that kﬁtsinto asingle word. ( ktakes wbits.)\\n\\x0fLetsbean integer inthe range 0 < s < 2w. (stakes wbits.)\\n\\x0fRestrict Ato beof the form s=2w.\\n×\\nbinary pointsDA\\x012wwbits\\nk\\nr0 r1\\nh.k/extract pbits\\n\\x0fMultiply kbys.\\n\\x0fSincewe’remultiplyingtwo w-bitwords,theresultis 2wbits,r12wCr0,where\\nr1isthe high-order wordof the product and r0isthe low-order word.\\n\\x0fr1holds the integer part of kA(bkAc) and r0holds the fractional part of kA\\n(k Amod1DkA/NULbkAc). Think of the “binary point” (analog of decimal\\npoint, butforbinaryrepresentation) asbeingbetween r1andr0. Sincewedon’t\\ncare about the integer part of kA, wecan forget about r1and just use r0.\\n\\x0fSince we wantbm .k Amod1/c, we could get that value by shifting r0to the\\nleft by pDlgmbits and then taking the pbits that were shifted to the left of\\nthe binary point.\\n\\x0fWe don’t need to shift. The pbits that would have been shifted to the left of\\nthe binary point are the pmost signiﬁcant bits of r0. So we can just take these\\nbits after having formed r0by multiplying kbys.\\n\\x0fExample: mD8(implies pD3),wD5,kD21. Must have 0 < s < 25;\\nchoose sD13)AD13=32.\\n\\x0fUsingjust theformula tocompute h.k/:kAD21\\x0113=32D273=32D817\\n32\\n)k Amod1D17=32)m .k Amod1/D8\\x0117=32D17=4D41\\n4)\\nbm .k Amod1/cD4, so that h.k/D4.\\n\\x0fUsing the implementation: ksD21\\x0113D273D8\\x0125C17)r1D8,\\nr0D17. Written in wD5bits,r0D10001. Take the pD3most signiﬁ-\\ncant bits of r0,get100inbinary, or 4indecimal, sothat h.k/D4.\\nHowtochoose A:\\n\\x0fThemultiplication method works withany legal value of A.\\n\\x0fBut it works better with some values than with others, depend ing on the keys\\nbeing hashed.\\n\\x0fKnuth suggests using A\\x19.p\\n5/NUL1/=2.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 175}),\n",
              " Document(page_content='11-10 Lecture Notes for Chapter 11: HashTables\\nUniversal hashing\\n[We just touch on universal hashing in these notes. See the bo ok for a full treat-\\nment.]\\nSuppose that a malicious adversary, who gets to choose the ke ys to be hashed, has\\nseenyourhashingprogramandknowsthehashfunctioninadva nce. Thenhecould\\nchoose keys that all hash tothe sameslot, giving worst-case behavior.\\nOne way to defeat the adversary is to use adifferent hash func tion each time. You\\nchoose one at random at the beginning of your program. Unless the adversary\\nknows how you’ll be randomly choosing which hash function to use, he cannot\\nintentionally defeat you.\\nJust because we choose a hash function randomly, that doesn’ t mean it’s a good\\nhash function. What we want is to randomly choose a single has h function from a\\nset of good candidates.\\nConsider aﬁnitecollection Hofhashfunctions thatmapauniverse Uofkeysinto\\ntherangef0; 1; : : : ; m/NUL1g.Hisuniversal ifforeachpairofkeys k; l2U,where\\nk¤l,the number of hash functions h2Hfor which h.k/Dh.l/is\\x14jHj=m.\\nPut another way, His universal if, with a hash function hchosen randomly\\nfrom H, the probability of a collision between two different keys i s no more than\\nthan1=mchance of just choosing twoslots randomly and independentl y.\\nWhyare universal hash functions good?\\n\\x0fTheygive good hashing behavior:\\nTheorem\\nUsing chaining and universal hashing on key k:\\n\\x0fIfkis not in the table, the expected length E Œnh.k/\\x8dof the list that khashes\\nto is\\x14˛.\\n\\x0fIfkis in the table, the expected length E Œnh.k/\\x8dof the list that holds kis\\n\\x141C˛.\\nCorollary\\nUsing chaining and universal hashing, the expected time for each SEARCHop-\\neration is O.1/.\\n\\x0fTheyare easy todesign.\\n[Seebookfordetailsofbehavioranddesignofauniversalcl assofhashfunctions.]\\nOpen addressing\\nAnalternative tochaining for handling collisions.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 176}),\n",
              " Document(page_content='Lecture Notes for Chapter 11: HashTables 11-11\\nIdea\\n\\x0fStore all keys inthe hash table itself.\\n\\x0fEach slot contains either akey or NIL.\\n\\x0fTosearch for key k:\\n\\x0fCompute h.k/andexamineslot h.k/. Examiningaslotisknownasa probe.\\n\\x0fIfslot h.k/contains key k,thesearch issuccessful. Ifthisslot contains NIL,\\nthe search isunsuccessful.\\n\\x0fThere’sathirdpossibility: slot h.k/containsakeythatisnot k. Wecompute\\nthe index of some other slot, based on kand on which probe (count from 0:\\n0th, 1st, 2nd, etc.) we’re on.\\n\\x0fKeep probing until we either ﬁnd key k(successful search) or weﬁnd a slot\\nholding NIL(unsuccessful search).\\n\\x0fWe need the sequence of slots probed to be a permutation of the slot numbers\\nh0; 1; : : : ; m/NUL1i(so that we examine all slots if we have to, and so that we\\ndon’t examine anyslot more than once).\\n\\x0fThus, thehash function is hWU\\x02f0; 1; : : : ; m/NUL1g„ƒ‚…\\nprobe number!f0; 1; : : : ; m/NUL1g„ƒ‚…\\nslot number.\\n\\x0fThe requirement that the sequence of slots be a permutation o fh0; 1; : : : ;\\nm/NUL1iis equivalent to requiring that the probe sequencehh.k; 0/; h.k; 1/;\\n: : : ; h.k; m/NUL1/ibea permutation of h0; 1; : : : ; m/NUL1i.\\n\\x0fToinsert, act asthough we’re searching, and insert at theﬁr stNILslot weﬁnd.\\nPseudocode for searching\\nHASH-SEARCH .T; k/\\niD0\\nrepeat\\njDh.k; i/\\nifT Œj \\x8d==k\\nreturn j\\niDiC1\\nuntil T Œj \\x8d==NILoriDm\\nreturn NIL\\nHASH-SEARCHreturns theindex of aslot containing key k,orNILifthe search is\\nunsuccessful.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 177}),\n",
              " Document(page_content='11-12 Lecture Notes for Chapter 11: HashTables\\nPseudocode for insertion\\nHASH-INSERT .T; k/\\niD0\\nrepeat\\njDh.k; i/\\nifT Œj \\x8d==NIL\\nT Œj \\x8dDk\\nreturn j\\nelseiDiC1\\nuntil i==m\\nerror“hash table overﬂow”\\nHASH-INSERTreturns the number of the slot that gets key k, or it ﬂags a “hash\\ntable overﬂow”error if there isno empty slot in which toput k eyk.\\nDeletion\\nCannot just put NILinto the slot containing thekey wewant to delete.\\n\\x0fSuppose wewant todelete key kinslot j.\\n\\x0fAndsuppose that sometimeafter inserting key k,wewereinserting key k0,and\\nduring this insertion wehad probed slot j(which contained key k).\\n\\x0fAndsuppose wethen deleted key kby storing NILinto slot j.\\n\\x0fAndthen wesearch for key k0.\\n\\x0fDuring the search, we would probe slot jbeforeprobing the slot into which\\nkeyk0waseventually stored.\\n\\x0fThus, the search would beunsuccessful, even though key k0is inthe table.\\nSolution: Use a special value DELETED instead of NILwhen marking a slot as\\nempty during deletion.\\n\\x0fSearchshouldtreat DELETED asthoughtheslotholdsakeythatdoesnotmatch\\nthe one being searched for.\\n\\x0fInsertion should treat DELETED asthough the slot wereempty, sothat itcanbe\\nreused.\\nThedisadvantageofusing DELETED isthatnowsearchtimeisnolongerdependent\\non the load factor ˛.\\nHow tocomputeprobe sequences\\nThe ideal situation is uniform hashing : each key is equally likely to have any of\\nthemŠpermutations ofh0; 1; : : : ; m/NUL1ias its probe sequence. (This generalizes\\nsimple uniform hashing for a hash function that produces a wh ole probe sequence\\nrather than just asingle number.)\\nIt’s hard toimplement true uniform hashing, soweapproxima te it withtechniques\\nthatatleastguarantee thattheprobesequenceisapermutat ionofh0;1;: : : ;m/NUL1i.\\nNoneofthese techniques canproduce all mŠprobe sequences. Theywillmakeuse\\nofauxiliary hash functions , which map U!f0; 1; : : : ; m/NUL1g.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 178}),\n",
              " Document(page_content='Lecture Notes for Chapter 11: HashTables 11-13\\nLinearprobing\\nGiven auxiliary hash function h0, the probe sequence starts at slot h0.k/and con-\\ntinues sequentially through the table, wrapping after slot m/NUL1toslot 0.\\nGivenkey kand probe number i(0\\x14i < m),h.k; i/D.h0.k/Ci/modm.\\nTheinitial probe determines the entire sequence )onlympossible sequences.\\nLinear probing suffers from primary clustering : long runs of occupied sequences\\nbuild up. And long runs tend to get longer, since an empty slot preceded by ifull\\nslots gets ﬁlled next with probability .iC1/=m. Result is that the average search\\nand insertion timesincrease.\\nQuadratic probing\\nAs in linear probing, the probe sequence starts at h0.k/. Unlike linear probing, it\\njumps around in the table according to a quadratic function o f the probe number:\\nh.k; i/D.h0.k/Cc1iCc2i2/modm,where c1; c2¤0are constants.\\nMust constrain c1,c2, and min order to ensure that we get a full permutation of\\nh0;1;: : : ;m/NUL1i. (Problem11-3exploresonewaytoimplementquadraticprob ing.)\\nCan getsecondary clustering : if two distinct keys have the same h0value, then\\nthey have the sameprobe sequence.\\nDoublehashing\\nUse two auxiliary hash functions, h1andh2.h1gives the initial probe, and h2\\ngives the remaining probes: h.k; i/D.h1.k/Cih2.k//modm.\\nMust have h2.k/be relatively prime to m(no factors in common other than 1) in\\nordertoguaranteethattheprobesequenceisafullpermutat ionofh0;1;: : : ;m/NUL1i.\\n\\x0fCould choose mto be a power of 2andh2to always produce an odd number\\n> 1.\\n\\x0fCould let mbe prime and have 1 < h 2.k/ < m.\\n‚.m2/different probe sequences, since each possible combinatio n of h1.k/\\nandh2.k/gives adifferent probe sequence.\\nAnalysis of open-address hashing\\nAssumptions\\n\\x0fAnalysis is in terms of load factor ˛. We will assume that the table never\\ncompletely ﬁlls, sowealways have 0\\x14n < m)0\\x14˛ < 1.\\n\\x0fAssume uniform hashing.\\n\\x0fNodeletion.\\n\\x0fIn asuccessful search, each key isequally likely to besearc hed for.\\nTheorem\\nTheexpected number of probes in anunsuccessful search is at most 1=.1/NUL˛/.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 179}),\n",
              " Document(page_content='11-14 Lecture Notes for Chapter 11: HashTables\\nProofSince the search is unsuccessful, every probe is to an occupi ed slot, except\\nfor the last probe, which is toan empty slot.\\nDeﬁnerandom variable XD#of probes made in anunsuccessful search.\\nDeﬁne events Ai, foriD1; 2; : : :, to be the event that there is an ith probe and\\nthat it’s to anoccupied slot.\\nX\\x15iif and only if probes 1; 2; : : : ; i/NUL1are made and are to occupied slots )\\nPrfX\\x15igDPrfA1\\\\A2\\\\\\x01\\x01\\x01\\\\ Ai/NUL1g.\\nByExercise C.2-5,\\nPrfA1\\\\A2\\\\\\x01\\x01\\x01\\\\ Ai/NUL1gDPrfA1g\\x01PrfA2jA1g\\x01PrfA3jA1\\\\A2g\\x01\\x01\\x01\\nPrfAi/NUL1jA1\\\\A2\\\\\\x01\\x01\\x01\\\\ Ai/NUL2g:\\nClaim\\nPrfAjjA1\\\\A2\\\\\\x01\\x01\\x01\\\\ Aj/NUL1gD.n/NULjC1/=.m/NULjC1/. Boundarycase: jD1\\n)PrfA1gDn=m.\\nProofFor the boundary case jD1, there are nstored keys and mslots, so the\\nprobability that the ﬁrst probe isto anoccupied slot is n=m.\\nGiventhat j/NUL1probesweremade,alltooccupiedslots,theassumptionofun iform\\nhashingsaysthattheprobesequenceisapermutationof h0;1;: : : ;m/NUL1i,whichin\\nturn implies that the next probe is to a slot that we have not ye t probed. There are\\nm/NULjC1slotsremaining, n/NULjC1of whichareoccupied. Thus, theprobability\\nthat the jthprobe is toan occupied slot is .n/NULjC1/=.m/NULjC1/.(claim)\\nUsing this claim,\\nPrfX\\x15igDn\\nm\\x01n/NUL1\\nm/NUL1\\x01n/NUL2\\nm/NUL2\\x01\\x01\\x01n/NULiC2\\nm/NULiC2„ ƒ‚ …\\ni/NUL1factors:\\nn < m).n/NULj /=.m/NULj /\\x14n=mforj\\x150, which implies\\nPrfX\\x15ig\\x14\\x10n\\nm\\x11i/NUL1\\nD˛i/NUL1:\\nByequation (C.25),\\nEŒX\\x8dD1X\\niD1PrfX\\x15ig\\n\\x141X\\niD1˛i/NUL1\\nD1X\\niD0˛i\\nD1\\n1/NUL˛(equation (A.6)) . (theorem)', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 180}),\n",
              " Document(page_content='Lecture Notes for Chapter 11: HashTables 11-15\\nInterpretation\\nIf˛is constant, an unsuccessful search takes O.1/time.\\n\\x0fIf˛D0:5, then an unsuccessful search takes an average of 1=.1/NUL0:5/D2\\nprobes.\\n\\x0fIf˛D0:9, takes an average of 1=.1/NUL0:9/D10probes.\\nCorollary\\nTheexpected number of probes to insert is at most 1=.1/NUL˛/.\\nProofSince there is no deletion, insertion uses the same probe seq uence as an\\nunsuccessful search.\\nTheorem\\nTheexpected number of probes in asuccessful search isat mos t1\\n˛ln1\\n1/NUL˛.\\nProofA successful search for key kfollows the same probe sequence as when\\nkeykwasinserted.\\nBy the previous corollary, if kwas the .iC1/st key inserted, then ˛equaled i=m\\nat the time. Thus, the expected number of probes made ina sear ch for kisat most\\n1=.1/NULi=m/Dm=.m/NULi/.\\nThat was assuming that kwas the .iC1/st key inserted. Weneed to average over\\nallnkeys:\\n1\\nnn/NUL1X\\niD0m\\nm/NULiDm\\nnn/NUL1X\\niD01\\nm/NULi\\nD1\\n˛mX\\nkDm/NULnC11\\nk\\n\\x141\\n˛Zm\\nm/NULn.1=x/ dx (byinequality (A.12))\\nD1\\n˛lnm\\nm/NULn\\nD1\\n˛ln1\\n1/NUL˛(theorem)', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 181}),\n",
              " Document(page_content='Solutionsfor Chapter11:\\nHash Tables\\nSolution to Exercise11.1-4\\nWe denote the huge array by Tand, taking the hint from the book, we also have a\\nstackimplementedbyanarray S. Thesizeof Sequalsthenumberofkeysactually\\nstored, so that Sshould be allocated at the dictionary’s maximum size. The st ack\\nhas an attribute S:top,so that only entries SŒ1 : : S:top\\x8dare valid.\\nThe idea of this scheme is that entries of TandSvalidate each other. If key kis\\nactually stored in T,then T Œk\\x8dcontains theindex, say j,of avalid entry in S, and\\nSŒj \\x8dcontains the value k. Let us call this situation, in which 1\\x14T Œk\\x8d\\x14S:top,\\nSŒT Œk\\x8d\\x8dDk,and T ŒSŒj \\x8d\\x8dDj,avalidating cycle .\\nAssuming that wealso need tostore pointers toobjects inour direct-address table,\\nwe can store them in an array that is parallel to either TorS. Since Sis smaller\\nthanT,we’lluseanarray S0,allocated tobethesamesizeas S,forthesepointers.\\nThus, if the dictionary contains an object xwith key k, then there is a validating\\ncycle and S0ŒT Œk\\x8d\\x8dpoints to x.\\nTheoperations onthe dictionary workas follows:\\n\\x0fInitialization: Simply set S:topD0, so that there are no valid entries in the\\nstack.\\n\\x0fSEARCH: Given key k, we check whether we have a validating cycle, i.e.,\\nwhether 1\\x14T Œk\\x8d\\x14S:topandSŒT Œk\\x8d\\x8dDk. If so, we return S0ŒT Œk\\x8d\\x8d, and\\notherwise wereturn NIL.\\n\\x0fINSERT: Toinsert object xwith key k, assuming that this object is not already\\nin the dictionary, we increment S:top, setSŒS:top\\x8dDk, setS0ŒS:top\\x8dDx,\\nand set T Œk\\x8dDS:top.\\n\\x0fDELETE: To delete object xwith key k, assuming that this object is in the\\ndictionary, we need to break the validating cycle. The trick is to also ensure\\nthat wedon’t leave a“hole” in the stack, and wesolve this pro blem by moving\\nthetopentryofthestackintothepositionthatwearevacati ng—and thenﬁxing\\nupthatentry’s validating cycle. That is, we execute the following sequence of\\nassignments:', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 182}),\n",
              " Document(page_content='Solutions forChapter 11: HashTables 11-17\\nSŒT Œk\\x8d\\x8dDSŒS:top\\x8d\\nS0ŒT Œk\\x8d\\x8dDS0ŒS:top\\x8d\\nT ŒSŒT Œk\\x8d\\x8d\\x8dDT Œk\\x8d\\nT Œk\\x8dD0\\nS:topDS:top/NUL1\\nEach of these operations—initialization, S EARCH, INSERT, and D ELETE—takes\\nO.1/time.\\nSolutionto Exercise 11.2-1\\nThissolutionisalsopostedpublicly\\nFor each pair of keys k; l, where k¤l, deﬁne the indicator random variable\\nXklDIfh.k/Dh.l/g. Sinceweassumesimpleuniformhashing,Pr fXklD1gD\\nPrfh.k/Dh.l/gD1=m, and so E ŒXkl\\x8dD1=m.\\nNow deﬁne the random variable Yto be the total number of collisions, so that\\nYDP\\nk¤lXkl. Theexpected number of collisions is\\nEŒY \\x8dDE\\x14X\\nk¤lXkl\\x15\\nDX\\nk¤lEŒXkl\\x8d(linearity of expectation)\\nD \\nn\\n2!\\n1\\nm\\nDn.n/NUL1/\\n2\\x011\\nm\\nDn.n/NUL1/\\n2m:\\nSolutionto Exercise 11.2-4\\nThissolutionisalsopostedpublicly\\nTheﬂagineach slot will indicate whether the slot isfree.\\n\\x0fA free slot is in the free list, a doubly linked list of all free slots in the table.\\nTheslot thus contains two pointers.\\n\\x0fAusedslotcontainsanelementandapointer(possibly NIL)tothenextelement\\nthat hashes to this slot. (Of course, that pointer points to a nother slot in the\\ntable.)', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 183}),\n",
              " Document(page_content='11-18 Solutions for Chapter 11: Hash Tables\\nOperations\\n\\x0fInsertion:\\n\\x0fIftheelement hashestoafreeslot,just removetheslotfrom thefreelist and\\nstore the element there (with a NILpointer). The free list must be doubly\\nlinked inorder for this deletion torun in O.1/time.\\n\\x0fIf the element hashes to a used slot j, check whether the element xalready\\nthere “belongs” there (its keyalso hashes toslot j).\\n\\x0fIf so, add the new element to the chain of elements in this slot . To do\\nso, allocate a free slot (e.g., take the head of the free list) for the new\\nelement and put this new slot at the head of the list pointed to by the\\nhashed-to slot ( j).\\n\\x0fIf not, Eis part of another slot’s chain. Move it to a new slot by allo-\\ncating one from the free list, copying the old slot’s ( j’s) contents (ele-\\nment xand pointer) to the new slot, and updating the pointer in the s lot\\nthatpointed to jtopoint tothenewslot. Theninsert thenewelement in\\nthe now-empty slot as usual.\\nToupdatethepointerto j,itisnecessarytoﬁnditbysearchingthechain\\nof elements starting inthe slot xhashes to.\\n\\x0fDeletion: Letjbethe slot the element xtobe deleted hashes to.\\n\\x0fIfxis the only element in j(jdoesn’t point to any other entries), just free\\nthe slot, returning it to the head of the free list.\\n\\x0fIfxis injbut there’s a pointer to a chain of other elements, move the ﬁr st\\npointed-to entry to slot jand free theslot it wasin.\\n\\x0fIfxisfoundbyfollowingapointer from j,justfree x’sslotandspliceitout\\nof the chain (i.e., update the slot that pointed to xto point to x’ssuccessor).\\n\\x0fSearching: Check the slot the key hashes to, and if that is not the desired\\nelement, follow the chain of pointers from the slot.\\nAll the operations take expected O.1/times for the same reason they do with\\nthe version in the book: The expected time to search the chain s isO.1C˛/\\nregardless of where the chains are stored, and the fact that a ll the elements are\\nstored in the table means that ˛\\x141. If the free list were singly linked, then\\noperations that involved removing an arbitrary slot from th e free list would not\\nrun in O.1/time.\\nSolution to Exercise11.2-6\\nWe can view the hash table as if it had mrows and Lcolumns; each row stores\\none chain. The array has mLentries storing nkeys, and mL/NULnempty values.\\nTheprocedure picksarray positions at random until itﬁndsa key, whichitreturns.\\nThe probability of success on one draw is n=mL, somL=nDL=˛trials are\\nneeded. Eachtrialtakestime O.1/,sincetheindividual chainsizesareknown. The\\nchain for the last draw needs to be scanned to ﬁnd the desired e lement, however,\\ncosting O.L/.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 184}),\n",
              " Document(page_content='Solutions forChapter 11: HashTables 11-19\\nSolutionto Exercise 11.3-3\\nFirst, we observe that we can generate any permutation by a se quence of inter-\\nchanges of pairs of characters. One can prove this property f ormally, but infor-\\nmally, consider that both heapsort and quicksort work by int erchanging pairs of\\nelements and that they have to be able to produce any permutat ion of their input\\narray. Thus, it sufﬁces to show that if string xcan be derived from string yby\\ninterchanging asingle pair of characters, then xandyhash to the samevalue.\\nLet us denote the ith character in xbyxi, and similarly for y. The interpreta-\\ntion of xin radix 2pisPn/NUL1\\niD0xi2ip, and so h.x/D/NULPn/NUL1\\niD0xi2ip\\x01\\nmod.2p/NUL1/.\\nSimilarly, h.y/D/NULPn/NUL1\\niD0yi2ip\\x01\\nmod.2p/NUL1/.\\nSupposethat xandyareidentical stringsof ncharacters exceptthatthecharacters\\nin positions aandbare interchanged: xaDybandyaDxb. Without loss of\\ngenerality, let a > b. Wehave\\nh.x//NULh.y/D n/NUL1X\\niD0xi2ip!\\nmod.2p/NUL1//NUL n/NUL1X\\niD0yi2ip!\\nmod.2p/NUL1/ :\\nSince 0\\x14h.x/; h.y/ < 2p/NUL1, wehave that/NUL.2p/NUL1/ < h.x//NULh.y/ < 2p/NUL1.\\nIf weshow that .h.x//NULh.y//mod.2p/NUL1/D0, then h.x/Dh.y/.\\nSince the sums in the hash functions are the same except for in dices aandb, we\\nhave\\n.h.x//NULh.y//mod.2p/NUL1/\\nD..xa2apCxb2bp//NUL.ya2apCyb2bp//mod.2p/NUL1/\\nD..xa2apCxb2bp//NUL.xb2apCxa2bp//mod.2p/NUL1/\\nD..xa/NULxb/2ap/NUL.xa/NULxb/2bp/mod.2p/NUL1/\\nD..xa/NULxb/.2ap/NUL2bp//mod.2p/NUL1/\\nD..xa/NULxb/2bp.2.a/NULb/p/NUL1//mod.2p/NUL1/ :\\nByequation (A.5),\\na/NULb/NUL1X\\niD02piD2.a/NULb/p/NUL1\\n2p/NUL1;\\nandmultiplying bothsidesby 2p/NUL1,weget 2.a/NULb/p/NUL1D/NULPa/NULb/NUL1\\niD02pi\\x01\\n.2p/NUL1/.\\nThus,\\n.h.x//NULh.y//mod.2p/NUL1/\\nD \\n.xa/NULxb/2bp a/NULb/NUL1X\\niD02pi!\\n.2p/NUL1/!\\nmod.2p/NUL1/\\nD0 ;\\nsince one of the factors is 2p/NUL1.\\nWehave shownthat .h.x//NULh.y//mod.2p/NUL1/D0, and so h.x/Dh.y/.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 185}),\n",
              " Document(page_content='11-20 Solutions for Chapter 11: Hash Tables\\nSolution to Exercise11.3-5\\nLetbDjBjanduDjUj. We start by showing that the total number of collisions\\nis minimized by a hash function that maps u=belements of Uto each of the b\\nvalues in B. For a given hash function, let ujbe the number of elements that map\\ntoj2B. We have uDP\\nj2Buj. We also have that the number of collisions for\\nagiven value of j2Bis/NULuj\\n2\\x01\\nDuj.uj/NUL1/=2.\\nLemma\\nThetotal number of collisions isminimized when ujDu=bfor each j2B.\\nProofIfuj\\x14u=b, let us call junderloaded , and if uj\\x15u=b, let us call j\\noverloaded . Consider an unbalanced situation in which uj¤u=bfor at least\\none value j2B. We can think of converting a balanced situation in which all\\nujequal u=binto the unbalanced situation by repeatedly moving an eleme nt that\\nmaps toan underloaded value tomapinstead toanoverloaded v alue. (If you think\\nof the values of Bas representing buckets, we are repeatedly moving elements\\nfrom buckets containing at most u=belements to buckets containing at least u=b\\nelements.)\\nWe now show that each such move increases the number of collis ions, so that\\nall the moves together must increase the number of collision s. Suppose that\\nwe move an element from an underloaded value jto an overloaded value k,\\nand we leave all other elements alone. Because jis underloaded and kis\\noverloaded, uj\\x14u=b\\x14uk. Considering just the collisions for values j\\nandk, we have uj.uj/NUL1/=2Cuk.uk/NUL1/=2collisions before the move and\\n.uj/NUL1/.u j/NUL2/=2C.ukC1/u k=2collisions afterward. We wish to show that\\nuj.uj/NUL1/=2Cuk.uk/NUL1/=2 < .u j/NUL1/.u j/NUL2/=2C.ukC1/u k=2. We have\\nthe following sequence of equivalent inequalities:\\nuj< u kC1\\n2uj< 2u kC2\\n/NULuk< u k/NUL2ujC2\\nu2\\nj/NULujCu2\\nk/NULuk< u2\\nj/NUL3ujC2Cu2\\nkCuk\\nuj.uj/NUL1/Cuk.uk/NUL1/ < .u j/NUL1/.u j/NUL2/C.ukC1/u k\\nuj.uj/NUL1/=2Cuk.uk/NUL1/=2 < .u j/NUL1/.u j/NUL2/=2C.ukC1/u k=2 :\\nThus, each moveincreases the number of collisions. Weconcl ude that thenumber\\nof collisions isminimized when ujDu=bfor each j2B.\\nBy the above lemma, for any hash function, the total number of collisions must\\nbe at least b.u=b/.u=b/NUL1/=2. The number of pairs of distinct elements is/NULu\\n2\\x01\\nD\\nu.u/NUL1/=2. Thus, thenumber ofcollisions per pairof distinct element s mustbeat\\nleast', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 186}),\n",
              " Document(page_content='Solutions forChapter 11: HashTables 11-21\\nb.u=b/.u=b/NUL1/=2\\nu.u/NUL1/=2Du=b/NUL1\\nu/NUL1\\n>u=b/NUL1\\nu\\nD1\\nb/NUL1\\nu:\\nThus,thebound \\x0fontheprobability of acollision for anypair of distinct ele ments\\ncan beno less than 1=b/NUL1=uD1=jBj/NUL1=jUj.\\nSolutionto Problem 11-1\\na.Since we assume uniform hashing, we can use the same observat ion as is used\\nin Corollary 11.7: that inserting a key entails an unsuccess ful search followed\\nby placing the key into the ﬁrst empty slot found. As in the pro of of Theo-\\nrem 11.6, if we let Xbe the random variable denoting the number of probes\\nin an unsuccessful search, then Pr fX\\x15ig\\x14˛i/NUL1. Since n\\x14m=2, we have\\n˛\\x141=2. Letting iDkC1, we have PrfX > kgDPrfX\\x15kC1g\\x14\\n.1=2/.kC1//NUL1D2/NULk.\\nb.Substituting kD2lgninto the statement of part (a) yields that the probability\\nthat the ith insertion requires more than kD2lgnprobes is at most 2/NUL2lgnD\\n.2lgn//NUL2Dn/NUL2D1=n2.\\nWe must deal with the possibility that 2lgnis not an integer, however. Then\\nthe event that the ith insertion requires more than 2lgnprobes is the same\\nas the event that the ith insertion requires more than b2lgncprobes. Since\\nb2lgnc> 2lgn/NUL1, we have that the probability of this event is at most\\n2/NULb2lgnc< 2/NUL.2lgn/NUL1/D2=n2DO.1=n2/.\\nc.Lettheevent AbeX > 2lgn,andfor iD1; 2; : : : ; n ,lettheevent AibeXi>\\n2lgn. In part (b), we showed that Pr fAigDO.1=n2/foriD1; 2; : : : ; n .\\nFrom how we deﬁned these events, ADA1[A2[\\x01\\x01\\x01[ An. Using Boole’s\\ninequality, (C.19), wehave\\nPrfAg\\x14PrfA1gCPrfA2gC\\x01\\x01\\x01CPrfAng\\n\\x14n\\x01O.1=n2/\\nDO.1=n/ :\\nd.Weuse the deﬁnition of expectation and break the sum into two parts:', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 187}),\n",
              " Document(page_content='11-22 Solutions for Chapter 11: Hash Tables\\nEŒX\\x8dDnX\\nkD1k\\x01PrfXDkg\\nDd2lgneX\\nkD1k\\x01PrfXDkgCnX\\nkDd2lgneC1k\\x01PrfXDkg\\n\\x14d2lgneX\\nkD1d2lgne\\x01PrfXDkgCnX\\nkDd2lgneC1n\\x01PrfXDkg\\nDd2lgned2lgneX\\nkD1PrfXDkgCnnX\\nkDd2lgneC1PrfXDkg:\\nSince Xtakes on exactly one value, we have thatPd2lgne\\nkD1PrfXDkgD\\nPrfX\\x14d2lgneg\\x141andPn\\nkDd2lgneC1PrfXDkg\\x14PrfX > 2lgngD\\nO.1=n/, bypart (c). Therefore,\\nEŒX\\x8d\\x14d2lgne\\x011Cn\\x01O.1=n/\\nDd2lgneCO.1/\\nDO.lgn/ :\\nSolution to Problem 11-2\\nThis solutionisalsopostedpublicly\\na.A particular key is hashed to a particular slot with probabil ity1=n. Suppose\\nweselect aspeciﬁc set of kkeys. Theprobability that these kkeysareinserted\\ninto the slot inquestion and that all other keys areinserted elsewhere is\\n\\x121\\nn\\x13k\\x12\\n1/NUL1\\nn\\x13n/NULk\\n:\\nSince there are/NULn\\nk\\x01\\nways tochoose our kkeys, weget\\nQkD\\x121\\nn\\x13k\\x12\\n1/NUL1\\nn\\x13n/NULk \\nn\\nk!\\n:\\nb.ForiD1; 2; : : : ; n , letXibe a random variable denoting the number of keys\\nthat hash to slot i, and let Aibe the event that XiDk, i.e., that exactly kkeys\\nhash to slot i. Frompart (a), wehave Pr fAgDQk. Then,\\nPkDPrfMDkg\\nDPrn\\x10\\nmax\\n1\\x14i\\x14nXi\\x11\\nDko\\nDPrfthere exists isuch that XiDkand that Xi\\x14kforiD1; 2; : : : ; ng\\n\\x14Prfthere exists isuch that XiDkg\\nDPrfA1[A2[\\x01\\x01\\x01[ Ang\\n\\x14PrfA1gCPrfA2gC\\x01\\x01\\x01CPrfAng(by inequality (C.19))\\nDnQ k:', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 188}),\n",
              " Document(page_content='Solutions forChapter 11: HashTables 11-23\\nc.We start by showing two facts. First, 1/NUL1=n < 1 , which implies\\n.1/NUL1=n/n/NULk< 1. Second, nŠ=.n/NULk/ŠDn\\x01.n/NUL1/\\x01.n/NUL2/\\x01\\x01\\x01.n/NULkC1/ < nk.\\nUsingthesefacts, alongwiththesimpliﬁcation kŠ > .k=e/kofequation (3.18),\\nwehave\\nQkD\\x121\\nn\\x13k\\x12\\n1/NUL1\\nn\\x13n/NULknŠ\\nkŠ.n/NULk/Š\\n<nŠ\\nnkkŠ.n/NULk/Š(.1/NUL1=n/n/NULk< 1)\\n<1\\nkŠ(nŠ=.n/NULk/Š < nk)\\n<ek\\nkk(kŠ > .k=e/k) .\\nd.Notice that when nD2, lglg nD0, so to be precise, we need to assume that\\nn\\x153.\\nInpart (c),weshowedthat Qk< ek=kkfor any k;inparticular, thisinequality\\nholds for k0. Thus, it sufﬁces to show that ek0=k0k0< 1=n3or, equivalently,\\nthatn3< k 0k0=ek0.\\nTaking logarithms of both sides gives anequivalent conditi on:\\n3lgn < k 0.lgk0/NULlge/\\nDclgn\\nlglgn.lgcClglgn/NULlglglg n/NULlge/ :\\nDividing both sides by lg ngives the condition\\n3 <c\\nlglgn.lgcClglgn/NULlglglg n/NULlge/\\nDc\\x12\\n1Clgc/NULlge\\nlglgn/NULlglglg n\\nlglgn\\x13\\n:\\nLetxbe the last expression inparentheses:\\nxD\\x12\\n1Clgc/NULlge\\nlglgn/NULlglglg n\\nlglgn\\x13\\n:\\nWeneed to show that there exists aconstant c > 1such that 3 < cx.\\nNotingthatlim n!1xD1,weseethatthereexists n0suchthat x\\x151=2forall\\nn\\x15n0. Thus, any constant c > 6works for n\\x15n0.\\nWe handle smaller values of n—in particular, 3\\x14n < n 0—as follows. Since\\nnis constrained to be an integer, there are a ﬁnite number of nin the range\\n3\\x14n < n 0. We can evaluate the expression xfor each such value of nand\\ndetermineavalueof cforwhich 3 < cxforallvaluesof n. Theﬁnalvalueof c\\nthat weuse isthe larger of\\n\\x0f6, which works for all n\\x15n0, and\\n\\x0fmax 3\\x14n<n 0fcW3 < cxg, i.e., the largest value of cthat we chose for the\\nrange 3\\x14n < n 0.\\nThus, wehave shown that Qk0< 1=n3, asdesired.\\nTo see that Pk< 1=n2fork\\x15k0, we observe that by part (b), Pk\\x14nQ k\\nfor all k. Choosing kDk0gives Pk0\\x14nQ k0< n\\x01.1=n3/D1=n2. For', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 189}),\n",
              " Document(page_content='11-24 Solutions for Chapter 11: Hash Tables\\nk > k 0, wewill show that wecan pick the constant csuch that Qk< 1=n3for\\nallk\\x15k0,and thus conclude that Pk< 1=n2for all k\\x15k0.\\nTopick casrequired, welet cbelarge enough that k0> 3 > e. Then e=k < 1\\nfor all k\\x15k0,and so ek=kkdecreases as kincreases. Thus,\\nQk< ek=kk\\n\\x14ek0=kk0\\n< 1=n3\\nfork\\x15k0.\\ne.Theexpectation of Mis\\nEŒM \\x8dDnX\\nkD0k\\x01PrfMDkg\\nDk0X\\nkD0k\\x01PrfMDkgCnX\\nkDk0C1k\\x01PrfMDkg\\n\\x14k0X\\nkD0k0\\x01PrfMDkgCnX\\nkDk0C1n\\x01PrfMDkg\\n\\x14k0k0X\\nkD0PrfMDkgCnnX\\nkDk0C1PrfMDkg\\nDk0\\x01PrfM\\x14k0gCn\\x01PrfM > k 0g;\\nwhich iswhat weneeded to show, since k0Dclgn=lglgn.\\nToshow that E ŒM \\x8dDO.lgn=lglgn/,note that PrfM\\x14k0g\\x141and\\nPrfM > k 0gDnX\\nkDk0C1PrfMDkg\\nDnX\\nkDk0C1Pk\\n<nX\\nkDk0C11=n2(bypart (d))\\n< n\\x01.1=n2/\\nD1=n :\\nWeconclude that\\nEŒM \\x8d\\x14k0\\x011Cn\\x01.1=n/\\nDk0C1\\nDO.lgn=lglgn/ :\\nSolution to Problem 11-3\\na.From how the probe-sequence computation is speciﬁed, it is e asy to see that\\nthe probe sequence is hh.k/; h.k/C1; h.k/C1C2; h.k/C1C2C3;', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 190}),\n",
              " Document(page_content='Solutions forChapter 11: HashTables 11-25\\n: : : ; h.k/C1C2C3C\\x01\\x01\\x01C i; : : :i, where all the arithmetic is modulo m.\\nStartingtheprobenumbersfrom 0,theithprobeisoffset(modulo m)from h.k/\\nby\\niX\\njD0jDi.iC1/\\n2D1\\n2i2C1\\n2i :\\nThus, wecan write the probe sequence as\\nh0.k; i/D\\x12\\nh.k/C1\\n2iC1\\n2i2\\x13\\nmodm ;\\nwhich demonstrates that this scheme isa special case of quad ratic probing.\\nb.Leth0.k; i/denote the ith probe of our scheme. We saw in part (a) that\\nh0.k; i/D.h.k/Ci.iC1/=2/modm. To show that our algorithm exam-\\nineseverytablepositionintheworstcase,weshowthatfora givenkey,eachof\\nthe ﬁrst mprobes hashes to a distinct value. That is, for any key kand for any\\nprobe numbers iandjsuch that 0\\x14i < j < m , wehave h0.k; i/¤h0.k; j /.\\nWedo soby showing that h0.k; i/Dh0.k; j /yields acontradiction.\\nLet us assume that there exists a key kand probe numbers iandjsatsifying\\n0\\x14i < j < m for which h0.k; i/Dh0.k; j /. Then\\nh.k/Ci.iC1/=2\\x11h.k/Cj.jC1/=2 .mod m/ ;\\nwhich in turn implies that\\ni.iC1/=2\\x11j.jC1/=2 .mod m/ ;\\nor\\nj.jC1/=2/NULi.iC1/=2\\x110 .mod m/ :\\nSince j.jC1/=2/NULi.iC1/=2D.j/NULi/.jCiC1/=2, wehave\\n.j/NULi/.jCiC1/=2\\x110 .mod m/ :\\nThe factors j/NULiandjCiC1must have different parities, i.e., j/NULiis\\neven if and only if jCiC1is odd. (Work out the various cases in which\\niandjare even and odd.) Since .j/NULi/.jCiC1/=2\\x110 .mod m/,\\nwe have .j/NULi/.jCiC1/=2Drmfor some integer ror, equivalently,\\n.j/NULi/.jCiC1/Dr\\x012m. Using the assumption that mis a power\\nof2, let mD2pfor some nonnegative integer p, so that now we have\\n.j/NULi/.jCiC1/Dr\\x012pC1. Because exactly one of the factors j/NULi\\nandjCiC1is even, 2pC1must divide one of the factors. It cannot be\\nj/NULi, since j/NULi < m < 2pC1. But it also cannot be jCiC1, since\\njCiC1\\x14.m/NUL1/C.m/NUL2/C1D2m/NUL2 < 2pC1. Thuswehave derived\\nthe contradiction that 2pC1divides neither of the factors j/NULiandjCiC1.\\nWeconclude that h0.k; i/¤h0.k; j /.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 191}),\n",
              " Document(page_content='Lecture Notes forChapter 12:\\nBinarySearch Trees\\nChapter 12overview\\nSearch trees\\n\\x0fData structures that support many dynamic-set operations.\\n\\x0fCanbe used as both adictionary and asapriority queue.\\n\\x0fBasic operations take timeproportional to theheight of the tree.\\n\\x0fFor complete binary tree with nnodes: worst case ‚.lgn/.\\n\\x0fFor linear chain of nnodes: worst case ‚.n/.\\n\\x0fDifferent typesofsearch treesinclude binarysearchtrees ,red-black trees(cov-\\nered in Chapter 13), and B-trees (covered in Chapter 18).\\nWewillcoverbinarysearchtrees,treewalks,andoperation sonbinarysearchtrees.\\nBinary search trees\\nBinary search trees are animportant data structure for dyna mic sets.\\n\\x0fAccomplish many dynamic-set operations in O.h/time, where hDheight of\\ntree.\\n\\x0fAsinSection10.4,werepresentabinarytreebyalinkeddata structureinwhich\\neach node isan object.\\n\\x0fT:rootpoints tothe root of tree T.\\n\\x0fEach node contains the attributes\\n\\x0fkey(and possibly other satellite data).\\n\\x0fleft: points to left child.\\n\\x0fright: points toright child.\\n\\x0fp: points toparent. T:root:pDNIL.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 192}),\n",
              " Document(page_content='12-2 Lecture Notes for Chapter 12: Binary Search Trees\\n\\x0fStored keys must satisfy the binary-search-tree property .\\n\\x0fIfyisinleft subtree of x,then y:key\\x14x:key.\\n\\x0fIfyisinright subtree of x,then y:key\\x15x:key.\\nDraw sample tree.\\n[This is Figure 12.1(a) from the text, using A,B,D,F,H,Kin place of 2, 3, 5,\\n5, 7, 8, with alphabetic comparisons. It’s OK to have duplica te keys, though there\\nare none in this example. Showthat the binary-search-tree p roperty holds.]\\nA DB\\nKHF\\nThe binary-search-tree property allows us to print keys in a binary search tree in\\norder, recursively, using an algorithm called an inorder tree walk . Elements are\\nprinted inmonotonically increasing order.\\nHow INORDER-TREE-WALKworks:\\n\\x0fCheck tomake sure that xis notNIL.\\n\\x0fRecursively, print the keys of the nodes in x’sleft subtree.\\n\\x0fPrint x’skey.\\n\\x0fRecursively, print the keys of the nodes in x’sright subtree.\\nINORDER-TREE-WALK.x/\\nifx¤NIL\\nINORDER-TREE-WALK.x:left/\\nprintkeyŒx\\x8d\\nINORDER-TREE-WALK.x:right/\\nExample\\nDothe inorder tree walk onthe example above, getting theout putABDFHK .\\nCorrectness\\nFollowsby induction directly from the binary-search-tree property.\\nTime\\nIntuitively, the walk takes ‚.n/time for a tree with nnodes, because we visit and\\nprint each node once. [Book has formal proof.]', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 193}),\n",
              " Document(page_content='Lecture Notes for Chapter 12: Binary Search Trees 12-3\\nQuerying abinary searchtree\\nSearching\\nTREE-SEARCH .x; k/\\nifx==NILork==keyŒx\\x8d\\nreturn x\\nifk < x:key\\nreturnTREE-SEARCH .x:left; k/\\nelse return TREE-SEARCH .x:right; k/\\nInitial call is T REE-SEARCH .T:root; k/.\\nExample\\nSearch for values DandCin theexample tree from above.\\nTime\\nThe algorithm recurses, visiting nodes on a downward path fr om the root. Thus,\\nrunning time is O.h/, where his the height of the tree.\\n[The text also gives an iterative version of TREE-SEARCH, which is more efﬁ-\\ncient on most computers. The above recursive procedure is mo re straightforward,\\nhowever.]\\nMinimumand maximum\\nThebinary-search-tree property guarantees that\\n\\x0fthe minimum key of abinary search tree islocated at theleftm ost node, and\\n\\x0fthe maximum keyof abinary search tree islocated at theright most node.\\nTraverse the appropriate pointers ( leftorright) untilNILisreached.\\nTREE-MINIMUM .x/\\nwhile x:left¤NIL\\nxDx:left\\nreturn x\\nTREE-MAXIMUM .x/\\nwhile x:right¤NIL\\nxDx:right\\nreturn x\\nTime\\nBoth procedures visit nodes that form a downward path from th e root to a leaf.\\nBothprocedures run in O.h/time, where histhe height of thetree.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 194}),\n",
              " Document(page_content='12-4 Lecture Notes for Chapter 12: Binary Search Trees\\nSuccessor andpredecessor\\nAssuming that all keys are distinct, the successor of a node xis the node ysuch\\nthaty:keyis the smallest key > x:key. (We can ﬁnd x’s successor based entirely\\non the tree structure. No key comparisons are necessary.) If xhas the largest key\\nin the binary search tree, then wesay that x’ssuccessor is NIL.\\nThere are twocases:\\n1. If node xhas a non-empty right subtree, then x’s successor is the minimum in\\nx’sright subtree.\\n2. If node xhasan empty right subtree, notice that:\\n\\x0fAs long as wemove to the left up the tree (move up through right children),\\nwe’re visiting smaller keys.\\n\\x0fx’s successor yis the node that xis the predecessor of ( xis the maximum\\niny’sleft subtree).\\nTREE-SUCCESSOR .x/\\nifx:right¤NIL\\nreturnTREE-MINIMUM .x:right/\\nyDx:p\\nwhile y¤NILandx==y:right\\nxDy\\nyDy:p\\nreturn y\\nTREE-PREDECESSOR is symmetric to T REE-SUCCESSOR .\\nExample\\n243\\n1376\\n17 201815\\n9\\n\\x0fFindthe successor of thenode withkey value 15. (Answer: Key value 17)\\n\\x0fFindthe successor of thenode withkey value 6. (Answer: Keyv alue 7)\\n\\x0fFindthe successor of thenode withkey value 4. (Answer: Keyv alue 6)\\n\\x0fFindthe predecessor of the node withkey value 6. (Answer: Ke yvalue 4)\\nTime\\nFor both the T REE-SUCCESSOR and TREE-PREDECESSOR procedures, in both\\ncases, we visit nodes on a path down the tree or up the tree. Thu s, running time is\\nO.h/, where histhe height of the tree.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 195}),\n",
              " Document(page_content='Lecture Notes for Chapter 12: Binary Search Trees 12-5\\nInsertionand deletion\\nInsertion and deletion allows the dynamic set represented b y a binary search tree\\ntochange. Thebinary-search-tree property mustholdafter thechange. Insertion is\\nmorestraightforward than deletion.\\nInsertion\\nTREE-INSERT .T; ´/\\nyDNIL\\nxDT:root\\nwhile x¤NIL\\nyDx\\nif´:key< x:key\\nxDx:left\\nelsexDx:right\\n´:pDy\\nify==NIL\\nT:rootD´//treeTwasempty\\nelseif ´:key< y:key\\ny:leftD´\\nelsey:rightD´\\n\\x0fToinsertvalue \\x17intothebinarysearchtree,theprocedure isgivennode ´,with\\n´:keyD\\x17,´:leftDNIL, and ´:rightDNIL.\\n\\x0fBeginning at root of the tree, trace adownward path, maintai ning twopointers.\\n\\x0fPointer x: traces thedownward path.\\n\\x0fPointer y: “trailing pointer” to keep track of parent of x.\\n\\x0fTraverse the tree downward by comparing the value of node at xwith\\x17, and\\nmoveto the left or right child accordingly.\\n\\x0fWhen xisNIL, it isat thecorrect position for node ´.\\n\\x0fCompare ´’svaluewith y’svalue, andinsert ´at either y’sleftorright,appro-\\npriately.\\nExample\\nRun TREE-INSERT .T; C /onthe ﬁrst sample binary search tree. Result:\\nA DB\\nKHF\\nC', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 196}),\n",
              " Document(page_content='12-6 Lecture Notes for Chapter 12: Binary Search Trees\\nTime\\nSameas T REE-SEARCH. Onatree of height h, procedure takes O.h/time.\\nTREE-INSERTcanbeusedwithI NORDER-TREE-WALKtosortagivensetofnum-\\nbers. (SeeExercise 12.3-3.)\\nDeletion\\n[Deletion from a binary search tree changed in the third edit ion. In the ﬁrst two\\neditions, when the node ´passed to TREE-DELETEhad two children, ´’s succes-\\nsorywasthenodeactuallyremoved,with y’scontentscopiedinto ´. Theproblem\\nwith that approach is that if there are external pointers int o the binary search tree,\\nthen a pointer to yfrom outside the binary search tree becomes stale. In the thi rd\\nedition, the node ´passed to TREE-DELETEisalways thenode actually removed,\\nso that all external pointers to nodes other than ´remain valid.]\\nConceptually, deleting node ´from binary search tree Thas three cases:\\n1. If ´has nochildren, just remove it.\\n2. If ´has just one child, then make that child take ´’s position in the tree, drag-\\nging the child’s subtree along.\\n3. If ´has two children, then ﬁnd ´’s successor yand replace ´byyin the tree.\\nymust be in ´’s right subtree and have no left child. The rest of ´’s original\\nright subtree becomes y’s new right subtree, and ´’s left subtree becomes y’s\\nnew left subtree.\\nThiscaseisalittle trickybecause theexact sequence of ste pstaken depends on\\nwhether yis´’sright child.\\nThe code organizes the cases a bit differently. Since it will move subtrees around\\nwithin the binary search tree, it uses a subroutine, T RANSPLANT , to replace one\\nsubtree asthe child of itsparent by another subtree.\\nTRANSPLANT .T; u; \\x17/\\nifu:p==NIL\\nT:rootD\\x17\\nelseif u==u:p:left\\nu:p:leftD\\x17\\nelseu:p:rightD\\x17\\nif\\x17¤NIL\\n\\x17:pDu:p\\nTRANSPLANT .T; u; \\x17/replaces the subtree rooted at ubythe subtree rooted at \\x17:\\n\\x0fMakes u’sparentbecome \\x17’sparent(unless uistheroot,inwhichcaseitmakes\\n\\x17the root).\\n\\x0fu’s parent gets \\x17as either its left or right child, depending on whether uwas a\\nleft or right child.\\n\\x0fDoesn’t update \\x17:leftor\\x17:right,leaving that upto T RANSPLANT ’s caller.\\nTREE-DELETE .T; ´/has four cases when deleting node ´from binary search\\ntreeT:', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 197}),\n",
              " Document(page_content='Lecture Notes for Chapter 12: Binary Search Trees 12-7\\n\\x0fIf´hasnoleftchild,replace ´byitsrightchild. Therightchildmayormaynot\\nbeNIL. (If´’sright child is NIL, then thiscase handles thesituation inwhich ´\\nhas no children.)\\nq q\\nz\\nNILr\\nr\\n\\x0fIf´has just one child, and that child is its left child, then repl ace´by its left\\nchild.\\nq q\\nz\\nl NILl\\n\\x0fOtherwise, ´has two children. Find ´’s successor y.ymust lie in ´’s right\\nsubtree andhavenoleft child (thesolution toExercise 12.2 -5 onpage 12-15 of\\nthis manual showswhy).\\nGoal isto replace ´byy, splicing yout of its current location.\\n\\x0fIfyis´’s right child, replace ´byyand leave y’s right child alone.\\nq\\nz\\nl\\nNILq\\ny\\nl y x\\nx\\n\\x0fOtherwise, ylies within ´’s right subtree but is not the root of this subtree.\\nReplace yby itsownright child. Thenreplace ´byy.\\nq\\nz\\nl rq\\nz\\nl NIL ryq\\nl ry\\nx\\nNILy\\nxx', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 198}),\n",
              " Document(page_content='12-8 Lecture Notes for Chapter 12: Binary Search Trees\\nTREE-DELETE .T; ´/\\nif´:left==NIL\\nTRANSPLANT .T; ´; ´:right///´has no left child\\nelseif ´:right==NIL\\nTRANSPLANT .T; ´; ´:left///´has just aleft child\\nelse//´has twochildren.\\nyDTREE-MINIMUM .´:right///yis´’ssuccessor\\nify:p¤´\\n//ylies within ´’sright subtree but is not theroot of this subtree.\\nTRANSPLANT .T; y; y:right/\\ny:rightD´:right\\ny:right:pDy\\n//Replace ´byy.\\nTRANSPLANT .T; ´; y/\\ny:leftD´:left\\ny:left:pDy\\nNotethatthelastthreelinesexecutewhen ´hastwochildren,regardlessofwhether\\nyis´’s right child.\\nExample\\nOnthis binary search tree T,\\nH\\nB\\nA\\nE\\nFCI\\nK\\nL\\nN\\nOMJG\\nD\\nrunthefollowing. [Youcaneitherstartwiththeoriginal treeeachtimeorstar twith\\nthe result of the previous call. Thetree is designed so that e ither way will elicit all\\nfour cases.]\\n\\x0fTREE-DELETE .T; I/showsthecaseinwhichthenodedeletedhasnoleftchild.\\n\\x0fTREE-DELETE .T; G/showsthecaseinwhichthenode deleted hasaleft child\\nbut no right child.\\n\\x0fTREE-DELETE .T; K/shows the case in which the node deleted has both chil-\\ndren and its successor isits right child.\\n\\x0fTREE-DELETE .T; B/shows the case in which the node deleted has both chil-\\ndren and its successor isnot its right child.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 199}),\n",
              " Document(page_content='Lecture Notes for Chapter 12: Binary Search Trees 12-9\\nTime\\nO.h/, on a tree of height h. Everything is O.1/except for the call to T REE-\\nMINIMUM.\\nMinimizingrunningtime\\nWe’ve been analyzing running time in terms of h(the height of the binary search\\ntree), instead of n(the number of nodes inthe tree).\\n\\x0fProblem: Worst case for binary search tree is ‚.n/—nobetter than linked list.\\n\\x0fSolution: Guarantee small height (balanced tree)— hDO.lgn/.\\nIn later chapters, by varying the properties of binary searc h trees, we will be able\\ntoanalyze running timein terms of n.\\n\\x0fMethod: Restructure the tree if necessary. Nothing special is required for\\nquerying, but there may be extra work when changing the struc ture of the tree\\n(inserting or deleting).\\nRed-black trees are a special class of binary trees that avoi ds the worst-case be-\\nhavior of O.n/that we can see in “plain” binary search trees. Red-black tre es are\\ncovered in detail inChapter 13.\\nExpected heightofa randomly builtbinary search tree\\n[These are notes on a starred section in the book. I covered th is material in an\\noptional lecture.]\\nGiven a set of ndistinct keys. Insert them in random order into an initially empty\\nbinary search tree.\\n\\x0fEach of the nŠpermutations isequally likely.\\n\\x0fDifferent from assuming that every binary search tree on nkeys is equally\\nlikely.\\nTry it for nD3. Will get 5 different binary search trees. When we look at the\\nbinarysearchtreesresulting fromeachofthe 3Šinputpermutations, 4treeswill\\nappear once and 1 tree will appear twice. [This gives the idea for the solution\\nto Exercise 12.4-3.]\\n\\x0fForget about deleting keys.\\nWe will show that the expected height of a randomly built bina ry search tree is\\nO.lgn/.\\nRandomvariables\\nDeﬁnethe following random variables:\\n\\x0fXnDheight of arandomly built binary search tree on nkeys.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 200}),\n",
              " Document(page_content='12-10 Lecture Notes for Chapter 12: Binary Search Trees\\n\\x0fYnD2XnDexponential height .\\n\\x0fRnDrank of the root within the set of nkeys used to build the binary search\\ntree.\\n\\x0fEqually likely tobe any element of f1; 2; : : : ; ng.\\n\\x0fIfRnDi, then\\n\\x0fLeft subtree is arandomly-built binary search tree on i/NUL1keys.\\n\\x0fRight subtree isarandomly-built binary search tree on n/NULikeys.\\nForeshadowing\\nWewill need to relate E ŒYn\\x8dtoEŒXn\\x8d.\\nWe’ll use Jensen’s inequality :\\nEŒf .X/\\x8d\\x15f .EŒX\\x8d/ ; [leave on board]\\nprovided\\n\\x0fthe expectations exist and areﬁnite, and\\n\\x0ff .x/isconvex: for all x; yand all 0\\x14\\x15\\x141\\nf .\\x15xC.1/NUL\\x15/y/\\x14\\x15f .x/C.1/NUL\\x15/f .y/ :\\nx yλx + (1–λ)yf(x)f(y)\\nf(λx + (1–λ)y)λf(x) + (1–λ)f(y)\\nConvex\\x11“curves upward”\\nWe’ll use Jensen’s inequality for f .x/D2x.\\nSince 2xcurves upward, it’s convex.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 201}),\n",
              " Document(page_content='Lecture Notes for Chapter 12: Binary Search Trees 12-11\\nFormulafor Yn\\nThink about Yn,if weknow that RnDi:\\ni–1\\nnodesn–i\\nnodes\\nHeight of root is1more than the maximum height of itschildre n:\\nYnD2\\x01max.Yi/NUL1; Yn/NULi/ :\\nBasecases:\\n\\x0fY1D1(expected height of a 1-node tree is 20D1).\\n\\x0fDeﬁne Y0D0.\\nDeﬁneindicator random variables Zn;1; Zn;2; : : : ; Z n;n:\\nZn;iDIfRnDig:\\nRnis equally likely tobe any element of f1; 2; : : : ; ng\\n)PrfRnDigD1=n\\n)EŒZn;i\\x8dD1=n [leave onboard]\\n(since E ŒIfAg\\x8dDPrfAg)\\nConsider a given n-node binary search tree (which could be a subtree). Exactly\\noneZn;iis1,and all others are 0. Hence,\\nYnDnX\\niD1Zn;i\\x01.2\\x01max.Yi/NUL1; Yn/NULi// :[leave on board]\\n[Recall: YnD2\\x01max.Yi/NUL1; Yn/NULi/wasassuming that RnDi.]\\nBoundingE ŒYn\\x8d\\nWe will show that E ŒYn\\x8dis polynomial in n, which will imply that E ŒXn\\x8dD\\nO.lgn/.\\nClaim\\nZn;iis independent of Yi/NUL1andYn/NULi.\\nJustiﬁcation Ifwechoosetherootsuchthat RnDi,theleftsubtreecontains i/NUL1\\nnodes, and it’s like any other randomly built binary search t ree with i/NUL1nodes.\\nOther than the number of nodes, the left subtree’s structure has nothing to do with\\nit being the left subtree of the root. Hence, Yi/NUL1andZn;iare independent.\\nSimilarly, Yn/NULiandZn;iareindependent. (claim)', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 202}),\n",
              " Document(page_content='12-12 Lecture Notes for Chapter 12: Binary Search Trees\\nFact\\nIfXandYarenonnegativerandomvariables,thenE Œmax.X; Y /\\x8d\\x14EŒX\\x8dCEŒY \\x8d.\\n[Leave on board. ThisisExercise C.3-4from thetext.]\\nThus,\\nEŒYn\\x8dDE\"nX\\niD1Zn;i.2\\x01max.Yi/NUL1; Yn/NULi//#\\nDnX\\niD1EŒZn;i\\x01.2\\x01max.Yi/NUL1; Yn/NULi//\\x8d(linearity of expectation)\\nDnX\\niD1EŒZn;i\\x8d\\x01EŒ2\\x01max.Yi/NUL1; Yn/NULi/\\x8d(independence)\\nDnX\\niD11\\nn\\x01EŒ2\\x01max.Yi/NUL1; Yn/NULi/\\x8d (EŒZn;i\\x8dD1=n)\\nD2\\nnnX\\niD1EŒmax.Yi/NUL1; Yn/NULi/\\x8d (EŒaX\\x8dDaEŒX\\x8d)\\n\\x142\\nnnX\\niD1.EŒYi/NUL1\\x8dCEŒYn/NULi\\x8d/ (earlier fact) .\\nObserve that thelast summation is\\n.EŒY0\\x8dCEŒYn/NUL1\\x8d/C.EŒY1\\x8dCEŒYn/NUL2\\x8d/C.EŒY2\\x8dCEŒYn/NUL3\\x8d/\\nC\\x01\\x01\\x01C .EŒYn/NUL1\\x8dCEŒY0\\x8d/D2n/NUL1X\\niD0EŒYi\\x8d ;\\nand so weget the recurrence\\nEŒYn\\x8d\\x144\\nnn/NUL1X\\niD0EŒYi\\x8d : [leave on board]\\nSolving therecurrence\\nWewill show that for all integers n > 0, this recurrence has the solution\\nEŒYn\\x8d\\x141\\n4 \\nnC3\\n3!\\n:\\nLemma\\nn/NUL1X\\niD0 \\niC3\\n3!\\nD \\nnC3\\n4!\\n:\\n[This lemmasolves Exercise 12.4-1.]', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 203}),\n",
              " Document(page_content='Lecture Notes for Chapter 12: Binary Search Trees 12-13\\nProofUsePascal’s identity (Exercise C.1-7): \\nn\\nk!\\nD \\nn/NUL1\\nk/NUL1!\\nC \\nn/NUL1\\nk!\\n.\\nAlsousing the simple identity \\n4\\n4!\\nD1D \\n3\\n3!\\n,wehave\\n \\nnC3\\n4!\\nD \\nnC2\\n3!\\nC \\nnC2\\n4!\\nD \\nnC2\\n3!\\nC \\nnC1\\n3!\\nC \\nnC1\\n4!\\nD \\nnC2\\n3!\\nC \\nnC1\\n3!\\nC \\nn\\n3!\\nC \\nn\\n4!\\n:::\\nD \\nnC2\\n3!\\nC \\nnC1\\n3!\\nC \\nn\\n3!\\nC\\x01\\x01\\x01C \\n4\\n3!\\nC \\n4\\n4!\\nD \\nnC2\\n3!\\nC \\nnC1\\n3!\\nC \\nn\\n3!\\nC\\x01\\x01\\x01C \\n4\\n3!\\nC \\n3\\n3!\\nDn/NUL1X\\niD0 \\niC3\\n3!\\n: (lemma)\\nWesolve therecurrence byinduction on n.\\nBasis: nD1.\\n1DY1DEŒY1\\x8d\\x141\\n4 \\n1C3\\n3!\\nD1\\n4\\x014D1 :\\nInductivestep: Assume that E ŒYi\\x8d\\x141\\n4 \\niC3\\n3!\\nfor all i < n. Then\\nEŒYn\\x8d\\x144\\nnn/NUL1X\\niD0EŒYi\\x8d(from before)\\n\\x144\\nnn/NUL1X\\niD01\\n4 \\niC3\\n3!\\n(inductive hypothesis)\\nD1\\nnn/NUL1X\\niD0 \\niC3\\n3!\\nD1\\nn \\nnC3\\n4!\\n(lemma)\\nD1\\nn\\x01.nC3/Š\\n4Š .n/NUL1/Š\\nD1\\n4\\x01.nC3/Š\\n3Š nŠ', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 204}),\n",
              " Document(page_content='12-14 Lecture Notes for Chapter 12: Binary Search Trees\\nD1\\n4 \\nnC3\\n3!\\n:\\nThus, we’veproven that E ŒYn\\x8d\\x141\\n4 \\nnC3\\n3!\\n.\\nBoundingE ŒXn\\x8d\\nWith our bound on E ŒYn\\x8d, weuse Jensen’s inequality to bound E ŒXn\\x8d:\\n2EŒXn\\x8d\\x14E\\x02\\n2Xn\\x03\\nDEŒYn\\x8d :\\nThus,\\n2EŒXn\\x8d\\x141\\n4 \\nnC3\\n3!\\nD1\\n4\\x01.nC3/.nC2/.nC1/\\n6\\nDO.n3/ :\\nTaking logs of both sides gives E ŒXn\\x8dDO.lgn/.\\nDone!', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 205}),\n",
              " Document(page_content='SolutionsforChapter 12:\\nBinarySearch Trees\\nSolutionto Exercise 12.1-2\\nThissolutionisalsopostedpublicly\\nIn a heap, a node’s key is \\x15both of its children’s keys. In a binary search tree, a\\nnode’s key is\\x15its left child’s key, but \\x14itsright child’s key.\\nThe heap property, unlike the binary-searth-tree property , doesn’t help print the\\nnodes in sorted order because it doesn’t tell which subtree o f a node contains the\\nelement to print before that node. In a heap, the largest elem ent smaller than the\\nnode could bein either subtree.\\nNote that if the heap property could be used to print the keys i n sorted order in\\nO.n/time, we would have an O.n/-time algorithm for sorting, because building\\nthe heap takes only O.n/time. But we know (Chapter 8) that a comparison sort\\nmust take \\x7f.nlgn/time.\\nSolutionto Exercise 12.2-5\\nLetxbe a node with two children. In an inorder tree walk, the nodes inx’s left\\nsubtree immediately precede xand the nodes in x’sright subtree immediately fol-\\nlowx. Thus, x’s predecessor is in its left subtree, and its successor is in its right\\nsubtree.\\nLetsbex’s successor. Then scannot have a left child, for a left child of swould\\ncome between xandsin the inorder walk. (It’s after xbecause it’s in x’s right\\nsubtree, and it’s before sbecause it’s in s’s left subtree.) If anynode wereto come\\nbetween xandsin an inorder walk, then swould not be x’s successor, as we had\\nsupposed.\\nSymmetrically, x’spredecessor has no right child.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 206}),\n",
              " Document(page_content='12-16 Solutions for Chapter 12: Binary Search Trees\\nSolution to Exercise12.2-7\\nThis solutionisalsopostedpublicly\\nNotethatacall to T REE-MINIMUMfollowedby n/NUL1callsto T REE-SUCCESSOR\\nperformsexactlythesameinorderwalkofthetreeasdoesthe procedureI NORDER-\\nTREE-WALK. INORDER-TREE-WALKprints the T REE-MINIMUM ﬁrst, and by\\ndeﬁnition, the T REE-SUCCESSOR of a node is the next node in the sorted order\\ndetermined by aninorder tree walk.\\nThis algorithm runs in ‚.n/timebecause:\\n\\x0fIt requires \\x7f.n/timetodo the nprocedure calls.\\n\\x0fIt traverses each of the n/NUL1tree edges at most twice, whichtakes O.n/time.\\nToseethateachedgeistraversedatmosttwice(oncegoingdo wnthetreeandonce\\ngoingup), consider theedgebetweenanynode uandeither ofitschildren, node \\x17.\\nBy starting at the root, we must traverse .u; \\x17/downward from uto\\x17, before\\ntraversing it upward from \\x17tou. The only time the tree is traversed downward is\\nin code of T REE-MINIMUM, and the only time the tree is traversed upward is in\\ncode of T REE-SUCCESSOR when we look for the successor of a node that has no\\nright subtree.\\nSuppose that \\x17isu’sleft child.\\n\\x0fBeforeprinting u,wemustprintallthenodesinitsleftsubtree, whichisroot ed\\nat\\x17,guaranteeing thedownward traversal of edge .u; \\x17/.\\n\\x0fAfterallnodesin u’sleftsubtreeareprinted, umustbeprintednext. Procedure\\nTREE-SUCCESSOR traverses an upward path to ufrom the maximum element\\n(whichhasnorightsubtree)inthesubtreerootedat \\x17. Thispathclearlyincludes\\nedge .u; \\x17/, and since all nodes in u’s left subtree are printed, edge .u; \\x17/is\\nnever traversed again.\\nNowsuppose that \\x17isu’sright child.\\n\\x0fAfter uis printed, T REE-SUCCESSOR .u/is called. To get to the minimum\\nelement in u’sright subtree (whoseroot is \\x17),theedge .u; \\x17/mustbetraversed\\ndownward.\\n\\x0fAfterall values in u’sright subtree areprinted, T REE-SUCCESSOR iscalled on\\nthe maximum element (again, which has no right subtree) inth e subtree rooted\\nat\\x17. TREE-SUCCESSOR traverses a path up the tree to an element after u,\\nsince uwasalreadyprinted. Edge .u; \\x17/mustbetraversed upwardonthispath,\\nand since all nodes in u’s right subtree have been printed, edge .u; \\x17/is never\\ntraversed again.\\nHence, no edge istraversed twice inthe samedirection.\\nTherefore, this algorithm runs in ‚.n/time.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 207}),\n",
              " Document(page_content='Solutions forChapter 12: Binary Search Trees 12-17\\nSolutionto Exercise 12.3-3\\nThissolutionisalsopostedpublicly\\nHere’s the algorithm:\\nTREE-SORT.A/\\nletTbe anempty binary search tree\\nforiD1ton\\nTREE-INSERT .T; AŒi\\x8d/\\nINORDER-TREE-WALK.T:root/\\nWorst case: ‚.n2/—occurs whenalinear chain of nodes results from the repeate d\\nTREE-INSERToperations.\\nBestcase: ‚.nlgn/—occurswhenabinary treeofheight ‚.lgn/resultsfromthe\\nrepeated T REE-INSERToperations.\\nSolutionto Exercise 12.4-2\\nWe will answer the second part ﬁrst. We shall show that if the a verage depth of a\\nnode is ‚.lgn/, then the height of the tree is O.p\\nnlgn/. Then we will answer\\ntheﬁrstpart byexhibiting that thisbound istight: thereis abinarysearchtreewith\\naverage node depth ‚.lgn/and height ‚.p\\nnlgn/D!.lgn/.\\nLemma\\nIf the average depth of a node in an n-node binary search tree is ‚.lgn/, then the\\nheight of the tree is O.p\\nnlgn/.\\nProofSuppose that an n-node binary search tree has average depth ‚.lgn/and\\nheight h. Thenthereexistsapathfromtheroottoanodeatdepth h,andthedepths\\nofthenodes onthis pathare 0; 1; : : : ; h . Let Pbetheset ofnodes onthis pathand\\nQbeall other nodes. Then the average depth of anode is\\n1\\nn X\\nx2Pdepth .x/CX\\ny2Qdepth .y/!\\n\\x151\\nnX\\nx2Pdepth .x/\\nD1\\nnhX\\ndD0d\\nD1\\nn\\x01‚.h2/ :\\nFor the purpose of contradiction, suppose that his not O.p\\nnlgn/, so that hD\\n!.p\\nnlgn/. Thenwehave\\n1\\nn\\x01‚.h2/D1\\nn\\x01!.nlgn/\\nD!.lgn/ ;', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 208}),\n",
              " Document(page_content='12-18 Solutions for Chapter 12: Binary Search Trees\\nwhich contradicts the assumption that the average depth is ‚.lgn/. Thus, the\\nheight is O.p\\nnlgn/.\\nHere is an example of an n-node binary search tree with average node depth\\n‚.lgn/but height !.lgn/:\\nn/NULp\\nnlgn\\nnodes\\np\\nnlgnnodes\\nIn this tree, n/NULp\\nnlgnnodes are a complete binary tree, and the otherp\\nnlgn\\nnodes protrude from below asasingle chain. This tree hashei ght\\n‚.lg.n/NULp\\nnlgn//Cp\\nnlgnD‚.p\\nnlgn/\\nD!.lgn/ :\\nTo compute an upper bound on the average depth of a node, we use O.lgn/as\\nan upper bound on the depth of each of the n/NULp\\nnlgnnodes in the complete\\nbinary tree part and O.lgnCp\\nnlgn/as an upper bound on the depth of each of\\nthep\\nnlgnnodes in the protruding chain. Thus, the average depth of a no de is\\nbounded from above by\\n1\\nn\\x01O.p\\nnlgn .lgnCp\\nnlgn/C.n/NULp\\nnlgn/lgn/D1\\nn\\x01O.nlgn/\\nDO.lgn/ :\\nTo bound the average depth of a node from below, observe that t he bottommost\\nlevel ofthe complete binary treepart has ‚.n/NULp\\nnlgn/nodes, and eachof these\\nnodes has depth ‚.lgn/. Thus, the average node depth isat least\\n1\\nn\\x01‚..n/NULp\\nnlgn/lgn/D1\\nn\\x01\\x7f.nlgn/\\nD\\x7f.lgn/ :\\nBecause the average node depth isboth O.lgn/and\\x7f.lgn/,it is ‚.lgn/.\\nSolution to Exercise12.4-4\\nWe’ll go one better than showing that the function 2xis convex. Instead, we’ll\\nshow that the function cxis convex, for any positive constant c. According to\\nthe deﬁnition of convexity on page 1199 of the text, a functio nf .x/is con-\\nvex if for all xandyand for all 0\\x14\\x15\\x141, we have f .\\x15xC.1/NUL\\x15/y/\\x14\\n\\x15f .x/C.1/NUL\\x15/f .y/. Thus, we need to show that for all 0\\x14\\x15\\x141, we have\\nc\\x15xC.1/NUL\\x15/y\\x14\\x15cxC.1/NUL\\x15/cy.\\nWestart by proving the following lemma.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 209}),\n",
              " Document(page_content='Solutions forChapter 12: Binary Search Trees 12-19\\nLemma\\nForany real numbers aandband anypositive real number c,\\nca\\x15cbC.a/NULb/cblnc :\\nProofWeﬁrstshowthatforallreal r,wehave cr\\x151Crlnc. Byequation (3.12)\\nfrom the text, we have ex\\x151Cxfor all real x. Let xDrlnc, so that exD\\nerlncD.elnc/rDcr. Then wehave crDerlnc\\x151Crlnc.\\nSubstituting a/NULbforrin the above inequality, we have ca/NULb\\x151C.a/NULb/lnc.\\nMultiplying both sides by cbgives ca\\x15cbC.a/NULb/cblnc. (lemma)\\nNow we can show that c\\x15xC.1/NUL\\x15/y\\x14\\x15cxC.1/NUL\\x15/cyfor all 0\\x14\\x15\\x141. For\\nconvenience, let ´D\\x15xC.1/NUL\\x15/y.\\nInthe inequality given by the lemma, substitute xforaand´forb,giving\\ncx\\x15c´C.x/NUL´/c´lnc :\\nAlsosubstitute yforaand´forb,giving\\ncy\\x15c´C.y/NUL´/c´lnc :\\nIf we multiply the ﬁrst inequality by \\x15and the second by 1/NUL\\x15and then add the\\nresulting inequalities, weget\\n\\x15cxC.1/NUL\\x15/cy\\n\\x15\\x15.c´C.x/NUL´/c´lnc/C.1/NUL\\x15/.c´C.y/NUL´/c´lnc/\\nD\\x15c´C\\x15xc´lnc/NUL\\x15´c´lncC.1/NUL\\x15/c´C.1/NUL\\x15/yc´lnc\\n/NUL.1/NUL\\x15/´c´lnc\\nD.\\x15C.1/NUL\\x15//c´C.\\x15xC.1/NUL\\x15/y/c´lnc/NUL.\\x15C.1/NUL\\x15//´c´lnc\\nDc´C´c´lnc/NUL´c´lnc\\nDc´\\nDc\\x15xC.1/NUL\\x15/y;\\naswewished to show.\\nSolutionto Problem 12-2\\nThissolutionisalsopostedpublicly\\nTosortthestringsof S,weﬁrstinsertthemintoaradixtree,andthenuseapreorder\\ntree walk to extract them in lexicographically sorted order . The tree walk outputs\\nstrings only for nodes that indicate the existence of a strin g (i.e., those that are\\nlightly shaded in Figure 12.5of the text).\\nCorrectness\\nThepreorder ordering is thecorrect order because:\\n\\x0fAny node’s string is a preﬁx of all its descendants’ strings a nd hence belongs\\nbefore them inthe sorted order (rule 2).', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 210}),\n",
              " Document(page_content='12-20 Solutions for Chapter 12: Binary Search Trees\\n\\x0fAnode’sleftdescendantsbelongbeforeitsrightdescendan tsbecausethecorre-\\nspondingstringsareidenticaluptothatparentnode,andin thenextpositionthe\\nleft subtree’s strings have 0whereas the right subtree’s st rings have 1(rule 1).\\nTime\\n‚.n/.\\n\\x0fInsertion takes ‚.n/time, since the insertion of each string takes time propor-\\ntional toitslength (traversing apaththrough thetreewhos elength isthelength\\nof the string), and the sum of all the string lengths is n.\\n\\x0fThe preorder tree walk takes O.n/time. It is just like I NORDER-TREE-WALK\\n(it prints the current node and calls itself recursively on t he left and right sub-\\ntrees), so it takes time proportional to the number of nodes i n the tree. The\\nnumber of nodes is at most 1 plus the sum ( n) of the lengths of the binary\\nstrings in the tree, because a length- istring corresponds to a path through the\\nroot and iother nodes, but a single node may be shared among many string\\npaths.\\nSolution to Problem 12-3\\na.The total path length P.T /is deﬁned asP\\nx2Td.x; T /. Dividing both quanti-\\nties by ngives the desired equation.\\nb.For any node xinTL, we have d.x; T L/Dd.x; T //NUL1, since the distance to\\nthe root of TLis one less than the distance to the root of T. Similarly, for any\\nnode xinTR, we have d.x; T R/Dd.x; T //NUL1. Thus, if Thasnnodes, we\\nhave\\nP.T /DP.T L/CP.T R/Cn/NUL1 ;\\nsince each of the nnodes of T(except theroot) isin either TLorTR.\\nc.IfTis a randomly built binary search tree, then the root is equal ly likely to\\nbe any of the nelements in the tree, since the root is the ﬁrst element inser ted.\\nIt follows that the number of nodes in subtree TLis equally likely to be any\\ninteger in the setf0; 1; : : : ; n/NUL1g. The deﬁnition of P.n/as the average total\\npath length of arandomly built binary search tree, along wit h part (b), gives us\\nthe recurrence\\nP.n/D1\\nnn/NUL1X\\niD0.P.i/CP.n/NULi/NUL1/Cn/NUL1/ :\\nd.Since P.0/D0, and since for kD1; 2; : : : ; n/NUL1, each term P.k/in the\\nsummation appears once as P.i/and once as P.n/NULi/NUL1/, wecan rewrite the\\nequation from part (c) as\\nP.n/D2\\nnn/NUL1X\\nkD1P.k/C‚.n/ :', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 211}),\n",
              " Document(page_content='Solutions forChapter 12: Binary Search Trees 12-21\\ne.Observe that if, in the recurrence (7.6) in part (c) of Proble m 7-3, we replace\\nEŒT .\\x01/\\x8dbyP.\\x01/andwereplace qbyk,wegetalmostthesamerecurrence asin\\npart (d) of Problem 12-3. The remaining difference is that in Problem 12-3(d),\\nthe summation starts at 1rather than 2. Observe, however, that a binary tree\\nwith just one node has a total path length of 0, so that P.1/D0. Thus, wecan\\nrewrite the recurrence in Problem 12-3(d) as\\nP.n/D2\\nnn/NUL1X\\nkD2P.k/C‚.n/\\nand use the same technique aswasused inProblem 7-3 tosolve i t.\\nWestart by solving part (d) of Problem 7-3: showing that\\nn/NUL1X\\nkD2klgk\\x141\\n2n2lgn/NUL1\\n8n2:\\nFollowing the hint in Problem 7-3(d), wesplit the summation into twoparts:\\nn/NUL1X\\nkD2klgkDdn=2e/NUL1X\\nkD2klgkCn/NUL1X\\nkDdn=2eklgk :\\nThelg kintheﬁrstsummation ontheright islessthan lg .n=2/Dlgn/NUL1,and\\nthe lg kinthe second summation isless than lg n. Thus,\\nn/NUL1X\\nkD2klgk < .lgn/NUL1/dn=2e/NUL1X\\nkD2kClgnn/NUL1X\\nkDdn=2ek\\nDlgnn/NUL1X\\nkD2k/NULdn=2e/NUL1X\\nkD2k\\n\\x141\\n2n.n/NUL1/lgn/NUL1\\n2\\x10n\\n2/NUL1\\x11n\\n2\\n\\x141\\n2n2lgn/NUL1\\n8n2\\nifn\\x152.\\nNowweshow that therecurrence\\nP.n/D2\\nnn/NUL1X\\nkD2P.k/C‚.n/\\nhas the solution P.n/DO.nlgn/. We use the substitution method. Assume\\ninductively that P.n/\\x14anlgnCbfor some positive constants aandbto be\\ndetermined. Wecanpick aandbsufﬁciently large sothat anlgnCb\\x15P.1/.\\nThen, for n > 1, wehave by substitution\\nP.n/D2\\nnn/NUL1X\\nkD2P.k/C‚.n/\\n\\x142\\nnn/NUL1X\\nkD2.aklgkCb/C‚.n/', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 212}),\n",
              " Document(page_content='12-22 Solutions for Chapter 12: Binary Search Trees\\nD2a\\nnn/NUL1X\\nkD2klgkC2b\\nn.n/NUL2/C‚.n/\\n\\x142a\\nn\\x121\\n2n2lgn/NUL1\\n8n2\\x13\\nC2b\\nn.n/NUL2/C‚.n/\\n\\x14anlgn/NULa\\n4nC2bC‚.n/\\nDanlgnCbC\\x10\\n‚.n/Cb/NULa\\n4n\\x11\\n\\x14anlgnCb ;\\nsince we can choose alarge enough so thata\\n4ndominates ‚.n/Cb. Thus,\\nP.n/DO.nlgn/.\\nf.We draw an analogy between inserting an element into a subtre e of a binary\\nsearch treeandsorting asubarray inquicksort. Observetha t once anelement x\\nis chosen as the root of a subtree T, all elements that will be inserted after x\\nintoTwill be compared to x. Similarly, observe that once an element yis\\nchosen as the pivot in a subarray S, all other elements in Swill be compared\\ntoy. Therefore, the quicksort implementation in which the comp arisons are\\nthe same as those made when inserting into a binary search tre e is simply to\\nconsider the pivots in the same order as the order in which the elements are\\ninserted into thetree.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 213}),\n",
              " Document(page_content='Lecture Notes forChapter 13:\\nRed-BlackTrees\\nChapter 13overview\\nRed-blacktrees\\n\\x0fA variation of binary search trees.\\n\\x0fBalanced : height is O.lgn/,where nis thenumber of nodes.\\n\\x0fOperations will take O.lgn/time inthe worst case.\\n[These notes are a bit simpler than the treatment in the book, to make them more\\namenable to a lecture situation. Our students ﬁrst see red-b lack trees in a course\\nthat precedes our algorithms course. This set of lecture not es is intended as a\\nrefresher for the students, bearing in mind that some time ma y have passed since\\nthey last sawred-black trees.\\nTheprocedures inthischapterareratherlongsequencesofp seudocode. Youmight\\nwanttomakearrangements toproject themrather thanspendi ng timewritingthem\\nonaboard.]\\nRed-black trees\\nAred-black tree is a binary search tree + 1 bit per node: an attribute color, which\\niseither red or black.\\nAll leaves are empty (nil) and colored black.\\n\\x0fWeuse asingle sentinel, T:nil,for all theleaves of red-black tree T.\\n\\x0fT:nil:colorisblack.\\n\\x0fTheroot’s parent isalso T:nil.\\nAllother attributes of binary search treesareinherited by red-black trees( key,left,\\nright, and p). Wedon’t care about the keyin T:nil.\\nRed-blackproperties\\n[Leave these up onthe board.]', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 214}),\n",
              " Document(page_content='13-2 Lecture Notes for Chapter 13: Red-BlackTrees\\n1. Everynode iseither red or black.\\n2. Theroot isblack.\\n3. Everyleaf ( T:nil) is black.\\n4. If a node is red, then both its children are black. (Hence no two reds in a row\\nona simple path from the root to aleaf.)\\n5. For each node, all paths from the node to descendant leaves contain the same\\nnumber of black nodes.\\nExample:\\n26\\n17 41\\n30\\n3847\\n50\\nT.nilh = 4\\nbh = 2\\nh = 1\\nbh = 1h = 3\\nbh = 2\\nh = 2\\nbh = 1h = 2\\nbh = 1\\nh = 1\\nbh = 1h = 1\\nbh = 1\\n[Nodeswithboldoutlineindicateblacknodes. Don’taddhei ghtsandblack-heights\\nyet. Wewon’t bother with drawing T:nilany more.]\\nHeight of ared-black tree\\n\\x0fHeight of anode is the number of edges in alongest path to aleaf.\\n\\x0fBlack-height ofanode x: bh.x/isthenumberofblacknodes(including T:nil)\\non the path from xto leaf, not counting x. By property 5, black-height is well\\ndeﬁned.\\n[Now label the example tree with height handbhvalues.]\\nClaim\\nAnynode withheight hhas black-height\\x15h=2.\\nProofBy property 4,\\x14h=2nodes on the path from the node to a leaf are red.\\nHence\\x15h=2are black. (claim)\\nClaim\\nThesubtree rooted at any node xcontains\\x152bh.x//NUL1internal nodes.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 215}),\n",
              " Document(page_content='Lecture Notes for Chapter 13: Red-BlackTrees 13-3\\nProofByinduction on height of x.\\nBasis:Height of xD0)xisaleaf)bh.x/D0. Thesubtree rooted at xhas0\\ninternal nodes. 20/NUL1D0.\\nInductive step: Let the height of xbehand bh .x/Db. Any child of xhas\\nheight h/NUL1and black-height either b(if the child is red) or b/NUL1(if the child is\\nblack). By the inductive hypothesis, each child has \\x152bh.x//NUL1/NUL1internal nodes.\\nThus,thesubtree rooted at xcontains\\x152\\x01.2bh.x//NUL1/NUL1/C1D2bh.x//NUL1internal\\nnodes. (TheC1is for xitself.) (claim)\\nLemma\\nAred-black tree with ninternal nodes has height \\x142lg.nC1/.\\nProofLethandbbe the height and black-height of the root, respectively. By the\\nabove twoclaims,\\nn\\x152b/NUL1\\x152h=2/NUL1 :\\nAdding 1toboth sides and then taking logs gives lg .nC1/\\x15h=2, which implies\\nthath\\x142lg.nC1/. (theorem)\\nOperations on red-black trees\\nThe non-modifying binary-search-tree operations M INIMUM, MAXIMUM, SUC-\\nCESSOR, PREDECESSOR , and S EARCHrun in O.height /time. Thus, they take\\nO.lgn/timeon red-black trees.\\nInsertion and deletion are not soeasy.\\nIf weinsert, what color to makethe new node?\\n\\x0fRed? Might violate property 4.\\n\\x0fBlack? Might violate property 5.\\nIf wedelete, thus removing anode, what color wasthe node tha t wasremoved?\\n\\x0fRed? OK, since we won’t have changed any black-heights, nor w ill we have\\ncreated two red nodes in a row. Also, cannot cause a violation of property 2,\\nsince if the removed node was red, it could not have been thero ot.\\n\\x0fBlack? Could cause there to be two reds in a row (violating pro perty 4), and\\ncan also cause a violation of property 5. Could also cause a vi olation of prop-\\nerty 2,iftheremoved nodewastheroot and itschild—which be comes thenew\\nroot—was red.\\nRotations\\n\\x0fThebasic tree-restructuring operation.\\n\\x0fNeeded tomaintain red-black trees as balanced binary searc h trees.\\n\\x0fChanges the local pointer structure. (Onlypointers are cha nged.)', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 216}),\n",
              " Document(page_content='13-4 Lecture Notes for Chapter 13: Red-BlackTrees\\n\\x0fWon’t upset thebinary-search-tree property.\\n\\x0fHaveboth left rotation and right rotation. Theyare inverse s of each other.\\n\\x0fArotation takes ared-black-tree and anode within thetree.\\ny\\nx\\nα βγx\\ny α\\nβ γLEFT-ROTATE(T, x)\\nRIGHT-ROTATE(T, y)\\nLEFT-ROTATE .T; x/\\nyDx:right//sety\\nx:rightDy:left//turny’sleft subtree into x’sright subtree\\nify:left¤T:nil\\ny:left:pDx\\ny:pDx:p//linkx’sparent to y\\nifx:p==T:nil\\nT:rootDy\\nelseif x==x:p:left\\nx:p:leftDy\\nelsex:p:rightDy\\ny:leftDx//putxony’sleft\\nx:pDy\\nThepseudocode for L EFT-ROTATEassumes that\\n\\x0fx:right¤T:nil,and\\n\\x0froot’s parent is T:nil.\\nPseudocodeforR IGHT-ROTATEissymmetric: exchange leftandrighteverywhere.\\nExample\\n[Use to demonstrate that rotation maintains inorder orderi ng of keys. Node colors\\nomitted.]\\n47\\n11\\n9 18\\n14\\n1719\\n22x\\ny\\n47\\n18\\n19\\n14\\n1722xy\\n11\\n9LEFT-ROTATE(T, x)', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 217}),\n",
              " Document(page_content='Lecture Notes for Chapter 13: Red-BlackTrees 13-5\\n\\x0fBeforerotation: keysof x’sleft subtree\\x1411\\x14keysof y’sleft subtree\\x1418\\x14\\nkeys of y’sright subtree.\\n\\x0fRotation makes y’s left subtree into x’sright subtree.\\n\\x0fAfter rotation: keysof x’sleft subtree\\x1411\\x14keysof x’sright subtree\\x1418\\x14\\nkeys of y’sright subtree.\\nTime\\nO.1/for both L EFT-ROTATEand RIGHT-ROTATE, since a constant number of\\npointers are modiﬁed.\\nNotes\\n\\x0fRotation is avery basic operation, also used in AVLtrees and splay trees.\\n\\x0fSomebooks talk of rotating on anedge rather than onanode.\\nInsertion\\nStart by doing regular binary-search-tree insertion:\\nRB-INSERT .T; ´/\\nyDT:nil\\nxDT:root\\nwhile x¤T:nil\\nyDx\\nif´:key< x:key\\nxDx:left\\nelsexDx:right\\n´:pDy\\nify==T:nil\\nT:rootD´\\nelseif ´:key< y:key\\ny:leftD´\\nelsey:rightD´\\n´:leftDT:nil\\n´:rightDT:nil\\n´:colorDRED\\nRB-INSERT-FIXUP.T; ´/\\n\\x0fRB-INSERTends bycoloring the new node ´red.\\n\\x0fThen it calls RB-I NSERT-FIXUPbecause we could have violated a red-black\\nproperty.\\nWhich property might be violated?\\n1. OK.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 218}),\n",
              " Document(page_content='13-6 Lecture Notes for Chapter 13: Red-BlackTrees\\n2. If ´isthe root, then there’s aviolation. Otherwise, OK.\\n3. OK.\\n4. If ´:pis red, there’s aviolation: both ´and´:parered.\\n5. OK.\\nRemovethe violation by calling RB-I NSERT-FIXUP:\\nRB-INSERT-FIXUP .T; ´/\\nwhile ´:p:color==RED\\nif´:p==´:p:p:left\\nyD´:p:p:right\\nify:color==RED\\n´:p:colorDBLACK //case 1\\ny:colorDBLACK //case 1\\n´:p:p:colorDRED //case 1\\n´D´:p:p //case 1\\nelse if ´==´:p:right\\n´D´:p //case 2\\nLEFT-ROTATE .T; ´/ //case 2\\n´:p:colorDBLACK //case 3\\n´:p:p:colorDRED //case 3\\nRIGHT-ROTATE .T; ´:p:p/ //case 3\\nelse(sameasthenclause with “right” and “left” exchanged)\\nT:root:colorDBLACK\\nLoop invariant:\\nAtthe start of each iteration of the whileloop,\\na.´is red.\\nb. There is at most one red-black violation:\\n\\x0fProperty 2: ´is ared root, or\\n\\x0fProperty 4: ´and´:pare both red.\\n[Thebook has athird part of the loop invariant, but weomit it for lecture.]\\nInitialization: We’ve already seen whythe loop invariant holds initially.\\nTermination: Theloopterminatesbecause ´:pisblack. Hence,property4isOK.\\nOnlyproperty 2might be violated, and the last line ﬁxesit.\\nMaintenance: Wedropoutwhen ´istheroot(sincethen ´:pisthesentinel T:nil,\\nwhichisblack). Whenwestarttheloopbody,theonlyviolati onisofproperty4.\\nThere are 6 cases, 3 of which are symmetric to the other 3. The c ases are not\\nmutually exclusive. We’ll consider cases inwhich ´:pis aleft child.\\nLetybe´’s uncle ( ´:p’s sibling).', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 219}),\n",
              " Document(page_content='Lecture Notes for Chapter 13: Red-BlackTrees 13-7\\nCase 1: yisred\\nzyC\\nD A\\nB α\\nβ γδ εC\\nD A\\nB α\\nβ γδ εnew z\\nyC\\nD B\\nδ εC\\nD B\\nA\\nα βγ δ εnew z\\nA\\nα βγ zIf z is a right child\\nIf z is a left child\\n\\x0f´:p:p(´’s grandparent) must be black, since ´and´:pare both red and\\nthere areno other violations of property 4.\\n\\x0fMake ´:pandyblack)now´and´:parenotbothred. Butproperty5\\nmight now be violated.\\n\\x0fMake ´:p:pred)restores property 5.\\n\\x0fThenext iteration has ´:p:pas the new ´(i.e.,´movesup 2levels).\\nCase 2: yisblack, ´is aright child\\nC\\nA\\nB α\\nβ γδ\\nCase 2zy B\\nA\\nα βγδ\\nCase 3zy z AB\\nC\\nα β γ δC\\n\\x0fLeft rotate around ´:p)now´is a left child, and both ´and´:pare\\nred.\\n\\x0fTakes usimmediately tocase 3.\\nCase 3: yisblack, ´is aleft child\\n\\x0fMake ´:pblack and ´:p:pred.\\n\\x0fThen right rotate on ´:p:p.\\n\\x0fNolonger have 2reds in arow.\\n\\x0f´:pis now black)no moreiterations.\\nAnalysis\\nO.lgn/timetoget through RB-I NSERTup tothe call of RB-I NSERT-FIXUP.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 220}),\n",
              " Document(page_content='13-8 Lecture Notes for Chapter 13: Red-BlackTrees\\nWithin RB-I NSERT-FIXUP:\\n\\x0fEachiteration takes O.1/time.\\n\\x0fEachiteration iseither the last one or it moves ´up2 levels.\\n\\x0fO.lgn/levels)O.lgn/time.\\n\\x0fAlsonote that there are at most 2rotations overall.\\nThus, insertion into ared-black tree takes O.lgn/time.\\nDeletion\\n[Because deletion from a binary search tree changed in the th ird edition, so did\\ndeletion from a red-black tree. As with deletion from a binar y search tree, the\\nnode ´deleted from a red-black tree is always the node ´passed to the deletion\\nprocedure.]\\nBased on the T REE-DELETEprocedure for binary search trees:\\nRB-DELETE .T; ´/\\nyD´\\ny-original-colorDy:color\\nif´:left==T:nil\\nxD´:right\\nRB-TRANSPLANT .T; ´; ´:right/\\nelseif ´:right==T:nil\\nxD´:left\\nRB-TRANSPLANT .T; ´; ´:left/\\nelseyDTREE-MINIMUM .´:right/\\ny-original-colorDy:color\\nxDy:right\\nify:p==´\\nx:pDy\\nelseRB-TRANSPLANT .T; y; y:right/\\ny:rightD´:right\\ny:right:pDy\\nRB-TRANSPLANT .T; ´; y/\\ny:leftD´:left\\ny:left:pDy\\ny:colorD´:color\\nify-original-color==BLACK\\nRB-DELETE-FIXUP .T; x/\\nRB-DELETEcallsaspecialversionofT RANSPLANT (usedindeletionfrombinary\\nsearch trees), customized for red-black trees:', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 221}),\n",
              " Document(page_content='Lecture Notes for Chapter 13: Red-BlackTrees 13-9\\nRB-TRANSPLANT .T; u; \\x17/\\nifu:p==T:nil\\nT:rootD\\x17\\nelseif u==u:p:left\\nu:p:leftD\\x17\\nelseu:p:rightD\\x17\\n\\x17:pDu:p\\nDifferences between RB-T RANSPLANT and TRANSPLANT :\\n\\x0fRB-TRANSPLANT references the sentinel T:nilinstead of NIL.\\n\\x0fAssignment to \\x17:poccursevenif \\x17pointstothesentinel. Infact,weexploitthe\\nability to assign to \\x17:pwhen \\x17points tothe sentinel.\\nRB-DELETEhas almost twice as many lines as T REE-DELETE, but you can ﬁnd\\neach line of T REE-DELETEwithin RB-D ELETE(withNILreplaced by T:niland\\ncalls to T RANSPLANT replaced bycalls to RB-T RANSPLANT ).\\nDifferences between RB-D ELETEand TREE-DELETE:\\n\\x0fyis the node either removed from the tree (when ´has fewer than 2children)\\nor movedwithin the tree (when ´has2children).\\n\\x0fNeedtosave y’soriginal color(in y-original-color)totestitattheend,because\\nif it’s black, then removing or moving ycould cause red-black properties to be\\nviolated.\\n\\x0fxisthe node that movesinto y’s original position. It’s either y’sonly child, or\\nT:nilifyhas nochildren.\\n\\x0fSetsx:pto point to the original position of y’s parent, even if xDT:nil.x:p\\nis set in one of twoways:\\n\\x0fIf´isnot y’soriginal parent, x:pissetinthelastlineof RB-T RANSPLANT .\\n\\x0fIf´isy’s original parent, then ywill move up to take ´’s position in the\\ntree. The assignment x:pDymakes x:ppoint to the original position of\\ny’sparent, even if xisT:nil.\\n\\x0fIfy’s original color was black, the changes to the tree structur e might cause\\nred-black properties tobe violated, and wecall RB-D ELETE-FIXUPat the end\\nto resolve theviolations.\\nIfywasoriginally black, what violations of red-black propert ies could arise?\\n1. Noviolation.\\n2. If yis the root and xisred, then the root has become red.\\n3. Noviolation.\\n4. Violation if x:pandxareboth red.\\n5. Anysimple path containing ynow has 1fewer black node.\\n\\x0fCorrect by giving xan“extra black.”\\n\\x0fAdd1to count of black nodes on paths containing x.\\n\\x0fNowproperty 5isOK,but property 1isnot.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 222}),\n",
              " Document(page_content='13-10 Lecture Notes for Chapter 13: Red-BlackTrees\\n\\x0fxiseitherdoublyblack (ifx:colorDBLACK)orred&black (ifx:colorD\\nRED).\\n\\x0fThe attribute x:coloris still either REDorBLACK. No new values for color\\nattribute.\\n\\x0fInotherwords,theextrablackness onanodeisbyvirtueof xpointing tothe\\nnode.\\nRemovethe violations by calling RB-D ELETE-FIXUP:\\nRB-DELETE-FIXUP .T; x/\\nwhile x¤T:rootandx:color==BLACK\\nifx==x:p:left\\nwDx:p:right\\nifw:color==RED\\nw:colorDBLACK //case 1\\nx:p:colorDRED //case 1\\nLEFT-ROTATE .T; x:p/ //case 1\\nwDx:p:right //case 1\\nifw:left:color==BLACKandw:right:color==BLACK\\nw:colorDRED //case 2\\nxDx:p //case 2\\nelse if w:right:color==BLACK\\nw:left:colorDBLACK //case 3\\nw:colorDRED //case 3\\nRIGHT-ROTATE .T; w/ //case 3\\nwDx:p:right //case 3\\nw:colorDx:p:color //case 4\\nx:p:colorDBLACK //case 4\\nw:right:colorDBLACK //case 4\\nLEFT-ROTATE .T; x:p/ //case 4\\nxDT:root //case 4\\nelse(sameasthenclause with “right” and “left” exchanged)\\nx:colorDBLACK\\nIdea\\nMovethe extra black up thetree until\\n\\x0fxpoints toared & black node )turn it into ablack node,\\n\\x0fxpoints tothe root)just remove theextra black, or\\n\\x0fwecando certain rotations and recolorings and ﬁnish.\\nWithin the whileloop:\\n\\x0fxalways points to anonroot doubly black node.\\n\\x0fwisx’ssibling.\\n\\x0fwcannot be T:nil, since that would violate property 5 at x:p.\\nThere are 8 cases, 4 of which are symmetric to the other 4. As wi th insertion, the\\ncases are not mutually exclusive. We’ll look at cases inwhic hxisaleft child.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 223}),\n",
              " Document(page_content='Lecture Notes for Chapter 13: Red-BlackTrees 13-11\\nCase1: wisred\\nAB\\nD\\nC E α β\\nγ δ ε ζx w\\nAB\\nCD\\nE\\nx new w\\nα β γ δε ζCase 1\\n\\x0fwmust have black children.\\n\\x0fMake wblack and x:pred.\\n\\x0fThenleft rotate on x:p.\\n\\x0fNewsibling of xwasachild of wbefore rotation)must beblack.\\n\\x0fGoimmediately tocase 2,3, or 4.\\nCase2: wisblack and both of w’schildren are black\\nAB\\nD\\nC E α β\\nγ δ ε ζx wc\\nAB\\nD\\nC E α β\\nγ δ ε ζc new xCase 2\\n[Node with gray outline isof unknown color, denoted by c.]\\n\\x0fTake1black off x()singly black) and off w()red).\\n\\x0fMovethat black to x:p.\\n\\x0fDothe next iteration with x:pasthe new x.\\n\\x0fIf entered this case from case 1, then x:pwas red)newxis red & black\\n)color attribute of new xisRED)loop terminates. Then new xis made\\nblack inthe last line.\\nCase3: wisblack, w’sleft child isred, and w’sright child is black\\nAB\\nD\\nC E α β\\nγ δ ε ζx wc\\nAB\\nC\\nD α β γ\\nδ\\nε ζxc\\nnew wCase 3\\nE\\n\\x0fMake wred and w’sleft child black.\\n\\x0fThenright rotate on w.\\n\\x0fNewsibling wofxisblack withared right child )case 4.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 224}),\n",
              " Document(page_content='13-12 Lecture Notes for Chapter 13: Red-BlackTrees\\nCase 4: wis black, w’sleft child is black, and w’sright child isred\\nAB\\nD\\nC E α β\\nγ δε ζx wc c\\nα βAB\\nCD\\nE\\nnew x = T.root γ δ ε ζCase 4\\nc′ c′\\n[Nowthere aretwo nodes of unknown colors, denoted by candc0.]\\n\\x0fMake wbex:p’scolor ( c).\\n\\x0fMake x:pblack and w’sright child black.\\n\\x0fThen left rotate on x:p.\\n\\x0fRemove the extra black on x()xis now singly black) without violating\\nany red-black properties.\\n\\x0fAll done. Setting xto root causes the loop to terminate.\\nAnalysis\\nO.lgn/time toget through RB-D ELETEup tothe call of RB-D ELETE-FIXUP.\\nWithin RB-D ELETE-FIXUP:\\n\\x0fCase2is the only case in which moreiterations occur.\\n\\x0fxmoves up1 level.\\n\\x0fHence, O.lgn/iterations.\\n\\x0fEachof cases 1, 3, and 4has 1rotation )\\x14 3rotations in all.\\n\\x0fHence, O.lgn/time.\\n[InChapter 14, we’llseeatheorem that relies onred-black t reeoperations causing\\nat most a constant number of rotations. This is where red-bla ck trees enjoy an\\nadvantage over AVL trees: in the worst case, an operation on a nn-node AVL tree\\ncauses \\x7f.lgn/rotations.]', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 225}),\n",
              " Document(page_content='SolutionsforChapter 13:\\nRed-BlackTrees\\nSolutionto Exercise 13.1-3\\nIf we color the root of a relaxed red-black tree black but make no other changes,\\nthe resulting tree is ared-black tree. Not even any black-he ights change.\\nSolutionto Exercise 13.1-4\\nThissolutionisalsopostedpublicly\\nAfter absorbing each red node into its black parent, the degr ee of each node black\\nnode is\\n\\x0f2, if both children were already black,\\n\\x0f3, if one child wasblack and one wasred, or\\n\\x0f4, if both children were red.\\nAll leaves of the resulting tree have the samedepth.\\nSolutionto Exercise 13.1-5\\nThissolutionisalsopostedpublicly\\nIn the longest path, at least every other node is black. In the shortest path, at most\\neverynodeisblack. Sincethetwopathscontainequal number sofblacknodes, the\\nlength of the longest path isat most twice thelength of the sh ortest path.\\nWecan say this more precisely, asfollows:\\nSince every path contains bh .x/black nodes, even the shortest path from xto a\\ndescendant leaf has length at least bh .x/. By deﬁnition, the longest path from x\\nto a descendant leaf has length height .x/. Since the longest path has bh .x/black\\nnodes and at least half the nodes on the longest path are black (by property 4),\\nbh.x/\\x15height .x/=2,so\\nlength of longest path Dheight .x/\\x142\\x01bh.x/\\x14twice length of shortest path :', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 226}),\n",
              " Document(page_content='13-14 Solutions for Chapter 13: Red-Black Trees\\nSolution to Exercise13.2-4\\nSince the exercise asks about binary search trees rather tha n the morespeciﬁc red-\\nblack trees, we assume here that leaves are full-ﬂedged node s, and we ignore the\\nsentinels.\\nTaking thebook’s hint, westart byshowing that withat most n/NUL1right rotations,\\nwecan convert any binary search tree into one that isjust ari ght-going chain.\\nThe idea is simple. Let us deﬁne the right spine as the root and all descendants of\\ntheroot thatarereachable byfollowingonly rightpointers fromtheroot. Abinary\\nsearch tree that is just aright-going chain has all nnodes in the right spine.\\nAs long as the tree is not just a right spine, repeatedly ﬁnd so me node yon the\\nright spine that has anon-leaf left child xand then perform aright rotation on y:\\nγy\\nx\\nα βRIGHT-ROTATE(T, y)\\nyx\\nα\\nβ γ\\n(In the above ﬁgure, note that anyof ˛,ˇ,and \\rcan be anempty subtree.)\\nObserve that this right rotation adds xto the right spine, and no other nodes leave\\nthe right spine. Thus, this right rotation increases the num ber of nodes in the right\\nspine by 1. Any binary search tree starts out with at least one node—the root—in\\ntherightspine. Moreover,ifthereareanynodesnotontheri ghtspine, thenatleast\\none such node has a parent on the right spine. Thus, at most n/NUL1right rotations\\nare needed to put all nodes in the right spine, so that the tree consists of a single\\nright-going chain.\\nIfweknewthesequenceofrightrotationsthattransformsan arbitrarybinarysearch\\ntreeTto a single right-going chain T0, then we could perform this sequence in\\nreverse—turning each right rotation into its inverse left r otation—to transform T0\\nback into T.\\nTherefore, here is how we can transform any binary search tre eT1into any\\nother binary search tree T2. Let T0be the unique right-going chain consist-\\ning of the nodes of T1(which is the same as the nodes of T2). Let rD\\nhr1; r2; : : : ; r kibe a sequence of right rotations that transforms T1toT0, and let\\nr0Dhr0\\n1; r0\\n2; : : : ; r0\\nk0ibe a sequence of right rotations that transforms T2toT0.\\nWe know that there exist sequences randr0with k; k0\\x14n/NUL1. For each right\\nrotation r0\\ni, letl0\\nibe the corresponding inverse left rotation. Then the sequen ce\\nhr1; r2; : : : ; r k; l0\\nk0; l0\\nk0/NUL1; : : : ; l0\\n2; l0\\n1itransforms T1toT2inat most 2n/NUL2rotations.\\nSolution to Exercise13.3-3\\nThis solutionisalsopostedpublicly\\nIn Figure 13.5, nodes A,B, and Dhave black-height kC1in all cases, because\\neach of their subtrees has black-height kand a black root. Node Chas black-', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 227}),\n",
              " Document(page_content='Solutions forChapter 13: Red-BlackTrees 13-15\\nheight kC1on the left (because its red children have black-height kC1) and\\nblack-height kC2ontheright(becauseitsblackchildrenhaveblack-height kC1).\\nC\\nD A\\nB α\\nβ γδ ε(a)C\\nD A\\nB α\\nβ γδ ε\\nC\\nD B\\nδ εC\\nD B\\nA\\nα βγ δ ε(b)\\nA\\nα βγk+1k+1\\nk+1k+1 k+1k+2\\nk+1\\nk+1k+1 k+1k+1\\nk+1k+1 k+1k+2k+1 zy\\nzy\\nInFigure13.6,nodes A,B,and Chaveblack-height kC1inall cases. Atleftand\\nin the middle, each of A’s and B’s subtrees has black-height kand a black root,\\nwhile Chasonesuchsubtree andaredchildwithblack-height kC1. Attheright,\\neach of A’s and C’s subtrees has black-height kand a black root, while B’s red\\nchildren each have black-height kC1.\\nC\\nA\\nB α\\nβ γδ\\nCase 2B\\nA\\nα βγδ\\nCase 3AB\\nC\\nα β γ δC\\nk+1k+1k+1 k+1\\nk+1\\nk+1k+1 k+1k+1\\nzy\\nzy\\nProperty 5 is preserved by the transformations. We have show n above that the\\nblack-height iswell-deﬁnedwithinthesubtreespictured, soproperty5ispreserved\\nwithin those subtrees. Property 5 is preserved for the tree c ontaining the subtrees\\npictured,becauseeverypaththroughthesesubtreestoalea fcontributes kC2black\\nnodes.\\nSolutionto Exercise 13.3-4\\nColors are set to red only in cases 1 and 3, and in both situatio ns, it is ´:p:pthat\\nis reddened. If ´:p:pis the sentinel, then ´:pis the root. By part (b) of the loop\\ninvariant andline1of RB-I NSERT-FIXUP, if´:pistheroot,thenwehavedropped\\nout of the loop. The only subtlety is in case 2, where we set ´D´:pbefore\\ncoloring ´:p:pred. Because we rotate before the recoloring, the identity o f´:p:p\\nisthe samebefore and after case 2, sothere’s no problem.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 228}),\n",
              " Document(page_content='13-16 Solutions for Chapter 13: Red-Black Trees\\nSolution to Exercise13.4-6\\nCase1occursonlyif x’ssibling wisred. If x:pwerered,thentherewouldbetwo\\nredsinarow,namely x:p(whichisalso w:p)and w,andwewouldhavehadthese\\ntworeds inarow even before calling RB-D ELETE.\\nSolution to Exercise13.4-7\\nNo, the red-black tree will not necessarily be the same. Here are two examples:\\none in which the tree’s shape changes, and one in which the sha pe remains the\\nsame but the node colors change.\\n3\\n22\\n1 32\\n3\\n3\\n2 43\\n2 43\\n2 4\\n1insert 1 delete 1\\ninsert 1 delete 1\\nSolution to Problem 13-1\\nThis solutionisalsopostedpublicly\\na.When inserting key k, all nodes on the path from the root to the added node\\n(a new leaf) must change, since the need for a new child pointe r propagates up\\nfrom the new node to all of its ancestors.\\nWhen deleting a node, let ybe the node actually removed and ´be the node\\ngiven tothe delete procedure.\\n\\x0fIf´hasatmostonechild,itwillbesplicedout,sothatallances tors of ´will\\nbe changed. (As with insertion, the need for a new child point er propagates\\nup from theremoved node.)\\n\\x0fIf´has two children, then its successor ywill be spliced out and moved\\nto´’s position. Therefore all ancestors of both ´andymust be changed.\\nBecause ´isanancestor of y,wecanjust saythatall ancestors of ymustbe\\nchanged.\\nIn either case, y’s children (if any) are unchanged, because we have assumed\\nthat there is noparent attribute.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 229}),\n",
              " Document(page_content='Solutions forChapter 13: Red-BlackTrees 13-17\\nb.Weassume that wecan call twoprocedures:\\n\\x0fMAKE-NEW-NODE.k/creates a new node whose keyattribute has value k\\nandwithleftandrightattributes NIL,anditreturnsapointertothenewnode.\\n\\x0fCOPY-NODE.x/createsanewnodewhose key,left,andrightattributeshave\\nthe samevalues asthose of node x,and it returns apointer to the new node.\\nHere are two ways to write P ERSISTENT -TREE-INSERT. The ﬁrst is a version\\nof TREE-INSERT, modiﬁed to create new nodes along the path to where the\\nnewnode willgo, andtonot useparent attributes. Itreturns theroot ofthenew\\ntree.\\nPERSISTENT -TREE-INSERT .T; k/\\n´DMAKE-NEW-NODE.k/\\nnew-rootDCOPY-NODE.T:root/\\nyDNIL\\nxDnew-root\\nwhile x¤NIL\\nyDx\\nif´:key< x:key\\nxDCOPY-NODE.x:left/\\ny:leftDx\\nelsexDCOPY-NODE.x:right/\\ny:rightDx\\nify==NIL\\nnew-rootD´\\nelseif ´:key< y:key\\ny:leftD´\\nelsey:rightD´\\nreturnnew-root\\nThe second isa rather elegant recursive procedure. Theinit ial call should have\\nT:rootasits ﬁrst argument. It returns the root of thenew tree.\\nPERSISTENT -TREE-INSERT .r; k/\\nifr==NIL\\nxDMAKE-NEW-NODE.k/\\nelsexDCOPY-NODE.r/\\nifk < r:key\\nx:leftDPERSISTENT -TREE-INSERT .r:left; k/\\nelsex:rightDPERSISTENT -TREE-INSERT .r:right; k/\\nreturn x\\nc.Like TREE-INSERT, PERSISTENT -TREE-INSERTdoes a constant amount of\\nwork at each node along the path from the root to the new node. S ince the\\nlength of the path isat most h, it takes O.h/time.\\nSince it allocates anew node (a constant amount of space) for each ancestor of\\nthe inserted node, it also needs O.h/space.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 230}),\n",
              " Document(page_content='13-18 Solutions for Chapter 13: Red-Black Trees\\nd.If there were parent attributes, then because of the new root , every node of the\\ntreewouldhave tobecopied whenanewnode isinserted. Tosee why,observe\\nthat the children of the root would change to point to the new r oot, then their\\nchildrenwouldchangetopoint tothem,andsoon. Sincethere arennodes, this\\nchange would cause insertion tocreate \\x7f.n/new nodes and totake \\x7f.n/time.\\ne.From parts (a) and (c), we know that insertion into a persiste nt binary search\\ntree of height h, like insertion into an ordinary binary search tree, takes w orst-\\ncasetime O.h/. Ared-blacktreehas hDO.lgn/,soinsertionintoanordinary\\nred-black tree takes O.lgn/time. Weneed to show that if the red-black tree is\\npersistent, insertion can still be done in O.lgn/time. To do this, we will need\\ntoshow twothings:\\n\\x0fHow to still ﬁnd the parent pointers we need in O.1/time without using a\\nparent attribute. We cannot use a parent attribute because a persistent tree\\nwith parent attributes uses \\x7f.n/time for insertion (by part (d)).\\n\\x0fThat the additional node changes made during red-black tree operations (by\\nrotation and recoloring) don’t cause more than O.lgn/additional nodes to\\nchange.\\nEachparent pointer needed during insertion canbefound in O.1/timewithout\\nhaving aparent attribute as follows:\\nTo insert into a red-black tree, we call RB-I NSERT, which in turn calls RB-\\nINSERT-FIXUP. Make the same changes to RB-I NSERTas we made to T REE-\\nINSERTfor persistence. Additionally, as RB-I NSERTwalks down the tree to\\nﬁnd the place to insert the new node, have it build a stack of th e nodes it tra-\\nverses and pass this stack to RB-I NSERT-FIXUP. RB-I NSERT-FIXUPneeds\\nparent pointers to walk back up the same path, and at any given time it needs\\nparent pointersonlytoﬁndtheparent andgrandparent ofthe nodeitisworking\\non. As RB-I NSERT-FIXUPmoves up the stack of parents, it needs only parent\\npointersthatareatknownlocationsaconstant distanceawa yinthestack. Thus,\\nthe parent information can be found in O.1/time, just as if it were stored in a\\nparent attribute.\\nRotation and recoloring change nodes as follows:\\n\\x0fRB-INSERT-FIXUPperforms at most 2 rotations, and each rotation changes\\nthe child pointers in 3 nodes (the node around which we rotate , that node’s\\nparent, andoneofthechildrenofthenodearoundwhichwerot ate). Thus,at\\nmost6nodesaredirectlymodiﬁedbyrotationduringRB-I NSERT-FIXUP. In\\napersistent tree, all ancestors ofachanged node arecopied , so RB-I NSERT-\\nFIXUP’s rotations take O.lgn/time to change nodes due to rotation. (Ac-\\ntually, the changed nodes in this case share a single O.lgn/-length path of\\nancestors.)\\n\\x0fRB-INSERT-FIXUPrecolors some of the inserted node’s ancestors, which\\nare being changed anyway in persistent insertion, and some c hildren of an-\\ncestors (the “uncles” referred to in the algorithm descript ion). There are\\nat most O.lgn/ancestors, hence at most O.lgn/color changes of uncles.\\nRecoloring uncles doesn’t cause any additional node change s due to persis-\\ntence, because the ancestors of the uncles are the same nodes (ancestors of', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 231}),\n",
              " Document(page_content='Solutions forChapter 13: Red-BlackTrees 13-19\\nthe inserted node) that are being changed anyway due to persi stence. Thus,\\nrecoloring does not affect the O.lgn/running time, even withpersistence.\\nWe could show similarly that deletion in a persistent tree al so takes worst-case\\ntimeO.h/.\\n\\x0fWealready sawin part (a) that O.h/nodes change.\\n\\x0fWe could write a persistent RB-D ELETEprocedure that runs in O.h/time,\\nanalogous to the changes we made for persistence in insertio n. But to do so\\nwithoutusingparentpointersweneedtowalkdownthetreeto thenodetobe\\ndeleted, to build up a stack of parents as discussed above for insertion. This\\nis a little tricky if the set’s keys are not distinct, because in order to ﬁnd the\\npath to the node to delete—a particular node with a given key— we have to\\nmakesomechanges tohowwestorethings inthetree, sothat du plicate keys\\ncan be distinguished. The easiest way is to have each key take a second part\\nthat is unique, and to use this second part as a tiebreaker whe n comparing\\nkeys.\\nThentheproblem ofshowing that deletion needs only O.lgn/timeinapersis-\\ntent red-black tree is the sameasfor insertion.\\n\\x0fAs for insertion, we can show that the parents needed by RB-D ELETE-\\nFIXUPcanbefoundin O.1/time(usingthesametechniqueasforinsertion).\\n\\x0fAlso, RB-D ELETE-FIXUPperforms at most 3rotations, which as discussed\\naboveforinsertionrequires O.lgn/timetochangenodesduetopersistence.\\nItalsodoes O.lgn/colorchanges, which(asforinsertion) takeonly O.lgn/\\ntime to change ancestors due to persistence, because the num ber of copied\\nnodes is O.lgn/.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 232}),\n",
              " Document(page_content='Lecture Notes forChapter 14:\\nAugmentingDataStructures\\nChapter 14overview\\nWe’ll be looking at methods for designing algorithms. In some cases, the design\\nwill be intermixed with analysis. In other cases, the analys is is easy, and it’s the\\ndesign that’s harder.\\nAugmentingdatastructures\\n\\x0fIt’s unusual tohave to design anall-new data structure from scratch.\\n\\x0fIt’s more common to take a data structure that you know and sto re additional\\ninformation in it.\\n\\x0fWith the new information, thedata structure can support new operations.\\n\\x0fButyouhavetoﬁgureouthowto correctly maintain thenewinformation with-\\nout loss of efﬁciency .\\nWe’ll look at acouple of situations inwhich weaugment red-b lack trees.\\nDynamicorderstatistics\\nWewant tosupport the usual dynamic-set operations from R-B trees, plus:\\n\\x0fOS-SELECT .x; i/: returnpointer tonodecontaining the ithsmallest keyofthe\\nsubtree rooted at x.\\n\\x0fOS-RANK.T; x/: return the rank of xin the linear order determined by an\\ninorder walkof T.\\nAugment bystoring in each node x:\\nx:sizeD#of nodes in subtree rooted at x :\\n\\x0fIncludes xitself.\\n\\x0fDoes not include leaves (sentinels).\\nDeﬁnefor sentinel T:nil:sizeD0.\\nThen x:sizeDx:left:sizeCx:right:sizeC1.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 233}),\n",
              " Document(page_content='14-2 Lecture Notes for Chapter 14: AugmentingDataStructur es\\nM\\n8B\\nB BB R\\nR RRi=5\\nr=6\\ni=3\\nr=2\\ni=1\\nr=1i=5\\nr=2M\\n8\\nM\\n8P\\n2\\nM\\n8Q\\n1M\\n8C\\n5\\nM\\n8A\\n1M\\n8F\\n3\\nM\\n8D\\n1M\\n8H\\n1\\n[Example above: Ignore colors, but legal coloring shown with “R” and “B” nota -\\ntions. Values of iandrare for theexample below.]\\nNote:OK for keys to not be distinct. Rank is deﬁned with respect to p osition in\\ninorder walk. Soif wechanged D to C, rank of original C is 2, ra nk of D changed\\nto Cis3.\\nOS-SELECT .x; i/\\nrDx:left:sizeC1\\nifi==r\\nreturn x\\nelseif i < r\\nreturnOS-SELECT .x:left; i/\\nelse return OS-SELECT .x:right; i/NULr/\\nInitial call: OS-S ELECT .T:root; i/\\nTry OS-S ELECT .T:root; 5/.[Values shown in ﬁgure above. Returns node whose\\nkey isH.]\\nCorrectness\\nrDrank of xwithin subtree rooted at x.\\n\\x0fIfiDr,then wewant x.\\n\\x0fIfi < r, then ith smallest element is in x’s left subtree, and we want the ith\\nsmallest element in thesubtree.\\n\\x0fIfi > r, then ith smallest element is in x’s right subtree, but subtract off the r\\nelements in x’ssubtree that precede those in x’sright subtree.\\n\\x0fLikethe randomized S ELECTalgorithm.\\nAnalysis\\nEach recursive call goes down one level. Since R-B tree has O.lgn/levels, have\\nO.lgn/calls)O.lgn/time.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 234}),\n",
              " Document(page_content='Lecture Notes for Chapter 14: AugmentingDataStructures 14 -3\\nOS-RANK.T; x/\\nrDx:left:sizeC1\\nyDx\\nwhile y¤T:root\\nify==y:p:right\\nrDrCy:p:left:sizeC1\\nyDy:p\\nreturn r\\nDemo:NodeD.\\nWhydoes this work?\\nLoop invariant: At start of each iteration of whileloop, rDrank of x:key\\nin subtree rooted at y.\\nInitialization: Initially, rDrank of x:keyin subtree rooted at x,and yDx.\\nTermination: Loop terminates when yDT:root)subtree rooted at yis entire\\ntree. Therefore, rDrank of x:keyin entire tree.\\nMaintenance: At end of each iteration, set yDy:p. So, show that if rDrank\\nofx:keyin subtree rooted at yat start of loop body, then rDrank of x:keyin\\nsubtree rooted at y:pat end of loop body.\\nxy\\n[rD# of nodes insubtree rooted at ypreceding xininorder walk]\\nMust add nodes in y’s sibling’s subtree.\\n\\x0fIfyis a left child, its sibling’s subtree follows all nodes in y’s subtree)\\ndon’t change r.\\n\\x0fIfyisarightchild, allnodesin y’ssibling’s subtreeprecede allnodesin y’s\\nsubtree)add size of y’s sibling’s subtree, plus 1for y:p,into r.\\ny y.p.lefty.p\\nAnalysis\\nygoes up one level in each iteration )O.lgn/time.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 235}),\n",
              " Document(page_content='14-4 Lecture Notes for Chapter 14: AugmentingDataStructur es\\nMaintaining subtree sizes\\n\\x0fNeedto maintain sizeattributes during insert and delete operations.\\n\\x0fNeed to maintain them efﬁciently. Otherwise, might have to r ecompute them\\nall, at acost of \\x7f.n/.\\nWill see how tomaintain without increasing O.lgn/timefor insert and delete.\\nInsert\\n\\x0fDuring pass downward, we know that the new node will be a desce ndant of\\neachnodewevisit, andonlyofthesenodes. Therefore, incre mentsizeattribute\\nof each node visited.\\n\\x0fThenthere’s the ﬁxup pass:\\n\\x0fGoes up thetree.\\n\\x0fChanges colors O.lgn/times.\\n\\x0fPerforms\\x142rotations.\\n\\x0fColor changes don’t affect subtree sizes.\\n\\x0fRotations do!\\n\\x0fBut wecan determine new sizes based on oldsizes and sizes of c hildren.\\nLEFT-ROTATE(T, x)x\\ny xyM\\n8C\\n5\\nM\\n8A\\n1M\\n8F\\n3\\nM\\n8D\\n1M\\n8H\\n1M\\n8D\\n1M\\n8C\\n3M\\n8F\\n5\\nM\\n8A\\n1M\\n8H\\n1\\ny:sizeDx:size\\nx:sizeDx:left:sizeCx:right:sizeC1\\n\\x0fSimilar for right rotation.\\n\\x0fTherefore, can update in O.1/time per rotation)O.1/time spent updating\\nsizeattributes during ﬁxup.\\n\\x0fTherefore, O.lgn/toinsert.\\nDelete\\nAlso 2phases:\\n1. Splice out somenode y.\\n2. Fixup.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 236}),\n",
              " Document(page_content='Lecture Notes for Chapter 14: AugmentingDataStructures 14 -5\\nAfter splicing out y, traverse a path y!root, decrementing sizein each node on\\npath. O.lgn/time.\\nDuring ﬁxup, like insertion, only color changes and rotatio ns.\\n\\x0f\\x143rotations)O.1/time spent updating sizeattributes during ﬁxup.\\n\\x0fTherefore, O.lgn/todelete.\\nDone!\\nMethodology foraugmenting a data structure\\n1. Choose anunderlying data structure.\\n2. Determine additional information tomaintain.\\n3. Verify that we can maintain additional information for ex isting data structure\\noperations.\\n4. Develop new operations.\\nDon’t need to dothese steps in strict order! Usually do alitt le of each, inparallel.\\nHowdid wedothem for OStrees?\\n1. R-B tree.\\n2.x:size.\\n3. Showed how tomaintain sizeduring insert and delete.\\n4. Developed OS-S ELECTand OS-R ANK.\\nRed-black trees are particularly amenable to augmentation .\\nTheorem\\nAugment a R-B tree with attribute f, where x:fdepends only on information in\\nx,x:left, and x:right(including x:left:fandx:right:f). Thencan maintain values\\noffin all nodes during insert and delete without affecting O.lgn/performance.\\nProofSince x:fdepends only on xand its children, when we alter information\\ninx,changes propagate only upward (to x:p; x:p:p; x:p:p:p; : : : ;root).\\nHeight = O.lgn/)O.lgn/updates, at O.1/each.\\nInsertion\\nInsert a node as child of existing node. Even if can’t update fon way down, can\\ngoupfrominsertednodetoupdate f. Duringﬁxup,onlychangescomefromcolor\\nchanges (noeffect on f)androtations. Eachrotation affects fof\\x143nodes ( x,y,\\nand parent), and can recompute each in O.1/time. Then, if necessary, propagate\\nchanges up the tree. Therefore, O.lgn/time per rotation. Since \\x142rotations,\\nO.lgn/timetoupdate fduring ﬁxup.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 237}),\n",
              " Document(page_content='14-6 Lecture Notes for Chapter 14: AugmentingDataStructur es\\nDelete\\nSameidea. Aftersplicing out anode, goupfrom theretoupdat ef. Fixuphas\\x143\\nrotations. O.lgn/perrotation)O.lgn/toupdate fduring ﬁxup. (theorem)\\nFor some attributes, canget away with O.1/per rotation. Example: sizeattribute.\\nInterval trees\\nMaintain aset of intervals. For instance, time intervals.\\n457\\n1517\\n21 2319\\n1810\\n11\\n8i=[7,10]low[i] = 7 high[i] = 10\\n[leave on board]\\nOperations\\n\\x0fINTERVAL -INSERT .T; x/:x:intalready ﬁlled in.\\n\\x0fINTERVAL -DELETE .T; x/\\n\\x0fINTERVAL -SEARCH .T; i/: returnpointer toanode xinTsuchthat x:intover-\\nlaps interval i. Any overlapping node in Tis OK. Return pointer to sen-\\ntinelT:nilif no overlapping node in T.\\nInterval ihasi:low,i:high.\\niandjoverlap if and only if\\ni:low\\x14j:highandj:low\\x14i:high.\\n(Go through examples of proper inclusion, overlap without p roper inclusion, no\\noverlap.)\\nAnother way: iandjdon’toverlap if and only if\\ni:low> j:highorj:low> i:high.\\n[leave this on board]\\nRecall the 4-part methodology.\\nForinterval trees\\n1. UseR-Btrees.\\n\\x0fEach node xcontains interval x:int.\\n\\x0fKeyislow endpoint ( x:int:low).\\n\\x0fInorder walkwould list intervals sorted bylow endpoint.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 238}),\n",
              " Document(page_content='Lecture Notes for Chapter 14: AugmentingDataStructures 14 -7\\n2. Each node xcontains\\nx:maxDmaxendpoint value in subtree rooted at x :\\nint\\nmaxM\\n8[17,19]\\n23\\nM\\n8[21,23]\\n23M\\n8[5,11]\\n18\\nM\\n8[15,18]\\n18M\\n8[4,8]\\n8\\nM\\n8[7,10]\\n10\\n[leave on board]\\nx:maxDmax8\\n<\\n:x:int:high;\\nx:left:max;\\nx:right:max\\nCould x:left:max> x:right:max? Sure. Position in tree is determined only by\\nlow endpoints, not high endpoints.\\n3. Maintaining the information.\\n\\x0fThisis easy— x:maxdepends only on:\\n\\x0finformation in x:x:int:high\\n\\x0finformation in x:left:x:left:max\\n\\x0finformation in x:right:x:right:max\\n\\x0fApply the theorem.\\n\\x0fIn fact, can update maxon waydown during insertion, and in O.1/timeper\\nrotation.\\n4. Developing newoperations.\\nINTERVAL -SEARCH .T; i/\\nxDT:root\\nwhile x¤T:nilandidoes not overlap x:int\\nifx:left¤T:nilandx:left:max\\x15i:low\\nxDx:left\\nelsexDx:right\\nreturn x\\nExamples\\nSearch for Œ14; 16\\x8dandŒ12; 14\\x8d.\\nTime\\nO.lgn/.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 239}),\n",
              " Document(page_content='14-8 Lecture Notes for Chapter 14: AugmentingDataStructur es\\nCorrectness\\nKeyidea: need check only 1of node’s 2children.\\nTheorem\\nIf search goes right, then either:\\n\\x0fThere isanoverlap in right subtree, or\\n\\x0fThere isno overlap in either subtree.\\nIf search goes left, then either:\\n\\x0fThere isanoverlap in left subtree, or\\n\\x0fThere isno overlap in either subtree.\\nProofIf search goes right:\\n\\x0fIf there isanoverlap in right subtree, done.\\n\\x0fIf there is no overlap in right, show there is no overlap in lef t. Went right\\nbecause\\n\\x0fx:leftDT:nil)no overlap in left.\\nOR\\n\\x0fx:left:max< i:low)no overlap in left.\\ni\\nx.left.max  = highest endpoint in left\\nIf search goes left:\\n\\x0fIf there isanoverlap in left subtree, done.\\n\\x0fIf there isno overlap in left, show there isno overlap inrigh t.\\n\\x0fWent left because:\\ni:low\\x14x:left:max\\nDj:highfor some jinleft subtree :\\n\\x0fSince there isno overlap in left, iandjdon’t overlap.\\n\\x0fRefer back to: nooverlap if\\ni:low> j:highorj:low> i:high:\\n\\x0fSince i:low\\x14j:high,must have j:low> i:high.\\n\\x0fNow consider anyinterval kinrightsubtree.\\n\\x0fBecause keys are low endpoint,\\nj:low„ƒ‚…\\ninleft\\x14k:low„ƒ‚…\\ninright:\\n\\x0fTherefore, i:high< j:low\\x14k:low.\\n\\x0fTherefore, i:high< k:low.\\n\\x0fTherefore, iandkdonot overlap. (theorem)', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 240}),\n",
              " Document(page_content='SolutionsforChapter 14:\\nAugmentingDataStructures\\nSolutionto Exercise 14.1-5\\nGiven an element xin an n-node order-statistic tree Tand a natural number i, the\\nfollowing procedure retrieves the ithsuccessor of xinthe linear order of T:\\nOS-SUCCESSOR .T; x; i/\\nrDOS-RANK.T; x/\\nsDrCi\\nreturnOS-SELECT .T:root; s/\\nSince OS-R ANKand OS-S ELECTeach take O.lgn/time, so does the procedure\\nOS-SUCCESSOR .\\nSolutionto Exercise 14.1-6\\nWhen inserting node ´, we search down the tree for the proper place for ´. For\\neach node xon this path, add 1tox:rankif´is inserted within x’s left subtree,\\nand leave x:rankunchanged if ´is inserted within x’s right subtree. Similarly\\nwhen deleting, subtract 1from x:rankwhenever the spliced-out node had been in\\nx’sleft subtree.\\nWealso need to handle the rotations that occur during the ﬁxu p procedures for in-\\nsertion and deletion. Consider a left rotation on node x, where the pre-rotation\\nright child of xisy(so that xbecomes y’s left child after the left rotation).\\nWe leave x:rankunchanged, and letting rDy:rankbefore the rotation, we set\\ny:rankDrCx:rank. Right rotations are handled in ananalogous manner.\\nSolutionto Exercise 14.1-7\\nThissolutionisalsopostedpublicly\\nLetAŒ1 : : n\\x8dbethe array of ndistinct numbers.\\nOnewaytocounttheinversionsistoaddup,foreachelement, thenumberoflarger\\nelements that precede it in the array:', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 241}),\n",
              " Document(page_content='14-10 Solutions for Chapter 14: Augmenting DataStructures\\n#of inversionsDnX\\njD1jIn\\x17.j /j;\\nwhereIn\\x17.j /DfiWi < jandAŒi\\x8d > AŒj \\x8dg.\\nNote thatjIn\\x17.j /jis related to AŒj \\x8d’s rank in the subarray AŒ1 : : j \\x8d because the\\nelements in In\\x17.j /are the reason that AŒj \\x8disnot positioned according to its rank.\\nLetr.j /be the rank of AŒj \\x8dinAŒ1 : : j \\x8d. Then jDr.j /CjIn\\x17.j /j, so we can\\ncompute\\njIn\\x17.j /jDj/NULr.j /\\nby inserting AŒ1\\x8d; : : : ; AŒn\\x8d into an order-statistic tree and using OS-R ANKto ﬁnd\\ntherank of each AŒj \\x8dinthetreeimmediately after itisinserted intothetree. (T his\\nOS-RANKvalue is r.j /.)\\nInsertion and OS-R ANKeach take O.lgn/time, and so the total time for nele-\\nments is O.nlgn/.\\nSolution to Exercise14.2-2\\nThis solutionisalsopostedpublicly\\nYes, we can maintain black-heights as attributes in the node s of a red-black tree\\nwithout affecting the asymptotic performance of the red-bl ack tree operations. We\\nappeal toTheorem14.1, because theblack-height ofanodeca nbecomputed from\\nthe information at the node and its two children. Actually, t he black-height can\\nbe computed from just one child’s information: the black-he ight of a node is the\\nblack-height of a red child, or the black height of a black chi ld plus one. The\\nsecond child does not need tobe checked because of property 5 of red-black trees.\\nWithin the RB-I NSERT-FIXUPand RB-D ELETE-FIXUPprocedures are color\\nchanges, each of which potentially cause O.lgn/black-height changes. Let us\\nshow that the color changes of the ﬁxup procedures cause only local black-height\\nchanges and thus are constant-time operations. Assume that the black-height of\\neach node xiskept inthe attribute x:bh.\\nFor RB-I NSERT-FIXUP, there are 3cases toexamine.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 242}),\n",
              " Document(page_content='Solutions forChapter 14: AugmentingDataStructures 14-11\\nCase1: ´’s uncle isred.\\nC\\nD A\\nB α\\nβ γδ ε(a)C\\nD A\\nB α\\nβ γδ ε\\nC\\nD B\\nδ εC\\nD B\\nA\\nα βγ δ ε(b)\\nA\\nα βγk+1k+1\\nk+1k+1 k+1k+2\\nk+1\\nk+1k+1 k+1k+1\\nk+1k+1 k+1k+2k+1 zy\\nzy\\n\\x0fBefore color changes, suppose that all subtrees ˛; ˇ; \\r; ı; \\x0f have the same\\nblack-height kwith a black root, so that nodes A,B,C, and Dhave black-\\nheights of kC1.\\n\\x0fAfter color changes, the only node whose black-height chang ed is node C.\\nToﬁxthat,add ´:p:p:bhD´:p:p:bhC1after line7inRB-I NSERT-FIXUP.\\n\\x0fSince the number of black nodes between ´:p:pand´remains the same,\\nnodes above ´:p:pare not affected by the color change.\\nCase2: ´’s uncle yis black, and ´isaright child.\\nCase3: ´0’suncle yisblack, and ´is aleft child.\\nC\\nA\\nB α\\nβ γδ\\nCase 2B\\nA\\nα βγδ\\nCase 3AB\\nC\\nα β γ δC\\nk+1k+1k+1 k+1\\nk+1\\nk+1k+1 k+1k+1\\nzy\\nzy\\n\\x0fWith subtrees ˛; ˇ; \\r; ı; \\x0f of black-height k, we see that even with color\\nchanges and rotations, the black-heights of nodes A,B, and Cremain the\\nsame( kC1).\\nThus, RB-I NSERT-FIXUPmaintains its original O.lgn/time.\\nFor RB-D ELETE-FIXUP, there are4 cases to examine.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 243}),\n",
              " Document(page_content='14-12 Solutions for Chapter 14: Augmenting DataStructures\\nCase 1: x’ssibling wisred.\\nAB\\nD\\nC E α β\\nγ δ ε ζx w\\nAB\\nCD\\nE\\nx new w\\nα β γ δε ζCase 1\\n\\x0fEven though case 1 changes colors of nodes and does a rotation , black-\\nheights are not changed.\\n\\x0fCase 1 changes the structure of the tree, but waits for cases 2 , 3, and 4 to\\ndeal withthe “extra black” on x.\\nCase 2: x’ssibling wisblack, and both of w’schildren are black.\\nAB\\nD\\nC E α β\\nγ δ ε ζx wc\\nAB\\nD\\nC E α β\\nγ δ ε ζc new xCase 2\\n\\x0fwiscolored red, and x’s“extra” black ismoved upto x:p.\\n\\x0fNow wecan add x:p:bhDx:bhafter line 10 in RB-D ELETE-FIXUP.\\n\\x0fThis is a constant-time update. Then, keep looping to deal wi th the extra\\nblack on x:p.\\nCase 3: x’ssibling wisblack, w’sleft child isred, and w’sright child isblack.\\nAB\\nD\\nC E α β\\nγ δ ε ζx wc\\nAB\\nC\\nD α β γ\\nδ\\nε ζxc\\nnew wCase 3\\nE\\n\\x0fRegardless of the color changes and rotation of this case, th e black-heights\\ndon’t change.\\n\\x0fCase3justsetsupthestructureofthetree,soitcanfallcor rectlyintocase4.\\nCase 4: x’ssibling wisblack, and w’sright child isred.\\nAB\\nD\\nC E α β\\nγ δε ζx wc c\\nα βAB\\nCD\\nE\\nnew x = root[T] γ δ ε ζCase 4\\nc′ c′', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 244}),\n",
              " Document(page_content='Solutions forChapter 14: AugmentingDataStructures 14-13\\n\\x0fNodes A,C, and Ekeep the same subtrees, so their black-heights don’t\\nchange.\\n\\x0fAdd these two constant-time assignments in RB-D ELETE-FIXUPafter\\nline 20:\\nx:p:bhDx:bhC1\\nx:p:p:bhDx:p:bhC1\\n\\x0fTheextra black istaken care of. Loop terminates.\\nThus, RB-D ELETE-FIXUPmaintains its original O.lgn/time.\\nTherefore, weconclude that black-heights ofnodescanbema intained asattributes\\nin red-black trees without affecting the asymptotic perfor mance of red-black tree\\noperations.\\nFor the second part of the question, no, we cannot maintain no de depths without\\naffecting the asymptotic performance of red-black tree ope rations. The depth of a\\nnode depends on the depth of its parent. When the depth of a nod e changes, the\\ndepths of all nodes below it in the tree must be updated. Updat ing the root node\\ncauses n/NUL1other nodes to be updated, which would mean that operations o n the\\ntree that change node depths might not run in O.nlgn/time.\\nSolutionto Exercise 14.3-3\\nAsittravelsdownthetree,I NTERVAL -SEARCHﬁrstcheckswhethercurrentnode x\\noverlapsthequeryinterval iand, ifitdoesnot,goesdowntoeither theleft orright\\nchild. If node xoverlaps i, and some node in the right subtree overlaps i, but\\nno node in the left subtree overlaps i, then because the keys are low endpoints,\\nthis order of checking (ﬁrst x, then one child) will return the overlapping interval\\nwith the minimum low endpoint. On the other hand, if there is a n interval that\\noverlaps iin the left subtree of x, then checking xbefore the left subtree might\\ncause the procedure to return an interval whose low endpoint is not the minimum\\nofthosethatoverlap i. Therefore, ifthereisapossibility thattheleftsubtreem ight\\ncontainaninterval thatoverlaps i,weneedtochecktheleftsubtreeﬁrst. Ifthereis\\nnooverlapintheleftsubtreebutnode xoverlaps i,thenwereturn x. Wecheckthe\\nright subtree under the same conditions as in I NTERVAL -SEARCH: the left subtree\\ncannot contain an interval that overlaps i, and node xdoes not overlap i,either.\\nBecausewemightsearchtheleftsubtreeﬁrst,itiseasierto writethepseudocodeto\\nusearecursiveprocedure M IN-INTERVAL -SEARCH-FROM.T; x; i/,whichreturns\\nthe node overlapping iwith the minimum low endpoint in the subtree rooted at x,\\norT:nilif there is nosuch node.\\nMIN-INTERVAL -SEARCH .T; i/\\nreturnMIN-INTERVAL -SEARCH-FROM.T; T:root; i/', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 245}),\n",
              " Document(page_content='14-14 Solutions for Chapter 14: Augmenting DataStructures\\nMIN-INTERVAL -SEARCH-FROM.T; x; i/\\nifx:left¤T:nilandx:left:max\\x15i:low\\nyDMIN-INTERVAL -SEARCH-FROM.T; x:left; i/\\nify¤T:nil\\nreturn y\\nelseif ioverlaps x:int\\nreturn x\\nelse return T:nil\\nelseif ioverlaps x:int\\nreturn x\\nelse return MIN-INTERVAL -SEARCH-FROM.T; x:right; i/\\nThe call M IN-INTERVAL -SEARCH .T; i/takes O.lgn/time, since each recursive\\ncall of M IN-INTERVAL -SEARCH-FROMgoes one node lower in the tree, and the\\nheight of the tree is O.lgn/.\\nSolution to Exercise14.3-6\\n1. Underlying data structure:\\nA red-black tree in which the numbers in the set are stored sim ply as the keys\\nof the nodes.\\nSEARCHisthenjusttheordinary T REE-SEARCHforbinarysearchtrees,which\\nruns in O.lgn/timeon red-black trees.\\n2. Additional information:\\nThered-black tree isaugmented by thefollowing attributes ineach node x:\\n\\x0fx:min-gapcontains the minimum gap in the subtree rooted at x. It has the\\nmagnitude of thedifference of thetwoclosest numbers inthe subtree rooted\\natx. Ifxis aleaf (its children are all T:nil), letx:min-gapD1.\\n\\x0fx:min-\\x17alcontains the minimum value (key) in thesubtree rooted at x.\\n\\x0fx:max-\\x17alcontains themaximum value (key) inthe subtree rooted at x.\\n3. Maintaining the information:\\nThe three attributes added to the tree can each be computed fr om information\\nin the node and its children. Hence by Theorem 14.1, they can b e maintained\\nduring insertion and deletion without affecting the O.lgn/running time:\\nx:min-\\x17alD(\\nx:left:min-\\x17alif there isaleft subtree ;\\nx:key otherwise ;\\nx:max-\\x17alD(\\nx:right:max-\\x17alif there isa right subtree ;\\nx:key otherwise ;\\nx:min-gapDmin„\\nx:left:min-gap (1if no left subtree) ;\\nx:right:min-gap (1if no right subtree) ;\\nx:key/NULx:left:max-\\x17al(1if no left subtree) ;\\nx:right:min-\\x17al/NULx:key(1if no right subtree) :', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 246}),\n",
              " Document(page_content='Solutions forChapter 14: AugmentingDataStructures 14-15\\nIn fact, the reason for deﬁning the min-\\x17alandmax-\\x17alattributes is to make it\\npossible tocompute min-gapfrom information at the node and its children.\\n4. Newoperation:\\nMIN-GAPsimply returns the min-gapstored at the tree root. Thus, its running\\ntime is O.1/.\\nNote that in addition (not asked for in the exercise), it is po ssible to ﬁnd the\\ntwoclosest numbers in O.lgn/time. Startingfrom theroot, lookfor wherethe\\nminimum gap (the onestored at the root) came from. At each nod ex,simulate\\nthe computation of x:min-gapto ﬁgure out where x:min-gapcame from. If it\\ncame from a subtree’s min-gapattribute, continue the search in that subtree. If\\nit came from a computation with x’s key, then xand that other number are the\\nclosest numbers.\\nSolutionto Exercise 14.3-7\\nThissolutionisalsopostedpublicly\\nGeneral idea: Move a sweep line from left to right, while main taining the set of\\nrectangles currently intersected by the line in an interval tree. The interval tree\\nwill organize all rectangles whose xinterval includes the current position of the\\nsweep line, and it will be based on the yintervals of the rectangles, so that any\\noverlapping yintervals in the interval tree correspond to overlapping re ctangles.\\nDetails:\\n1. Sort the rectangles by their x-coordinates. (Actually, each rectangle must ap-\\npeartwiceinthesortedlist—once foritsleft x-coordinateandonceforitsright\\nx-coordinate.)\\n2. Scan the sorted list (from lowest tohighest x-coordinate).\\n\\x0fWhen an x-coordinate of aleft edge is found, check whether the rectan gle’s\\ny-coordinate interval overlapsaninterval inthetree,andi nsert therectangle\\n(keyed onits y-coordinate interval) into the tree.\\n\\x0fWhenan x-coordinate of aright edge isfound, delete therectangle fr om the\\ninterval tree.\\nTheinterval treealwayscontainsthesetof“open”rectangl es intersected bythe\\nsweepline. Ifanoverlapiseverfoundintheinterval tree,t hereareoverlapping\\nrectangles.\\nTime: O.nlgn/\\n\\x0fO.nlgn/to sort the rectangles (wecan use merge sort or heap sort).\\n\\x0fO.nlgn/for interval-tree operations (insert, delete, and check fo r overlap).\\nSolutionto Problem 14-1\\na.Assume for the purpose of contradiction that there is no poin t of maximum\\noverlap in an endpoint of a segment. The maximum overlap poin tpis in the', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 247}),\n",
              " Document(page_content='14-16 Solutions for Chapter 14: Augmenting DataStructures\\ninterior of msegments. Actually, pisintheinterior oftheintersection ofthose\\nmsegments. Now look at one of the endpoints p0of the intersection of the m\\nsegments. Point p0hasthesameoverlapas pbecause itisinthesameintersec-\\ntionof msegments,andso p0isalsoapointofmaximumoverlap. Moreover, p0\\nisintheendpoint ofasegment(otherwisetheintersection w ouldnotendthere),\\nwhichcontradicts our assumption that thereisnopoint of ma ximum overlap in\\nan endpoint of a segment. Thus, there is always a point of maxi mum overlap\\nwhich isan endpoint of one of the segments.\\nb.Keep a balanced binary search tree of the endpoints. That is, to insert an in-\\nterval, we insert its endpoints separately. With each left e ndpoint e, associate\\na value p.e/DC1(increasing the overlap by 1). With each right endpoint e\\nassociateavalue p.e/D/NUL1(decreasingtheoverlapby1). Whenmultipleend-\\npoints have the same value, insert all the left endpoints wit h that value before\\ninserting anyof the right endpoints with that value.\\nHere’s some intuition. Let e1,e2, ..., enbe the sorted sequence of endpoints\\ncorresponding to our intervals. Let s.i; j /denote the sum p.e i/Cp.e iC1/C\\n\\x01\\x01\\x01C p.e j/for1\\x14i\\x14j\\x14n. Wewish toﬁndan imaximizing s.1; i/.\\nFor each node xin the tree, let l.x/andr.x/be the indices in the sorted order\\noftheleftmost andrightmost endpoints, respectively, int hesubtreerooted at x.\\nThenthe subtree rooted at xcontains theendpoints el.x/; el.x/C1; : : : ; e r.x/.\\nEach node xstores three new attributes. We store x:\\x17Ds.l.x/; r.x// , the\\nsum of the values of all nodes in the subtree rooted at x. We also store\\nx:m, the maximum value obtained by the expression s.l.x/; i/ for any iin\\nfl.x/; l.x/C1; : : : ; r.x/g. Finally, we store x:oas the value of ifor which\\nx:machievesitsmaximum. Forthesentinel, wedeﬁne T:nil:\\x17DT:nil:mD0.\\nWe can compute these attributes in a bottom-up fashion to sat isfy the require-\\nments of Theorem 14.1:\\nx:\\x17Dx:left:\\x17Cp.x/Cx:right:\\x17 ;\\nx:mDmax\\x80\\nx:left:m (maxis in x’sleft subtree) ;\\nx:left:\\x17Cp.x/ (maxis at x);\\nx:left:\\x17Cp.x/Cx:right:m(maxis in x’sright subtree) :\\nComputing x:\\x17is straightforward. Computing x:mbears further explanation.\\nRecall that it is the maximum value of the sum of the pvalues for the nodes\\nin the subtree rooted at x, starting at the node for el.x/, which is the leftmost\\nendpoint in x’s subtree, and ending at any node for eiinx’s subtree. The\\nendpoint eithat maximizes this sum—let’s call it ei\\x03—corresponds to either a\\nnodein x’sleftsubtree, xitself,oranodein x’srightsubtree. If ei\\x03corresponds\\ntoanode in x’sleft subtree, then x:left:mrepresents asum starting at thenode\\nforel.x/and ending at a node in x’s left subtree, and hence x:mDx:left:m.\\nIfei\\x03corresponds to xitself, then x:mrepresents the sum of all pvalues in\\nx’s left subtree, plus p.x/, so that x:mDx:left:\\x17Cp.x/. Finally, if ei\\x03\\ncorresponds toanodein x’sright subtree, then x:mrepresents thesumof all p\\nvaluesin x’sleftsubtree, plus p.x/,plusthesumofsomesubset of pvaluesin\\nx’s right subtree. Moreover, the values taken from x’s right subtree must start\\nfrom the leftmost endpoint stored in the right subtree. To ma ximize this sum,', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 248}),\n",
              " Document(page_content='Solutions forChapter 14: AugmentingDataStructures 14-17\\nweneedtomaximizethesumfromtheright subtree, andthatva lueisprecisely\\nx:right:m. Hence, inthis case, x:mDx:left:\\x17Cp.x/Cx:right:m.\\nOnceweunderstand howtocompute x:m,it isstraightforward tocompute x:o\\nfrom the information in xand its two children. Thus, we can implement the\\noperations asfollows:\\n\\x0fINTERVAL -INSERT: insert twonodes, one for each endpoint of the interval.\\n\\x0fFIND-POM: return the interval whose endpoint isrepresented by T:root:o.\\n(Notethat because wearebuilding abinary search treeof all theendpoints and\\nthen determining T:root:o, wehave noneedtodelete anynodes from thetree.)\\nBecause of how we have deﬁned the new attributes, Theorem 14. 1 says that\\neachoperation runsin O.lgn/time. Infact, F IND-POM takesonly O.1/time.\\nSolutionto Problem 14-2\\na.Weuseacircular listinwhicheachelementhastwoattribute s,keyandnext. At\\nthe beginning, we initialize the list to contain the keys 1; 2; : : : ; n in that order.\\nThis initialization takes O.n/time, since there is only a constant amount of\\nwork per element (i.e., setting its keyand itsnextattributes). We make the list\\ncircular bylettingthe nextattributeofthelastelementpointtotheﬁrstelement.\\nWe then start scanning the list from the beginning. We output and then delete\\nevery mth element, until the list becomes empty. The output sequenc e is the\\n.n; m/-Josephus permutation. This process takes O.m/time per element, for a\\ntotal time of O.mn/. Since mis a constant, we get O.mn/DO.n/time, as\\nrequired.\\nb.We can use an order-statistic tree, straight out of Section 1 4.1. Why? Suppose\\nthat weare at aparticular spot in the permutation, and let’s say that it’s the jth\\nlargestremainingperson. Supposethatthereare k\\x14npeopleremaining. Then\\nwe will remove person j, decrement kto reﬂect having removed this person,\\nandthengoontothe .jCm/NUL1/thlargestremainingperson(subtract1because\\nwehavejustremovedthe jthlargest). Butthatassumesthat jCm\\x14k. Ifnot,\\nthen weuse alittle modular arithmetic, as shown below.\\nIn detail, we use an order-statistic tree T, and we call the procedures OS-\\nINSERT, OS-D ELETE, OS-R ANK, and OS-S ELECT:', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 249}),\n",
              " Document(page_content='14-18 Solutions for Chapter 14: Augmenting DataStructures\\nJOSEPHUS .n; m/\\ninitialize Ttobe empty\\nforjD1ton\\ncreate anode xwithx:key==j\\nOS-INSERT .T; x/\\nkDn\\njDm\\nwhile k > 2\\nxDOS-SELECT .T:root; j /\\nprint x:key\\nOS-DELETE .T; x/\\nkDk/NUL1\\njD..jCm/NUL2/modk/C1\\nprint OS-S ELECT .T:root; 1/:key\\nTheabove procedure is easier to understand. Here’s astream lined version:\\nJOSEPHUS .n; m/\\ninitialize Ttobe empty\\nforjD1ton\\ncreate anode xwithx:key==j\\nOS-INSERT .T; x/\\njD1\\nforkDndownto 1\\njD..jCm/NUL2/modk/C1\\nxDOS-SELECT .T:root; j /\\nprint x:key\\nOS-DELETE .T; x/\\nEither way, it takes O.nlgn/time to build up the order-statistic tree T, and\\nthen we make O.n/calls to the order-statistic-tree procedures, each of whic h\\ntakes O.lgn/time. Thus, the total time is O.nlgn/.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 250}),\n",
              " Document(page_content='Lecture Notes forChapter 15:\\nDynamicProgramming\\nDynamicProgramming\\n\\x0fNot aspeciﬁc algorithm, but atechnique (like divide-and-c onquer).\\n\\x0fDeveloped back inthe day when“programming” meant “tabular method” (like\\nlinear programming). Doesn’t really refer tocomputer prog ramming.\\n\\x0fUsed for optimization problems:\\n\\x0fFindasolution with theoptimal value.\\n\\x0fMinimization or maximization. (We’ll see both.)\\nFour-step method\\n1. Characterize the structure of an optimal solution.\\n2. Recursively deﬁne thevalue of an optimal solution.\\n3. Compute the value of an optimal solution, typically in abo ttom-up fashion.\\n4. Construct anoptimal solution from computed information .\\nRodcutting\\n[Newin thethird edition of thebook.]\\nHow to cut steel rods into pieces in order to maximize the reve nue you can get?\\nEachcut isfree. Rodlengths are always an integral number of inches.\\nInput:Alength nand table of prices pi,foriD1; 2; : : : ; n .\\nOutput: Themaximumrevenueobtainableforrodswhoselengthssumto n,com-\\nputed asthe sum of the prices for the individual rods.\\nIfpnis large enough, an optimal solution might require no cuts, i .e., just leave the\\nrod as ninches long.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 251}),\n",
              " Document(page_content='15-2 Lecture Notes for Chapter 15: Dynamic Programming\\nExample: [Using theﬁrst 8values from the example in thebook.]\\nlength i1 2 3 4 5 6 7 8\\nprice pi1 5 8 9 10 17 17 20\\nCan cut up a rod in 2n/NUL1different ways, because can choose to cut or not cut after\\neach of the ﬁrst n/NUL1inches.\\nHere areall 8ways tocut arod of length 4,with the costs from theexample:\\n9 1 8\\n1 1 1 15 5 1 8\\n511 5 1 1 5 1 1\\nThe best way is to cut it into two 2-inch pieces, getting a revenue of p2Cp2D\\n5C5D10.\\nLetribe the maximum revenue for a rod of length i. Can express a solution as a\\nsum of individual rod lengths.\\nCandetermine optimal revenues rifor the example, by inspection:\\ni rioptimal solution\\n11 1(no cuts)\\n25 2(no cuts)\\n38 3(no cuts)\\n410 2C2\\n513 2C3\\n617 6(no cuts)\\n718 1C6or2C2C3\\n822 2C6\\nCandetermine optimal revenue rnbytaking the maximum of\\n\\x0fpn: the price weget by not making acut,\\n\\x0fr1Crn/NUL1: themaximumrevenuefromarodof 1inchandarodof n/NUL1inches,\\n\\x0fr2Crn/NUL2: the maximum revenue from a rod of 2inches and a rod of n/NUL2\\ninches, ...\\n\\x0frn/NUL1Cr1.\\nThat is,\\nrnDmax.pn; r1Crn/NUL1; r2Crn/NUL2; : : : ; r n/NUL1Cr1/ :\\nOptimal substructure: Tosolvetheoriginal problem ofsize n,solvesubproblems\\non smaller sizes. After making a cut, we have two subproblems . The optimal\\nsolution totheoriginal problem incorporates optimal solu tionstothesubproblems.\\nWemaysolve the subproblems independently.\\nExample: FornD7, one of the optimal solutions makes a cut at 3inches, giving\\ntwosubproblems, oflengths 3and4. Weneedtosolvebothofthemoptimally. The\\noptimalsolutionfortheproblemoflength 4,cuttinginto2pieces, eachoflength 2,\\nis used inthe optimal solution tothe original problem withl ength 7.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 252}),\n",
              " Document(page_content='Lecture Notes for Chapter 15: Dynamic Programming 15-3\\nA simpler way to decompose the problem: Every optimal solution has a leftmost\\ncut. In other words, there’s some cut that gives a ﬁrst piece o f length icut off the\\nleft end, and aremaining piece of length n/NULionthe right.\\n\\x0fNeed todivide only the remainder, not the ﬁrst piece.\\n\\x0fLeaves only one subproblem tosolve, rather than twosubprob lems.\\n\\x0fSay that the solution with no cuts has ﬁrst piece size iDnwith revenue pn,\\nand remainder size 0withrevenue r0D0.\\n\\x0fGivesa simpler version of the equation for rn:\\nrnDmax\\n1\\x14i\\x14n.piCrn/NULi/ :\\nRecursive top-down solution\\nDirect implementation of thesimpler equation for rn.\\nThecall C UT-ROD.p; n/returns the optimal revenue rn:\\nCUT-ROD.p; n/\\nifn==0\\nreturn0\\nqD/NUL1\\nforiD1ton\\nqDmax.q; pŒi\\x8dCCUT-ROD.p; n/NULi//\\nreturn q\\nThis procedure works, but it is terribly inefﬁcient . If you code it up and run it, it\\ncould take more than an hour for nD40. Running time almost doubles each time\\nnincreases by 1.\\nWhysoinefﬁcient?: CUT-RODcalls itself repeatedly, evenonsubproblems it has\\nalready solved. Here’s a tree of recursive calls for nD4. Inside each node is the\\nvalue of nfor the call represented by the node:\\n3\\n10\\n0\\n00 12 0\\n012\\n0104\\nLots of repeated subproblems. Solve the subproblem for size 2twice, for size 1\\nfour times, and for size 0eight times.\\nExponential growth: LetT .n/equal thenumber of calls to C UT-RODwithsecond\\nparameter equal to n. Then', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 253}),\n",
              " Document(page_content='15-4 Lecture Notes for Chapter 15: Dynamic Programming\\nT .n/D\\x801 ifnD0 ;\\n1Cn/NUL1X\\njD0T .j /ifn\\x151 :\\nSummation counts calls where second parameter is jDn/NULi.\\nSolution to recurrence is T .n/D2n.\\nDynamic-programming solution\\nInstead of solving the same subproblems repeatedly, arrang e to solve each sub-\\nproblem just once.\\nSave the solution to a subproblem in a table, and refer back to the table whenever\\nwerevisit the subproblem.\\n“Store, don’t recompute” )time-memory trade-off.\\nCanturn anexponential-time solution into apolynomial-ti me solution.\\nTwobasic approaches: top-down with memoization, and botto m-up.\\nTop-down withmemoization\\nSolve recursively, but store each result in atable.\\nTo ﬁnd the solution to a subproblem, ﬁrst look in the table. If the answer is there,\\nuse it. Otherwise, compute the solution to the subproblem an d then store the solu-\\ntion in the table for future use.\\nMemoizing is remembering what wehave computed previously.\\nMemoizedversion oftherecursive solution, storing thesol ution tothesubproblem\\nof length iinarray entry rŒi\\x8d:\\nMEMOIZED -CUT-ROD.p; n/\\nletrŒ0 : : n\\x8dbe anew array\\nforiD0ton\\nrŒi\\x8dD/NUL1\\nreturnMEMOIZED -CUT-ROD-AUX.p; n; r/\\nMEMOIZED -CUT-ROD-AUX.p; n; r/\\nifrŒn\\x8d\\x150\\nreturn rŒn\\x8d\\nifn==0\\nqD0\\nelseqD/NUL1\\nforiD1ton\\nqDmax.q; pŒi\\x8dCMEMOIZED -CUT-ROD-AUX.p; n/NULi; r//\\nrŒn\\x8dDq\\nreturn q', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 254}),\n",
              " Document(page_content='Lecture Notes for Chapter 15: Dynamic Programming 15-5\\nBottom-up\\nSort the subproblems by size and solve the smaller ones ﬁrst. That way, when\\nsolving asubproblem, have already solved the smaller subpr oblems weneed.\\nBOTTOM-UP-CUT-ROD.p; n/\\nletrŒ0 : : n\\x8dbeanew array\\nrŒ0\\x8dD0\\nforjD1ton\\nqD/NUL1\\nforiD1toj\\nqDmax.q; pŒi\\x8dCrŒj/NULi\\x8d/\\nrŒj \\x8dDq\\nreturn rŒn\\x8d\\nRunningtime\\nBoththe top-down and bottom-up versions run in ‚.n2/time.\\n\\x0fBottom-up: Doubly nested loops. Number of iterations of inn erforloop forms\\nan arithmetic series.\\n\\x0fTop-down: M EMOIZED -CUT-RODsolves each subproblem just once, and it\\nsolves subproblems for sizes 0; 1; : : : ; n . To solve a subproblem of size n, the\\nforloop iterates ntimes)over all recursive calls, total number of iterations\\nforms an arithmetic series. [Actually using aggregate analysis, which Chap-\\nter 17 covers.]\\nSubproblemgraphs\\nHowtounderstand the subproblems involved and how they depe nd on each other.\\nDirected graph:\\n\\x0fOnevertex for each distinct subproblem.\\n\\x0fHas a directed edge .x; y/if computing an optimal solution to subproblem x\\ndirectlyrequires knowing anoptimal solution to subproblem y.\\nExample: Forrod-cutting problem with nD4:\\n3\\n0124', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 255}),\n",
              " Document(page_content='15-6 Lecture Notes for Chapter 15: Dynamic Programming\\nCan think of the subproblem graph as a collapsed version of th e tree of recursive\\ncalls, where all nodes for the same subproblem are collapsed into a single vertex,\\nand all edges gofrom parent tochild.\\nSubproblem graph can help determine running time. Because w e solve each sub-\\nproblem just once, running timeis sum of times needed tosolv e each subproblem.\\n\\x0fTime to compute solution to a subproblem is typically linear in the out-degree\\n(number of outgoing edges) of its vertex.\\n\\x0fNumber of subproblems equals number of vertices.\\nWhentheseconditionshold,runningtimeislinearinnumber ofverticesandedges.\\nReconstructing asolution\\nSofar,havefocused oncomputing the valueofanoptimal solution, rather thanthe\\nchoicesthat produced an optimal solution.\\nExtend the bottom-up approach to record not just optimal val ues, but optimal\\nchoices. Save the optimal choices in a separate table. Then u se a separate pro-\\ncedure to print the optimal choices.\\nEXTENDED -BOTTOM-UP-CUT-ROD.p; n/\\nletrŒ0 : : n\\x8dandsŒ0 : : n\\x8dbe new arrays\\nrŒ0\\x8dD0\\nforjD1ton\\nqD/NUL1\\nforiD1toj\\nifq < pŒi\\x8dCrŒj/NULi\\x8d\\nqDpŒi\\x8dCrŒj/NULi\\x8d\\nsŒj \\x8dDi\\nrŒj \\x8dDq\\nreturn rands\\nSaves the ﬁrst cut madein anoptimal solution for aproblem of sizeiinsŒi\\x8d.\\nToprint out the cuts made inan optimal solution:\\nPRINT-CUT-ROD-SOLUTION .p; n/\\n.r; s/DEXTENDED -BOTTOM-UP-CUT-ROD.p; n/\\nwhile n > 0\\nprint sŒn\\x8d\\nnDn/NULsŒn\\x8d\\nExample: For the example, E XTENDED -BOTTOM-UP-CUT-RODreturns\\ni0 1 2 3 4 5 6 7 8\\nrŒi\\x8d 0 1 5 8 10 13 17 18 22\\nsŒi\\x8d 0 1 2 3 2 2 6 1 2\\nA call to P RINT-CUT-ROD-SOLUTION .p; 8/calls E XTENDED -BOTTOM-UP-\\nCUT-RODto compute the above randstables. Then it prints 2, sets nto6,\\nprints 6, and ﬁnishes (because nbecomes 0).', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 256}),\n",
              " Document(page_content=\"Lecture Notes for Chapter 15: Dynamic Programming 15-7\\nLongestcommonsubsequence\\nProblem: Given 2sequences, XDhx1; : : : ; x miandYDhy1; : : : ; y ni. Find\\na subsequence common to both whose length is longest. A subse quence doesn’t\\nhave tobe consecutive, but it has to bein order.\\n[To come up with examples of longest common subsequences, se arch the dictio-\\nnary for all words that contain the word you are looking for as a subsequence. On\\na UNIX system, for example, to ﬁnd all the words with pineas a subsequence,\\nuse the commandgrep'.*p.*i.*n.*e.*'dict,wheredictis your lo-\\ncal dictionary. Thencheck ifthat wordisactually alongest commonsubsequence.\\nWorking Ccode for ﬁnding alongest commonsubsequence of two strings appears\\nat http://www.cs.dartmouth.edu/˜thc/code/lcs.c]\\nExamples\\n[Theexamples are of different types of trees.]\\nh e r o i c a l l ys p r i n g t i m e\\np i o n e e rh o r s e b a c k\\ns n o w f l a k e\\nm a e l s t r o m\\nb e c a l m s c h o l a r l y\\nBrute-force algorithm:\\nForevery subsequence of X,check whether it’s asubsequence of Y.\\nTime: ‚.n2m/.\\n\\x0f2msubsequences of Xto check.\\n\\x0fEach subsequence takes ‚.n/time to check: scan Yfor ﬁrst letter, from there\\nscan for second, and so on.\\nOptimalsubstructure\\nNotation:\\nXiDpreﬁxhx1; : : : ; x ii\\nYiDpreﬁxhy1; : : : ; y ii\\nTheorem\\nLetZDh´1; : : : ; ´ kibeany LCSof XandY.\\n1. If xmDyn,then ´kDxmDynandZk/NUL1is anLCSof Xm/NUL1andYn/NUL1.\\n2. If xm¤yn,then ´k¤xm)Zisan LCSof Xm/NUL1andY.\\n3. If xm¤yn,then ´k¤yn)Zisan LCSof XandYn/NUL1.\", metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 257}),\n",
              " Document(page_content='15-8 Lecture Notes for Chapter 15: Dynamic Programming\\nProof\\n1. First show that ´kDxmDyn. Suppose not. Then make a subsequence\\nZ0D h´1; : : : ; ´ k; xmi. It’s a common subsequence of XandYand has\\nlength kC1)Z0is a longer common subsequence than Z)contradicts Z\\nbeing anLCS.\\nNow show Zk/NUL1is an LCS of Xm/NUL1andYn/NUL1. Clearly, it’s a common subse-\\nquence. Nowsupposethereexistsacommonsubsequence WofXm/NUL1andYn/NUL1\\nthat’s longer than Zk/NUL1)length of W\\x15k. Make subsequence W0by ap-\\npending xmtoW.W0iscommonsubsequence of XandY,haslength\\x15kC1\\n)contradicts Zbeing an LCS.\\n2. If ´k¤xm, then Zis a common subsequence of Xm/NUL1andY. Suppose there\\nexistsasubsequence WofXm/NUL1andYwithlength > k. Then Wisacommon\\nsubsequence of XandY)contradicts Zbeing an LCS.\\n3. Symmetric to 2. (theorem)\\nTherefore, an LCSof twosequences contains asapreﬁx an LCSo f preﬁxes of the\\nsequences.\\nRecursive formulation\\nDeﬁne cŒi; j \\x8dDlength of LCSof XiandYj. Wewant cŒm; n\\x8d.\\ncŒi; j \\x8dD\\x80\\n0 ifiD0orjD0 ;\\ncŒi/NUL1; j/NUL1\\x8dC1 ifi; j > 0andxiDyj;\\nmax.cŒi/NUL1; j \\x8d; cŒi; j/NUL1\\x8d/ifi; j > 0andxi¤yj:\\nAgain, wecould writea recursive algorithm based onthis for mulation.\\nTrywithbozo, bat.\\n0,3 1,2 1,2 2,1 1,2 2,1 2,1 3,01,3 2,2 2,2 3,1 2,2 3,1 3,1 4,02,3 3,2 3,2 4,13,3 3,34,3\\n\\x0fLotsof repeated subproblems.\\n\\x0fInstead of recomputing, store ina table.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 258}),\n",
              " Document(page_content='Lecture Notes for Chapter 15: Dynamic Programming 15-9\\nComputelength of optimal solution\\nLCS-L ENGTH .X; Y; m; n/\\nletbŒ1 : : m; 1 : : n\\x8d andcŒ0 : : m; o : : n\\x8d be new tables\\nforiD1tom\\ncŒi; 0\\x8dD0\\nforjD0ton\\ncŒ0; j \\x8dD0\\nforiD1tom\\nforjD1ton\\nifxi==yj\\ncŒi; j \\x8dDcŒi/NUL1; j/NUL1\\x8dC1\\nbŒi; j \\x8dD“-”\\nelse if cŒi/NUL1; j \\x8d\\x15cŒi; j/NUL1\\x8d\\ncŒi; j \\x8dDcŒi/NUL1; j \\x8d\\nbŒi; j \\x8dD“\"”\\nelsecŒi; j \\x8dDcŒi; j/NUL1\\x8d\\nbŒi; j \\x8dD“ ”\\nreturn candb\\nPRINT-LCS .b; X; i; j /\\nifi==0orjD0\\nreturn\\nifbŒi; j \\x8d==“-”\\nPRINT-LCS .b; X; i/NUL1; j/NUL1/\\nprint xi\\nelseif bŒi; j \\x8d==“\"”\\nPRINT-LCS .b; X; i/NUL1; j /\\nelsePRINT-LCS .b; X; i; j/NUL1/\\n\\x0fInitial call is P RINT-LCS .b; X; m; n/ .\\n\\x0fbŒi; j \\x8dpoints to table entry whose subproblem we used in solving LCS ofXi\\nandYj.\\n\\x0fWhen bŒi; j \\x8dD-, we have extended LCS by one character. So longest com-\\nmon subsequenceDentries with-in them.\\nDemonstration\\nWhat do spanking andamputation have in common? [Showonly cŒi; j \\x8d.]', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 259}),\n",
              " Document(page_content='15-10 Lecture Notes for Chapter 15: Dynamic Programming\\n433221111104332211111033322111110322221111103222211111022222111110111111110000000000000000000000000\\ngniknapsnoitatupma\\nn i a p\\nAnswer: pain.\\nTime\\n‚.mn/\\nOptimal binary search trees\\n[Added inthe second edition.]\\n\\x0fGiven sequence KDhk1; k2; : : : ; k niofndistinct keys, sorted ( k1< k 2<\\n\\x01\\x01\\x01< k n).\\n\\x0fWant to build a binary search tree from the keys.\\n\\x0fForki, have probability pithat asearch isfor ki.\\n\\x0fWant BSTwithminimum expected search cost.\\n\\x0fActual costD#of items examined.\\nForkey ki, costDdepthT.ki/C1, wheredepthT.ki/Ddepth of kiinBST T.\\nEŒsearch cost in T \\x8d\\nDnX\\niD1.depthT.ki/C1/\\x01pi\\nDnX\\niD1depthT.ki/\\x01piCnX\\niD1pi\\nD1CnX\\niD1depthT.ki/\\x01pi(since probabilities sum to 1) ( \\x03)\\n[Keep equation (*) on board.]', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 260}),\n",
              " Document(page_content='Lecture Notes for Chapter 15: Dynamic Programming 15-11\\n[Similar to optimal BSTproblem in the book, but simpliﬁed he re: we assume that\\nall searches are successful. Book has probabilities of sear ches between keys in\\ntree.]\\nExample\\ni1 2 3 4 5\\npi.25 .2 .05 .2 .3\\nk2\\nk1 k4\\nk3 k5\\nidepthT.ki/depthT.ki/\\x01pi\\n1 1 .25\\n2 0 0\\n3 2 .1\\n4 1 .2\\n5 2 .6\\n1.15\\nTherefore, E Œsearch cost \\x8dD2:15.\\nk2\\nk1 k5\\nk4\\nk3\\nidepthT.ki/depthT.ki/\\x01pi\\n1 1 .25\\n2 0 0\\n3 3 .15\\n4 2 .4\\n5 1 .3\\n1.10\\nTherefore, E Œsearch cost \\x8dD2:10, which turns out tobe optimal.\\nObservations\\n\\x0fOptimal BSTmight not have smallest height.\\n\\x0fOptimal BSTmight not have highest-probability key at root.\\nBuild byexhaustive checking?', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 261}),\n",
              " Document(page_content=\"15-12 Lecture Notes for Chapter 15: Dynamic Programming\\n\\x0fConstruct each n-node BST.\\n\\x0fForeach, put in keys.\\n\\x0fThencompute expected search cost.\\n\\x0fBut there are \\x7f.4n=n3=2/different BSTswith nnodes.\\nOptimalsubstructure\\nConsider any subtree of a BST. It contains keys in a contiguou s range ki; : : : ; k j\\nfor some 1\\x14i\\x14j\\x14n.\\nT\\nT'\\nIfTis an optimal BST and Tcontains subtree T0with keys ki; : : : ; k j, then T0\\nmust bean optimal BSTfor keys ki; : : : ; k j.\\nProofCut and paste.\\nUse optimal substructure to construct an optimal solution t o the problem from op-\\ntimal solutions tosubproblems:\\n\\x0fGivenkeys ki; : : : ; k j(the problem).\\n\\x0fOneof them, kr,where i\\x14r\\x14j,must bethe root.\\n\\x0fLeft subtree of krcontains ki; : : : ; k r/NUL1.\\n\\x0fRight subtree of krcontains krC1; : : : ; k j.\\nkr\\nki kr–1kr+1 kj\\n\\x0fIf\\n\\x0fweexamine all candidate roots kr,fori\\x14r\\x14j,and\\n\\x0fwe determine all optimal BSTs containing ki; : : : ; k r/NUL1and containing\\nkrC1; : : : ; k j,\\nthen we’re guaranteed toﬁndan optimal BSTfor ki; : : : ; k j.\", metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 262}),\n",
              " Document(page_content='Lecture Notes for Chapter 15: Dynamic Programming 15-13\\nRecursive solution\\nSubproblem domain:\\n\\x0fFind optimal BSTfor ki; : : : ; k j,where i\\x151; j\\x14n; j\\x15i/NUL1.\\n\\x0fWhen jDi/NUL1, thetree is empty.\\nDeﬁne eŒi; j \\x8dDexpected search cost of optimal BSTfor ki; : : : ; k j.\\nIfjDi/NUL1, then eŒi; j \\x8dD0.\\nIfj\\x15i,\\n\\x0fSelect aroot kr, for some i\\x14r\\x14j.\\n\\x0fMake an optimal BSTwith ki; : : : ; k r/NUL1as the left subtree.\\n\\x0fMake an optimal BSTwith krC1; : : : ; k jas theright subtree.\\n\\x0fNote: when rDi, left subtree is ki; : : : ; k i/NUL1; when rDj, right subtree is\\nkjC1; : : : ; k j.\\nWhen asubtree becomes asubtree of anode:\\n\\x0fDepth of every node in subtree goes up by1.\\n\\x0fExpected search cost increases by\\nw.i; j /DjX\\nlDipl(refer to equation (\\x03)) .\\nIfkristhe root of an optimal BSTfor ki; : : : ; k j:\\neŒi; j \\x8dDprC.eŒi; r/NUL1\\x8dCw.i; r/NUL1//C.eŒrC1; j \\x8dCw.rC1; j // :\\nButw.i; j /Dw.i; r/NUL1/CprCw.rC1; j /.\\nTherefore, eŒi; j \\x8dDeŒi; r/NUL1\\x8dCeŒrC1; j \\x8dCw.i; j /.\\nThisequation assumes that wealready know which key is kr.\\nWedon’t.\\nTryall candidates, and pick thebest one:\\neŒi; j \\x8dD(\\n0 ifjDi/NUL1 ;\\nmin\\ni\\x14r\\x14jfeŒi; r/NUL1\\x8dCeŒrC1; j \\x8dCw.i; j /gifi\\x14j :\\nCould writearecursive algorithm...\\nComputingan optimal solution\\nAs“usual,” we’ll store the values inatable:\\neŒ 1 : : nC1„ƒ‚…\\ncan store\\neŒnC1; n\\x8d; 0 : : n„ƒ‚…\\ncan store\\neŒ1; 0\\x8d\\x8d\\n\\x0fWill use only entries eŒi; j \\x8d, where j\\x15i/NUL1.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 263}),\n",
              " Document(page_content='15-14 Lecture Notes for Chapter 15: Dynamic Programming\\n\\x0fWill also compute\\nrootŒi; j \\x8dDroot of subtree with keys ki; : : : ; k j, for1\\x14i\\x14j\\x14n :\\nOne other table: don’t recompute w.i; j /from scratch every time we need it.\\n(Would take ‚.j/NULi/additions.)\\nInstead:\\n\\x0fTable wŒ1 : : nC1; 0 : : n\\x8d\\n\\x0fwŒi; i/NUL1\\x8dD0for1\\x14i\\x14n\\n\\x0fwŒi; j \\x8dDwŒi; j/NUL1\\x8dCpjfor1\\x14i\\x14j\\x14n\\nCancompute all ‚.n2/values in O.1/time each.\\nOPTIMAL-BST .p; q; n/\\nleteŒ1 : : nC1; 0 : : n\\x8d,wŒ1 : : nC1; 0 : : n\\x8d,androotŒ1 : : n; 1 : : n\\x8d be new tables\\nforiD1tonC1\\neŒi; i/NUL1\\x8dD0\\nwŒi; i/NUL1\\x8dD0\\nforlD1ton\\nforiD1ton/NULlC1\\njDiCl/NUL1\\neŒi; j \\x8dD1\\nwŒi; j \\x8dDwŒi; j/NUL1\\x8dCpj\\nforrDitoj\\ntDeŒi; r/NUL1\\x8dCeŒrC1; j \\x8dCwŒi; j \\x8d\\nift < eŒi; j \\x8d\\neŒi; j \\x8dDt\\nrootŒi; j \\x8dDr\\nreturn eandroot\\nFirstforloop initializes e; wentries for subtrees with 0keys.\\nMainforloop:\\n\\x0fIteration for lworks on subtrees with lkeys.\\n\\x0fIdea: compute inorder of subtree sizes, smaller ( 1key) to larger ( nkeys).\\nFor example at beginning:\\ne\\n1\\n2\\n3\\n4\\n5\\n60 1 2 3 4 5\\nij\\n0\\n0\\n0\\n0\\n0\\n0.25 .65 .8 1.25 2.10\\n.2 .3 .75 1.35\\n.3.2.05 .3 .85\\n.7 pi', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 264}),\n",
              " Document(page_content='Lecture Notes for Chapter 15: Dynamic Programming 15-15\\nw\\n1\\n2\\n3\\n4\\n5\\n60 1 2 3 4 5\\nij\\n0\\n0\\n0\\n0\\n0\\n0.25 .45 .5 .7 1.0\\n.2 .25 .45 .75\\n.3.2.05 .25 .55\\n.5\\nroot\\n1\\n2\\n3\\n4\\n51 2 3 4 5\\nij\\n1\\n2\\n3\\n4\\n51 1 2 2\\n2 2 4\\n54 5\\nTime\\nO.n3/: for loops nested 3deep, each loop index takes on \\x14nvalues. Can also\\nshow \\x7f.n3/. Therefore, ‚.n3/.\\nConstructan optimal solution\\nCONSTRUCT -OPTIMAL-BST .root/\\nrDrootŒ1; n\\x8d\\nprint “ k”r“is the root”\\nCONSTRUCT -OPT-SUBTREE .1; r/NUL1; r;“left” ;root/\\nCONSTRUCT -OPT-SUBTREE .rC1; n; r;“right” ;root/\\nCONSTRUCT -OPT-SUBTREE .i; j; r;dir;root/\\nifi\\x14j\\ntDrootŒi; j \\x8d\\nprint “ k”t“is”dir“child of k”r\\nCONSTRUCT -OPT-SUBTREE .i; t/NUL1; t;“left” ;root/\\nCONSTRUCT -OPT-SUBTREE .tC1; j; t;“right” ;root/\\nElements ofdynamic programming\\nMentioned already:\\n\\x0foptimal substructure\\n\\x0foverlapping subproblems\\nOptimalsubstructure\\n\\x0fShow that a solution to a problem consists of making a choice, which leaves\\none or subproblems to solve.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 265}),\n",
              " Document(page_content='15-16 Lecture Notes for Chapter 15: Dynamic Programming\\n\\x0fSuppose that you are given this last choice that leads to an op timal solution.\\n[We ﬁnd that students often have trouble understanding the r elationship be-\\ntween optimal substructure and determining which choice is made in an opti-\\nmal solution. One way that helps them understand optimal sub structure is to\\nimagine that the dynamic-programming gods tell you what was the last choice\\nmadein anoptimal solution.]\\n\\x0fGiven this choice, determine which subproblems arise and ho w to characterize\\nthe resulting space of subproblems.\\n\\x0fShow that the solutions to the subproblems used within the op timal solution\\nmust themselves beoptimal. Usually usecut-and-paste:\\n\\x0fSuppose that one of the subproblem solutions isnot optimal.\\n\\x0fCutit out.\\n\\x0fPasteinan optimal solution.\\n\\x0fGetabetter solution totheoriginal problem. Contradicts o ptimality ofprob-\\nlem solution.\\nThat wasoptimal substructure.\\nNeed to ensure that you consider a wide enough range of choice s and subprob-\\nlems that you get them all. [The dynamic-programming gods are too busy to tell\\nyou what that last choice really was.] Try all the choices, solve all the subprob-\\nlems resulting from each choice, and pick the choice whose so lution, along with\\nsubproblem solutions, isbest.\\nHowto characterize the space of subproblems?\\n\\x0fKeepthe space assimple as possible.\\n\\x0fExpand it asnecessary.\\nExamples\\nRod cutting\\n\\x0fSpace of subproblems wasrods of length n/NULi, for1\\x14i\\x14n.\\n\\x0fNoneed totry amoregeneral space of subproblems.\\nOptimalbinary search trees\\n\\x0fSuppose we had tried to constrain space of subproblems to sub trees with\\nkeysk1; k2; : : : ; k j.\\n\\x0fAnoptimal BSTwould have root kr,for some 1\\x14r\\x14j.\\n\\x0fGet subproblems k1; : : : ; k r/NUL1andkrC1; : : : ; k j.\\n\\x0fUnlesswecouldguarantee that rDj,sothatsubproblem with krC1; : : : ; k j\\nis empty, then this subproblem is notof the form k1; k2; : : : ; k j.\\n\\x0fThus, needed to allow the subproblems to vary at “both ends,” i.e., allow\\nbothiandjto vary.\\nOptimal substructure varies across problem domains:\\n1.Howmany subproblems are used in anoptimal solution.\\n2.Howmany choices indetermining which subproblem(s) to use.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 266}),\n",
              " Document(page_content='Lecture Notes for Chapter 15: Dynamic Programming 15-17\\n\\x0fRod cutting:\\n\\x0f1subproblem (of size n/NULi)\\n\\x0fnchoices\\n\\x0fLongest common subsequence:\\n\\x0f1subproblem\\n\\x0fEither\\n\\x0f1choice (if xiDyj,LCSof Xi/NUL1andYj/NUL1),or\\n\\x0f2choices (if xi¤yj, LCSof Xi/NUL1andY, and LCSof XandYj/NUL1)\\n\\x0fOptimal binary search tree:\\n\\x0f2subproblems ( ki; : : : ; k r/NUL1andkrC1; : : : ; k j)\\n\\x0fj/NULiC1choices for krinki; : : : ; k j. Oncewedetermine optimal solutions\\ntosubproblems, wechoose from among the j/NULiC1candidates for kr.\\nInformally, running timedepends on(# of subproblems overa ll)\\x02(#of choices).\\n\\x0fRod cutting: ‚.n/subproblems,\\x14nchoices for each\\n)O.n2/running time.\\n\\x0fLongest common subsequence: ‚.mn/subproblems,\\x142choices for each\\n)‚.mn/running time.\\n\\x0fOptimal binary search tree: ‚.n2/subproblems, O.n/choices for each\\n)O.n3/running time.\\nCanusethesubproblem graphtogetthesameanalysis: countt henumberofedges.\\n\\x0fEach vertex corresponds toasubproblem.\\n\\x0fChoices for asubproblem arevertices that thesubproblem ha s edges going to.\\n\\x0fFor rod cutting, subproblem graph has nvertices and\\x14nedges per vertex\\n)O.n2/running time.\\nIn fact, can get an exact count of the edges: for iD0; 1; : : : ; n , vertex for\\nsubproblem size ihas out-degree i)#of edgesDPn\\niD0iDn.nC1/=2.\\n\\x0fSubproblem graph for matrix-chain multiplication would ha ve‚.n2/vertices,\\neach withdegree\\x14n/NUL1\\n)O.n3/running time.\\nDynamic programming uses optimal substructure bottom up .\\n\\x0fFirstﬁnd optimal solutions to subproblems.\\n\\x0fThenchoose which touse in optimal solution tothe problem.\\nWhen welook at greedy algorithms, we’ll see that they work top down:ﬁrstmake\\nachoice that looks best, thensolve the resulting subproblem.\\nDon’tbefooledintothinkingoptimalsubstructure applies toalloptimization prob-\\nlems. It doesn’t.\\nHere are two problems that look similar. In both, we’re given anunweighted,\\ndirected graph GD.V; E/.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 267}),\n",
              " Document(page_content='15-18 Lecture Notes for Chapter 15: Dynamic Programming\\n\\x0fVisaset of vertices.\\n\\x0fEisaset of edges.\\nAnd we ask about ﬁnding a path(sequence of connected edges) from vertex uto\\nvertex \\x17.\\n\\x0fShortest path : ﬁndpath u;\\x17withfewest edges. Must be simple(nocycles),\\nsince removing acycle from apath gives apath withfewer edge s.\\n\\x0fLongestsimplepath : ﬁndsimplepathu;\\x17withmostedges. Ifdidn’t require\\nsimple, could repeatedly traverse acycle to make anarbitra rily long path.\\nShortest path has optimal substructure.\\nu v wp1p2\\np\\n\\x0fSuppose pisshortest path u;\\x17.\\n\\x0fLetwbeany vertex on p.\\n\\x0fLetp1be the portion of pgoing u;w.\\n\\x0fThen p1isa shortest path u;w.\\nProofSuppose there exists a shorter path p0\\n1going u;w. Cut out p1, replace it\\nwithp0\\n1,get path up0\\n1;wp2;\\x17withfewer edges than p.\\nTherefore, canﬁndshortest path u;\\x17byconsidering allintermediatevertices w,\\nthen ﬁnding shortest paths u;wandw;\\x17.\\nSameargument applies to p2.\\nDoes longest path have optimal substructure?\\n\\x0fIt seems like it should.\\n\\x0fIt doesnot.\\nq r\\ns t\\nConsider q!r!tDlongest path q;t. Areitssubpaths longest paths?\\nNo!\\n\\x0fSubpath q;risq!r.\\n\\x0fLongest simple path q;risq!s!t!r.\\n\\x0fSubpath r;tisr!t.\\n\\x0fLongest simple path r;tisr!q!s!t.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 268}),\n",
              " Document(page_content='Lecture Notes for Chapter 15: Dynamic Programming 15-19\\nNot only isn’t there optimal substructure, but we can’t even assemble a legal solu-\\ntion from solutions to subproblems.\\nCombine longest simple paths:\\nq!s!t!r!q!s!t\\nNot simple!\\nInfact, this problem isNP-complete (so it probably has noop timal substructure to\\nﬁnd.)\\nWhat’s the big difference between shortest path and longest path?\\n\\x0fShortest path has independent subproblems.\\n\\x0fSolution to one subproblem does not affect solution to anoth er subproblem of\\nthe same problem.\\n\\x0fLongest simple path: subproblems are notindependent.\\n\\x0fConsider subproblems of longest simple paths q;randr;t.\\n\\x0fLongest simple path q;rusessandt.\\n\\x0fCannot use sandtto solve longest simple path r;t, since if wedo, the path\\nisn’t simple.\\n\\x0fBut wehaveto use tto ﬁndlongest simple path r;t!\\n\\x0fUsingresources (vertices) tosolveonesubproblem renders themunavailable to\\nsolve theother subproblem.\\n[For shortest paths, if we look at a shortest path up1;wp2;\\x17, no vertex other\\nthanwcan appear in p1andp2. Otherwise, wehave acycle.]\\nIndependent subproblems in our examples:\\n\\x0fRod cutting and longest common subsequence\\n\\x0f1subproblem)automatically independent.\\n\\x0fOptimal binary search tree\\n\\x0fki; : : : ; k r/NUL1andkrC1; : : : ; k j)independent.\\nOverlapping subproblems\\nThese occur when arecursive algorithm revisits thesame pro blem over and over.\\nGooddivide-and-conquer algorithmsusuallygenerateabra ndnewproblemateach\\nstage of recursion.\\nExample: merge sort\\n1..8\\n1..4 5..8\\n1..2 3..4 5..6 7..8\\n1..12..23..3 4..4 5..5 6..67..78..8', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 269}),\n",
              " Document(page_content='15-20 Lecture Notes for Chapter 15: Dynamic Programming\\nWon’t go through exercise of showing repeated subproblems.\\nBook has agood example for matrix-chain multiplication.\\nAlternative approach to dynamic programming: memoization\\n\\x0f“Store, don’t recompute.”\\n\\x0fMake atable indexed by subproblem.\\n\\x0fWhen solving asubproblem:\\n\\x0fLookup intable.\\n\\x0fIf answer isthere, use it.\\n\\x0fElse, compute answer, then store it.\\n\\x0fIn bottom-up dynamic programming, we go one step further. We determine in\\nwhat order we’d want to access thetable, and ﬁllit inthat way .', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 270}),\n",
              " Document(page_content='SolutionsforChapter 15:\\nDynamicProgramming\\nSolutionto Exercise 15.1-1\\nWecan verify that T .n/D2nisa solution to the given recurrence bythe substitu-\\ntion method. Wenote that for nD0, the formula is true since 20D1. For n > 0,\\nsubstituting into the recurrence and using the formula for s umming a geometric\\nseries yields\\nT .n/D1Cn/NUL1X\\njD02j\\nD1C.2n/NUL1/\\nD2n:\\nSolutionto Exercise 15.1-2\\nHereisa counterexample for the “greedy” strategy:\\nlength i1 2 3 4\\nprice pi1 20 33 36\\npi=i1 10 11 1\\nLetthegivenrodlength be4. According toagreedy strategy, weﬁrstcut outarod\\nof length 3for aprice of 33, which leaves uswitharod of lengt h 1of price 1. The\\ntotal price for the rod is 34. The optimal way is to cut it into t wo rods of length 2\\neach fetching us40 dollars.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 271}),\n",
              " Document(page_content='15-22 Solutions for Chapter 15: Dynamic Programming\\nSolution to Exercise15.1-3\\nMODIFIED-CUT-ROD.p; n; c/\\nletrŒ0 : : n\\x8dbe anew array\\nrŒ0\\x8dD0\\nforjD1ton\\nqDpŒj \\x8d\\nforiD1toj/NUL1\\nqDmax.q; pŒi\\x8dCrŒj/NULi\\x8d/NULc/\\nrŒj \\x8dDq\\nreturn rŒn\\x8d\\nThe major modiﬁcation required is in the body of the inner forloop, which now\\nreads qDmax.q; pŒi\\x8dCrŒj/NULi\\x8d/NULc/. This change reﬂects the ﬁxed cost of\\nmaking the cut, which is deducted from the revenue. We also ha ve to handle the\\ncase in which we make no cuts (when iequals j); the total revenue in this case is\\nsimply pŒj \\x8d. Thus, wemodify the inner forloop to run from itoj/NUL1instead of\\ntoj. Theassignment qDpŒj \\x8dtakescareofthecaseofnocuts. Ifwedidnotmake\\nthesemodiﬁcations,theneveninthecaseofnocuts,wewould bededucting cfrom\\nthe total revenue.\\nSolution to Exercise15.1-4\\nMEMOIZED -CUT-ROD.p; n/\\nletrŒ0 : : n\\x8dandsŒ0 : : n\\x8dbe new arrays\\nforiD0ton\\nrŒi\\x8dD/NUL1\\n.\\x17al; s/DMEMOIZED -CUT-ROD-AUX.p; n; r; s/\\nprint “Theoptimal value is ” \\x17al“and the cuts are at ”\\njDn\\nwhile j > 0\\nprint sŒj \\x8d\\njDj/NULsŒj \\x8d', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 272}),\n",
              " Document(page_content='Solutions forChapter 15: Dynamic Programming 15-23\\nMEMOIZED -CUT-ROD-AUX.p; n; r; s/\\nifrŒn\\x8d\\x150\\nreturn rŒn\\x8d\\nifn==0\\nqD0\\nelseqD/NUL1\\nforiD1ton\\n.\\x17al; s/DMEMOIZED -CUT-ROD-AUX.p; n/NULi; r; s/\\nifq < pŒi\\x8dC\\x17al\\nqDpŒi\\x8dC\\x17al\\nsŒn\\x8dDi\\nrŒn\\x8dDq\\nreturn .q; s/\\nPRINT-CUT-ROD-SOLUTION constructstheactuallengthswhereacutshouldhap-\\npen. Array entry sŒi\\x8dcontains the value jindicating that an optimal cut for a rod\\nof length iisjinches. Thenext cut isgiven by sŒi/NULj \\x8d,and soon.\\nSolutionto Exercise 15.1-5\\nFIBONACCI .n/\\nletﬁbŒ0 : : n\\x8dbeanew array\\nﬁbŒ0\\x8dDﬁbŒ1\\x8dD1\\nforiD2ton\\nﬁbŒi\\x8dDﬁbŒi/NUL1\\x8dCﬁbŒi/NUL2\\x8d\\nreturnﬁbŒn\\x8d\\nFIBONACCI directlyimplementstherecurrencerelationoftheFibonac ci sequence.\\nEach number in the sequence is the sum of the two previous numb ers in the se-\\nquence. Therunning timeisclearly O.n/.\\nThe subproblem graph consists of nC1vertices, \\x170; \\x171; : : : ; \\x17 n. For iD\\n2; 3; : : : ; n , vertex \\x17ihas two leaving edges: to vertex \\x17i/NUL1and to vertex \\x17i/NUL2.\\nNoedges leave vertices \\x170or\\x171. Thus, the subproblem graph has 2n/NUL2edges.\\nSolutionto Exercise 15.2-4\\nThe vertices of the subproblem graph are the ordered pairs \\x17ij, where i\\x14j. If\\niDj, then there are no edges out of \\x17ij. Ifi < j, then for every ksuch that\\ni\\x14k < j,thesubproblem graphcontainsedges .\\x17ij; \\x17ik/and.\\x17ij; \\x17kC1;j/. These\\nedgesindicatethattosolvethesubproblem ofoptimallypar enthesizing theproduct\\nAi\\x01\\x01\\x01Aj, we need to solve subproblems of optimally parenthesizing t he products\\nAi\\x01\\x01\\x01AkandAkC1\\x01\\x01\\x01Aj. Thenumber of vertices is\\nnX\\niD1nX\\njDi1Dn.nC1/\\n2;', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 273}),\n",
              " Document(page_content='15-24 Solutions for Chapter 15: Dynamic Programming\\nand the number of edges is\\nnX\\niD1nX\\njDi.j/NULi/DnX\\niD1n/NULiX\\ntD0t(substituting tDj/NULi)\\nDnX\\niD1.n/NULi/.n/NULiC1/\\n2:\\nSubstituting rDn/NULiand reversing the order of summation, weobtain\\nnX\\niD1.n/NULi/.n/NULiC1/\\n2\\nD1\\n2n/NUL1X\\nrD0.r2Cr/\\nD1\\n2\\x12.n/NUL1/n.2n/NUL1/\\n6C.n/NUL1/n\\n2\\x13\\n(by equations (A.3)and (A.1))\\nD.n/NUL1/n.nC1/\\n6:\\nThus, thesubproblem graph has ‚.n2/vertices and ‚.n3/edges.\\nSolution to Exercise15.2-5\\nThis solutionisalsopostedpublicly\\nEach time the l-loop executes, the i-loop executes n/NULlC1times. Each time the\\ni-loop executes, the k-loop executes j/NULiDl/NUL1times, each time referencing\\nmtwice. Thus the total number of times that an entry of mis referenced while\\ncomputing other entries isPn\\nlD2.n/NULlC1/.l/NUL1/2. Thus,\\nnX\\niD1nX\\njDiR.i; j /DnX\\nlD2.n/NULlC1/.l/NUL1/2\\nD2n/NUL1X\\nlD1.n/NULl/l\\nD2n/NUL1X\\nlD1nl/NUL2n/NUL1X\\nlD1l2\\nD2n.n/NUL1/n\\n2/NUL2.n/NUL1/n.2n/NUL1/\\n6\\nDn3/NULn2/NUL2n3/NUL3n2Cn\\n3\\nDn3/NULn\\n3:', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 274}),\n",
              " Document(page_content='Solutions forChapter 15: Dynamic Programming 15-25\\nSolutionto Exercise 15.3-1\\nThissolutionisalsopostedpublicly\\nRunning R ECURSIVE -MATRIX-CHAINis asymptotically more efﬁcient than enu-\\nmerating all the ways of parenthesizing the product and comp uting the number of\\nmultiplications for each.\\nConsider the treatment of subproblems by the twoapproaches .\\n\\x0fFor each possible place to split the matrix chain, the enumer ation approach\\nﬁnds all ways to parenthesize the left half, ﬁnds all ways to p arenthesize the\\nright half, and looks at all possible combinations of the lef t half with the right\\nhalf. The amount of work to look at each combination of left- a nd right-half\\nsubproblem results isthusthe product of thenumber of wayst odothe left half\\nand the number of ways todo the right half.\\n\\x0fForeachpossibleplacetosplitthematrixchain, R ECURSIVE -MATRIX-CHAIN\\nﬁndsthebestwaytoparenthesizethelefthalf,ﬁndsthebest waytoparenthesize\\nthe right half, and combines just those tworesults. Thusthe amount of work to\\ncombine the left- and right-half subproblem results is O.1/.\\nSection 15.2 argued that the running time for enumeration is \\x7f.4n=n3=2/. Wewill\\nshow that the running timefor R ECURSIVE -MATRIX-CHAINisO.n3n/NUL1/.\\nTogetanupperboundontherunningtimeofR ECURSIVE -MATRIX-CHAIN, we’ll\\nuse the same approach used in Section 15.2 to get a lower bound : Derive a recur-\\nrence of the form T .n/\\x14: : :and solve it by substitution. For the lower-bound\\nrecurrence, the book assumed that the execution of lines 1–2 and 6–7 each take at\\nleast unit time. For the upper-bound recurrence, we’ll assu me those pairs of lines\\neach take at most constant time c. Thus, wehave the recurrence\\nT .n/\\x14\\x80\\nc ifnD1 ;\\ncCn/NUL1X\\nkD1.T .k/CT .n/NULk/Cc/ifn\\x152 :\\nThisisjustlikethebook’s \\x15recurrence except thatithas cinstead of1,andsowe\\ncan berewrite it as\\nT .n/\\x142n/NUL1X\\niD1T .i/Ccn :\\nWe shall prove that T .n/DO.n3n/NUL1/using the substitution method. (Note: Any\\nupperboundon T .n/thatis o.4n=n3=2/willsufﬁce. Youmightprefertoproveone\\nthat is easier to think up, such as T .n/DO.3:5n/.) Speciﬁcally, we shall show\\nthatT .n/\\x14cn3n/NUL1for all n\\x151. Thebasis iseasy, since T .1/\\x14cDc\\x011\\x0131/NUL1.\\nInductively, for n\\x152wehave', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 275}),\n",
              " Document(page_content='15-26 Solutions for Chapter 15: Dynamic Programming\\nT .n/\\x142n/NUL1X\\niD1T .i/Ccn\\n\\x142n/NUL1X\\niD1ci3i/NUL1Ccn\\n\\x14c\\x01 \\n2n/NUL1X\\niD1i3i/NUL1Cn!\\nDc\\x01\\x12\\n2\\x01\\x12n3n/NUL1\\n3/NUL1C1/NUL3n\\n.3/NUL1/2\\x13\\nCn\\x13\\n(see below)\\nDcn3n/NUL1Cc\\x01\\x121/NUL3n\\n2Cn\\x13\\nDcn3n/NUL1Cc\\n2.2nC1/NUL3n/\\n\\x14cn3n/NUL1for all c > 0,n\\x151.\\nRunning R ECURSIVE -MATRIX-CHAINtakes O.n3n/NUL1/time, andenumerating all\\nparenthesizations takes \\x7f.4n=n3=2/time, and so R ECURSIVE -MATRIX-CHAINis\\nmore efﬁcient than enumeration.\\nNote: The above substitution uses the following fact:\\nn/NUL1X\\niD1ixi/NUL1Dnxn/NUL1\\nx/NUL1C1/NULxn\\n.x/NUL1/2:\\nThis equation can be derived from equation (A.5)by taking th ederivative. Let\\nf .x/Dn/NUL1X\\niD1xiDxn/NUL1\\nx/NUL1/NUL1 :\\nThen\\nn/NUL1X\\niD1ixi/NUL1Df0.x/Dnxn/NUL1\\nx/NUL1C1/NULxn\\n.x/NUL1/2:\\nSolution to Exercise15.3-5\\nWe say that a problem exhibits the optimal substructure prop erty when optimal\\nsolutions toaproblem incorporate optimalsolutions torel atedsubproblems, which\\nwe may solve independently (i.e., they do not share resources). When we impose\\na limit lion the number of pieces of size ithat we are permitted to produce, the\\nsubproblems can no longer be solved independently . For example, consider a rod\\nof length 4withthe following prices and limits:\\nlength i1 2 3 4\\nprice pi15 20 33 36\\nlimit li2 1 1 1\\nThis instance has only three solutions that do not violate th e limits: length 4 with\\nprice 36; lengths 1 and 3 with price 48; and lengths 1, 1, and 2 w ith price 50. The', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 276}),\n",
              " Document(page_content='Solutions forChapter 15: Dynamic Programming 15-27\\noptimal solution, therefore is to cut into lengths 1, 1, and 2 . When we look at the\\nsubproblem forlength2,ithastwosolutions thatdonotviol atethelimits: length2\\nwithprice20,andlengths 1and1withprice30. Theoptimal so lution forlength2,\\ntherefore, istocutintolengths1and1. Butwecannot usethi soptimal solution for\\nthe subproblem in the optimal solution for the original prob lem, because it would\\nresult in using four rods of length 1 to solve the original pro blem, violating the\\nlimit of twolength-1 rods.\\nSolutionto Exercise 15.3-6\\nAnysolution must add the additional assumption that no curr ency can be repeated\\ninasequenceoftrades. Withoutthisassumption,if rij> 1=r jiforsomecurrencies\\niandj, we could repeatedly exchange i!j!i!j!\\x01\\x01\\x01and make an\\nunbounded proﬁt.\\nTo see that this problem has optimal substructure when ckD0for all k, observe\\nthat theproblem ofexchanging currency aforcurrency bisequivalent toﬁndinga\\nsequence of currencies k1; k2; : : : ; k msuch that k1Da,kmDb, and the product\\nrk1k2rk2k3\\x01\\x01\\x01rkm/NUL1kmismaximized.\\nWe use the usual cut-and-paste argument. Suppose that an opt imal solution con-\\ntains a sequencehki; kiC1; : : : ; k jiof currencies, and suppose that there exists a\\nsequencehk0\\ni; k0\\niC1; : : : ; k0\\nji, such that k0\\niDki,k0\\njDkj, and rk0\\nik0\\niC1\\x01\\x01\\x01rk0\\nj/NUL1k0\\nj>\\nrkikiC1\\x01\\x01\\x01rkj/NUL1kj. Thenwecould substitute thesequence hk0\\ni; k0\\niC1; : : : ; k0\\njiforthe\\nsequencehki;kiC1;: : : ;k jiintheoptimalsolutiontocreateanevenbettersolution.\\nWe show that optimal substructure does not hold when the ckare arbitrary values\\nby means of an example. Suppose we have four currencies, with the following\\nexchange rates:\\nj\\nrij1 2 3 4\\n11 2 5/2 6\\n21/2 1 3/2 3\\ni32/5 2/3 1 3\\n41/6 1/3 1/3 1\\nLetc1D2andc2Dc3D3. Note that this example is not too badly contrived, in\\nthatrjiD1=r ijfor all iandj.\\nTo see how this example does not exhibit optimal substructur e, let’s examine an\\noptimal solution for exchanging currency 1 for currency 4. T here are ﬁve possible\\nexchange sequences, with the following costs:\\nh1; 4i W 6/NUL2D4 ;\\nh1; 2; 4i W 2\\x013/NUL3D3 ;\\nh1; 3; 4i W 5=2\\x013/NUL3D9=2 ;\\nh1;2;3;4i W2\\x013=2\\x013/NUL3D6\\nh1; 3; 2; 4i W 5=2\\x012=3\\x013/NUL3D2\\nTheoptimal exchange sequence, h1; 2; 3; 4i, appears inboldface.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 277}),\n",
              " Document(page_content='15-28 Solutions for Chapter 15: Dynamic Programming\\nLet’s examine the subproblem of exchanging currency 1 for cu rrency 3. Allow-\\ning currency 4 to be part of the exchange sequence, there are a gain ﬁve possible\\nexchange sequences with thefollowing costs and the optimal one in boldface:\\nh1;3i W 5=2/NUL2D1=2\\nh1; 2; 3i W 2\\x013=2/NUL3D0\\nh1; 4; 3i W 6\\x011=3/NUL3D /NUL 1\\nh1; 2; 4; 3i W 2\\x013\\x011=3/NUL3D /NUL 1\\nh1; 4; 2; 3i W 6\\x011=3\\x013=2D3D0\\nWe see that the solution to the original problem includes the subproblem of ex-\\nchanging currency 1 for currency 3, yet the solution h1; 2; 3ito the subproblem\\nusedintheoptimalsolutiontotheoriginalproblemisnotth eoptimalsolutionh1;3i\\nto the subproblem onits own.\\nSolution to Exercise15.4-4\\nThis solutionisalsopostedpublicly\\nWhen computing a particular row of the ctable, no rows before the previous row\\nareneeded. Thusonlytworows— 2\\x01Y:lengthentries—need tobekept inmemory\\natatime. (Note: Eachrowof cactuallyhas Y:lengthC1entries,butwedon’tneed\\nto store the column of 0’s—instead we can make the program “kn ow” that those\\nentries are 0.) With this idea, we need only 2\\x01min.m; n/entries if we always call\\nLCS-L ENGTHwiththe shorter sequence as the Yargument.\\nWecan thus do awaywiththe ctable asfollows:\\n\\x0fUsetwoarrays of length min .m; n/,pre\\x17ious-rowandcurrent-row,tohold the\\nappropriate rowsof c.\\n\\x0fInitialize pre\\x17ious-rowto all 0and compute current-rowfrom left toright.\\n\\x0fWhencurrent-rowis ﬁlled, if there are still more rows to compute, copy\\ncurrent-rowintopre\\x17ious-rowand compute thenew current-row.\\nActually only a little more than one row’s worth of centries—min .m; n/C1en-\\ntries—are needed during the computation. The only entries n eeded in the table\\nwhen it is time to compute cŒi; j \\x8darecŒi; k\\x8dfork\\x14j/NUL1(i.e., earlier entries in\\nthecurrent row,whichwillbeneeded tocompute thenext row) ; and cŒi/NUL1; k\\x8dfor\\nk\\x15j/NUL1(i.e., entries intheprevious rowthat arestill needed toco mpute therest\\nof the current row). This is one entry for each kfrom 1 to min .m; n/except that\\ntherearetwoentrieswith kDj/NUL1,hencetheadditional entryneeded besidesthe\\none row’s worth of entries.\\nWecan thus do awaywiththe ctable asfollows:\\n\\x0fUsean array aof length min .m; n/C1to hold the appropriate entries of c. At\\nthe time cŒi; j \\x8dis tobe computed, awill hold the following entries:\\n\\x0faŒk\\x8dDcŒi; k\\x8dfor1\\x14k < j/NUL1(i.e.,earlier entries in the current “row”),\\n\\x0faŒk\\x8dDcŒi/NUL1; k\\x8dfork\\x15j/NUL1(i.e., entries inthe previous “row”),', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 278}),\n",
              " Document(page_content='Solutions forChapter 15: Dynamic Programming 15-29\\n\\x0faŒ0\\x8dDcŒi; j/NUL1\\x8d(i.e., the previous entry computed, which couldn’t be put\\ninto the “right” place in awithout erasing the still-needed cŒi/NUL1; j/NUL1\\x8d).\\n\\x0fInitialize ato all 0and compute the entries from left toright.\\n\\x0fNote that the 3 values needed to compute cŒi; j \\x8dforj > 1are in aŒ0\\x8dD\\ncŒi; j/NUL1\\x8d,aŒj/NUL1\\x8dDcŒi/NUL1; j/NUL1\\x8d, and aŒj \\x8dDcŒi/NUL1; j \\x8d.\\n\\x0fWhen cŒi; j \\x8dhas been computed, move aŒ0\\x8d(cŒi; j/NUL1\\x8d) to its “correct”\\nplace, aŒj/NUL1\\x8d, and put cŒi; j \\x8dinaŒ0\\x8d.\\nSolutionto Problem 15-1\\nWe will make use of the optimal substructure property of long est paths in acyclic\\ngraphs. Let ube some vertex of the graph. If uDt, then the longest path from u\\ntothas zero weight. If u¤t, letpbe a longest path from utot. Path phas at\\nleast two vertices. Let \\x17be the second vertex on the path. Let p0be the subpath\\nofpfrom \\x17tot(p0might be a zero-length path). That is, the path plooks like\\nu!\\x17p0\\n;t.\\nWeclaim that p0isalongest path from \\x17tot.\\nTo prove the claim, we use a cut-and-paste argument. If p0were not a longest\\npath, then there exists a longer path p00from \\x17tot. We could cut out p0and paste\\ninp00to produce a path u!\\x17p00\\n;twhich is longer than p, thus contradicting the\\nassumption that pisalongest path from utot.\\nIt is important to note that the graph is acyclic. Because the graph is acyclic,\\npathp00cannot include the vertex u, for otherwise there would be a cycle of the\\nform u!\\x17;uin the graph. Thus, we can indeed use p00to construct a longer\\npath. The acyclicity requirement ensures that by pasting in pathp00, the overall\\npath is still a simplepath (there is no cycle in the path). This difference between\\nthe cyclic and the acyclic case allows us touse dynamic progr amming tosolve the\\nacyclic case.\\nLetdistŒu\\x8ddenotetheweightofalongestpathfrom utot. Theoptimalsubstructure\\nproperty allows us towrite arecurrence for distŒu\\x8das\\ndistŒu\\x8dD(0 ifuDt ;\\nmax\\n.u;\\x17/ 2E˚\\nw.u; \\x17/CdistŒ\\x17\\x8d/TAB\\notherwise :\\nThisrecurrence allows us toconstruct the following proced ure:', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 279}),\n",
              " Document(page_content='15-30 Solutions for Chapter 15: Dynamic Programming\\nLONGEST-PATH-AUX.G; u; t;dist;next/\\nifu==t\\ndistŒu\\x8dD0\\nreturn .dist;next/\\nelseifnextŒu\\x8d\\x150\\nreturn .dist;next/\\nelsenextŒu\\x8dD0\\nforeach vertex \\x172G:AdjŒu\\x8d\\n.dist;next/DLONGEST-PATH-AUX.G; \\x17; t;dist;next/\\nifw.u; \\x17/CdistŒ\\x17\\x8d >distŒu\\x8d\\ndistŒu\\x8dDw.u; \\x17/CdistŒ\\x17\\x8d\\nnextŒu\\x8dD\\x17\\nreturn .dist;next/\\n(See Section 22.1 for anexplanation of the notation G:AdjŒu\\x8d.)\\nLONGEST-PATH-AUXisamemoized,recursiveprocedure, whichreturnsthetuple\\n.dist;next/. The array distis the memoized array that holds the solution to sub-\\nproblems. That is, after the procedure returns, distŒu\\x8dwill hold the weight of a\\nlongest path from utot. Thearray nextserves twopurposes:\\n\\x0fIt holds information necessary for printing out an actual pa th. Speciﬁcally, if u\\nisavertex onthelongest path that theprocedure found, then nextŒu\\x8disthenext\\nvertex on thepath.\\n\\x0fThe value in nextŒu\\x8dis used to check whether the current subproblem has been\\nsolved earlier. A value of at least zero indicates that this s ubproblem has been\\nsolved earlier.\\nThe ﬁrst ifcondition checks for the base case uDt. The second ifcondition\\nchecks whether the current subproblem has already been solv ed. The forloop\\niterates over each adjacent edge .u; \\x17/and updates the longest distance in distŒu\\x8d.\\nWhat is the running time of L ONGEST-PATH-AUX? Each subproblem represented\\nby a vertex uis solved at most once due to the memoization. For each vertex , we\\nexamine its adjacent edges. Thus, each edge is examined at mo st once, and the\\noverall running time is O.E/. (Section 22.1 discusses how weachieve O.E/time\\nby representing the graph withadjacency lists.)\\nThePRINT-PATHprocedureprintsoutthepathusinginformationstoredinth enext\\narray:\\nPRINT-PATH.s; t;next/\\nuDs\\nprint u\\nwhile u¤t\\nprint “!”next[u]\\nuDnextŒu\\x8d\\nThe LONGEST-PATH-MAINprocedure is the main driver. It creates and initializes\\nthedistand thenextarrays. It then calls L ONGEST-PATH-AUXto ﬁnd a path and\\nPRINT-PATHtoprint out the actual path.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 280}),\n",
              " Document(page_content='Solutions forChapter 15: Dynamic Programming 15-31\\nLONGEST-PATH-MAIN.G; s; t/\\nnDjG:Vj\\nletdistŒ1 : : n\\x8dandnextŒ1 : : n\\x8dbe new arrays\\nforiD1ton\\ndistŒi\\x8dD/NUL1\\nnextŒi\\x8dD/NUL1\\n.dist;next/DLONGEST-PATH-AUX.G; s; t;dist;next/\\nifdistŒs\\x8d==/NUL1\\nprint “No path exists”\\nelseprint “The weight of the longest path is” distŒs\\x8d\\nPRINT-PATH.s; t;next/\\nInitializating the distandnextarrays takes O.V /time. Thus the overall running\\ntimeof L ONGEST-PATH-MAINisO.VCE/.\\nAlternative solution\\nWe can also solve the problem using a bottom-up aproach. To do so, we need\\nto ensure that we solve “smaller” subproblems before we solv e “larger” ones. In\\nour case, we can use a topological sort (see Section 22.4) to obtain a bottom-up\\nprocedure, imposing the required ordering on thevertices i n‚.VCE/time.\\nLONGEST-PATH2.G; s; t/\\nletdistŒ1 : : n\\x8dandnextŒ1 : : n\\x8dbe new arrays\\ntopologically sort the vertices of G\\nforiD1tojG:Vj\\ndistŒi\\x8dD/NUL1\\ndistŒs\\x8dD0\\nforeachuin topological order, starting from s\\nforeach edge .u; \\x17/2G:AdjŒu\\x8d\\nifdistŒu\\x8dCw.u; \\x17/ > distŒ\\x17\\x8d\\ndistŒ\\x17\\x8dDdistŒu\\x8dCw.u; \\x17/\\nnextŒu\\x8dD\\x17\\nprint “The longest distance is” distŒt\\x8d\\nPRINT-PATH.s; t;next/\\nTherunning time of L ONGEST-PATH2 is‚.VCE/.\\nSolutionto Problem 15-2\\nWe solve the longest palindrome subsequence (LPS) problem i n a manner similar\\ntohow wecompute the longest common subsequence inSection 1 5.4.\\nStep1: Characterizing alongest palindrome subsequence\\nThe LPS problem has an optimal-substructure property, wher e the subproblems\\ncorrespond to pairs of indices, starting and ending, of thei nput sequence.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 281}),\n",
              " Document(page_content='15-32 Solutions for Chapter 15: Dynamic Programming\\nForasequence XDhx1;x2;: : : ;x ni,wedenotethesubsequence startingat xiand\\nending at xjbyXijDhxi; xiC1; : : : ; x ji.\\nTheorem (Optimal substructure of an LPS)\\nLetXDhx1; x2; : : : ; x nibe the input sequence, and let ZDh´1; ´2; : : : ; ´ mibe\\nany LPSof X.\\n1. If nD1,then mD1and´1Dx1.\\n2. If nD2andx1Dx2, then mD2and´1D´2Dx1Dx2.\\n3. If nD2andx1¤x2, then mD1and´1is equal to either x1orxn.\\n4. If n > 2andx1Dxn,then m > 2,´1D´mDx1Dxn,and Z2;m/NUL1isanLPS\\nofX2;n/NUL1.\\n5. If n > 2andx1¤xn, then ´1¤x1implies that Z1;mis anLPSof X2;n.\\n6. If n > 2andx1¤xn, then ´m¤xnimplies that Z1;mis anLPSof X1;n/NUL1.\\nProofProperties (1), (2), and (3) follow trivially from the deﬁni tion of LPS.\\n(4) If n > 2andx1Dxn, then we can choose x1andxnas the ends of Zand\\nat least one more element of Xas part of Z. Thus, it follows that m > 2. If\\n´1¤x1, then we could append x1Dxnto the ends of Zto obtain a palindrome\\nsubsequence of Xwith length mC2, contradicting the supposition that Zis a\\nlongestpalindromesubsequence of X. Thus,wemusthave ´1Dx1.DxnD´m/.\\nNow, Z2;m/NUL1is a length- .m/NUL2/palindrome subsequence of X2;n/NUL1. We wish to\\nshow that it is an LPS. Suppose for the purpose of contradicti on that there exists\\na palindrome subsequence WofX2;n/NUL1with length greater than m/NUL2. Then,\\nappending x1Dxnto the ends of Wproduces a palindrome subsequence of X\\nwhose length is greater than m,which is acontradiction.\\n(5) If ´1¤x1, then Zis a palindrome subsequence of X2;n. If there were a\\npalindromesubsequence WofX2;nwithlengthgreaterthan m,then Wwouldalso\\nbe apalindrome subsequence of X,contradicting the assumption that ZisanLPS\\nofX.\\n(6) Theproof issymmetric to(2).\\nThe way that the theorem characterizes longest palindrome s ubsequences tells us\\nthat an LPS of a sequence contains within it an LPS of a subsequ ence of the se-\\nquence. Thus, the LPSproblem has anoptimal-substructure p roperty.\\nStep 2: Arecursive solution\\nThe theorem implies that weshould examine either one or twos ubproblems when\\nﬁnding anLPSof XDhx1; x2; : : : ; x ni,depending on whether x1Dxn.\\nLet us deﬁne pŒi; j \\x8dto be the length of an LPS of the subsequence Xij. IfiDj,\\ntheLPShaslength1. If jDiC1,thentheLPShaslengtheither1or2,depending\\non whether xiDxj. The optimal substructure of the LPS problem gives the\\nfollowing recursive formula:', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 282}),\n",
              " Document(page_content='Solutions forChapter 15: Dynamic Programming 15-33\\npŒi; j \\x8dD˚\\n1 ifiDj ;\\n2 ifjDiC1andxiDxj;\\n1 ifjDiC1andxi¤xj;\\npŒiC1; j/NUL1\\x8dC2 ifj > iC1andxiDxj;\\nmax.pŒi; j/NUL1\\x8d; pŒiC1; j \\x8d/ifj > iC1andxi¤xj:\\nStep3: Computingthelength of an LPS\\nProcedure L ONGEST-PALINDROME takes a sequence XDhx1; x2; : : : ; x nias\\ninput. The procedure ﬁlls cells pŒi; i\\x8d, where 1\\x14i\\x14n, and pŒi; iC1\\x8d, where\\n1\\x14i\\x14n/NUL1,asthebasecases. Itthenstartsﬁllingcells pŒi; j \\x8d,where j > iC1.\\nTheprocedure ﬁllsthe ptable row byrow,starting withrow n/NUL2and moving to-\\nwardrow1. (Rows n/NUL1andnarealready ﬁlledaspart ofthebase cases.) Within\\neachrow,theprocedureﬁllstheentriesfromlefttoright. T heprocedurealsomain-\\ntains the table bŒ1 : : n; 1 : : n\\x8d to help us construct an optimal solution. Intuitively,\\nbŒi; j \\x8dpoints to the table entry corresponding to the optimal subpr oblem solution\\nchosen when computing pŒi; j \\x8d. The procedure returns the bandptables; pŒ1; n\\x8d\\ncontains thelength ofanLPSof X. Therunning timeof L ONGEST-PALINDROME\\nisclearly ‚.n2/.\\nLONGEST-PALINDROME .X/\\nnDX:length\\nletbŒ1 : : n; 1 : : n\\x8d andpŒ0 : : n; 0 : : n\\x8d be new tables\\nforiD1ton/NUL1\\npŒi; i\\x8dD1\\njDiC1\\nifxi==xj\\npŒi; j \\x8dD2\\nbŒi; j \\x8dD“.”\\nelsepŒi; j \\x8dD1\\nbŒi; j \\x8dD“#”\\npŒn; n\\x8dD1\\nforiDn/NUL2downto 1\\nforjDiC2ton\\nifxi==xj\\npŒi; j \\x8dDpŒiC1; j/NUL1\\x8dC2\\nbŒi; j \\x8dD“.”\\nelseif pŒiC1; j \\x8d\\x15pŒi; j/NUL1\\x8d\\npŒi; j \\x8dDpŒiC1; j \\x8d\\nbŒi; j \\x8dD“#”\\nelsepŒi; j \\x8dDpŒi; j/NUL1\\x8d\\nbŒi; j \\x8dD“ ”\\nreturn pandb', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 283}),\n",
              " Document(page_content='15-34 Solutions for Chapter 15: Dynamic Programming\\nStep 4: Constructingan LPS\\nThebtable returned by L ONGEST-PALINDROME enables us to quickly construct\\nan LPS of XDhx1; x2; : : : ; x mi. We simply begin at bŒ1; n\\x8dand trace through\\nthe table by following the arrows. Whenever we encounter a “ .” in entry bŒi; j \\x8d,\\nit implies that xiDyjare the ﬁrst and last elements of the LPS that L ONGEST-\\nPALINDROME found. Thefollowingrecursive procedure returns asequenc eSthat\\ncontains an LPS of X. The initial call is G ENERATE -LPS .b; X; 1; X: length ;hi/,\\nwherehidenotes an empty sequence. Within the procedure, the symbol jjdenotes\\nconcatenation of asymbol and asequence.\\nGENERATE -LPS .b; X; i; j; S/\\nifi > j\\nreturn S\\nelseif i==j\\nreturn Sjjxi\\nelseif bŒi; j \\x8d==“.”\\nreturn xijjGENERATE -LPS .b; X; iC1; j/NUL1; S/jjxi\\nelseif bŒi; j \\x8d==“#”\\nreturnGENERATE -LPS .b; X; iC1; j; S/\\nelse return GENERATE -LPS .b; X; i; j/NUL1; S/\\nSolution to Problem 15-3\\nTakingthebook’shint,wesortthepointsby x-coordinate, lefttoright,in O.nlgn/\\ntime. Let the sorted points be, left to right, hp1; p2; p3; : : : ; p ni. Therefore, p1is\\nthe leftmost point, and pnisthe rightmost.\\nWe deﬁne as our subproblems paths of the following form, whic h we call bitonic\\npaths. A bitonic path Pi;j, where i\\x14j, includes all points p1; p2; : : : ; p j; it\\nstarts at some point pi, goes strictly left to point p1, and then goes strictly right to\\npoint pj. By“goingstrictlyleft,”wemeanthateachpointinthepath hasalower x-\\ncoordinatethanthepreviouspoint. Lookedatanotherway,t heindicesofthesorted\\npoints form a strictly decreasing sequence. Likewise, “goi ng strictly right” means\\nthat the indices of the sorted points form astrictly increas ing sequence. Moreover,\\nPi;jcontains all the points p1; p2; p3; : : : ; p j. Note that pjis the rightmost point\\ninPi;jandisontherightgoing subpath. Theleftgoing subpath mayb edegenerate,\\nconsisting of just p1.\\nLet us denote the euclidean distance between any two points piandpjbyjpipjj.\\nAnd let us denote by bŒi; j \\x8d, for1\\x14i\\x14j\\x14n, the length of the shortest bitonic\\npathPi;j. Since the leftgoing subpath may be degenerate, we can easil y compute\\nallvalues bŒ1; j \\x8d. Theonlyvalueof bŒi; i\\x8dthatwewillneedis bŒn; n\\x8d,whichisthe\\nlength oftheshortest bitonictour. Wehavethefollowingfo rmulation of bŒi; j \\x8dfor\\n1\\x14i\\x14j\\x14n:\\nbŒ1; 2\\x8dDjp1p2j;\\nbŒi; j \\x8dDbŒi; j/NUL1\\x8dCjpj/NUL1pjjfori < j/NUL1 ;\\nbŒj/NUL1; j \\x8dDmin\\n1\\x14k<j /NUL1fbŒk; j/NUL1\\x8dCjpkpjjg:', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 284}),\n",
              " Document(page_content='Solutions forChapter 15: Dynamic Programming 15-35\\nWhyarethese formulas correct? Anybitonic path ending at p2hasp2as its right-\\nmost point, soit consists only of p1andp2. Itslength, therefore, is jp1p2j.\\nNow consider a shortest bitonic path Pi;j. The point pj/NUL1is somewhere on this\\npath. If it is on the rightgoing subpath, then it immediately preceeds pjon this\\nsubpath. Otherwise, it is on the leftgoing subpath, and it mu st be the rightmost\\npoint on this subpath, so iDj/NUL1. In the ﬁrst case, the subpath from pitopj/NUL1\\nmust be a shortest bitonic path Pi;j/NUL1, for otherwise we could use a cut-and-paste\\nargument tocomeupwithashorter bitonic paththan Pi;j. (Thisispartofouropti-\\nmalsubstructure.) Thelengthof Pi;j,therefore, isgivenby bŒi; j/NUL1\\x8dCjpj/NUL1pjj.\\nIn the second case, pjhas an immediate predecessor pk, where k < j/NUL1, on\\nthe rightgoing subpath. Optimal substructure again applie s: the subpath from pk\\ntopj/NUL1mustbeashortest bitonic path Pk;j/NUL1,forotherwise wecouldusecut-and-\\npaste to come up with a shorter bitonic path than Pi;j. (We have implicitly relied\\non paths having the same length regardless of which directio n we traverse them.)\\nThelength of Pi;j, therefore, isgiven by min 1\\x14k\\x14j/NUL1fbŒk; j/NUL1\\x8dCjpkpjjg.\\nWe need to compute bŒn; n\\x8d. In an optimal bitonic tour, one of the points adjacent\\ntopnmust be pn/NUL1,and so wehave\\nbŒn; n\\x8dDbŒn/NUL1; n\\x8dCjpn/NUL1pnj:\\nTo reconstruct the points on the shortest bitonic tour, we de ﬁnerŒi; j \\x8dto be the\\nindexoftheimmediatepredecessor of pjontheshortestbitonicpath Pi;j. Because\\nthe immediate predecessor of p2onP1;2isp1, we know that rŒ1; 2\\x8dmust be 1.\\nThe pseudocode below shows how we compute bŒi; j \\x8dandrŒi; j \\x8d. It ﬁlls in only\\nentries bŒi; j \\x8dwhere 1\\x14i\\x14n/NUL1andiC1\\x14j\\x14n, or where iDjDn, and\\nonly entries rŒi; j \\x8dwhere 1\\x14i\\x14n/NUL2andiC2\\x14j\\x14n.\\nEUCLIDEAN -TSP .p/\\nsort the points sothat hp1; p2; p3; : : : ; p niare inorder of increasing x-coordinate\\nletbŒ1 : : n; 2 : : n\\x8d andrŒ1 : : n/NUL2; 3 : : n\\x8dbe new arrays\\nbŒ1; 2\\x8dDjp1p2j\\nforjD3ton\\nforiD1toj/NUL2\\nbŒi; j \\x8dDbŒi; j/NUL1\\x8dCjpj/NUL1pjj\\nrŒi; j \\x8dDj/NUL1\\nbŒj/NUL1; j \\x8dD1\\nforkD1toj/NUL2\\nqDbŒk; j/NUL1\\x8dCjpkpjj\\nifq < bŒj/NUL1; j \\x8d\\nbŒj/NUL1; j \\x8dDq\\nrŒj/NUL1; j \\x8dDk\\nbŒn; n\\x8dDbŒn/NUL1; n\\x8dCjpn/NUL1pnj\\nreturn bandr\\nWe print out the tour we found by starting at pn, then a leftgoing subpath that\\nincludes pn/NUL1,fromright toleft, until wehit p1. Thenweprint right-to-left there-\\nmainingsubpath, whichdoesnotinclude pn/NUL1. FortheexampleinFigure15.11(b)\\non page 405, we wish to print the sequence p7; p6; p4; p3; p1; p2; p5. Our code is\\nrecursive. The right-to-left subpath is printed as we go dee per into the recursion,\\nand the left-to-right subpath isprinted asweback out.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 285}),\n",
              " Document(page_content='15-36 Solutions for Chapter 15: Dynamic Programming\\nPRINT-TOUR.r; n/\\nprint pn\\nprint pn/NUL1\\nkDrŒn/NUL1; n\\x8d\\nPRINT-PATH(r; k; n/NUL1)\\nprint pk\\nPRINT-PATH.r; i; j /\\nifi < j\\nkDrŒi; j \\x8d\\nifk¤i\\nprint pk\\nifk > 1\\nPRINT-PATH(r; i; k)\\nelsekDrŒj; i\\x8d\\nifk > 1\\nPRINT-PATH(r; k; j)\\nprint pk\\nThe relative values of the parameters iandjin each call of P RINT-PATHindicate\\nwhich subpath we’re working on. If i < j, we’re on the right-to-left subpath, and\\nifi > j, we’re on the left-to-right subpath. The test for k¤iprevents us from\\nprinting p1anextra time, which could occur whenwecall P RINT-PATH.r; 1; 2/.\\nThetimetorun E UCLIDEAN -TSP is O.n2/sincetheouter loopon jiterates n/NUL2\\ntimesandtheinner loops on iandkeachrunatmost n/NUL2times. Thesorting step\\nat the beginning takes O.nlgn/time, which the loop times dominate. The timeto\\nrun PRINT-TOURisO.n/, since each point is printed just once.\\nSolution to Problem 15-4\\nThis solutionisalsopostedpublicly\\nNote: We assume that no word is longer than will ﬁt into a line, i.e.,li\\x14Mfor\\nalli.\\nFirst,we’llmakesomedeﬁnitionssothatwecanstatethepro blemmoreuniformly.\\nSpecialcasesaboutthelastlineandworriesaboutwhethera sequenceofwordsﬁts\\ninalinewillbehandledinthesedeﬁnitions, sothatwecanfo rgetaboutthemwhen\\nframing our overall strategy.\\n\\x0fDeﬁneextras Œi; j \\x8dDM/NULjCi/NULPj\\nkDilkto be the number of extra spaces\\nat the end of a line containing words ithrough j. Note that extrasmay be\\nnegative.\\n\\x0fNowdeﬁnethecostofincluding alinecontaining words ithrough jinthesum\\nwewant to minimize:\\nlcŒi; j \\x8dD\\x80\\n1 ifextras Œi; j \\x8d < 0 (i.e., words i; : : : ; jdon’t ﬁt) ;\\n0 ifjDnandextras Œi; j \\x8d\\x150(last line costs 0) ;\\n.extras Œi; j \\x8d/3otherwise :', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 286}),\n",
              " Document(page_content='Solutions forChapter 15: Dynamic Programming 15-37\\nBymakingthe linecost inﬁnite whenthewordsdon’t ﬁtonit, w eprevent such\\nanarrangement frombeingpartofaminimalsum,andbymaking thecost0for\\nthe last line (if the words ﬁt), we prevent the arrangement of the last line from\\ninﬂuencing the sum being minimized.\\nWewant tominimize thesum of lcover all lines of theparagraph.\\nOur subproblems are how to optimally arrange words 1; : : : ; j, where jD\\n1; : : : ; n.\\nConsider an optimal arrangement of words 1; : : : ; j. Suppose we know that the\\nlastline,whichendsinword j,beginswithword i. Thepreceding lines,therefore,\\ncontain words 1; : : : ; i/NUL1. In fact, they must contain an optimal arrangement of\\nwords 1; : : : ; i/NUL1. (The usual type of cut-and-paste argument applies.)\\nLetcŒj \\x8dbe the cost of an optimal arrangement of words 1; : : : ; j. If weknow that\\nthelastlinecontainswords i; : : : ; j,then cŒj \\x8dDcŒi/NUL1\\x8dClcŒi; j \\x8d. Asabasecase,\\nwhen we’re computing cŒ1\\x8d,we need cŒ0\\x8d. If weset cŒ0\\x8dD0, then cŒ1\\x8dDlcŒ1; 1\\x8d,\\nwhich iswhat wewant.\\nBut of course we have to ﬁgure out which word begins the last li ne for the sub-\\nproblem of words 1; : : : ; j. So we try all possibilities for word i, and we pick the\\nonethatgivesthelowestcost. Here, irangesfrom 1toj. Thus,wecandeﬁne cŒj \\x8d\\nrecursively by\\ncŒj \\x8dD(\\n0 ifjD0 ;\\nmin\\n1\\x14i\\x14j.cŒi/NUL1\\x8dClcŒi; j \\x8d/ifj > 0 :\\nNotethat the waywedeﬁned lcensures that\\n\\x0fall choices made will ﬁt on the line (since an arrangement wit hlcD1cannot\\nbe chosen asthe minimum), and\\n\\x0fthecost ofputting words i; : : : ; jonthelastlinewillnot be0unless thisreally\\nis thelast line of the paragraph ( jDn)or words i : : : jﬁll the entire line.\\nWe can compute a table of cvalues from left to right, since each value depends\\nonly on earlier values.\\nTo keep track of what words go on what lines, we can keep a paral lelptable that\\npoints to where each cvalue came from. When cŒj \\x8dis computed, if cŒj \\x8dis based\\non the value of cŒk/NUL1\\x8d, setpŒj \\x8dDk. Then after cŒn\\x8dis computed, we can trace\\nthe pointers to see where to break the lines. The last line sta rts at word pŒn\\x8dand\\ngoes through word n. The previous line starts at word pŒpŒn\\x8d\\x8dand goes through\\nword pŒn\\x8d/NUL1, etc.\\nInpseudocode, here’s how weconstruct the tables:', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 287}),\n",
              " Document(page_content='15-38 Solutions for Chapter 15: Dynamic Programming\\nPRINT-NEATLY .l; n; M /\\nletextras Œ1 : : n; 1 : : n\\x8d ,lcŒ1 : : n; 1 : : n\\x8d ,and cŒ0 : : n\\x8dbe new arrays\\n//Compute extras Œi; j \\x8dfor1\\x14i\\x14j\\x14n.\\nforiD1ton\\nextras Œi; i\\x8dDM/NULli\\nforjDiC1ton\\nextras Œi; j \\x8dDextras Œi; j/NUL1\\x8d/NULlj/NUL1\\n//Compute lcŒi; j \\x8dfor1\\x14i\\x14j\\x14n.\\nforiD1ton\\nforjDiton\\nifextras Œi; j \\x8d < 0\\nlcŒi; j \\x8dD1\\nelseif j==nandextras Œi; j \\x8d\\x150\\nlcŒi; j \\x8dD0\\nelselcŒi; j \\x8dD.extras Œi; j \\x8d/3\\n//Compute cŒj \\x8dandpŒj \\x8dfor1\\x14j\\x14n.\\ncŒ0\\x8dD0\\nforjD1ton\\ncŒj \\x8dD1\\nforiD1toj\\nifcŒi/NUL1\\x8dClcŒi; j \\x8d < cŒj \\x8d\\ncŒj \\x8dDcŒi/NUL1\\x8dClcŒi; j \\x8d\\npŒj \\x8dDi\\nreturn candp\\nQuite clearly, both the timeand space are ‚.n2/.\\nInfact,wecandoabitbetter: wecangetboththetimeandspac edownto ‚.nM /.\\nThe key observation is that at most dM=2ewords can ﬁt on a line. (Each word is\\nat least one character long, and there’s a space between word s.) Since a line with\\nwords i; : : : ; jcontains j/NULiC1words, if j/NULiC1 >dM=2ethen we know\\nthatlcŒi; j \\x8dD1. We need only compute and store extras Œi; j \\x8dandlcŒi; j \\x8dfor\\nj/NULiC1\\x14dM=2e. And the inner forloop header in the computation of cŒj \\x8d\\nandpŒj \\x8dcan run from max .1; j/NULdM=2eC1/toj.\\nWe can reduce the space even further to ‚.n/. We do so by not storing the lc\\nandextrastables, and instead computing the value of lcŒi; j \\x8das needed in the last\\nloop. The idea is that we could compute lcŒi; j \\x8dinO.1/time if we knew the\\nvalue ofextras Œi; j \\x8d. And if we scan for the minimum value in descending order\\nofi, we can compute that as extras Œi; j \\x8dDextras ŒiC1; j \\x8d/NULli/NUL1. (Initially,\\nextras Œj; j \\x8dDM/NULlj.) This improvement reduces the space to ‚.n/, since now\\nthe only tables westore are candp.\\nHere’s how we print which words are on which line. The printed output of\\nGIVE-LINES.p; j /isasequence of triples .k; i; j /, indicating that words i; : : : ; j\\nare printed on line k. Thereturn value isthe line number k.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 288}),\n",
              " Document(page_content='Solutions forChapter 15: Dynamic Programming 15-39\\nGIVE-LINES.p; j /\\niDpŒj \\x8d\\nifi==1\\nkD1\\nelsekDGIVE-LINES.p; i/NUL1/C1\\nprint .k; i; j /\\nreturn k\\nTheinitial callis G IVE-LINES.p; n/. Sincethevalueof jdecreases ineachrecur-\\nsivecall, G IVE-LINEStakes atotal of O.n/time.\\nSolutionto Problem 15-5\\na.Dynamic programming is the ticket. This problem is slightly similar to the\\nlongest-common-subsequence problem. Infact,we’lldeﬁne thenotational con-\\nveniences XiandYjin the similar manner as we did for the LCS problem:\\nXiDxŒ1 : : i\\x8dandYjDyŒ1 : : j \\x8d.\\nOur subproblems will be determining an optimal sequence of o perations that\\nconverts XitoYj, for0\\x14i\\x14mand0\\x14j\\x14n. We’ll call this the “ Xi!Yj\\nproblem.” Theoriginal problem isthe Xm!Ynproblem.\\nLet’ssuppose for themomentthat weknowwhatwasthelastope ration usedto\\nconvert XitoYj. Therearesixpossibilities. Wedenote by cŒi; j \\x8dthecost ofan\\noptimal solution tothe Xi!Yjproblem.\\n\\x0fIfthelastoperationwasacopy,thenwemusthavehad xŒi\\x8dDyŒj \\x8d. Thesub-\\nproblem that remains isconverting Xi/NUL1toYj/NUL1. Andanoptimal solution to\\ntheXi!Yjproblem must include an optimal solution to the Xi/NUL1!Yj/NUL1\\nproblem. The cut-and-paste argument applies. Thus, assumi ng that the last\\noperation wasacopy, wehave cŒi; j \\x8dDcŒi/NUL1; j/NUL1\\x8dCcost.copy/.\\n\\x0fIf it was a replace, then we must have had xŒi\\x8d¤yŒj \\x8d. (Here, we assume\\nthat we cannot replace a character with itself. It is a straig htforward mod-\\niﬁcation if we allow replacement of a character with itself. ) We have the\\nsame optimal substructure argument as for copy, and assumin g that the last\\noperation wasareplace, wehave cŒi; j \\x8dDcŒi/NUL1; j/NUL1\\x8dCcost.replace /.\\n\\x0fIf it was a twiddle, then we must have had both xŒi\\x8dDyŒj/NUL1\\x8dand\\nxŒi/NUL1\\x8dDyŒj \\x8d, along with the implicit assumption that i; j\\x152. Now\\nour subproblem is Xi/NUL2!Yj/NUL2and, assuming that the last operation was a\\ntwiddle, wehave cŒi; j \\x8dDcŒi/NUL2; j/NUL2\\x8dCcost.twiddle /.\\n\\x0fIf it was adelete, then wehave no restrictions on xory. Since wecan view\\ndelete as removing a character from Xiand leaving Yjalone, our subprob-\\nlem is Xi/NUL1!Yj. Assuming that the last operation was a delete, we have\\ncŒi; j \\x8dDcŒi/NUL1; j \\x8dCcost.delete /.\\n\\x0fIf it was an insert, then we have no restrictions on xory. Our subproblem\\nisXi!Yj/NUL1. Assuming that the last operation was an insert, we have\\ncŒi; j \\x8dDcŒi; j/NUL1\\x8dCcost.insert /.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 289}),\n",
              " Document(page_content='15-40 Solutions for Chapter 15: Dynamic Programming\\n\\x0fIf it was a kill, then we had to have completed converting XmtoYn, so that\\nthecurrentproblemmustbethe Xm!Ynproblem. Inotherwords,wemust\\nhave iDmandjDn. If we think of a kill as amultiple delete, wecan get\\nanyXi!Yn, where 0\\x14i < m, as a subproblem. We pick the best one,\\nand so assuming that the last operation wasakill, wehave\\ncŒm; n\\x8dDmin\\n0\\x14i<mfcŒi; n\\x8dgCcost.kill/ :\\nWe have not handled the base cases, in which iD0orjD0. These are\\neasy. X0andY0are the empty strings. We convert an empty string into Yjby\\nasequence of jinserts, so that cŒ0; j \\x8dDj\\x01cost.insert /. Similarly, weconvert\\nXiintoY0by a sequence of ideletes, so that cŒi; 0\\x8dDi\\x01cost.delete /. When\\niDjD0, either formula gives us cŒ0; 0\\x8dD0, which makes sense, since\\nthere’s no cost toconvert theempty string to theempty strin g.\\nFori; j > 0,ourrecursive formulation for cŒi; j \\x8dapplies theaboveformulasin\\nthe situations inwhich they hold:\\ncŒi; j \\x8dDmin†\\ncŒi/NUL1; j/NUL1\\x8dCcost.copy/ifxŒi\\x8dDyŒj \\x8d ;\\ncŒi/NUL1; j/NUL1\\x8dCcost.replace /ifxŒi\\x8d¤yŒj \\x8d ;\\ncŒi/NUL2; j/NUL2\\x8dCcost.twiddle /ifi; j\\x152;\\nxŒi\\x8dDyŒj/NUL1\\x8d;\\nandxŒi/NUL1\\x8dDyŒj \\x8d ;\\ncŒi/NUL1; j \\x8dCcost.delete / always ;\\ncŒi; j \\x8dDcŒi; j/NUL1\\x8dCcost.insert /always ;\\nmin\\n0\\x14i<mfcŒi; n\\x8dgCcost.kill/ ifiDmandjDn :\\nLike we did for LCS, our pseudocode ﬁlls in the table in row-ma jor order, i.e.,\\nrow-by-row from top to bottom, and left to right within each r ow. Column-\\nmajor order (column-by-column from left to right, and top to bottom within\\neach column) wouldalso work. Along withthe cŒi; j \\x8dtable, weﬁllin thetable\\nopŒi; j \\x8d,holding which operation wasused.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 290}),\n",
              " Document(page_content='Solutions forChapter 15: Dynamic Programming 15-41\\nEDIT-DISTANCE .x; y; m; n/\\nletcŒ0 : : m; 0 : : n\\x8d andopŒ0 : : m; 0 : : n\\x8d benew arrays\\nforiD0tom\\ncŒi; 0\\x8dDi\\x01cost.delete /\\nopŒi; 0\\x8dDDELETE\\nforjD0ton\\ncŒ0; j \\x8dDj\\x01cost.insert /\\nopŒ0; j \\x8dDINSERT\\nforiD1tom\\nforjD1ton\\ncŒi; j \\x8dD1\\nifxŒi\\x8d==yŒj \\x8d\\ncŒi; j \\x8dDcŒi/NUL1; j/NUL1\\x8dCcost.copy/\\nopŒi; j \\x8dDCOPY\\nifxŒi\\x8d¤yŒj \\x8dandcŒi/NUL1; j/NUL1\\x8dCcost.replace / < cŒi; j \\x8d\\ncŒi; j \\x8dDcŒi/NUL1; j/NUL1\\x8dCcost.replace /\\nopŒi; j \\x8dDREPLACE (byyŒj \\x8d)\\nifi\\x152andj\\x152andxŒi\\x8d==yŒj/NUL1\\x8dand\\nxŒi/NUL1\\x8d==yŒj \\x8dand\\ncŒi/NUL2; j/NUL2\\x8dCcost.twiddle / < cŒi; j \\x8d\\ncŒi; j \\x8dDcŒi/NUL2; j/NUL2\\x8dCcost.twiddle /\\nopŒi; j \\x8dDTWIDDLE\\nifcŒi/NUL1; j \\x8dCcost.delete / < cŒi; j \\x8d\\ncŒi; j \\x8dDcŒi/NUL1; j \\x8dCcost.delete /\\nopŒi; j \\x8dDDELETE\\nifcŒi; j/NUL1\\x8dCcost.insert / < cŒi; j \\x8d\\ncŒi; j \\x8dDcŒi; j/NUL1\\x8dCcost.insert /\\nopŒi; j \\x8dDINSERT(yŒj \\x8d)\\nforiD0tom/NUL1\\nifcŒi; n\\x8dCcost.kill/ < cŒm; n\\x8d\\ncŒm; n\\x8dDcŒi; n\\x8dCcost.kill/\\nopŒm; n\\x8dDKILL i\\nreturn candop\\nThe time and space are both ‚.mn/. If we store a KILLoperation in opŒm; n\\x8d,\\nwe also include the index iafter which we killed, to help us reconstruct the\\noptimal sequence of operations. (Wedon’t need tostore yŒi\\x8dintheoptable for\\nreplace or insert operations.)\\nToreconstruct thissequence, weusethe optablereturned by E DIT-DISTANCE.\\nThe procedure O P-SEQUENCE .op; i; j /reconstructs the optimal operation se-\\nquence that we found to transform XiintoYj. The base case is when\\niDjD0. Theﬁrst call is O P-SEQUENCE .op; m; n/.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 291}),\n",
              " Document(page_content='15-42 Solutions for Chapter 15: Dynamic Programming\\nOP-SEQUENCE .op; i; j /\\nifi==0andjD0\\nreturn\\nifopŒi; j \\x8d==COPYoropŒi; j \\x8dDREPLACE\\ni0Di/NUL1\\nj0Dj/NUL1\\nelseifopŒi; j \\x8d==TWIDDLE\\ni0Di/NUL2\\nj0Dj/NUL2\\nelseifopŒi; j \\x8d==DELETE\\ni0Di/NUL1\\nj0Dj\\nelseifopŒi; j \\x8d==INSERT//don’t care yet what character is inserted\\ni0Di\\nj0Dj/NUL1\\nelse//must be KILL, and must have iDmandjDn\\nletopŒi; j \\x8d==KILL k\\ni0Dk\\nj0Dj\\nOP-SEQUENCE .op; i0; j0/\\nprintopŒi; j \\x8d\\nThisprocedure determines which subproblem weused, recurs es onit,and then\\nprints itsownlast operation.\\nb.TheDNA-alignment problem isjust the edit-distance proble m, with\\ncost.copy/D /NUL 1 ;\\ncost.replace /D C 1 ;\\ncost.delete /D C 2 ;\\ncost.insert /D C 2 ;\\nand the twiddle and kill operations arenot permitted.\\nThe score that we are trying to maximize in the DNA-alignment problem is\\nprecisely thenegative of the cost weare trying tominimize i n the edit-distance\\nproblem. The negative cost of copy is not an impediment, sinc e we can only\\napply thecopy operation when thecharacters areequal.\\nSolution to Problem 15-8\\na.Let us set up a recurrence for the number of valid seams as a fun ction of m.\\nSuppose we are in the process of carving out a seam row by row, s tarting from\\nthe ﬁrst row. Let the last pixel carved out be AŒi; j \\x8d. Howmany choices do we\\nhaveforthepixelinrow iC1suchthatthepixelcontinuestheseam? Ifthelast\\npixel AŒi; j \\x8dwere on the column boundary ( iD1oriDn), then there would\\nbe two choices for the next pixel. For example, when iD1, the two choices\\nfor the next pixel are AŒiC1; j \\x8dandAŒiC1; jC1\\x8d. Otherwise, there would', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 292}),\n",
              " Document(page_content='Solutions forChapter 15: Dynamic Programming 15-43\\nbethreechoicesforthenextpixel: AŒiC1; j/NUL1\\x8d; AŒiC1; j \\x8d; AŒiC1; jC1\\x8d.\\nThus, for a general pixel AŒi; j \\x8d, there are at least two possible choices for a\\npixel pinthenextrowsuchthat pcontinues aseamendingin AŒi; j \\x8d. Let T .i/\\ndenote the number of possible seams from row 1 to row i. Then, for iD1, we\\nhave T .i/Dn, and for i > 1,\\nT .i/\\x152T .i/NUL1/ :\\nIt is easy to guess that T .i/\\x15n2i/NUL1, which we verify by direct substitution.\\nForiD1, wehave T .1/Dn\\x15n\\x0120. For i > 1, wehave\\nT .i/\\x152T .i/NUL1/\\n\\x152\\x01n2i/NUL2\\nDn2i/NUL1:\\nThus, the total number T .m/of seams is at least n2m/NUL1. We conclude that the\\nnumber of seams grows at least exponentially in m.\\nb.As proved in the previous part, it is infeasible to systemati cally check every\\nseam, since the number of possible seams grows exponentiall y.\\nThe structure of the problem allows us to build the solution r ow by row. Con-\\nsider a pixel AŒi; j \\x8d. We ask the question: “If iwere the ﬁrst row of the\\npicture, what is the minimum disruptive measure of seams tha t start with the\\npixel AŒi; j \\x8d?”\\nLetS\\x03be a seam of minimum disruptive measure among all seams that s tart\\nwith pixel AŒi; j \\x8d. Let AŒiC1; p\\x8d, where p2fj/NUL1; j; jC1g, be the pixel\\nofS\\x03inthenextrow. Let S0bethesub-seam of S\\x03thatstartswith AŒiC1; p\\x8d.\\nWe claim that S0has the minimum disruptive measure among seams that start\\nwith AŒiC1; p\\x8d. Why? Suppose there exists another seam S00that starts\\nwithAŒiC1; p\\x8dandhasdisruptivemeasurelessthanthatof S0. Byusing S00as\\nthe sub-seam instead of S0, we can obtain another seam that starts with AŒi; j \\x8d\\nand has a disruptive measure which is less than that of S\\x03. Thus, we obtain a\\ncontradiction to our assumption that S\\x03is a seam of minimum disruptive mea-\\nsure.\\nLetdisrŒi; j \\x8dbethevalueof theminimumdisruptive measure amongall seam s\\nthat start with pixel AŒi; j \\x8d. For row m, the seam with the minimum disruptive\\nmeasure consists of just one point. Wecan now state arecurre nce fordisrŒi; j \\x8d\\nas follows. In the base case, disrŒm; j \\x8dDdŒm; j \\x8dforjD1; 2; : : : ; n . In the\\nrecursive case, for jD1; 2; : : : ; n ,\\ndisrŒi; j \\x8dDdŒi; j \\x8dCmin\\nk2KfdisrŒiCi; jCk\\x8dg;\\nwhere the set Kof index offsets is\\nKD\\x80\\nf0; 1gifjD1 ;\\nf/NUL1; 0; 1gif1 < j < m ;\\nf/NUL1; 0gifjDn :\\nSince every seam has to start with a pixel of the ﬁrst row, we si mply ﬁnd the\\nminimum disrŒ1; j \\x8dfor pixels intheﬁrst rowtoobtain the minimum disruptive\\nmeasure.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 293}),\n",
              " Document(page_content='15-44 Solutions for Chapter 15: Dynamic Programming\\nCOMPRESS -IMAGE .d/\\nmDd:rows\\nnDd:columns\\nletdisrŒ1 : : m; 1 : : n\\x8d andnextŒ1 : : m; 1 : : n\\x8d benew tables\\nforjD1ton\\ndisrŒm; j \\x8dDdŒm; j \\x8d\\nforiDm/NUL1downto 1\\nforjD1ton\\nlowDmax./NUL1; 1/NULj /\\nhighDmin.1; n/NULj /\\ndisrŒi; j \\x8dD1\\nforkDlowtohigh\\nifdisrŒiC1; jCk\\x8d <disrŒi; j \\x8d\\ndisrŒi; j \\x8dDdisrŒiC1; jCk\\x8d\\nnextŒi; j \\x8dDjCk\\ndisrŒi; j \\x8dDdisrŒi; j \\x8dCdŒi; j \\x8d\\n\\x17alD1\\nstartD1\\nforjD1ton\\nifdisrŒ1; j \\x8d < \\x17 al\\n\\x17alDdisrŒ1; j \\x8d\\nstartDj\\nprint “The minimum value of the disruptive measure is ” \\x17al\\nforiD1tom\\nprint “cut point at ” .i;start/\\nstartDnextŒi;start\\x8d\\nThe procedure C OMPRESS -IMAGEis simply an implementation of this recur-\\nrence in abottom-up fashion.\\nWe ﬁrst carry out the initialization of the base cases, which are the cases when\\nrowiDm. The minimum disruptive measure for the base cases is sim-\\nplydŒm; j \\x8d.\\nThe next forloop runs down from m/NUL1to1. Thus,disrŒiC1; j \\x8dis already\\navailable before computing disrŒi; j \\x8dfor pixels of row i.\\nThe assignments to lowandhighallow the index offset kto range over the\\ncorrect set Kfrom above. Weset lowto0when jD1and to/NUL1when j > 1,\\nandweset highto0when jDnandto 1when j < n. Theinnermost forloop\\nsetsdisrŒi; j \\x8dtotheminimumvalueof disrŒiC1; jCk\\x8dforall k2K,andthe\\nline that follows this loop adds in dŒi; j \\x8d.\\nWeusethe nexttabletoreconstruct theactualseam. Foragivenpixel,itre cords\\nwhich pixel was used as the next pixel. Speciﬁcally, for a pix elAŒi; j \\x8d, if\\nnextŒi; j \\x8dDp, where p2fj/NUL1; j; jC1g, then the next pixel of the seam\\nisAŒiC1; p\\x8d.\\nThe last line of the forloop adds the disruptive measure of the current pixel to\\nthe disruptive measure of the seam.\\nThe next forloop ﬁnds the minimum disruptive measure of pixels in the ﬁrs t\\nrow. Weprint the minimum disruptive measure asthe answer.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 294}),\n",
              " Document(page_content='Solutions forChapter 15: Dynamic Programming 15-45\\nThe rest of the code reconstructs the actual seam, using the i nformation stored\\nin thenextarray.\\nNoting that the innermost forloop runs over at most three values of k, we see\\nthat the running timeof C OMPRESS -IMAGEisO.mn/. Thespace requirement\\nis also O.mn/. Wecan improve upon the space requirement by observing that\\nrowiof thedisrtable depends on only row iC1. Therefore, we can store\\njust two rows at any time. Thus, we can improve the space requi rement of\\nCOMPRESS -IMAGEtoO.n/.\\nSolutionto Problem 15-9\\nOur ﬁrst step will be to identify the subproblems that satisf y the optimal-\\nsubstructure property. Before weframe the subproblem, wem ake twosimplifying\\nmodiﬁcations to the input:\\n\\x0fWesort Lsothat the indices in Larein ascending order.\\n\\x0fWeprepend the index 0tothe beginning of Land append nto theend of L.\\nLetLŒi : : j \\x8ddenote a subarray of Lthat starts from index iand ends at index j.\\nDeﬁne the subproblem denoted by .i; j /as “What is the cheapest sequence of\\nbreaks to break the substring SŒLŒi\\x8dC1 : : LŒj \\x8d\\x8d ?” Note that the ﬁrst and last\\nelements of the subarray LŒi : : j \\x8ddeﬁne the ends of the substring, and we have to\\nworry about only the indices of the subarray LŒiC1 : : j/NUL1\\x8d.\\nFor example, let LDh20; 17; 14; 11; 25iandnD30. First, we sort L. Then, we\\nprepend 0and append nas explained to get LDh0; 11; 14; 17; 20; 25; 30 i. Now,\\nwhatisthesubproblem .2; 6/? Weobtainasubstring bybreaking Saftercharacter\\nLŒ2\\x8dD11and character LŒ6\\x8dD25. We ask “What is the cheapest sequence of\\nbreaks tobreak thesubstring SŒ12 : : 25\\x8d ?” Wehave toworry about only indices in\\nthe subarray LŒ3 : : 5\\x8dDh14; 17; 20i, since the other indices are not present in the\\nsubstring.\\nAt this point, the problem looks similar to matrix-chain mul tiplication (see Sec-\\ntion 15.2). Wecan makethe ﬁrst break at any element of LŒiC1 : : j/NUL1\\x8d.\\nSupposethatanoptimalsequenceofbreaks \\x1bforsubproblem .i; j /makestheﬁrst\\nbreak at LŒk\\x8d,where i < k < j . This break gives rise to twosubproblems:\\n\\x0fThe“preﬁx” subproblem .i; k/, covering the subarray LŒiC1 : : k/NUL1\\x8d,\\n\\x0fThe“sufﬁx” subproblem .k; j /,covering the subarray LŒkC1 : : j/NUL1\\x8d.\\nThe overall cost can be expressed as the sum of the length of th e substring, the\\npreﬁx cost, and the sufﬁx cost.\\nWeshow optimal substructure byclaiming that thesequence o f breaks in \\x1bfor the\\npreﬁx subproblem .i; k/must be an optimal one. Why? If there were a less costly\\nwaytobreakthesubstring SŒLŒi\\x8dC1 : : LŒk\\x8d\\x8d representedbythesubproblem .i; k/,\\nthen substituting that sequence of breaks in \\x1bwould produce another sequence of\\nbreaks whose cost is lower than that of \\x1b, which would be acontradiction. A sim-\\nilar observation holds for the sequence of breaks for the suf ﬁx subproblem .k; j /:\\nit must bean optimal sequence of breaks.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 295}),\n",
              " Document(page_content='15-46 Solutions for Chapter 15: Dynamic Programming\\nLetcostŒi; j \\x8ddenote the cost of the cheapest solution to subproblem .i; j /. We\\nwrite the recurrence relation for costas\\ncostŒi; j \\x8dD˚\\n0 ifj/NULi\\x141 ;\\nmin\\ni<k<jn\\ncostŒi; k\\x8dCcostŒk; j \\x8dC.LŒj \\x8d/NULLŒi\\x8d/o\\nifj/NULi > 1 :\\nThus, our approach to solving the subproblem .i; j /will be to try to split the re-\\nspective substring at all possible values of kandthen choosing abreak that results\\nin the minimum cost. We need to be careful to solve smaller sub problems before\\nwe solve larger subproblems. In particular, we solve subpro blems in increasing\\norder of the length j/NULi.\\nBREAK-STRING .n; L/\\nprepend 0to thestart of Land append ntothe end of L\\nmDL:length\\nsortLinto increasing order\\nletcostŒ1 : : m; 1 : : m\\x8d andbreak Œ1 : : m; 1 : : m\\x8d be new tables\\nforiD1tom/NUL1\\ncostŒi; i\\x8dDcostŒi; iC1\\x8dD0\\ncostŒm; m\\x8dD0\\nforlenD3tom\\nforiD1tom/NULlenC1\\njDiClen/NUL1\\ncostŒi; j \\x8dD1\\nforkDiC1toj/NUL1\\nifcostŒi; k\\x8dCcostŒk; j \\x8d <costŒi; j \\x8d\\ncostŒi; j \\x8dDcostŒi; k\\x8dCcostŒk; j \\x8d\\nbreak Œi; j \\x8dDk\\ncostŒi; j \\x8dDcostŒi; j \\x8dCLŒj \\x8d/NULLŒi\\x8d\\nprint “Theminimum cost of breaking the string is” costŒ1; m\\x8d\\nPRINT-BREAKS .L;break ; 1; m/\\nAfter sorting L, weinitialize the base cases, inwhich iDjorjDiC1.\\nThenested forloops represent themaincomputation. Theoutermost forloop runs\\nforlenD3tom,whichmeansthatweneedtoconsider subarrays of Lwithlength\\nat least 3, since the ﬁrst and the last element deﬁne the substring, and we need at\\nleastonemoreelementtospecifyabreak. Theincreasingval uesoflenalsoensures\\nthat we solve subproblems with smaller length before we solv e subproblems with\\ngreater length.\\nTheinner forloopon irunsfrom 1tom/NULlenC1. Theupperboundof m/NULlenC1\\nis thelargest value that the start index ican take such that iClen/NUL1\\x14m.\\nIn the innermost forloop, wetry each possible location kas the place to make the\\nﬁrstbreakforsubproblem .i; j /. Theﬁrstsuchplaceis LŒiC1\\x8d,andnot LŒi\\x8d,since\\nLŒi\\x8drepresents the start of the substring (and thus not a valid pl ace for a break).\\nSimilarly, the last valid place is LŒj/NUL1\\x8d, because LŒj \\x8drepresents the end of the\\nsubstring.\\nTheifcondition tests whether kis the best place for a break found so far, and\\nit updates the best value in costŒi; j \\x8dif so. We use break Œi; j \\x8dto record that the', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 296}),\n",
              " Document(page_content='Solutions forChapter 15: Dynamic Programming 15-47\\nbest place for the ﬁrst break is k. Speciﬁcally, if break Œi; j \\x8dDk, then an optimal\\nsequence of breaks for .i; j /makes the ﬁrst break at LŒk\\x8d.\\nFinally, we add the length of the substring LŒj \\x8d/NULLŒi\\x8dtocostŒi; j \\x8dbecause, irre-\\nspective of what wechoose as the ﬁrst break, it costs us apric e equal tothe length\\nof the substring to makeabreak.\\nThelowestcostfortheoriginalproblemendsupin costŒ1; m\\x8d. Byourinitialization,\\nLŒ1\\x8dD0andLŒm\\x8dDn. Thus,costŒ1; m\\x8dwill hold the optimum price of cutting\\nthe substring from LŒ1\\x8dC1D1toLŒm\\x8dDn,which is theentire string.\\nThe running time is ‚.m3/, and it is dictated by the three nested forloops. They\\nﬁll in the entries above the main diagonal of the two tables, e xcept for entries in\\nwhich jDiC1. That is, they ﬁll in rows iD1; 2; : : : ; m/NUL2, entries jD\\niC2; iC3; : : : ; m. When ﬁlling in entry Œi; j \\x8d, we check values of krunning\\nfrom iC1toj/NUL1,orj/NULi/NUL1entries. Thus,thetotal number ofiterations ofthe\\ninnermost forloop is\\nm/NUL2X\\niD1mX\\njDiC2.j/NULi/NUL1/Dm/NUL2X\\niD1m/NULi/NUL1X\\ndD1d(dDj/NULi/NUL1)\\nDm/NUL2X\\niD1‚..m/NULi/2/(equation (A.2))\\nDm/NUL1X\\nhD2‚.h2/ (hDm/NULi)\\nD‚.m3/ (equation (A.3)) .\\nSinceeachiterationoftheinnermost forlooptakesconstant time,thetotalrunning\\ntimeis ‚.m3/. Noteinparticular thattherunningtimeisindependent oft helength\\nof the string n.\\nPRINT-BREAKS .L;break ; i; j /\\nifj/NULi\\x152\\nkDbreak Œi; j \\x8d\\nprint “Break at ” LŒk\\x8d\\nPRINT-BREAKS .L;break ; i; k/\\nPRINT-BREAKS .L;break ; k; j /\\nPRINT-BREAKSuses the information stored in breakto print out the actual se-\\nquence of breaks.\\nSolutionto Problem 15-11\\nWe state the subproblem .k; s/as “What is the cheapest way to satisfy all the de-\\nmandsofmonths k; : : : ; nwhenwestartwithasurplusof sbeforethe kthmonth?”\\nAplanfor the subproblem .k; s/would specify the number of machines to manu-\\nfacture for each month k; : : : ; nsuch that demands aresatisﬁed.\\nIn some optimal plan Pto.k; s/, letf\\x03machines be maufactured in month k.\\nThus, the surplus s0in month kC1issCf\\x03/NULdk. Let P0be the part of the', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 297}),\n",
              " Document(page_content='15-48 Solutions for Chapter 15: Dynamic Programming\\nplan Pfor months kC1; : : : ; n. We claim that P0is an optimal plan for the\\nsubproblem .kC1; s0/. Why? Suppose P0were not an optimal plan and let P00\\nbe an optimal plan for .kC1; s0/. If we modify plan Pby cutting out P0and\\npasting in P00(i.e., by using plan P00for months kC1; : : : ; n), we obtain another\\nplan for .k; s/which is cheaper than plan P. Thus, we obtain a contradiction to\\nthe assumption that plan Pwas optimal.\\nLetcostŒk; s\\x8ddenote the cost of an optimal plan for .k; s/, and let fdenote the\\nnumber of machines that can be manufactured in month k. The bounds for fare\\nas follows:\\n\\x0fAtleastthenumber ofmachinessothat(along withsurplus s)thereareenough\\nmachinestosatisfythecurrentmonth’sdemand. Letusdenot ethislowerbound\\nbyL.k; s/. Wehave\\nL.k; s/Dmax.dk/NULs; 0/ :\\n\\x0fAt most the number of machines such that there are enough mach ines to sat-\\nisfy the demands of all the following months. Let us denote th is upper bound\\nbyU.k; s/. Wehave\\nU.k; s/D nX\\niDkdi!\\n/NULs :\\nFor the last month, we need only manufacture the minimum requ ired number of\\nmachines, given by L.n; s/. For other months, we examine the costs of manufac-\\nturingallfeasiblenumbersofmachinesandseewhichchoice givesusthecheapest\\nplan. Wecan now write the recurrence for costas the following:\\ncostŒk; s\\x8dD‚\\nc\\x01max.L.n; s//NULm; 0/\\nCh.sCL.n; s//NULdn/ ifkDn ;\\nmin\\nL.k;s/ \\x14f\\x14U.k;s/n\\ncostŒkC1; sCf/NULdk\\x8d\\nCc\\x01max.f/NULm; 0/\\nCh.sCf/NULdk/o\\nif0 < k < n :\\nThe recurrence suggests how to build an optimal plan in a bott om-up fashion. We\\nnow present thealgorithm for constructing anoptimal plan.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 298}),\n",
              " Document(page_content='Solutions forChapter 15: Dynamic Programming 15-49\\nINVENTORY -PLANNING .n; m; c; D; d; h/\\nletcostŒ1 : : n; 0 : : D\\x8d andmakeŒ1 : : n; 0 : : D\\x8d benew tables\\n//Compute costŒn; 0 : : D\\x8d andmake Œn; 0 : : D\\x8d .\\nforsD0toD\\nfDmax.dn/NULs; 0/\\ncostŒn; s\\x8dDc\\x01max.f/NULm; 0/Ch.sCf/NULdn/\\nmake Œn; s\\x8dDf\\n//Compute costŒ1 : : n/NUL1; 0 : : D\\x8d andmake Œ1 : : n/NUL1; 0 : : D\\x8d.\\nUDdn\\nforkDn/NUL1downto1\\nUDUCdk\\nforsD0toD\\ncostŒk; s\\x8dD1\\nforfDmax.dk/NULs; 0/toU/NULs\\n\\x17alDcostŒkC1; sCf/NULdk\\x8d\\nCc\\x01max.f/NULm; 0/Ch.sCf/NULdk/\\nif\\x17al<costŒk; s\\x8d\\ncostŒk; s\\x8dD\\x17al\\nmake Œk; s\\x8dDf\\nprintcost[1,0]\\nPRINT-PLAN.make ; n; d/\\nPRINT-PLAN.make ; n; d/\\nsD0\\nforkD1ton\\nprint “For month ” k“manufacture ” makeŒk; s\\x8d“machines”\\nsDsCmake Œk; s\\x8d/NULdk\\nIn INVENTORY -PLANNING , we build the solution month by month, starting from\\nmonth n,movingbackwardtowardmonth 1. First,wesolvethesubproblem forthe\\nlast month, for all surpluses. Then, for each month and for ea ch surplus entering\\nthat month, we calculate the cheapest way to satisfy demand f or that month based\\nonthe solved subproblems of the next month.\\n\\x0ffis the number of machines that wetry tomanufacture inmonth k.\\n\\x0fcostŒk; s\\x8dholdsthecheapest waytosatisfy demandsof months k; : : : ; n,witha\\nnet surplus of sleft over at the beginning of month k.\\n\\x0fmake Œk; s\\x8dholds the number of machines to manufacture in month kand the\\nsurplus sof an optimal plan. We will use this table to reconstruct the o ptimal\\nplan.\\nWe ﬁrst initialize the base cases, which are the cases for mon thnstarting with\\nsurplus s, for sD0; : : : ; D. Ifdn> s, it sufﬁces to manufacture dn/NULsma-\\nchines, since we need not keep any surplus after month n. Ifdn\\x14s, we need not\\nmanufacture any machines at all.\\nWe then calculate the total cost for month nas the sum of hiring extra labor\\nc\\x01max.f/NULm; 0/andtheinventorycostsforleftoversurplus h.sCf/NULdn/,which\\ncan benonzero if wehad started out withalarge surplus.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 299}),\n",
              " Document(page_content='15-50 Solutions for Chapter 15: Dynamic Programming\\nTheouter forloopofthenextblockofcoderunsdownfrommonth n/NUL1to1,thus\\nensuring that whenweconsider month k,wehave already solved thesubproblems\\nof month kC1.\\nThenext inner forloop iterates through all possible values of fas described.\\nFor every choice of ffor a given month k, the total cost of .k; s/is given by the\\ncost of extra labor (if any) plus the cost of inventory (if the re is a surplus) plus the\\ncost of the subproblem .kC1; sCf/NULdk/. Thisvalue ischecked and updated.\\nFinally, the required answer is the answer to the subproblem .1; 0/, which ap-\\npears incostŒ1; 0\\x8d. That is, it is the cheapest way to satisfy all the demands of\\nmonths 1; : : : ; nwhen westart withasurplus of 0.\\nThe running time of I NVENTORY -PLANNING is clearly O.nD2/. The space re-\\nquirement is O.nD/. We can improve upon the space requirement by noting that\\nweneedonlystorethesolution tosubproblems ofthenextmon th. Withthisobser-\\nvation, wecan construct an algorithm that uses O.nCD/space.\\nSolution to Problem 15-12\\nLetp:costdenote the cost and p:\\x17orpdenote the VORP of player p. We shall\\nassume that all dollar amounts are expressed inunits of $100 ,000.\\nSince the order of choosing players for the positions does no t matter, we may\\nassume that we make our decisions starting from position 1, m oving toward posi-\\ntionN. For each position, we decide to either sign one player or sig n no players.\\nSuppose we decide to sign player p, who plays position 1. Then, we are left with\\nanamount of X/NULp:costdollars tosign players at positions 2; : : : ; N. Thisobser-\\nvation guides us inhow to framethe subproblems.\\nWe deﬁne the cost and VORP of a setof players as the sum of costs and the sum\\nof VORPs of all players in that set. Let .i; x/denote the following subproblem:\\n“Suppose we consider only positions i; iC1; : : : ; N and we can spend at most x\\ndollars. What set of players (with at most one player for each position under con-\\nsideration) has the maximum VORP?” A validset of players for .i; x/is one in\\nwhicheachplayerinthesetplaysoneofthepositions i; iC1; : : : ; n,eachposition\\nhas at most one player, and the cost of the players in the set is at most xdollars.\\nAnoptimalset of players for .i; x/is a valid set with the maximum VORP. We\\nnow show that the problem exhibits optimal substructure.\\nTheorem (Optimal substructure of theVORPmaximization pro blem)\\nLetLDfp1; p2; : : : ; p kgbe a set of players, possibly empty, with maximum\\nVORPfor thesubproblem .i; x/.\\n1. If iDN, then Lhas at most one player. If all players in position Nhave cost\\nmore than x, then Lhas no players. Otherwise, LDfp1g, where p1has the\\nmaximum VORPamong players for position Nwithcost at most x.\\n2. If i < NandLincludes player pfor position i, then L0DL/NULfpgis an\\noptimal set for the subproblem .iC1; x/NULp:cost/.\\n3. If i < NandLdoes not include a player for position i, then Lis an optimal\\nset for the subproblem .iC1; x/.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 300}),\n",
              " Document(page_content='Solutions forChapter 15: Dynamic Programming 15-51\\nProofProperty (1) follows trivially from the problem statement.\\n(2) Suppose that L0is not an optimal set for the subproblem .iC1; x/NULp:cost/.\\nThen, there exists another valid set L00for.iC1; x/NULp:cost/that has VORP\\nmore than L0. Let L000DL00[fpg. The cost of L000is at most x, since L00has a\\ncost at most x/NULp:cost. Moreover, L000has at most one player for each position\\ni; iC1; : : : ; N. Thus, L000isavalid set for .i; x/. But L000hasVORPmorethan L,\\nthus contradicting the assumption that Lhad the maximum VORPfor .i; x/.\\n(3) Clearly, any valid set for .iC1; x/is also a valid set for .i; x/. IfLwere not\\nan optimal set for .iC1; x/, then there exists another valid set L0for.iC1; x/\\nwith VORP more than L. The set L0would also be a valid set for .i; x/, which\\ncontradicts the assumption that Lhad the maximum VORPfor .i; x/.\\nThetheorem suggests that when i < N,weexamine twosubproblems and choose\\nthebetter of thetwo. Let \\x17Œi; x\\x8ddenote themaximum VORPfor .i; x/. Let S.i; x/\\nbe the set of players who play position iand cost at most x. In the following\\nrecurrence for \\x17Œi; x\\x8d,weassume that the maxfunction returns /NUL1when invoked\\nover an empty set:\\n\\x17Œi; x\\x8dD˚\\nmax\\np2S.N;x/˚\\np:\\x17orp/TAB\\nifiDN ;\\nmax\\x1a\\n\\x17ŒiC1; x\\x8d;\\nmax\\np2S.i;x/˚\\np:\\x17orpC\\x17ŒiC1; x/NULp:cost\\x8d/TAB\\x1b\\nifi < N :\\nThis recurrence lends itself to implementation in a straigh tforward way. Let pij\\ndenote the jthplayer whoplays position i.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 301}),\n",
              " Document(page_content='15-52 Solutions for Chapter 15: Dynamic Programming\\nFREE-AGENT-VORP .p; N; P; X/\\nlet\\x17Œ1 : : N \\x8dŒ0 : : X\\x8d andwhoŒ1 : : N \\x8dŒ0 : : X\\x8d benew tables\\nforxD0toX\\n\\x17ŒN; x\\x8dD/NUL1\\nwhoŒN; x\\x8dD0\\nforkD1toP\\nifpN k:cost\\x14xandpN k:\\x17orp> \\x17ŒN; x\\x8d\\n\\x17ŒN; x\\x8dDpN k:\\x17orp\\nwhoŒN; x\\x8dDk\\nforiDN/NUL1downto 1\\nforxD0toX\\n\\x17Œi; x\\x8dD\\x17ŒiC1; x\\x8d\\nwhoŒi; x\\x8dD0\\nforkD1toP\\nifpik:cost\\x14xand\\x17ŒiC1; x/NULpik:cost\\x8dCpik:\\x17orp> \\x17Œi; x\\x8d\\n\\x17Œi; x\\x8dD\\x17ŒiC1; x/NULpik:cost\\x8dCpik:\\x17orp\\nwhoŒi; x\\x8dDk\\nprint “Themaximum value of VORPis” \\x17Œ1; X\\x8d\\namtDX\\nforiD1toN\\nkDwhoŒi;amt\\x8d\\nifk¤0\\nprint “sign player ” pik\\namtDamt/NULpik:cost\\nprint “Thetotal moneyspent is” X/NULamt\\nThe input to F REE-AGENT-VORP is the list of players pandN,P, and X, as\\ngiven in the problem. The table \\x17Œi; x\\x8dholds the maximum VORP for the sub-\\nproblem .i; x/. The table whoŒi; x\\x8dholds information necessary to reconstruct the\\nactual solution. Speciﬁcally, whoŒi; x\\x8dholds the index of player to sign for posi-\\ntioni, or0if no player should be signed for position i. The ﬁrst set of nested for\\nloops initializes the base cases, in which iDN. For every amount x, the inner\\nloop simply picks the player with the highest VORP who plays p osition Nand\\nwhose cost is at most x.\\nThenext set of three nested forloops represents the maincomputation. Theouter-\\nmostforloopruns downfrom position N/NUL1to1. Thisorder ensures that smaller\\nsubproblems aresolvedbeforelarger ones. Weinitialize \\x17Œi; x\\x8das\\x17ŒiC1; x\\x8d. This\\nway, we already take care of the case in which we decide not to s ign any player\\nwho plays position i. The innermost forloop tries to sign each player (if we have\\nenough money) inturn, and it keeps track of the maximum VORPp ossible.\\nThemaximum VORPfor theentire problem ends upin \\x17Œ1; X\\x8d. Theﬁnal forloop\\nuses the information in whotable to print out which players to sign. The running\\ntime of F REE-AGENT-VORP isclearly ‚.NPX/ ,and it uses ‚.NX/space.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 302}),\n",
              " Document(page_content='Lecture Notes forChapter 16:\\nGreedy Algorithms\\nChapter 16Introduction\\nSimilar to dynamic programming.\\nUsedfor optimization problems.\\nIdea\\nWhen we have a choice to make, make the one that looks best right now . Make a\\nlocally optimal choice in hope of getting a globally optimal solution .\\nGreedy algorithms don’t always yield an optimal solution. B ut sometimes they\\ndo. We’ll see a problem for which they do. Then we’ll look at so me general\\ncharacteristics of when greedy algorithms give optimal sol utions.\\n[Wedonot cover Huffman codes or matroids inthese notes.]\\nActivityselection\\nnactivities requireexclusive use of a common resource. For example, scheduling\\nthe useof aclassroom.\\nSet of activities SDfa1; : : : ; a ng.\\naineeds resource during period Œsi; fi/, which is a half-open interval, where siD\\nstart timeand fiDﬁnish time.\\nGoal\\nSelect thelargest possible set of nonoverlapping ( mutually compatible ) activities.\\nNote\\nCould have manyother objectives:\\n\\x0fSchedule room for longest time.\\n\\x0fMaximize income rental fees.\\nAssumethat activities aresorted byﬁnishtime: f1\\x14f2\\x14f3\\x14\\x01\\x01\\x01\\x14 fn/NUL1\\x14fn.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 303}),\n",
              " Document(page_content='16-2 Lecture Notes for Chapter 16: Greedy Algorithms\\nExample\\nSsorted by ﬁnish time: [Leave onboard]\\ni1 2 3 4 5 6 7 8 9\\nsi1 2 4 1 5 8 9 11 13\\nfi3 5 7 8 9 10 11 14 16\\n0 1 2 3 4 5 6 7 8 9 10 11 12 13 14a1a4a5\\na6a7\\na8a9 a2\\na3\\n1615\\nMaximum-size mutually compatible set: fa1; a3; a6; a8g.\\nNot unique: alsofa2; a5; a7; a9g.\\nOptimalsubstructure of activity selection\\nSijDfak2SWfi\\x14sk< f k\\x14sjg [Leave on board]\\nDactivities that start after aiﬁnishes and ﬁnishbefore ajstarts :\\nai ak ajfisk fksj. . . . . .\\nActivities in Sijare compatible with\\n\\x0fall activities that ﬁnish by fi,and\\n\\x0fall activities that start noearlier than sj.\\nLetAijbeamaximum-size set of mutually compatible activities in Sij.\\nLetak2Aijbe some activity in Aij. Then wehave two subproblems:\\n\\x0fFind mutually compatible activities in Sik(activities that start after aiﬁnishes\\nand that ﬁnish before akstarts).\\n\\x0fFind mutually compatible activities in Skj(activities that start after akﬁnishes\\nand that ﬁnish before ajstarts).\\nLet\\nAikDAij\\\\SikDactivities in Aijthat ﬁnish before akstarts ;\\nAkjDAij\\\\SkjDactivities in Aijthat start afer akﬁnishes :\\nThen AijDAik[fakg[Akj\\n)jAijjDjAikjCjAkjjC1.\\nClaim\\nOptimal solution Aijmust include optimal solutions for the two subproblems for\\nSikandSkj.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 304}),\n",
              " Document(page_content='Lecture Notes for Chapter 16: Greedy Algorithms 16-3\\nProofUse the usual cut-and-paste argument. Will show the claim fo rSkj; proof\\nforSikis symmetric.\\nSuppose we could ﬁnd a set A0\\nkjof mutually compatible activities in Skj, whereˇˇA0\\nkjˇˇ>jAkjj. Then use A0\\nkjinstead of Akjwhen solving the subproblem for Sij.\\nSizeofresultingsetofmutuallycompatibleactivitieswou ldbejAikjCˇˇA0\\nkjˇˇC1 >\\njAikjCjAkjjC1DjAj. Contradicts assumption that Aijisoptimal. (claim)\\nOnerecursive solution\\nSince optimal solution Aijmust include optimal solutions to the subproblems for\\nSikandSkj, could solve bydynamic programming.\\nLetcŒi; j \\x8dDsize of optimal solution for Sij. Then\\ncŒi; j \\x8dDcŒi; k\\x8dCcŒk; j \\x8dC1 :\\nBut wedon’t know which activity aktochoose, so wehave totry them all:\\ncŒi; j \\x8dD(0 ifSijD;;\\nmax\\nak2SijfcŒi; k\\x8dCcŒk; j \\x8dC1gifSij¤;:\\nCould then develop a recursive algorithm and memoize it. Or c ould develop a\\nbottom-up algorithm and ﬁllin table entries.\\nInstead, wewill look at agreedy approach.\\nMakingthegreedy choice\\nChoose an activity to add to optimal solution beforesolving subproblems. For\\nactivity-selection problem, we can get away with consideri ng only the greedy\\nchoice: the activity that leaves the resource available for as many other activities\\naspossible.\\nQuestion: Whichactivityleavestheresourceavailablefor themostotheractivities?\\nAnswer: The ﬁrst activity to ﬁnish. (If more than one activit y has earliest ﬁnish\\ntime, can choose any such activity.)\\nSinceactivities are sorted byﬁnish time, just choose activ itya1.\\nThatleavesonlyonesubproblem tosolve: ﬁndingamaximumsi zesetofmutually\\ncompatible activities that start after a1ﬁnishes. (Don’t have to worry about activ-\\nities that ﬁnish before a1starts, because s1< f 1and no activity aihas ﬁnish time\\nfi< f 1)no activity aihasfi\\x14s1.)\\nSincehave only subproblem tosolve, simplify notation:\\nSkDfai2SWsi\\x15fkgDactivities that start after akﬁnishes :\\nMaking greedy choice of a1)S1remains as only subproblem to solve. [Slight\\nabuseofnotation: referring to Sknotonlyasasetofactivities butasasubproblem\\nconsisting of these activities.]\\nByoptimal substructure, if a1isinanoptimal solution, thenanoptimal solution to\\nthe original problem consists of a1plus all activities in anoptimal solution to S1.\\nBut need to prove that a1is always part of someoptimal solution.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 305}),\n",
              " Document(page_content='16-4 Lecture Notes for Chapter 16: Greedy Algorithms\\nTheorem\\nIfSkis nonempty and amhas the earliest ﬁnish time in Sk, then amis included in\\nsome optimal solution.\\nProofLetAkbe an optimal solution to Sk, and let ajhave the earliest ﬁnish time\\nof any activity in Ak. IfajDam, done. Otherwise, let A0\\nkDAk/NULfajg[famg\\nbeAkbut with amsubstituted for aj.\\nClaim\\nActivities in A0\\nkaredisjoint.\\nProofActivities in Akare disjoint, ajis ﬁrst activity in Akto ﬁnish, and\\nfm\\x14fj. (claim)\\nSincejA0\\nkjDjAkj, conclude that A0\\nkis an optimal solution to Sk, and it in-\\ncludes am. (theorem)\\nSo, don’t need full power of dynamic programming. Don’t need to work bottom-\\nup.\\nInstead, can just repeatedly choose the activity that ﬁnish es ﬁrst, keep only the\\nactivities that are compatible withthat one, and repeat unt il no activities remain.\\nCan worktop-down: make a choice, then solve a subproblem. Do n’t have to solve\\nsubproblems before making achoice.\\nRecursive greedy algorithm\\nStart andﬁnishtimesarerepresented byarrays sandf,where fisassumed tobe\\nalready sorted in monotonically increasing order.\\nTo start, add ﬁctitious activity a0withf0D0, so that S0DS, the entire set of\\nactivities.\\nProcedure R EC-ACTIVITY-SELECTOR takes asparameters the arrays sandf, in-\\ndexkof current subproblem, and number nof activities in theoriginal problem.\\nREC-ACTIVITY-SELECTOR .s; f; k; n/\\nmDkC1\\nwhile m\\x14nandsŒm\\x8d < f Œk\\x8d //ﬁnd theﬁrst activity in Sktoﬁnish\\nmDmC1\\nifm\\x14n\\nreturnfamg[REC-ACTIVITY-SELECTOR .s; f; m; n/\\nelse return;\\nInitial call\\nREC-ACTIVITY-SELECTOR .s; f; 0; n/ .', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 306}),\n",
              " Document(page_content='Lecture Notes for Chapter 16: Greedy Algorithms 16-5\\nIdea\\nThewhileloop checks akC1; akC2; : : : ; a nuntil it ﬁnds an activity amthat is com-\\npatible with ak(need sm\\x15fk).\\n\\x0fIf the loop terminates because amis found ( m\\x14n), then recursively solve Sm,\\nand return this solution, along with am.\\n\\x0fIf theloop never ﬁndsacompatible am(m > n), then just return empty set.\\nGothrough example given earlier. Should get fa1; a3; a6; a8g.\\nTime\\n‚.n/—each activity examined exactly once, assuming that activi ties are already\\nsorted byﬁnish times.\\nIterative greedy algorithm\\nCan convert the recursive algorithm to an iterative one. It’ s already almost tail\\nrecursive.\\nGREEDY-ACTIVITY-SELECTOR .s; f /\\nnDs:length\\nADfa1g\\nkD1\\nformD2ton\\nifsŒm\\x8d\\x15f Œk\\x8d\\nADA[famg\\nkDm\\nreturn A\\nGothrough example given earlier. Should again get fa1; a3; a6; a8g.\\nTime\\n‚.n/,if activities are already sorted byﬁnish times.\\nForboththerecursiveanditerativealgorithms,add O.nlgn/timeifactivitiesneed\\ntobe sorted.\\nGreedy strategy\\nThechoice that seems best at the moment isthe one wegowith.\\nWhat did wedo for activity selection?\\n1. Determine the optimal substructure.\\n2. Develop arecursive solution.\\n3. Show that if wemake thegreedy choice, only one subproblem remains.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 307}),\n",
              " Document(page_content='16-6 Lecture Notes for Chapter 16: Greedy Algorithms\\n4. Provethat it’s always safe to make thegreedy choice.\\n5. Develop arecursive greedy algorithm.\\n6. Convert it toan iterative algorithm.\\nAtﬁrst,itlookedlikedynamicprogramming. Intheactivity -selection problem,we\\nstartedoutbydeﬁningsubproblems Sij,whereboth iandjvaried. Butthenfound\\nthat making the greedy choice allowed us to restrict the subp roblems to be of the\\nform Sk.\\nCould instead have gone straight for the greedy approach: in our ﬁrst crack at\\ndeﬁning subproblems, use the Skform. Could then have proven that the greedy\\nchoice am(the ﬁrst activity to ﬁnish), combined with optimal solutio n to the re-\\nmaining compatible activities Sm, gives anoptimal solution to Sk.\\nTypically, westreamline these steps:\\n1. Cast the optimization problem as one in which we make a choi ce and are left\\nwithone subproblem to solve.\\n2. Prove that there’s always an optimal solution that makes t he greedy choice, so\\nthat the greedy choice isalways safe.\\n3. Demonstrate optimal substructure by showing that, havin g made the greedy\\nchoice, combining an optimal solution to the remaining subp roblem with the\\ngreedy choice gives anoptimal solution tothe original prob lem.\\nNogeneral waytotell whether agreedy algorithm isoptimal, but twokeyingredi-\\nents are\\n1. greedy-choice property and\\n2. optimal substructure.\\nGreedy-choice property\\nCan assemble a globally optimal solution by making locally o ptimal (greedy)\\nchoices.\\nDynamicprogramming\\n\\x0fMake achoice at each step.\\n\\x0fChoice depends on knowing optimal solutions tosubproblems . Solvesubprob-\\nlemsﬁrst.\\n\\x0fSolvebottom-up .\\nGreedy\\n\\x0fMake achoice at each step.\\n\\x0fMake thechoice beforesolving thesubproblems.\\n\\x0fSolvetop-down .\\nTypically show thegreedy-choice property bywhat wedidfor activity selection:\\n\\x0fLookat anoptimal solution.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 308}),\n",
              " Document(page_content='Lecture Notes for Chapter 16: Greedy Algorithms 16-7\\n\\x0fIf it includes thegreedy choice, done.\\n\\x0fOtherwise, modify the optimal solution to include the greed y choice, yielding\\nanother solution that’s just as good.\\nCanget efﬁciency gains from greedy-choice property.\\n\\x0fPreprocess input to put it into greedy order.\\n\\x0fOr, if dynamic data, use apriority queue.\\nOptimalsubstructure\\nJust show that optimal solution tosubproblem and greedy cho ice)optimal solu-\\ntion toproblem.\\nGreedy vs. dynamicprogramming\\nTheknapsack problem isagood example of the difference.\\n0-1 knapsack problem\\n\\x0fnitems.\\n\\x0fItemiis worth $ \\x17i, weighs wipounds.\\n\\x0fFind amost valuable subset of items withtotal weight \\x14W.\\n\\x0fHave toeither take an item or not take it—can’t take part of it .\\nFractional knapsack problem\\nLikethe 0-1 knapsack problem, but can take fraction of anite m.\\nBothhave optimal substructure.\\nBut the fractional knapsack problem has the greedy-choice p roperty, and the 0-1\\nknapsack problem does not.\\nTo solve the fractional problem, rank items by value/weight :\\x17i=wi. Let\\n\\x17i=wi\\x15\\x17iC1=wiC1for all i. Takeitems in decreasing order of value/weight. Will\\ntake all of the items with the greatest value/weight, and pos sibly a fraction of the\\nnext item.\\nFRACTIONAL -KNAPSACK .\\x17; w; W /\\nloadD0\\niD1\\nwhileload< Wandi\\x14n\\nifwi\\x14W/NULload\\ntake all of item i\\nelsetake.W/NULload/=w iof item i\\nadd what was taken to load\\niDiC1', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 309}),\n",
              " Document(page_content='16-8 Lecture Notes for Chapter 16: Greedy Algorithms\\nTime: O.nlgn/tosort, O.n/thereafter.\\nGreedydoesn’t workforthe0-1knapsack problem. Mightget e mptyspace, which\\nlowers the average value per pound of the itemstaken.\\ni1 2 3\\n\\x17i60 100 120\\nwi10 20 30\\n\\x17i=wi6 5 4\\nWD50.\\nGreedy solution:\\n\\x0fTakeitems 1and2.\\n\\x0fvalueD160, weightD30.\\nHave 20pounds of capacity left over.\\nOptimal solution:\\n\\x0fTakeitems 2and3.\\n\\x0fvalueD220, weightD50.\\nNoleftover capacity.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 310}),\n",
              " Document(page_content='SolutionsforChapter 16:\\nGreedy Algorithms\\nSolutionto Exercise 16.1-1\\nThe tricky part is determining which activities are in the se tSij. If activity kis\\ninSij, then we must have i < k < j , which means that j/NULi\\x152, but we must\\nalsohave that fi\\x14skandfk\\x14sj. If westart katj/NUL1and decrement k,wecan\\nstop once kreaches i, but we can also stop once we ﬁnd that fk\\x14fi, since then\\nactivities iC1through kcannot be compatible withactivity i.\\nWe create two ﬁctitious activities, a0with f0D0andanC1with snC1D1.\\nWe are interested in a maximum-size set A0;nC1of mutually compatible activities\\ninS0;nC1. We’ll use tables cŒ0 : : nC1; 0 : : nC1\\x8d, as in recurrence (16.2) (so that\\ncŒi; j \\x8dDjAijj),andactŒ0 : : nC1; 0 : : nC1\\x8d,whereactŒi; j \\x8distheactivity kthat\\nwechoose toput into Aij.\\nWeﬁllthetables inaccording toincreasing difference j/NULi,whichwedenoteby l\\nin the pseudocode. Since SijD;ifj/NULi < 2, we initialize cŒi; i\\x8dD0for all i\\nandcŒi; iC1\\x8dD0for0\\x14i\\x14n. As in R ECURSIVE -ACTIVITY-SELECTOR and\\nGREEDY-ACTIVITY-SELECTOR , the start and ﬁnish times are given as arrays s\\nandf,whereweassumethat thearraysalready includethetwoﬁcti tious activities\\nand that the activities are sorted by monotonically increas ing ﬁnish time.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 311}),\n",
              " Document(page_content='16-10 Solutions for Chapter 16: Greedy Algorithms\\nDYNAMIC-ACTIVITY-SELECTOR .s; f; n/\\nletcŒ0 : : nC1; 0 : : nC1\\x8dandactŒ0 : : nC1; 0 : : nC1\\x8dbenew tables\\nforiD0ton\\ncŒi; i\\x8dD0\\ncŒi; iC1\\x8dD0\\ncŒnC1; nC1\\x8dD0\\nforlD2tonC1\\nforiD0ton/NULlC1\\njDiCl\\ncŒi; j \\x8dD0\\nkDj/NUL1\\nwhile f Œi\\x8d < f Œk\\x8d\\niff Œi\\x8d\\x14sŒk\\x8dandf Œk\\x8d\\x14sŒj \\x8dandcŒi; k\\x8dCcŒk; j \\x8dC1 > cŒi; j \\x8d\\ncŒi; j \\x8dDcŒi; k\\x8dCcŒk; j \\x8dC1\\nactŒi; j \\x8dDk\\nkDk/NUL1\\nprint “Amaximum size set of mutually compatible activities has size ” cŒ0; nC1\\x8d\\nprint “Theset contains ”\\nPRINT-ACTIVITIES .c;act; 0; nC1/\\nPRINT-ACTIVITIES .c;act; i; j /\\nifcŒi; j \\x8d > 0\\nkDactŒi; j \\x8d\\nprint k\\nPRINT-ACTIVITIES .c;act; i; k/\\nPRINT-ACTIVITIES .c;act; k; j /\\nThe PRINT-ACTIVITIES procedure recursively prints the set of activities placed\\ninto the optimal solution Aij. It ﬁrst prints the activity kthat achieved the maxi-\\nmumvalueof cŒi; j \\x8d,andthenitrecursestoprinttheactivitiesin AikandAkj. The\\nrecursion bottoms out when cŒi; j \\x8dD0,so that AijD;.\\nWhereas G REEDY-ACTIVITY-SELECTOR runs in ‚.n/time, the D YNAMIC-\\nACTIVITY-SELECTOR procedure runs in O.n3/time.\\nSolution to Exercise16.1-2\\nThe proposed approach—selecting the last activity to start that is compatible with\\nall previously selected activities—is really the greedy al gorithm but starting from\\nthe end rather than the beginning.\\nAnother way to look at it is as follows. We are given a set SDfa1; a2; : : : ; a ng\\nof activities, where aiDŒsi; fi/, and we propose to ﬁnd an optimal solution by\\nselecting the last activity to start that is compatible with all previously selected\\nactivities. Instead, let us create a set S0Dfa0\\n1; a0\\n2; : : : ; a0\\nng, where a0\\niDŒfi; si/.\\nThat is, a0\\niisaiin reverse. Clearly, a subset of fai1; ai2; : : : ; a ikg\\x12Sis mutually\\ncompatible if and only if the corresponding subset˚\\na0\\ni1; a0\\ni2; : : : ; a0\\nik/TAB\\n\\x12S0is also', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 312}),\n",
              " Document(page_content='Solutions forChapter 16: Greedy Algorithms 16-11\\nmutually compatible. Thus, an optimal solution for Smaps directly to an optimal\\nsolution for S0and vice versa.\\nTheproposed approach of selecting thelast activity tostar t that iscompatible with\\nall previously selected activities, when run on S, gives the same answer as the\\ngreedy algorithm fromthetext—selecting theﬁrstactivity toﬁnishthat iscompat-\\nible with all previously selected activities—when run on S0. The solution that the\\nproposed approach ﬁnds for Scorresponds to the solution that the text’s greedy\\nalgorithm ﬁnds for S0,and so it is optimal.\\nSolutionto Exercise 16.1-3\\n\\x0fFor the approach of selecting the activity of least duration from those that are\\ncompatible withpreviously selected activities:\\ni 1 2 3\\nsi 0 2 3\\nfi 3 4 6\\nduration 3 2 3\\nThis approach selects just fa2g, but theoptimal solution selects fa1; a3g.\\n\\x0fFor the approach of always selecting the compatible activit y that overlaps the\\nfewest other remaining activities:\\ni 1 2 3 4 5 6 7 8 9 10 11\\nsi 0 1 1 1 2 3 4 5 5 5 6\\nfi 2 3 3 3 4 5 6 7 7 7 8\\n#of overlapping activities 3 4 4 4 4 2 4 4 4 4 3\\nThis approach ﬁrst selects a6, and after that choice it can select only two other\\nactivities (oneof a1; a2; a3; a4andoneof a8; a9; a10; a11). Anoptimal solution\\nisfa1; a5; a7; a11g.\\n\\x0fFor the approach of always selecting the compatible remaini ng activity with\\nthe earliest start time, just add one more activity with the i nterval Œ0; 14/to\\nthe example in Section 16.1. It will be the ﬁrst activity sele cted, and no other\\nactivities are compatible with it.\\nSolutionto Exercise 16.1-4\\nThissolutionisalsopostedpublicly\\nLetSbethe set of nactivities.\\nThe “obvious” solution of using G REEDY-ACTIVITY-SELECTOR to ﬁnd a maxi-\\nmum-sizeset S1ofcompatibleactivitiesfrom Sfortheﬁrstlecturehall,thenusing\\nitagaintoﬁndamaximum-size set S2ofcompatible activities from S/NULS1forthe\\nsecond hall, (and so on until all the activities are assigned ), requires ‚.n2/time\\nin the worst case. Moreover, it can produce a result that uses more lecture halls', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 313}),\n",
              " Document(page_content='16-12 Solutions for Chapter 16: Greedy Algorithms\\nthan necessary. Consider activities with the intervals fŒ1; 4/; Œ2; 5/; Œ6; 7/; Œ4; 8/ g.\\nGREEDY-ACTIVITY-SELECTOR would choose the activities with intervals Œ1; 4/\\nandŒ6; 7/for the ﬁrst lecture hall, and then each of the activities wit h intervals\\nŒ2; 5/andŒ4; 8/would have to go into its own hall, for a total of three halls us ed.\\nAnoptimalsolutionwouldputtheactivitieswithintervals Œ1; 4/andŒ4; 8/intoone\\nhall and the activities withintervals Œ2; 5/andŒ6; 7/into another hall, for only two\\nhalls used.\\nThere is a correct algorithm, however, whose asymptotic tim e is just the time\\nneeded to sort the activities by time— O.nlgn/time for arbitrary times, or pos-\\nsibly asfast as O.n/if the times aresmall integers.\\nThe general idea is to go through the activities in order of st art time, assigning\\neach to any hall that is available at that time. To do this, mov e through the set\\nof events consisting of activities starting and activities ﬁnishing, in order of event\\ntime. Maintain two lists of lecture halls: Halls that are bus y at the current event-\\ntime t(because they have been assigned an activity ithat started at si\\x14tbut\\nwon’t ﬁnish until fi> t) and halls that are free at time t. (As in the activity-\\nselection problem in Section 16.1, we are assuming that acti vity time intervals are\\nhalf open—i.e., that if si\\x15fj, then activities iandjare compatible.) When t\\nis the start time of some activity, assign that activity to a f ree hall and move the\\nhall from the free list to the busy list. When tis the ﬁnish time of some activity,\\nmove the activity’s hall from the busy list to the free list. ( The activity is certainly\\nin some hall, because the event times are processed in order a nd the activity must\\nhave started before its ﬁnish time t, hence must have been assigned toahall.)\\nTo avoid using more halls than necessary, always pick a hall t hat has already had\\nanactivityassigned toit,ifpossible, beforepickinganev er-used hall. (Thiscanbe\\ndone by always working at the front of the free-halls list—pu tting freed halls onto\\nthe front of the list and taking halls from the front of the lis t—so that a new hall\\ndoesn’t come tothe front and get chosen if there are previous ly-used halls.)\\nThis guarantees that the algorithm uses as few lecture halls as possible: The algo-\\nrithm will terminate with a schedule requiring m\\x14nlecture halls. Let activity i\\nbe the ﬁrst activity scheduled in lecture hall m. The reason that iwas put in the\\nmth lecture hall is that the ﬁrst m/NUL1lecture halls were busy at time si. Soat this\\ntime there are mactivities occurring simultaneously. Therefore any sched ule must\\nuse at least mlecture halls, sothe schedule returned bythe algorithm is o ptimal.\\nRun time:\\n\\x0fSort the 2nactivity-starts/activity-ends events. (Inthe sorted ord er, anactivity-\\nending event should precede anactivity-starting event tha t isat the sametime.)\\nO.nlgn/timeforarbitrarytimes,possibly O.n/ifthetimesarerestricted(e.g.,\\ntosmall integers).\\n\\x0fProcesstheeventsin O.n/time: Scanthe 2nevents, doing O.1/workforeach\\n(moving a hall from one list to the other and possibly associa ting an activity\\nwithit).\\nTotal: O.nCtime tosort /\\n[The idea of this algorithm is related to the rectangle-over lap algorithm in Exer-\\ncise 14.3-7.]', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 314}),\n",
              " Document(page_content='Solutions forChapter 16: Greedy Algorithms 16-13\\nSolutionto Exercise 16.1-5\\nWecan no longer use the greedy algorithm to solve this proble m. However, as we\\nshow, the problem still has an optimal substructure which al lows us to formulate a\\ndynamic programming solution. The analysis here follows cl osely the analysis of\\nSection 16.1 in the book. We deﬁne the value of a set of compati ble events as the\\nsumofvaluesofeventsinthatset. Let SijbedeﬁnedasinSection16.1. An optimal\\nsolutiontoSijis a subset of mutually compatible events of Sijthat has maximum\\nvalue. Let Aijbeanoptimal solution to Sij. Suppose Aijincludesanevent ak. Let\\nAikandAkjbedeﬁnedasinSection16.1. Thus,wehave AijDAik[fakg[Akj,\\nand so the value of maximum-value set Aijis equal to the value of Aikplus the\\nvalue of Akjplus\\x17k.\\nThe usual cut-and-paste argument shows that the optimal sol ution Aijmust also\\ninclude optimal solutions tothetwosubproblems for SikandSkj. Ifwecould ﬁnd\\nasetA0\\nkjof mutually compatible activities in Skjwherethevalue of A0\\nkjisgreater\\nthan the value of Akj, then we could use A0\\nkj, rather than Akj, in a solution to\\nthe subproblem for Sij. We would have constructed a set of mutually compatible\\nactivities withgreater valuethanthat of Aij,whichcontradicts theassumption that\\nAijis anoptimal solution. Asymmetric argument applies to the a ctivities in Sik.\\nLetusdenote thevalueofanoptimal solution fortheset Sijby\\x17alŒi; j \\x8d. Then, we\\nwould have the recurrence\\n\\x17alŒi; j \\x8dD\\x17alŒi; k\\x8dC\\x17alŒk; j \\x8dC\\x17k:\\nOf course, since we do not know that an optimal solution for th e set Sijincludes\\nactivity ak, we would have to examine all activities in Sijto ﬁnd which one to\\nchoose, sothat\\n\\x17alŒi; j \\x8dD(0 ifSijD;;\\nmax\\nak2Sijf\\x17alŒi; k\\x8dC\\x17alŒk; j \\x8dC\\x17kgifSij¤;:\\nWhile implementing the recurrence, the tricky part isdeter mining which activities\\nareintheset Sij. Ifactivity kisinSij,thenwemusthave i < k < j ,whichmeans\\nthatj/NULi\\x152, but we must also have that fi\\x14skandfk\\x14sj. If we start kat\\nj/NUL1and decrement k, we can stop once kreaches i, but we can also stop once\\nwe ﬁnd that fk\\x14fi, since then activities iC1through kcannot be compatible\\nwithactivity i.\\nWe create two ﬁctitious activities, a0with f0D0andanC1with snC1D1.\\nWe are interested in a maximum-size set A0;nC1of mutually compatible activities\\ninS0;nC1. We’ll use tables \\x17alŒ0 : : nC1; 0 : : nC1\\x8d, as in the recurrence, and\\nactŒ0 : : nC1; 0 : : nC1\\x8d, whereactŒi; j \\x8dis the activity kthat we choose to put\\nintoAij.\\nWeﬁllthetables inaccording toincreasing difference j/NULi,whichwedenoteby l\\nin the pseudocode. Since SijD;ifj/NULi < 2, we initialize \\x17alŒi; i\\x8dD0for all i\\nand\\x17alŒi; iC1\\x8dD0for0\\x14i\\x14n. As in R ECURSIVE -ACTIVITY-SELECTOR\\nandGREEDY-ACTIVITY-SELECTOR , thestartandﬁnishtimesaregivenasarrays s\\nandf,whereweassumethat thearraysalready includethetwoﬁcti tious activities', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 315}),\n",
              " Document(page_content='16-14 Solutions for Chapter 16: Greedy Algorithms\\nand that the activities are sorted by monotonically increas ing ﬁnish time. The\\narray \\x17speciﬁes the value of each activity.\\nMAX-VALUE-ACTIVITY-SELECTOR .s; f; \\x17; n/\\nlet\\x17alŒ0 : : nC1; 0 : : nC1\\x8dandactŒ0 : : nC1; 0 : : nC1\\x8dbenew tables\\nforiD0ton\\n\\x17alŒi; i\\x8dD0\\n\\x17alŒi; iC1\\x8dD0\\n\\x17alŒnC1; nC1\\x8dD0\\nforlD2tonC1\\nforiD0ton/NULlC1\\njDiCl\\n\\x17alŒi; j \\x8dD0\\nkDj/NUL1\\nwhile f Œi\\x8d < f Œk\\x8d\\niff Œi\\x8d\\x14sŒk\\x8dandf Œk\\x8d\\x14sŒj \\x8dand\\n\\x17alŒi; k\\x8dC\\x17alŒk; j \\x8dC\\x17k> \\x17alŒi; j \\x8d\\n\\x17alŒi; j \\x8dD\\x17alŒi; k\\x8dC\\x17alŒk; j \\x8dC\\x17k\\nactŒi; j \\x8dDk\\nkDk/NUL1\\nprint “Amaximum-value set of mutually compatible activiti es has value ”\\n\\x17alŒ0; nC1\\x8d\\nprint “Theset contains ”\\nPRINT-ACTIVITIES .\\x17al;act; 0; nC1/\\nPRINT-ACTIVITIES .\\x17al;act; i; j /\\nif\\x17alŒi; j \\x8d > 0\\nkDactŒi; j \\x8d\\nprint k\\nPRINT-ACTIVITIES .\\x17al;act; i; k/\\nPRINT-ACTIVITIES .\\x17al;act; k; j /\\nThe PRINT-ACTIVITIES procedure recursively prints the set of activities placed\\ninto the optimal solution Aij. It ﬁrst prints the activity kthat achieved the maxi-\\nmum value of \\x17alŒi; j \\x8d, and then it recurses to print the activities in AikandAkj.\\nTherecursion bottoms out when \\x17alŒi; j \\x8dD0, sothat AijD;.\\nWhereas G REEDY-ACTIVITY-SELECTOR runs in ‚.n/time, the M AX-VALUE-\\nACTIVITY-SELECTOR procedure runs in O.n3/time.\\nSolution to Exercise16.2-2\\nThis solutionisalsopostedpublicly\\nThe solution is based on the optimal-substructure observat ion in the text: Let i\\nbe the highest-numbered item in an optimal solution SforWpounds and items\\n1; : : : ; n. Then S0DS/NULfigmust be an optimal solution for W/NULwipounds\\nand items 1; : : : ; i/NUL1, and the value of the solution Sis\\x17iplus the value of the\\nsubproblem solution S0.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 316}),\n",
              " Document(page_content='Solutions forChapter 16: Greedy Algorithms 16-15\\nWecan express this relationship in the following formula: D eﬁne cŒi; w\\x8dto be the\\nvalue of thesolution for items 1; : : : ; iand maximum weight w. Then\\ncŒi; w\\x8dD\\x80\\n0 ifiD0orwD0 ;\\ncŒi/NUL1; w\\x8d ifwi> w ;\\nmax.\\x17iCcŒi/NUL1; w/NULwi\\x8d; cŒi/NUL1; w\\x8d/ifi > 0andw\\x15wi:\\nThe last case says that the value of a solution for iitems either includes item i,\\nin which case it is \\x17iplus a subproblem solution for i/NUL1items and the weight\\nexcluding wi, or doesn’t include item i, in which case it is a subproblem solution\\nfori/NUL1items and the same weight. That is, if the thief picks item i, he takes \\x17i\\nvalue, and he can choose from items 1; : : : ; i/NUL1up to the weight limit w/NULwi,\\nand get cŒi/NUL1; w/NULwi\\x8dadditional value. On the other hand, if he decides not to\\ntakeitem i,hecanchoose fromitems 1; : : : ; i/NUL1uptotheweightlimit w,andget\\ncŒi/NUL1; w\\x8dvalue. Thebetter of these twochoices should be made.\\nThealgorithm takesasinputs themaximumweight W,thenumber ofitems n,and\\nthe two sequences \\x17Dh\\x171; \\x172; : : : ; \\x17 niandwDhw1; w2; : : : ; w ni. It stores the\\ncŒi; j \\x8dvalues in a table cŒ0 : : n; 0 : : W \\x8d whose entries are computed in row-major\\norder. (That is, the ﬁrst row of cis ﬁlled in from left to right, then the second row,\\nand so on.) At the end of the computation, cŒn; W \\x8dcontains the maximum value\\nthe thief can take.\\nDYNAMIC-0-1-KNAPSACK .\\x17; w; n; W /\\nletcŒ0 : : n; 0 : : W \\x8d be anew array\\nforwD0toW\\ncŒ0; w\\x8dD0\\nforiD1ton\\ncŒi; 0\\x8dD0\\nforwD1toW\\nifwi\\x14w\\nif\\x17iCcŒi/NUL1; w/NULwi\\x8d > cŒi/NUL1; w\\x8d\\ncŒi; w\\x8dD\\x17iCcŒi/NUL1; w/NULwi\\x8d\\nelsecŒi; w\\x8dDcŒi/NUL1; w\\x8d\\nelsecŒi; w\\x8dDcŒi/NUL1; w\\x8d\\nWecan use the ctable todeduce the set of items totake by starting at cŒn; W \\x8dand\\ntracingwheretheoptimalvaluescamefrom. If cŒi; w\\x8dDcŒi/NUL1; w\\x8d,thenitem iis\\nnotpartofthesolution, andwecontinue tracingwith cŒi/NUL1; w\\x8d. Otherwiseitem i\\nispart of thesolution, and wecontinue tracing with cŒi/NUL1; w/NULwi\\x8d.\\nTheabove algorithm takes ‚.nW /timetotal:\\n\\x0f‚.nW /toﬁllinthe ctable: .nC1/\\x01.WC1/entries, eachrequiring ‚.1/time\\nto compute.\\n\\x0fO.n/time to trace the solution (since it starts in row nof the table and moves\\nup one row at each step).', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 317}),\n",
              " Document(page_content='16-16 Solutions for Chapter 16: Greedy Algorithms\\nSolution to Exercise16.2-4\\nThe optimal strategy is the obvious greedy one. Starting wit h both bottles full,\\nProfessor Gekko should go to the westernmost place that he ca n reﬁll his bottles\\nwithin mmilesof Grand Forks. Fillup there. Then gotothe westernmos t reﬁlling\\nlocation hecanget towithin mmilesof whereheﬁlledup, ﬁllupthere, andsoon.\\nLooked at another way, at each reﬁlling location, Professor Gekko should check\\nwhether he can make it to the next reﬁlling location without s topping at this one.\\nIf he can, skip this one. If he cannot, then ﬁll up. Professor G ekko doesn’t need to\\nknow howmuchwater hehasor howfar thenext reﬁllinglocatio n istoimplement\\nthis approach, since at each ﬁllup, he can determine which is the next location at\\nwhich he’ll need tostop.\\nThisproblem hasoptimal substructure. Supposethereare npossible reﬁllingloca-\\ntions. Consider an optimal solution with sreﬁlling locations and whose ﬁrst stop\\nis at the kth location. Then the rest of the optimal solution must be an o ptimal\\nsolution to the subproblem of the remaining n/NULkstations. Otherwise, if there\\nwere a better solution to the subproblem, i.e., one with fewe r than s/NUL1stops, we\\ncoulduseittocomeupwithasolution withfewerthan sstopsforthefullproblem,\\ncontradicting our supposition of optimality.\\nThis problem also has the greedy-choice property. Suppose t here are kreﬁlling\\nlocations beyond the start that are within mmiles of the start. The greedy solution\\nchooses the kthlocation asitsﬁrst stop. Nostation beyond the kthworks asaﬁrst\\nstop, since Professor Gekko would run out of water ﬁrst. If a s olution chooses a\\nlocation j < kasitsﬁrststop,thenProfessor Gekkocouldchoosethe kthlocation\\ninstead, having at least as much water when he leaves the kth location as if he’d\\nchosen the jth location. Therefore, he would get at least as far without ﬁ lling up\\nagain if hehad chosen the kth location.\\nIfthereare nreﬁlling locations onthemap,Professor Gekkoneedstoinsp ect each\\none just once. Therunning timeis O.n/.\\nSolution to Exercise16.2-6\\nUse a linear-time median algorithm to calculate the median mof the \\x17i=wira-\\ntios. Next, partition the items into three sets: GDfiW\\x17i=wi> mg,ED\\nfiW\\x17i=wiDmg, and LDfiW\\x17i=wi< mg; this step takes linear time. Com-\\nputeWGDP\\ni2GwiandWEDP\\ni2Ewi, the total weight of the items in sets G\\nandE, respectively.\\n\\x0fIfWG> W,then donot yet take anyitemsin set G, and instead recurse onthe\\nset of items Gand knapsack capacity W.\\n\\x0fOtherwise ( WG\\x14W), take all items inset G, and take asmuch of the itemsin\\nsetEas will ﬁtin the remaining capacity W/NULWG.\\n\\x0fIfWGCWE\\x15W(i.e., there is no capacity left after taking all the items in\\nsetGandalltheitemsinset Ethatﬁtintheremainingcapacity W/NULWG),then\\nwearedone.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 318}),\n",
              " Document(page_content='Solutions forChapter 16: Greedy Algorithms 16-17\\n\\x0fOtherwise ( WGCWE< W), then after taking all the items in sets GandE,\\nrecurse on the set of items Land knapsack capacity W/NULWG/NULWE.\\nToanalyze this algorithm, note that each recursive call tak es linear time, exclusive\\nofthetimeforarecursivecallthatitmaymake. Whenthereis arecursivecall,there\\nisjust one, andit’sfor aproblem ofatmost half thesize. Thu s, therunning timeis\\ngivenby therecurrence T .n/\\x14T .n=2/C‚.n/,whosesolution is T .n/DO.n/.\\nSolutionto Exercise 16.2-7\\nThissolutionisalsopostedpublicly\\nSortAandBinto monotonically decreasing order.\\nHere’s a proof that this method yields an optimal solution. C onsider any indices i\\nandjsuch that i < j,and consider the terms aibiandajbj. Wewant toshow that\\nitisnoworsetoincludethesetermsinthepayoffthantoincl udeaibjandajbi,i.e.,\\nthataibiajbj\\x15aibjajbi. Since AandBare sorted into monotonically decreasing\\norder and i < j, we have ai\\x15ajandbi\\x15bj. Since aiandajare positive\\nandbi/NULbjis nonnegative, we have aibi/NULbj\\x15ajbi/NULbj. Multiplying both sides by\\naibjajbjyields aibiajbj\\x15aibjajbi.\\nSince the order of multiplication doesn’t matter, sorting AandBinto monotoni-\\ncally increasing order works as well.\\nSolutionto Exercise 16.3-1\\nWearegiventhat x:freq\\x14y:freqarethetwolowest frequencies inorder, andthat\\na:freq\\x14b:freq. Now,\\nb:freqDx:freq\\n)a:freq\\x14x:freq\\n)a:freqDx:freq(since x:freqisthe lowest frequency) ,\\nand since y:freq\\x14b:freq,\\nb:freqDx:freq\\n)y:freq\\x14x:freq\\n)y:freqDx:freq(since x:freqisthe lowest frequency) .\\nThus, if weassume that x:freqDb:freq, then wehave that each of a:freq,b:freq,\\nandy:freqequals x:freq,and so a:freqDb:freqDx:freqDy:freq.\\nSolutionto Exercise 16.4-2\\nWeneed toshow three things toprove that .S;/TAB/is amatroid:\\n1.Sis ﬁnite. That’s because Sisthe set of of mcolumns of matrix T.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 319}),\n",
              " Document(page_content='16-18 Solutions for Chapter 16: Greedy Algorithms\\n2./TABishereditary. That’sbecause if B2/TAB,then thecolumns in Barelinearly in-\\ndependent. If A\\x12B,thenthecolumnsof Amustalsobelinearlyindependent,\\nand so A2/TAB.\\n3..S;/TAB/satisﬁestheexchangeproperty. Toseewhy,letussupposeth atA; B2/TAB\\nandjAj<jBj.\\nWewill use thefollowing properties of matrices:\\n\\x0fThe rank of a matrix is the number of columns in a maximal set of linearly\\nindependent columns (see page 1223 of the text). The rank is a lso equal to\\nthe dimension of the column space of thematrix.\\n\\x0fIf the column space of matrix Bis a subspace of the column space of ma-\\ntrixA, then rank .B/\\x14rank.A/.\\nBecause the columns in Aare linearly independent, if we take just these\\ncolumns as a matrix A, we have that rank .A/DjAj. Similarly, if we take\\nthe columns of Bas amatrix B, wehave rank .B/DjBj. SincejAj<jBj, we\\nhave rank .A/ <rank.B/.\\nWeshall showthat thereissomecolumn b2Bthat isnotalinear combination\\nofthecolumnsin A,andso A[fbgislinearlyindependent. Theproofproceeds\\nby contradiction. Assume that each column in Bis a linear combination of\\nthe columns of A. That means that any vector that is a linear combination\\nof the columns of Bis also a linear combination of the columns of A, and\\nso, treating the columns of AandBas matrices, the column space of Bis a\\nsubspace of the column space of A. By the second property above, we have\\nrank.B/\\x14rank.A/. But we have already shown that rank .A/ <rank.B/, a\\ncontradiction. Therefore, some column in Bis not a linear combination of the\\ncolumns of A,and .S;/TAB/satisﬁes theexchange property.\\nSolution to Exercise16.4-3\\n[This exercise deﬁnes what is commonly known as the dual of a m atroid, and it\\nasks to prove that the dual of a matroid is itself a matroid. Th e literature contains\\nsimpler proofs of this fact, but they depend on other (equiva lent) deﬁnitions of\\na matroid. The proof given here is more complicated, but it re lies only on the\\ndeﬁnition given inthe text.]\\nWeneed to show three things toprove that .S;/TAB0/is amatroid:\\n1.Sisﬁnite. Wearegiven that.\\n2./TAB0is hereditary. Suppose that B02/TAB0andA0\\x12B0. Since B02/TAB0, there is\\nsome maximal set B2/TABsuch that B\\x12S/NULB0. But A0\\x12B0implies that\\nS/NULB0\\x12S/NULA0,and so B\\x12S/NULB0\\x12S/NULA0. Thus, there exists amaximal\\nsetB2/TABsuch that B\\x12S/NULA0,proving that A02/TAB0.\\n3..S;/TAB0/satisﬁes the exchange property. We start with two prelimina ry facts\\nabout sets. Theproofs of these facts are omitted.\\nFact 1:jX/NULYjDjXj/NULjX\\\\Yj.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 320}),\n",
              " Document(page_content='Solutions forChapter 16: Greedy Algorithms 16-19\\nFact 2:LetSbetheuniverseofelements. If X/NULY\\x12ZandZ\\x12S/NULY,then\\njX\\\\ZjDjXj/NULjX\\\\Yj.\\nToshowthat .S;/TAB0/satisﬁestheexchangeproperty, letusassumethat A02/TAB0,\\nB02/TAB0, and thatjA0j<jB0j. We need to show that there exists some x2\\nB0/NULA0such that A0[fxg2/TAB0. Because A02/TAB0andB02/TAB0, there are\\nmaximal sets A\\x12S/NULA0andB\\x12S/NULB0such that A2/TABandB2/TAB.\\nDeﬁne the set XDB0/NULA0/NULA, so that Xconsists of elements in B0but not\\ninA0orA.\\nIfXis nonempty, then let xbe any element of X. By how we deﬁned set X,\\nwe know that x2B0andx62A0, so that x2B0/NULA0. Since x62A, we also\\nhave that A\\x12S/NULA0/NULfxgDS/NUL.A0[fxg/, and so A0[fxg2/TAB0.\\nIfXisempty, the situation ismore complicated. Because jA0j<jB0j,wehave\\nthatB0/NULA0¤;,and so Xbeing empty means that B0/NULA0\\x12A.\\nClaim\\nThere isan element y2B/NULA0such that .A/NULB0/[fyg2/TAB.\\nProofFirst,observethatbecause A/NULB0\\x12AandA2/TAB,wehavethat A/NULB02\\n/TAB. Similarly, B/NULA0\\x12BandB2/TAB, and so B/NULA02/TAB. If we show\\nthatjA/NULB0j<jB/NULA0j, the assumption that .S;/TAB/is a matroid proves the\\nexistence of y.\\nBecause B0/NULA0\\x12AandA\\x12S/NULA0, we can apply Fact 2 to conclude\\nthatjB0\\\\AjDjB0j/NULjB0\\\\A0j. We claim thatjB\\\\A0j\\x14jA0/NULB0j. To\\nsee why, observe that A0/NULB0DA0\\\\.S/NULB0/andB\\x12S/NULB0, and so\\nB\\\\A0\\x12.S/NULB0/\\\\A0DA0\\\\.S/NULB0/DA0/NULB0. Applying Fact 1, we\\nseethatjA0/NULB0jDjA0j/NULjA0\\\\B0jDjA0j/NULjB0\\\\A0j,andhencejB\\\\A0j\\x14\\njA0j/NULjB0\\\\A0j.\\nNow,wehave\\njA0j<jB0j (by assumption)\\njA0j/NULjB0\\\\A0j<jB0j/NULjB0\\\\A0j(subtracting samequantity)\\njB\\\\A0j<jB0j/NULjB0\\\\A0j(jB\\\\A0j\\x14jA0j/NULjB0\\\\A0j)\\njB\\\\A0j<jB0\\\\Aj (jB0\\\\AjDjB0j/NULjB0\\\\A0j)\\njBj/NULjB\\\\A0j>jAj/NULjB0\\\\Aj(jAjDjBj)\\njB/NULA0j>jA/NULB0j (Fact 1) (claim)\\nNow we know there is an element y2B/NULA0such that .A/NULB0/[fyg2/TAB.\\nMoreover, we claim that y62A. To see why, we know that by the exchange\\nproperty, we can, without loss of generality, choose yso that y62A/NULB0. In\\norder for ytobe in A,it would have to bein A\\\\B0. But y2B,which means\\nthaty62B0,and hence y62A\\\\B0. Therefore y62A.\\nApplyingtheexchangeproperty, weaddsuchanelement yinB/NULA0toA/NULB0,\\nmaintaining that the set we get, say C, is in /TAB. Then we keep applying the\\nexchange property, adding anewelement in A/NULCtoC,maintaining that Cis\\nin/TAB, untiljCjDjAj. OncejCjDjAj, there must exist some element x2A', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 321}),\n",
              " Document(page_content='16-20 Solutions for Chapter 16: Greedy Algorithms\\nthat we have not added into C. We know that such an element exists because\\nthe element ythat we ﬁrst added into Cwas not in A, and so some element x\\ninAmust be left over. Also, we must have x2B0because all the elements\\ninA/NULB0areinitially in C. Therefore, wehave x2B0/NULA0.\\nThe set Cso constructed is maximal, because it has the same cardinali ty as A,\\nwhich is maximal, and C2/TAB. All the elements but one in Care also in A;\\nthe one exception is in B/NULA0, and so Ccontains no elements in A0. Because\\nwe never added xtoC, we have that C\\x12S/NULA0/NULfxgDS/NUL.A0[fxg/.\\nTherefore, A0[fxg2/TAB0,as weneeded to show.\\nSolution to Problem 16-1\\nBefore we go into the various parts of this problem, let us ﬁrs t prove once and for\\nall that the coin-changing problem has optimal substructur e.\\nSuppose wehave an optimal solution for a problem of making ch ange for ncents,\\nand we know that this optimal solution uses a coin whose value isccents; let this\\noptimal solution use kcoins. We claim that this optimal solution for the problem\\nofncentsmustcontainwithinitanoptimalsolutionfortheprob lemof n/NULccents.\\nWe use the usual cut-and-paste argument. Clearly, there are k/NUL1coins in the\\nsolution tothe n/NULccents problem used within our optimal solution tothe ncents\\nproblem. Ifwehadasolutiontothe n/NULccentsproblem thatusedfewerthan k/NUL1\\ncoins, then wecould use this solution to produce a solution t othe ncents problem\\nthat uses fewer than kcoins, which contradicts the optimality of our solution.\\na.Agreedyalgorithm tomakechange usingquarters, dimes,nic kels, andpennies\\nworks asfollows:\\n\\x0fGive qDbn=25cquarters. That leaves nqDnmod25cents to make\\nchange.\\n\\x0fThen give dDbnq=10cdimes. That leaves ndDnqmod10cents to make\\nchange.\\n\\x0fThen give kDbnd=5cnickels. That leaves nkDndmod5cents to make\\nchange.\\n\\x0fFinally, give pDnkpennies.\\nAn equivalent formulation is the following. The problem we w ish to solve is\\nmaking change for ncents. If nD0, the optimal solution is to give no coins.\\nIfn > 0, determine the largest coin whose value is less than or equal ton.\\nLet this coin have value c. Give one such coin, and then recursively solve the\\nsubproblem of making change for n/NULccents.\\nTo prove that this algorithm yields an optimal solution, we ﬁ rst need to show\\nthat the greedy-choice property holds, that is, that some op timal solution to\\nmaking change for ncents includes one coin of value c, where cis the largest\\ncoin value such that c\\x14n. Consider some optimal solution. If this optimal\\nsolution includes a coin of value c, then we are done. Otherwise, this optimal\\nsolution does not include acoin of value c. Wehave four cases to consider:', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 322}),\n",
              " Document(page_content='Solutions forChapter 16: Greedy Algorithms 16-21\\n\\x0fIf1\\x14n < 5, then cD1. A solution may consist only of pennies, and so it\\nmust contain the greedy choice.\\n\\x0fIf5\\x14n < 10, then cD5. By supposition, this optimal solution does not\\ncontain a nickel, and so it consists of only pennies. Replace ﬁve pennies by\\none nickel togive asolution withfour fewer coins.\\n\\x0fIf10\\x14n < 25, then cD10. Bysupposition, this optimal solution does not\\ncontain a dime, and so it contains only nickels and pennies. S ome subset of\\nthe nickels and pennies in this solution adds up to 10cents, and so we can\\nreplace these nickels and pennies byadimetogiveasolution with(between\\n1and 9) fewer coins.\\n\\x0fIf25\\x14n, then cD25. By supposition, this optimal solution does not\\ncontain a quarter, and so it contains only dimes, nickels, an d pennies. If\\nit contains three dimes, we can replace these three dimes by a quarter and\\na nickel, giving a solution with one fewer coin. If it contain s at most two\\ndimes, then some subset of the dimes, nickels, and pennies ad ds up to 25\\ncents, and so we can replace these coins by one quarter to give a solution\\nwithfewer coins.\\nThus, we have shown that there is always an optimal solution t hat includes the\\ngreedychoice,andthatwecancombinethegreedychoicewith anoptimalsolu-\\ntiontotheremainingsubproblem toproduce anoptimal solut iontoouroriginal\\nproblem. Therefore, the greedy algorithm produces an optim al solution.\\nFor the algorithm that chooses one coin at a time and then recu rses on sub-\\nproblems, the running time is ‚.k/, where kis the number of coins used in\\nan optimal solution. Since k\\x14n, the running time is O.n/. For our ﬁrst de-\\nscription ofthealgorithm, weperform aconstant number ofc alculations (since\\nthere areonly 4coin types), and the running time is O.1/.\\nb.When the coin denominations are c0; c1; : : : ; ck, the greedy algorithm to make\\nchange for ncents works by ﬁnding the denomination cjsuch that jD\\nmaxf0\\x14i\\x14kWci\\x14ng, giving one coin of denomination cj, and recurs-\\ning on the subproblem of making change for n/NULcjcents. (An equivalent,\\nbut more efﬁcient, algorithm is to give\\x04\\nn=ck˘\\ncoins of denomination ckand\\nb.nmodciC1/=ciccoins of denomination ciforiD0; 1; : : : ; k/NUL1.)\\nTo show that the greedy algorithm produces an optimal soluti on, we start by\\nproving the following lemma:\\nLemma\\nForiD0; 1; : : : ; k , letaibe the number of coins of denomination ciused\\nin an optimal solution to the problem of making change for ncents. Then for\\niD0; 1; : : : ; k/NUL1, wehave ai< c.\\nProofIfai\\x15cforsome 0\\x14i < k,thenwecanimprovethesolutionbyusing\\nonemorecoinofdenomination ciC1andcfewercoinsofdenomination ci. The\\namount for which we make change remains the same, but we use c/NUL1 > 0\\nfewer coins. (lemma)\\nTo show that the greedy solution is optimal, we show that any n on-greedy so-\\nlution is not optimal. As above, let jDmaxf0\\x14i\\x14kWci\\x14ng, so that the', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 323}),\n",
              " Document(page_content='16-22 Solutions for Chapter 16: Greedy Algorithms\\ngreedy solution uses at least one coin of denomination cj. Consider a non-\\ngreedysolution, whichmustusenocoinsofdenomination cjorhigher. Letthe\\nnon-greedy solution use aicoins of denomination ci, foriD0; 1; : : : ; j/NUL1;\\nthus we havePj/NUL1\\niD0aiciDn. Since n\\x15cj, we have thatPj/NUL1\\niD0aici\\x15cj.\\nNow suppose that the non-greedy solution is optimal. By the a bove lemma,\\nai\\x14c/NUL1foriD0; 1; : : : ; j/NUL1. Thus,\\nj/NUL1X\\niD0aici\\x14j/NUL1X\\niD0.c/NUL1/ci\\nD.c/NUL1/j/NUL1X\\niD0ci\\nD.c/NUL1/cj/NUL1\\nc/NUL1\\nDcj/NUL1\\n< cj;\\nwhich contradicts our earlier assertion thatPj/NUL1\\niD0aici\\x15cj. We conclude that\\nthe non-greedy solution isnot optimal.\\nSince any algorithm that does not produce the greedy solutio n fails to be opti-\\nmal, only the greedy algorithm produces the optimal solutio n.\\nTheproblem didnotaskfortherunningtime,butforthemoree fﬁcientgreedy-\\nalgorithm formulation, it is easy to see that the running tim e isO.k/, since we\\nhave toperform at most keach of the division, ﬂoor, and modoperations.\\nc.With actual U.S.coins, wecan use coins of denomination 1, 10 , and 25. When\\nnD30cents, thegreedy solution givesonequarter andﬁvepennies , for atotal\\nof six coins. Thenon-greedy solution of three dimes is bette r.\\nThe smallest integer numbers we can use are 1, 3, and 4. When nD6cents,\\nthe greedy solution gives one 4-cent coin and two 1-cent coin s, for a total of\\nthree coins. Thenon-greedy solution of two3-cent coins isb etter.\\nd.Since we have optimal substructure, dynamic programming mi ght apply. And\\nindeed it does.\\nLetusdeﬁne cŒj \\x8dtobetheminimumnumber ofcoins weneedtomakechange\\nforjcents. Let the coin denominations be d1; d2; : : : ; d k. Since one of the\\ncoins is apenny, there isawayto make change for anyamount j\\x151.\\nBecause of the optimal substructure, if we knew that an optim al solution for\\nthe problem of making change for jcents used a coin of denomination di, we\\nwould have cŒj \\x8dD1CcŒj/NULdi\\x8d. Asbase cases, wehave that cŒj \\x8dD0for all\\nj\\x140.\\nTodeveloparecursiveformulation, wehavetocheckalldeno minations, giving\\ncŒj \\x8dD(\\n0 ifj\\x140 ;\\n1Cmin\\n1\\x14i\\x14kfcŒj/NULdi\\x8dgifj > 1 :\\nWe can compute the cŒj \\x8dvalues in order of increasing jby using a table. The\\nfollowing procedure does so, producing a table cŒ1 : : n\\x8d. It avoids even exam-\\nining cŒj \\x8dforj\\x140by ensuring that j\\x15dibefore looking up cŒj/NULdi\\x8d. The', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 324}),\n",
              " Document(page_content='Solutions forChapter 16: Greedy Algorithms 16-23\\nprocedure also produces a table denom Œ1 : : n\\x8d, wheredenom Œj \\x8dis the denomi-\\nnation of a coin used in an optimal solution to the problem of m aking change\\nforjcents.\\nCOMPUTE-CHANGE .n; d; k/\\nletcŒ1 : : n\\x8danddenom Œ1 : : n\\x8dbe new arrays\\nforjD1ton\\ncŒj \\x8dD1\\nforiD1tok\\nifj\\x15diand1CcŒj/NULdi\\x8d < cŒj \\x8d\\ncŒj \\x8dD1CcŒj/NULdi\\x8d\\ndenom Œj \\x8dDdi\\nreturn canddenom\\nThis procedure obviously runs in O.nk/time.\\nWeusethefollowing procedure tooutput thecoinsusedinthe optimal solution\\ncomputed by C OMPUTE-CHANGE:\\nGIVE-CHANGE .j;denom /\\nifj > 0\\ngive one coin of denomination denom Œj \\x8d\\nGIVE-CHANGE .j/NULdenom Œj \\x8d;denom /\\nThe initial call is G IVE-CHANGE .n;denom /. Since the value of the ﬁrst pa-\\nrameter decreases ineach recursive call, this procedure ru ns in O.n/time.\\nSolutionto Problem 16-5\\na.The procedure C ACHE-MANAGER is a generic procedure, which initializes a\\ncache by calling I NITIALIZE -CACHEand then calls A CCESSwith each data\\nelement in turn. The inputs are a sequence RDhr1; r2; : : : ; r niof memory\\nrequests and acache size k.\\nCACHE-MANAGER .R; k/\\nINITIALIZE -CACHE .R; k/\\nforiD1ton\\nACCESS .ri/\\nThe running time of C ACHE-MANAGER of course depends heavily on how\\nACCESSis implemented. We have several choices for how to implement the\\ngreedy strategy outlined in the problem. A straightforward way of implement-\\ning the greedy strategy is that when processing request ri, for each of the at\\nmost kelements currently in the cache, scan through requests riC1; : : : ; r nto\\nﬁnd which of the elements in the cache and rihas its next access furthest in\\nthe future, and evict this element. Because each scan takes O.n/time, each\\nrequest entails O.k/scans, and there are nrequests, the running time of this\\nstraightforward approach is O.kn2/.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 325}),\n",
              " Document(page_content='16-24 Solutions for Chapter 16: Greedy Algorithms\\nInstead, wedescribe anasymptotically faster algorithm, w hichusesared-black\\ntree to check whether a given element is currently in the cach e, a max-priority\\nqueuetoretrievethedataelementwiththefurthestaccesst ime,andahashtable\\n(resolving collisions by chaining) to map data elements to i nteger indices. We\\nassume that the data elements can be linearly ordered, so tha t it makes sense\\nto put them into a red-black tree and a max-priority queue. Th e following pro-\\ncedure I NITIALIZE -CACHEcreates and initializes some global data structures\\nthat are used by A CCESS.\\nINITIALIZE -CACHE .R; k/\\nletTbe anew red-black tree\\nletPbeanew max-priority queue\\nletHbe anew hash table\\nindD1\\nforiD1ton\\njDHASH-SEARCH .ri/\\nifj==NIL\\nHASH-INSERT .ri;ind/\\nletSindbeanew linked list\\njDind\\nindDindC1\\nappend itoSj\\nInthe above procedure, here is themeaning of various variab les:\\n\\x0fThered-blacktree Thasatmost knodesandholdsthedistinctdataelements\\nthat arecurrently inthecache. Weassumethat thered-black treeprocedures\\nare modiﬁed to keep track of the number of nodes currently in t he tree, and\\nthat the procedure T REE-SIZEreturns this value. Because red-black tree T\\nhasatmost knodes,wecaninsertinto,deletefrom,orsearchinitin O.lgk/\\nworst-case time.\\n\\x0fThe max-priority queue Pcontains elements with two attributes: keyis the\\nnext access time of a data element, and \\x17alueis the actual data element\\nfor each data element in the cache. keygives the key and \\x17alueis satellite\\ndata in the priority queue. Likethe red-black tree T, the max-priority queue\\ncontains onlyelements currently inthecache. Weneedtomai ntain TandP\\nseparately, however,because Tiskeyedonthedataelementsand Piskeyed\\non access times. Using a max-heap to implement P, we can extract the\\nmaximum element or insert a new element in O.lgk/time, and we can ﬁnd\\nthe maximum element in ‚.1/time.\\n\\x0fThehashtable Hisadictionaryoramap,whichmapseachdataelementtoa\\nunique integer. Thisinteger isusedtoindexlinked lists,w hicharedescribed\\nnext. Weassumethat the H ASH-INSERTprocedure usesthetable-expansion\\ntechnique of Section 17.4.1 tokeep thehash table’s load fac tor tobeat most\\nsome constant ˛. In this way, the amortized cost per insertion is ‚.1/and,\\nunder the assumption of simple uniform hashing, then by Theo rems 11.1\\nand 11.2, the average-case search timeisalso ‚.1/.\\n\\x0fFor every distinct data element ri, we create a linked list Sind(where\\nindis obtained through the hash table) holding the indices in th e in-', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 326}),\n",
              " Document(page_content='Solutions forChapter 16: Greedy Algorithms 16-25\\nput array where rioccurs. For example, if the input sequence is\\nhd; b; d; b; d; a; c; d; b; a; c; b i, then we create four linked lists: S1fora,\\nS2forb,S3forc, and S4ford.S1holds the indices where ais accessed,\\nand so S1D h6; 10i. Similarly, S2D h2; 4; 9; 12i,S3D h7; 11iand\\nS4Dh1; 3; 5; 8i.\\nFor each data element ri, we ﬁrst check whether there is already a linked list\\nassociated with riand create a new linked list if not. Weretrieve the linked lis t\\nassociated with riand append ito it, indicating that an access to rioccurs at\\naccess i.\\nACCESS .ri/\\n//Compute the next access timefor ri.\\nindDHASH-SEARCH .ri/\\ntimeD1\\ndelete the head of Sind\\nifSindisnot empty\\ntimeDhead of Sind\\n//Check tosee whether riiscurrently inthe cache.\\nifTREE-SEARCH .T:root; ri/¤NIL\\nprint “cache hit”\\nelseifTREE-SIZE.T / < k\\n//Insert inan empty slot inthe cache.\\nlet´bea newnode for T\\n´:keyDri\\nRB-INSERT .T; ´/\\nlete\\x17entbeanew object for P\\ne\\x17ent:keyDtime\\ne\\x17ent:\\x17alueDri\\nINSERT .P;e\\x17ent/\\nprint “cache miss, inserted ” ri“inempty slot”\\nelsee\\x17entDMAXIMUM .P /\\nife\\x17ent:key\\x14time//rihasthe furthest access time\\nprint “cache miss, no data element evicted”\\nelse//evict the element withfurthest access time\\nprint “cache miss, evict data element ” e\\x17ent:\\x17alue\\ne\\x17entDEXTRACT-MAX.P /\\nRB-DELETE .T;TREE-SEARCH .T:root;e\\x17ent:\\x17alue//\\ne\\x17ent:keyDtime\\ne\\x17ent:\\x17alueDri\\nINSERT .P;e\\x17ent/\\nlet´be anew node for T\\n´:keyDri\\nRB-INSERT .T; ´/\\nThe procedure A CCESStakes an input riand decides which element to evict,\\nif any, from the cache. The ﬁrst ifcondition properly sets timeto the next\\naccess time of ri. The head of the linked list associated with ricontains i; we\\nremove this element from the list, and the new head contains t he next access', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 327}),\n",
              " Document(page_content='16-26 Solutions for Chapter 16: Greedy Algorithms\\ntime for ri. Then, we check to see whether riis already present in the cache.\\nIfriis not present in the cache, we check to see whether we can stor eriin\\nan empty slot. If there are no empty slots, we have to evict the element with\\nthe furthest access time. We retrieve the element with the fu rthest access time\\nfrom the max-priority queue and compare it with that of ri. Ifri’s next access\\nis sooner, we evict the element with the furthest access time from the cache\\n(deleting the element from the tree and from the priority que ue) and insert ri\\ninto the tree and priority queue.\\nUnder the assumption of simple uniform hashing, the average -case running\\ntime of A CCESSisO.lgk/, since it performs a constant number of operations\\non the red-black tree, priority queue, and hash table. Thus, the average-case\\nrunning time of C ACHE-MANAGER isO.nlgk/.\\nb.Toshowthat theproblem exhibits optimal substructure, wed eﬁnethesubprob-\\nlem.C; i/as the contents of the cache just before the ith request, where Cis a\\nsubsetofthesetofinputdataelementscontainingatmost kofthem. A solution\\nto.C; i/isasequenceofdecisionsthatspeciﬁeswhichelementtoevi ct(ifany)\\nfor each request i; iC1; : : : ; n. Anoptimal solution to.C; i/is asolution that\\nminimizes the number of cache misses.\\nLetSbe an optimal solution to .C; i/. Let S0be the subsolution of Sfor\\nrequests iC1; iC2; : : : ; n. If a cache hit occurs on the ith request, then the\\ncacheremainsunchanged. Ifacachemissoccurs, thenthe ithrequest resultsin\\nthe contents of the cache changing to C0(possibly with C0DCif no element\\nwasevicted). Weclaimthat S0isanoptimal solutionto .C0; iC1/. Why? If S0\\nwerenotanoptimalsolutionto .C0; iC1/,thenthereexistsanothersolution S00\\nto.C0; iC1/thatmakesfewercachemissesthan S0. Bycombining S00withthe\\ndecision of Sat the ith request, we obtain another solution that makes fewer\\ncache misses than S, which contradicts our assumption that Sis an optimal\\nsolution to .C; i/.\\nSuppose the ith request results in a cache miss. Let PCbe the set of all cache\\nstates that can be reached from Cthrough a single decision of the cache man-\\nager. The set PCcontains up to kC1states: kof them arising from different\\nelements of the cache being evicted and one arising from the d ecision of evict-\\ningnoelement. Forexample,if CDfr1; r2; r3gandtherequesteddataelement\\nisr4, then PCDffr1; r2; r3g;fr1; r2; r4g;fr1; r3; r4g;fr2; r3; r4gg.\\nLetmiss.C; i/denote the minimumnumber of cache missesfor .C; i/. Wecan\\nstate arecurrence for miss.C; i/as\\nmiss.C; i/D†\\n0 ifiDnandrn2C ;\\n1 ifiDnandrn62C ;\\nmiss.C; iC1/ ifi < nandri2C ;\\n1Cmin\\nC02PCfmiss.C0; iC1/gifi < nandri62C :\\nThus, weconclude that the problem exhibits optimal substru cture.\\nc.To prove that the furthest-in-future strategy yields an opt imal solution, we\\nshow that the problem exhibits the greedy-choice property. Combined with the\\noptimal-substructure property from part (b), the greedy-c hoice property will', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 328}),\n",
              " Document(page_content='Solutions forChapter 16: Greedy Algorithms 16-27\\nprove that furthest-in-future produces the minimum possib le number of cache\\nmisses.\\nWe use the deﬁnitions of subproblem, solution, and optimal s olution from\\npart (b). Since we will be comparing different solutions, le t us deﬁne CAias\\nthe state of the cache for solution Ajust before the ith request. The following\\ntheorem isthe key.\\nTheorem (Greedy-choice property)\\nLetAbe some optimal solution to .C; i/. Let bbe the element in CAi[frig\\nwhose next access at the time of the ith request is furthest in the future, at\\ntimem. Then, we can construct another solution A0to.C; i/that has the fol-\\nlowing properties:\\n1. Onthe ithrequest, A0evicts b.\\n2. For iC1\\x14j\\x14m, the caches CAjandCA0jdiffer by at most one element.\\nIf they differ, then b2CAjis always the element in CAjthat is not in CA0j.\\nEquivalently, if CAjandCA0jdiffer, we can write CAjDDj[fbgand\\nCA0jDDj[fxg, where Djis a size-( k/NUL1) set and x¤bis some data\\nelement.\\n3. For requests i; : : : ; m/NUL1, ifAhas acache hit, then A0has acache hit.\\n4.CAjDCA0jforj > m.\\n5. Forrequests i; : : : ; m,thenumber ofcachemissesproduced by A0isatmost\\nthe number of cache misses produced by A.\\nProofIfAevicts bat request i,then the proof of the theorem istrivial. There-\\nfore,suppose Aevictsdataelement aonrequest i,where a¤b. Wewillprove\\nthe theorem byconstructing A0inductively for each request.\\n(1) At request i,A0evicts binstead of a.\\n(2) We proceed with induction on j, where iC1\\x14j\\x14m. The construction\\nfor property 1 establishes the base case because CA;iC1andCA0;iC1differ by\\njust one element and bis the element in CA;iC1that isnot in CA0;iC1.\\nFor the induction step, suppose property 2 is true for some re quest j, where\\niC1\\x14j < m. IfAdoes not evict any element or evicts an element in Dj,\\nthen construct A0to make the same decision on request jasAmakes. If A\\nevicts bon request j, then construct A0to evict xand keep the same element\\nasAkeeps, namely rj. This construction conserves property 2for jC1. Note\\nthat this construction might sometimes insert duplicate el ements in the cache.\\nThis situation caneasily be dealt with byintroducing adumm y element for x.\\n(3) Suppose Ahas a cache hit for request j, where i\\x14j\\x14m/NUL1. Then,\\nrj2Djsince rj¤b. Thus, rj2CA0jandA0has acache hit, too.\\n(4) By property 2, the cache CAmdiffers from CA0mby at most one element,\\nwith bbeing the element in CAmthat might not be in CA0m. IfCAmDCA0m,\\nthen construct A0to make the same decision on request masA. Otherwise,\\nCAm¤CA0mandb2CAm. Construct A0to evict xand keep bon request m.\\nSincethe mthrequest isforelement bandb2CAm,Ahasacache hit sothat it\\ndoes not evict anyelement. Thus, wecan ensure that CA;mC1DCA0;mC1. From\\nthe.mC1/st request on, A0simply makes the samedecisions as A.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 329}),\n",
              " Document(page_content='16-28 Solutions for Chapter 16: Greedy Algorithms\\n(5) By property 3, for requests i; : : : ; m/NUL1, whenever we have a cache hit\\nforA,wealsohaveacachehit for A0. Thus,wehavetoconcern ourselves with\\nonly the mth request. If Ahas a cache miss on the mth request, we are done.\\nOtherwise, Ahas a cache hit on the mth request, and we will prove that there\\nexistsatleastonerequest j,where iC1\\x14j\\x14m/NUL1,suchthatthe jthrequest\\nresults in a cache miss for Aand a cache hit for A0. Because Aevicts data\\nelement ain request i, then, by our construction of A0,CA0;iC1DDiC1[fag.\\nThemth request is for data element b. IfAhas a cache hit, then because\\nnone of the requests iC1; : : : ; m/NUL1were for b,Acould not have evicted b\\nand brought it back. Moreover, because Ahas a cache hit on the mth request,\\nb2CAm. Therefore, Adid not evict bin any of requests i; : : : ; m/NUL1. By\\nour construction, A0did not evict a. But a request for aoccurs at least once\\nbefore the mthrequest. Consider theﬁrstsuchinstance. Atthisinstanc e,Ahas\\nacache missand A0has acache hit.\\nThe above theorem and the optimal-substructure property pr oved in part (b)\\nimply that furthest-in-future produces theminimum number of cache misses.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 330}),\n",
              " Document(page_content='Lecture Notes forChapter 17:\\nAmortized Analysis\\nChapter 17overview\\nAmortized analysis\\n\\x0fAnalyze a sequence of operations ona data structure.\\n\\x0fGoal:Show that although some individual operations may be expens ive,on\\naveragethecost per operation issmall.\\nAveragein this context does not mean that we’re averaging over a dist ribution of\\ninputs.\\n\\x0fNoprobability isinvolved.\\n\\x0fWe’re talking about average cost inthe worst case .\\nOrganization\\nWe’ll look at 3methods:\\n\\x0faggregate analysis\\n\\x0faccounting method\\n\\x0fpotential method\\nUsing 3examples:\\n\\x0fstack withmultipop operation\\n\\x0fbinary counter\\n\\x0fdynamic tables (later on)\\nAggregateanalysis\\nStack operations\\n\\x0fPUSH.S; x/:O.1/each)O.n/for any sequence of noperations.\\n\\x0fPOP.S/:O.1/each)O.n/for any sequence of noperations.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 331}),\n",
              " Document(page_content='17-2 Lecture Notes for Chapter 17: AmortizedAnalysis\\n\\x0fMULTIPOP .S; k/\\nwhile Sisnot empty and k > 0\\nPOP.S/\\nkDk/NUL1\\nRunning time of M ULTIPOP:\\n\\x0fLinear in#of P OPoperations.\\n\\x0fLet each P USH/POPcost 1.\\n\\x0f#of iterations of whileloop is min .s; k/, where sD#of objects onstack.\\n\\x0fTherefore, total cost Dmin.s; k/.\\nSequence of nPUSH, POP, MULTIPOP operations:\\n\\x0fWorst-case cost of M ULTIPOP isO.n/.\\n\\x0fHave noperations.\\n\\x0fTherefore, worst-case cost of sequence is O.n2/.\\nObservation\\n\\x0fEachobject can be popped only once per timethat it’s pushed.\\n\\x0fHave\\x14nPUSHes)\\x14 nPOPs, including those in M ULTIPOP.\\n\\x0fTherefore, total cost DO.n/.\\n\\x0fAverage over the noperations)O.1/per operation on average.\\nAgain, notice no probability.\\n\\x0fShowedworst-case O.n/cost for sequence.\\n\\x0fTherefore, O.1/per operation on average.\\nThis technique iscalled aggregate analysis .\\nBinary counter\\n\\x0fk-bit binary counter AŒ0 : : k/NUL1\\x8dof bits, where AŒ0\\x8dis the least signiﬁcant bit\\nandAŒk/NUL1\\x8dis themost signiﬁcant bit.\\n\\x0fCounts upward from 0.\\n\\x0fValue of counter isk/NUL1X\\niD0AŒi\\x8d\\x012i.\\n\\x0fInitially, counter value is 0, soAŒ0 : : k/NUL1\\x8dD0.\\n\\x0fToincrement, add 1 .mod 2k/:\\nINCREMENT .A; k/\\niD0\\nwhile i < kandAŒi\\x8d==1\\nAŒi\\x8dD0\\niDiC1\\nifi < k\\nAŒi\\x8dD1', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 332}),\n",
              " Document(page_content='Lecture Notes for Chapter 17: AmortizedAnalysis 17-3\\nExample: kD3\\n[Underlined bits ﬂip. Show costs later.]\\ncounter A\\nvalue 210 cost\\n0 000 0\\n1 001 1\\n2 010 3\\n3 011 4\\n4 100 7\\n5 101 8\\n6 110 10\\n7 111 11\\n0 000 14\\n::::::15\\nCost of I NCREMENTD‚(# of bits ﬂipped) .\\nAnalysis\\nEachcall could ﬂip kbits, so nINCREMENT s takes O.nk/time.\\nObservation\\nNot every bit ﬂipsevery time.\\n[Show costs from above.]\\nbit ﬂips how often times in nINCREMENT s\\n0 every time n\\n1 1=2the time bn=2c\\n2 1=4the time bn=4c\\n:::\\ni 1=2ithe time bn=2ic\\n:::\\ni\\x15knever 0\\nTherefore, total #of ﬂips Dk/NUL1X\\niD0\\x04\\nn=2i˘\\n< n1X\\niD01=2i\\nDn\\x121\\n1/NUL1=2\\x13\\nD2n :\\nTherefore, nINCREMENT s costs O.n/.\\nAverage cost per operation DO.1/.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 333}),\n",
              " Document(page_content='17-4 Lecture Notes for Chapter 17: AmortizedAnalysis\\nAccounting method\\nAssign different charges todifferent operations.\\n\\x0fSomearecharged more than actual cost.\\n\\x0fSomearecharged less.\\nAmortized costDamount wecharge.\\nWhen amortized cost >actual cost, store the difference on speciﬁc objects in the\\ndata structure as credit.\\nUsecredit later to pay for operations whose actual cost >amortized cost.\\nDiffers from aggregate analysis:\\n\\x0fInthe accounting method, different operations can have dif ferent costs.\\n\\x0fInaggregate analysis, all operations have same cost.\\nNeed credit tonever go negative.\\n\\x0fOtherwise, have a sequence of operations for which the amort ized cost is not\\nanupper bound on actual cost.\\n\\x0fAmortized cost would tell us nothing.\\nLetciDactual cost of ith operation ;\\nyciDamortized cost of ith operation :\\nThen requirenX\\niD1yci\\x15nX\\niD1ciforallsequences of noperations.\\nTotal credit storedDnX\\niD1yci/NULnX\\niD1ci\\x15\\n„ƒ‚…\\nhadbetter be0.\\nStack\\noperation actual cost amortized cost\\nPUSH 1 2\\nPOP 1 0\\nMULTIPOP min.k; s/ 0\\nIntuition\\nWhen pushing anobject, pay $2.\\n\\x0f$1pays for the P USH.\\n\\x0f$1is prepayment for it being popped byeither P OPor MULTIPOP.\\n\\x0fSince each object has $1, which iscredit, the credit can neve r go negative.\\n\\x0fTherefore, totalamortized cost, DO.n/,isanupper boundontotalactual cost.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 334}),\n",
              " Document(page_content='Lecture Notes for Chapter 17: AmortizedAnalysis 17-5\\nBinarycounter\\nCharge $2 toset abit to1.\\n\\x0f$1 pays for setting abit to1.\\n\\x0f$1 isprepayment for ﬂipping it back to 0.\\n\\x0fHave $1of credit for every 1in the counter.\\n\\x0fTherefore, credit\\x150.\\nAmortized cost of I NCREMENT :\\n\\x0fCost of resetting bits to 0ispaid by credit.\\n\\x0fAt most 1 bit is set to 1.\\n\\x0fTherefore, amortized cost \\x14$2.\\n\\x0fFornoperations, amortized cost DO.n/.\\nPotential method\\nLike the accounting method, but think of the credit as potential stored with the\\nentire data structure.\\n\\x0fAccounting method stores credit withspeciﬁc objects.\\n\\x0fPotential method stores potential in the data structure asa whole.\\n\\x0fCanrelease potential to pay for future operations.\\n\\x0fMost ﬂexible of the amortized analysis methods.\\nLetDiDdata structure after ith operation ;\\nD0Dinitial data structure ;\\nciDactual cost of ithoperation ;\\nyciDamortized cost of ith operation :\\nPotential function ˆWDi!R\\nˆ.D i/is thepotential associated withdata structure Di.\\nyciDciCˆ.D i//NULˆ.D i/NUL1/\\nDciC\\x81ˆ.D i/„ƒ‚…:\\nincrease inpotential due to ith operation\\nTotal amortized cost DnX\\niD1yci\\nDnX\\niD1.ciCˆ.D i//NULˆ.D i/NUL1//\\n(telescoping sum: every term other than D0andDn\\nisadded once and subtracted once)\\nDnX\\niD1ciCˆ.D n//NULˆ.D 0/ :', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 335}),\n",
              " Document(page_content='17-6 Lecture Notes for Chapter 17: AmortizedAnalysis\\nIf we require that ˆ.D i/\\x15ˆ.D 0/for all i, then the amortized cost is always an\\nupper bound on actual cost.\\nIn practice: ˆ.D 0/D0,ˆ.D i/\\x150for all i.\\nStack\\nˆD#of objects instack\\n.D#of $1 bills in accounting method)\\nD0Dempty stack)ˆ.D 0/D0.\\nSince #of objects instack isalways \\x150,ˆ.D i/\\x150Dˆ.D 0/for all i.\\noperation actual cost \\x81ˆ amortized cost\\nPUSH 1 .sC1//NULsD1 1 C1D2\\nwhere sD#of objects initially\\nPOP 1 .s/NUL1//NULsD/NUL1 1/NUL1D0\\nMULTIPOP k0Dmin.k; s/ .s/NULk0//NULsD/NULk0k0/NULk0D0\\nTherefore, amortized cost of asequence of noperationsDO.n/.\\nBinary counter\\nˆDbiD#of 1’safter ith INCREMENT\\nSuppose ith operation resets tibits to 0.\\nci\\x14tiC1(resets tibits, sets\\x141bit to1)\\n\\x0fIfbiD0, theithoperation reset all kbits and didn’t set one, so\\nbi/NUL1DtiDk)biDbi/NUL1/NULti.\\n\\x0fIfbi> 0, theith operation reset tibits, set one, so\\nbiDbi/NUL1/NULtiC1.\\n\\x0fEither way, bi\\x14bi/NUL1/NULtiC1.\\n\\x0fTherefore,\\n\\x81ˆ.D i/\\x14.bi/NUL1/NULtiC1//NULbi/NUL1\\nD1/NULti:\\nyciDciC\\x81ˆ.D i/\\n\\x14.tiC1/C.1/NULti/\\nD2 :\\nIf counter starts at 0, ˆ.D 0/D0.\\nTherefore, amortized cost of noperationsDO.n/.\\nDynamictables\\nA nice use of amortized analysis.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 336}),\n",
              " Document(page_content='Lecture Notes for Chapter 17: AmortizedAnalysis 17-7\\nScenario\\n\\x0fHave atable—maybe a hash table.\\n\\x0fDon’t know inadvance how manyobjects will be stored init.\\n\\x0fWhenitﬁlls,mustreallocatewithalargersize,copyingall objectsintothenew,\\nlarger table.\\n\\x0fWhen it gets sufﬁciently small, mightwant to reallocate withasmaller size.\\nDetails of table organization not important.\\nGoals\\n1.O.1/amortized time per operation.\\n2. Unused space always \\x14constant fraction of allocated space.\\nLoadfactor ˛Dnum=size,wherenumD#items stored, sizeDallocated size.\\nIfsizeD0, thennumD0. Call ˛D1.\\nNever allow ˛ > 1.\\nKeep ˛ >aconstant fraction)goal (2).\\nTableexpansion\\nConsider only insertion.\\n\\x0fWhen the table becomes full, double its size and reinsert all existing items.\\n\\x0fGuarantees that ˛\\x151=2.\\n\\x0fEachtimeweactuallyinsertanitemintothetable,it’san elementaryinsertion .\\nTABLE-INSERT .T; x/\\nifT:size==0\\nallocate T:tablewith1slot\\nT:sizeD1\\nifT:num==T:size //expand?\\nallocatenew-tablewith2\\x01T:sizeslots\\ninsert all items in T:tableintonew-table//T:numelem insertions\\nfreeT:table\\nT:tableDnew-table\\nT:sizeD2\\x01T:size\\ninsert xintoT:table //1elem insertion\\nT:numDT:numC1\\nInitially, T:numDT:sizeD0.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 337}),\n",
              " Document(page_content='17-8 Lecture Notes for Chapter 17: AmortizedAnalysis\\nRunningtime\\nCharge 1 per elementary insertion. Count only elementary in sertions, since all\\nother costs together are constant per call.\\nciDactual cost of ith operation\\n\\x0fIf not full, ciD1.\\n\\x0fIf full, have i/NUL1items in the table at the start of the ith operation. Have to\\ncopy all i/NUL1existing items, then insert ith item)ciDi.\\nnoperations)ciDO.n/)O.n2/timefor noperations.\\nOf course, wedon’t always expand:\\nciD(\\niifi/NUL1is exact power of 2 ;\\n1otherwise :\\nTotal costDnX\\niD1ci\\n\\x14nCblgncX\\njD02j\\nDnC2blgncC1/NUL1\\n2/NUL1\\n< nC2n\\nD3n\\nTherefore, aggregate analysis says amortized cost per operation D3.\\nAccountingmethod\\nCharge $3 per insertion of x.\\n\\x0f$1pays for x’sinsertion.\\n\\x0f$1pays for xtobe movedinthe future.\\n\\x0f$1pays for someother item to bemoved.\\nSuppose we’ve just expanded, sizeDmbefore next expansion, sizeD2mafter\\nnext expansion.\\n\\x0fAssumethattheexpansion usedupallthecredit, sothatther e’snocredit stored\\nafter the expansion.\\n\\x0fWill expand again after another minsertions.\\n\\x0fEachinsertion willput $1ononeofthe mitemsthat wereinthetable just after\\nexpansion and will put $1 onthe item inserted.\\n\\x0fHave $ 2mof credit by next expansion, when there are 2mitems to move. Just\\nenough to pay for the expansion, withno credit left over!', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 338}),\n",
              " Document(page_content='Lecture Notes for Chapter 17: AmortizedAnalysis 17-9\\nPotential method\\nˆ.T /D2\\x01T:num/NULT:size\\n\\x0fInitially,numDsizeD0)ˆD0.\\n\\x0fJust after expansion, sizeD2\\x01num)ˆD0.\\n\\x0fJust before expansion, sizeDnum)ˆDnum)have enough potential to\\npay for moving all items.\\n\\x0fNeed ˆ\\x150, always.\\nAlways have\\nsize\\x15num\\x15size=2)\\n2\\x01num\\x15size)\\nˆ\\x150.\\nAmortized cost of ithoperation\\nnum iDnumafterith operation ,\\nsize iDsizeafterith operation ,\\nˆiDˆafterith operation .\\n\\x0fIf noexpansion:\\nsize iDsize i/NUL1;\\nnum iDnum i/NUL1C1 ;\\nciD1 :\\nThen wehave\\nyciDciCˆi/NULˆi/NUL1\\nD1C.2\\x01num i/NULsize i//NUL.2\\x01num i/NUL1/NULsize i/NUL1/\\nD1C.2\\x01num i/NULsize i//NUL.2.num i/NUL1//NULsize i/\\nD1C2\\nD3 :\\n\\x0fIf expansion:\\nsize iD2\\x01size i/NUL1;\\nsize i/NUL1Dnum i/NUL1Dnum i/NUL1 ;\\nciDnum i/NUL1C1Dnum i:\\nThen wehave\\nyciDciCˆiCˆi/NUL1\\nDnum iC.2\\x01num i/NULsize i//NUL.2\\x01num i/NUL1/NULsize i/NUL1/\\nDnum iC.2\\x01num i/NUL2.num i/NUL1///NUL.2.num i/NUL1//NUL.num i/NUL1//\\nDnum iC2/NUL.num i/NUL1/\\nD3 :', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 339}),\n",
              " Document(page_content='17-10 Lecture Notes for Chapter 17: AmortizedAnalysis\\nΦinumi sizei\\n0 8 16 24 3208162432\\ni\\nExpansion and contraction\\nWhen ˛drops too low, contract the table.\\n\\x0fAllocate anew,smaller one.\\n\\x0fCopy all items.\\nStill want\\n\\x0f˛bounded from below by aconstant,\\n\\x0famortized cost per operation DO.1/.\\nMeasure cost in termsof elementary insertions and deletion s.\\n“Obvious strategy”\\n\\x0fDoublesizewheninserting intoafulltable(when ˛D1,sothatafter insertion\\n˛would become > 1).\\n\\x0fHalve size when deletion would make table less than half full (when ˛D1=2,\\nsothat after deletion ˛would become < 1=2).\\n\\x0fThenalways have 1=2\\x14˛\\x141.\\n\\x0fSuppose weﬁll table.\\nTheninsert)double\\n2deletes)halve\\n2inserts)double\\n2deletes)halve\\n\\x01\\x01\\x01\\nNot performing enough operations after expansion or contra ction to pay for the\\nnext one.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 340}),\n",
              " Document(page_content='Lecture Notes for Chapter 17: AmortizedAnalysis 17-11\\nSimplesolution\\n\\x0fDouble asbefore: wheninserting with ˛D1)after doubling, ˛D1=2.\\n\\x0fHalve size whendeleting with ˛D1=4)after halving, ˛D1=2.\\n\\x0fThus, immediately after either expansion or contraction, h ave˛D1=2.\\n\\x0fAlways have 1=4\\x14˛\\x141.\\nIntuition\\n\\x0fWant to make sure that we perform enough operations between c onsecutive\\nexpansions/contractions to pay for the change in table size .\\n\\x0fNeed todelete half the items before contraction.\\n\\x0fNeed todouble number of items before expansion.\\n\\x0fEither way, number of operations between expansions/contr actions is at least a\\nconstant fraction of number of itemscopied.\\nˆ.T /D(\\n2\\x01T:num/NULT:sizeif˛\\x151=2 ;\\nT:size=2/NULT:numif˛ < 1=2 :\\nTempty)ˆD0.\\n˛\\x151=2)num\\x15size=2)2\\x01num\\x15size)ˆ\\x150.\\n˛ < 1=2)num<size=2)ˆ\\x150.\\nFurtherintuition\\nˆmeasures how far from ˛D1=2weare.\\n\\x0f˛D1=2)ˆD2\\x01num/NUL2\\x01numD0.\\n\\x0f˛D1)ˆD2\\x01num/NULnumDnum.\\n\\x0f˛D1=4)ˆDsize=2/NULnumD4\\x01num=2/NULnumDnum.\\n\\x0fTherefore, when we double or halve, have enough potential to pay for moving\\nallnumitems.\\n\\x0fPotential increases linearly between ˛D1=2and˛D1, and it also increases\\nlinearly between ˛D1=2and˛D1=4.\\n\\x0fSince ˛has different distances to go to get to 1or1=4, starting from 1=2, rate\\nof increase of ˆdiffers.\\n\\x0fFor˛to go from 1=2to1,numincreases from size=2tosize, for a total\\nincrease of size=2.ˆincreases from 0tosize. Thus, ˆneeds to increase\\nby2for each item inserted. That’s why there’s a coefﬁcient of 2on the\\nT:numterm in theformula for ˆwhen ˛\\x151=2.\\n\\x0fFor˛togofrom 1=2to1=4,numdecreases from size=2tosize=4,foratotal\\ndecrease of size=4.ˆincreases from 0tosize=4. Thus, ˆneeds to increase\\nby1for each item deleted. That’s why there’s a coefﬁcient of /NUL1on the\\nT:numterm in theformula for ˆwhen ˛ < 1=2.\\nAmortized costs: morecases\\n\\x0finsert, delete\\n\\x0f˛\\x151=2,˛ < 1=2(use˛i,since ˛can vary alot)\\n\\x0fsizedoes/doesn’t change', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 341}),\n",
              " Document(page_content='17-12 Lecture Notes for Chapter 17: AmortizedAnalysis\\nInsert\\n\\x0f˛i/NUL1\\x151=2, same analysis as before )yciD3.\\n\\x0f˛i/NUL1< 1=2)no expansion (only occurs when ˛i/NUL1D1).\\n\\x0fIf˛i/NUL1< 1=2and˛i< 1=2:\\nyciDciCˆiCˆi/NUL1\\nD1C.size i=2/NULnum i//NUL.size i/NUL1=2/NULnum i/NUL1/\\nD1C.size i=2/NULnum i//NUL.size i=2/NUL.num i/NUL1//\\nD0 :\\n\\x0fIf˛i/NUL1< 1=2and˛i\\x151=2:\\nyciD1C.2\\x01num i/NULsize i//NUL.size i/NUL1=2/NULnum i/NUL1/\\nD1C.2.num i/NUL1C1//NULsize i/NUL1//NUL.size i/NUL1=2/NULnum i/NUL1/\\nD3\\x01num i/NUL1/NUL3\\n2\\x01size i/NUL1C3\\nD3\\x01˛i/NUL1size i/NUL1/NUL3\\n2\\x01size i/NUL1C3\\n<3\\n2\\x01size i/NUL1/NUL3\\n2\\x01size i/NUL1C3\\nD3 :\\nTherefore, amortized cost of insert is < 3.\\nDelete\\n\\x0fIf˛i/NUL1< 1=2, then ˛i< 1=2.\\n\\x0fIf nocontraction:\\nyciD1C.size i=2/NULnum i//NUL.size i/NUL1=2/NULnum i/NUL1/\\nD1C.size i=2/NULnum i//NUL.size i=2/NUL.num iC1//\\nD2 :\\n\\x0fIf contraction:\\nyciD.num iC1„ƒ‚…/C.size i=2/NULnum i//NUL.size i/NUL1=2/NULnum i/NUL1/\\nmove+ delete\\nŒsize i=2Dsize i/NUL1=4Dnum i/NUL1Dnum iC1\\x8d\\nD.num iC1/C..num iC1//NULnum i//NUL..2\\x01num iC2//NUL.num iC1//\\nD1 :\\n\\x0fIf˛i/NUL1\\x151=2, then nocontraction.\\n\\x0fIf˛i\\x151=2:\\nyciD1C.2\\x01num i/NULsize i//NUL.2\\x01num i/NUL1/NULsize i/NUL1/\\nD1C.2\\x01num i/NULsize i//NUL.2\\x01num iC2/NULsize i/\\nD /NUL 1 :', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 342}),\n",
              " Document(page_content='Lecture Notes for Chapter 17: AmortizedAnalysis 17-13\\n\\x0fIf˛i< 1=2, since ˛i/NUL1\\x151=2, have\\nnum iDnum i/NUL1/NUL1\\x151\\n2\\x01size i/NUL1/NUL1D1\\n2\\x01size i/NUL1 :\\nThus,\\nyciD1C.size i=2/NULnum i//NUL.2\\x01num i/NUL1/NULsize i/NUL1/\\nD1C.size i=2/NULnum i//NUL.2\\x01num iC2/NULsize i/\\nD /NUL 1C3\\n2\\x01size i/NUL3\\x01num i\\n\\x14 /NUL 1C3\\n2\\x01size i/NUL3\\x121\\n2\\x01size i/NUL1\\x13\\nD2 :\\nTherefore, amortized cost of delete is \\x142.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 343}),\n",
              " Document(page_content='Solutionsfor Chapter17:\\nAmortized Analysis\\nSolution to Exercise17.1-3\\nThis solutionisalsopostedpublicly\\nLetciDcost of ithoperation.\\nciD(\\niifiisan exact power of 2 ;\\n1otherwise :\\nOperation Cost\\n1 1\\n2 2\\n3 1\\n4 4\\n5 1\\n6 1\\n7 1\\n8 8\\n9 1\\n10 1\\n::::::\\nnoperations cost\\nnX\\niD1ci\\x14nClgnX\\njD02jDnC.2n/NUL1/ < 3n :\\n(Note: Ignoring ﬂoor inupper bound ofP2j.)\\nAverage cost of operation DTotal cost\\n#operations< 3.\\nByaggregate analysis, the amortized cost per operation DO.1/.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 344}),\n",
              " Document(page_content='Solutions forChapter 17: AmortizedAnalysis 17-15\\nSolutionto Exercise 17.2-1\\n[We assume that the only way in which COPYis invoked is automatically, after\\nevery sequence of kPUSHandPOPoperations.]\\nCharge$2foreach P USHand POPoperation and$0foreach C OPY. Whenwecall\\nPUSH, we use $1 to pay for the operation, and we store the other $1 on the item\\npushed. Whenwecall P OP, weagain use$1topayfor theoperation, and westore\\nthe other $1 in the stack itself. Because the stack size never exceeds k, the actual\\ncost of a C OPYoperation isat most $ k, which ispaid bythe$ kfound inthe items\\nin the stack and the stack itself. Since kPUSHand POPoperations occur between\\ntwo consecutive C OPYoperations, $ kof credit are stored, either on individual\\nitems (from P USHoperations) or in the stack itself (from P OPoperations) by the\\ntime a C OPYoccurs. Since the amortized cost of each operation is O.1/and the\\namount of credit never goes negative, the total cost of noperations is O.n/.\\nSolutionto Exercise 17.2-2\\nThissolutionisalsopostedpublicly\\nLetciDcost of ith operation.\\nciD(\\niifiis anexact power of 2 ;\\n1otherwise :\\nCharge each operation $3(amortized cost yci).\\n\\x0fIfiis not anexact power of 2, pay $1, and store $2 ascredit.\\n\\x0fIfiis anexact power of 2,pay $ i, using stored credit.\\nOperation Cost Actual cost Credit remaining\\n1 3 1 2\\n2 3 2 3\\n3 3 1 5\\n4 3 4 4\\n5 3 1 6\\n6 3 1 8\\n7 3 1 10\\n8 3 8 5\\n9 3 1 7\\n10 3 1 9\\n::::::::::::\\nSincethe amortized cost is$3 per operation,nX\\niD1yciD3n.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 345}),\n",
              " Document(page_content='17-16 Solutions for Chapter 17: AmortizedAnalysis\\nWeknow from Exercise 17.1-3 thatnX\\niD1ci< 3n.\\nThen wehavenX\\niD1yci\\x15nX\\niD1ci)creditDamortized cost/NULactual cost\\x150.\\nSincethe amortized cost of each operation is O.1/, and theamount of credit never\\ngoes negative, thetotal cost of noperations is O.n/.\\nSolution to Exercise17.2-3\\nThis solutionisalsopostedpublicly\\nWeintroduceanewﬁeld A:maxtoholdtheindexofthehigh-order 1inA. Initially,\\nA:maxissetto/NUL1,sincethelow-order bitof Aisatindex0,andthereareinitially\\nno 1’s in A. The value of A:maxis updated as appropriate when the counter is\\nincremented orreset, andweusethisvaluetolimithowmucho fAmustbelooked\\nat to reset it. By controlling the cost of R ESETin this way, we can limit it to an\\namount that can becovered by credit from earlier I NCREMENT s.\\nINCREMENT .A/\\niD0\\nwhile i < A:lengthandAŒi\\x8d==1\\nAŒi\\x8dD0\\niDiC1\\nifi < A:length\\nAŒi\\x8dD1\\n//Additions to book’s I NCREMENT start here.\\nifi > A:max\\nA:maxDi\\nelseA:maxD/NUL1\\nRESET.A/\\nforiD0toA:max\\nAŒi\\x8dD0\\nA:maxD/NUL1\\nAs for the counter in the book, we assume that it costs $1 to ﬂip a bit. In addition,\\nweassume it costs $1 toupdate A:max.\\nSetting and resetting of bits by I NCREMENT will work exactly as for the original\\ncounter in the book: $1 will pay to set one bit to 1; $1 will be pl aced on the bit\\nthat is set to 1 as credit; the credit on each 1 bit will pay to re set the bit during\\nincrementing.\\nInaddition, we’ll use$1topaytoupdate max,andifmaxincreases, we’ll placean\\nadditional $1 of credit on the new high-order 1. (If maxdoesn’t increase, we can\\njustwastethat$1—itwon’tbeneeded.) SinceR ESETmanipulates bitsatpositions\\nonlyupto A:max,andsinceeachbituptotheremusthavebecomethehigh-orde r 1', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 346}),\n",
              " Document(page_content='Solutions forChapter 17: AmortizedAnalysis 17-17\\nat some time before the high-order 1 got up to A:max, every bit seen by R ESET\\nhas$1ofcredit onit. Sothezeroingofbitsof Aby RESETcanbecompletely paid\\nfor by the credit stored onthe bits. Wejust need $1 topay for r esettingmax.\\nThuscharging $4foreach I NCREMENT and$1foreach R ESETissufﬁcient, sothe\\nsequence of nINCREMENT and RESEToperations takes O.n/time.\\nSolutionto Exercise 17.3-3\\nLetDibethe heap after the ithoperation, andlet Diconsist of nielements. Also,\\nletkbe a constant such that each I NSERTor EXTRACT-MINoperation takes at\\nmost klnntime, where nDmax.ni/NUL1; ni/. (Wedon’t want to worry about taking\\nthelogof 0,andatleast oneof ni/NUL1andniisatleast 1. We’llseelater whyweuse\\nthe natural log.)\\nDeﬁne\\nˆ.D i/D(\\n0 ifniD0 ;\\nknilnniifni> 0 :\\nThisfunction exhibits thecharacteristics welikeinapote ntial function: ifwestart\\nwithan empty heap, then ˆ.D 0/D0, and wealways maintain that ˆ.D i/\\x150.\\nBeforeprovingthatweachievethedesiredamortizedtimes, weshowthatif n\\x152,\\nthennlnn\\nn/NUL1\\x142. Wehave\\nnlnn\\nn/NUL1Dnln\\x12\\n1C1\\nn/NUL1\\x13\\nDln\\x12\\n1C1\\nn/NUL1\\x13n\\n\\x14ln\\x10\\ne1\\nn/NUL1\\x11n\\n(since 1Cx\\x14exfor all real x)\\nDlnen\\nn/NUL1\\nDn\\nn/NUL1\\n\\x142 ;\\nassuming that n\\x152. (The equation ln en\\nn/NUL1Dn\\nn/NUL1is whyweuse the natural log.)\\nIf the ith operation is an I NSERT, then niDni/NUL1C1. If the ith operation inserts\\ninto anempty heap, then niD1,ni/NUL1D0, and the amortized cost is\\nyciDciCˆ.D i//NULˆ.D i/NUL1/\\n\\x14kln1Ck\\x011ln1/NUL0\\nD0 :\\nIf the ith operation inserts into a nonempty heap, then niDni/NUL1C1, and the\\namortized cost is\\nyciDciCˆ.D i//NULˆ.D i/NUL1/\\n\\x14klnniCknilnni/NULkni/NUL1lnni/NUL1\\nDklnniCknilnni/NULk.n i/NUL1/ln.ni/NUL1/', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 347}),\n",
              " Document(page_content='17-18 Solutions for Chapter 17: AmortizedAnalysis\\nDklnniCknilnni/NULkniln.ni/NUL1/Ckln.ni/NUL1/\\n< 2klnniCknilnni\\nni/NUL1\\n\\x142klnniC2k\\nDO.lgni/ :\\nIf the ith operation is an E XTRACT-MIN, then niDni/NUL1/NUL1. If the ith operation\\nextracts theoneandonlyheapitem,then niD0,ni/NUL1D1,andtheamortized cost\\nis\\nyciDciCˆ.D i//NULˆ.D i/NUL1/\\n\\x14kln1C0/NULk\\x011ln1\\nD0 :\\nIfthe ithoperation extractsfromaheapwithmorethan 1item,then niDni/NUL1/NUL1\\nandni/NUL1\\x152, and the amortized cost is\\nyciDciCˆ.D i//NULˆ.D i/NUL1/\\n\\x14klnni/NUL1Cknilnni/NULkni/NUL1lnni/NUL1\\nDklnni/NUL1Ck.n i/NUL1/NUL1/ln.ni/NUL1/NUL1//NULkni/NUL1lnni/NUL1\\nDklnni/NUL1Ckni/NUL1ln.ni/NUL1/NUL1//NULkln.ni/NUL1/NUL1//NULkni/NUL1lnni/NUL1\\nDklnni/NUL1\\nni/NUL1/NUL1Ckni/NUL1lnni/NUL1/NUL1\\nni/NUL1\\n< klnni/NUL1\\nni/NUL1/NUL1Ckni/NUL1ln1\\nDklnni/NUL1\\nni/NUL1/NUL1\\n\\x14kln2(since ni/NUL1\\x152)\\nDO.1/ :\\nA slightly different potential function—which may be easie r to work with—is as\\nfollows. Foreach node xinthe heap, let di.x/be the depth of xinDi. Deﬁne\\nˆ.D i/DX\\nx2Dik.d i.x/C1/\\nDk \\nniCX\\nx2Didi.x/!\\n;\\nwhere kisdeﬁned as before.\\nInitially, theheaphasnoitems,whichmeansthatthesumiso veranemptyset,and\\nsoˆ.D 0/D0. Wealways have ˆ.D i/\\x150, asrequired.\\nObserve that after an I NSERT, the sum changes only by an amount equal to the\\ndepth of the new last node of the heap, which is blgnic. Thus, the change\\nin potential due to an I NSERTisk.1Cblgnic/, and so the amortized cost is\\nO.lgni/CO.lgni/DO.lgni/DO.lgn/.\\nAfter an E XTRACT-MIN, the sum changes by the negative of the depth of the old\\nlast node in the heap, and so the potential decreases byk.1Cblgni/NUL1c/. The\\namortized cost isat most klgni/NUL1/NULk.1Cblgni/NUL1c/DO.1/.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 348}),\n",
              " Document(page_content='Solutions forChapter 17: AmortizedAnalysis 17-19\\nSolutionto Problem 17-2\\na.The SEARCHoperation canbeperformed bysearching eachoftheindividu ally\\nsorted arrays. Since all the individual arrays are sorted, s earching one of them\\nusing a binary search algorithm takes O.lgm/time, where mis the size of the\\narray. Inanunsuccessful search,thetimeis ‚.lgm/. Intheworstcase,wemay\\nassume that all the arrays A0; A1; : : : ; A k/NUL1are full, kDdlg.nC1/e, and we\\nperform anunsuccessful search. Thetotal timetaken is\\nT .n/D‚.lg2k/NUL1Clg2k/NUL2C\\x01\\x01\\x01Clg21Clg20/\\nD‚..k/NUL1/C.k/NUL2/C\\x01\\x01\\x01C 1C0/\\nD‚.k.k/NUL1/=2/\\nD‚.dlg.nC1/e.dlg.nC1/e/NUL1/=2/\\nD‚/NUL\\nlg2n\\x01\\n:\\nThus, theworst-case running time is ‚.lg2n/.\\nb.Wecreateanewsortedarrayofsize1containingtheneweleme nttobeinserted.\\nIfarray A0(whichhassize1)isempty, thenwereplace A0withthenewsorted\\narray. Otherwise, we merge sort the two arrays into another s orted array of\\nsize 2. If A1is empty, then we replace A1with the new array; otherwise we\\nmerge sort the arrays as before and continue. Since array Aiisof size 2i, if we\\nmerge sort two arrays of size 2ieach, we obtain one of size 2iC1, which is the\\nsize of AiC1. Thus, this method will result in another list of arrays in th e same\\nstructure that wehad before.\\nLet us analyze its worst-case running time. We will assume th at merge sort\\ntakes 2mtime to merge two sorted lists of size meach. If all the arrays\\nA0; A1; : : : ; A k/NUL2are full, then the running time toﬁll array Ak/NUL1would be\\nT .n/D2/NUL\\n20C21C\\x01\\x01\\x01C 2k/NUL2\\x01\\nD2.2k/NUL1/NUL1/\\nD2k/NUL2\\nD‚.n/ :\\nTherefore, the worst-case time to insert an element into thi s data structure\\nis‚.n/.\\nHowever, let us now analyze the amortized running time. Usin g the aggregate\\nmethod, we compute the total cost of a sequence of ninserts, starting with\\nthe empty data structure. Let rbe the position of the rightmost 0in the binary\\nrepresentationhnk/NUL1; nk/NUL2; : : : ; n 0iofn,sothat njD1forjD0; 1; : : : ; r/NUL1.\\nThecost of an insertion when nitemshave already been inserted is\\nr/NUL1X\\njD02\\x012jDO.2r/ :\\nFurthermore, rD0half the time, rD1a quarter of the time, and so on.\\nThere are at mostdn=2reinsertions for each value of r. The total cost of the n\\noperations istherefore bounded by', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 349}),\n",
              " Document(page_content='17-20 Solutions for Chapter 17: AmortizedAnalysis\\nO dlg.nC1/eX\\nrD0\\x10ln\\n2rm\\x11\\n2r!\\nDO.nlgn/ :\\nTheamortized cost per I NSERToperation, therefore is O.lgn/.\\nWe can also use the accounting method to analyze the running t ime. We can\\ncharge $ kto insert an element. $1 pays for the insertion, and we put $ .k/NUL1/\\non the inserted item to pay for it being involved in merges lat er on. Each time\\nit is merged, it moves to a higher-indexed array, i.e., from AitoAiC1. It can\\nmovetoahigher-indexed array at most k/NUL1times, and sothe $ .k/NUL1/onthe\\nitem sufﬁces to pay for all the times it will ever be involved i n merges. Since\\nkD‚.lgn/,wehave an amortized cost of ‚.lgn/per insertion.\\nc.DELETE .x/will beimplemented asfollows:\\n1. Findthesmallest jfor whichthearray Ajwith2jelements isfull. Let ybe\\nthe last element of Aj.\\n2. Let xbe in the array Ai. If necessary, ﬁnd which array this is by using the\\nsearch procedure.\\n3. Remove xfrom Aiandput yintoAi. Thenmove ytoitscorrectplacein Ai.\\n4. Divide Aj(which nowhas 2j/NUL1elements left): Theﬁrst element goes into\\narray A0, the next 2 elements go into array A1, the next 4 elements go into\\narray A2, and so forth. Mark array Ajas empty. The new arrays are created\\nalready sorted.\\nThe cost of D ELETEis‚.n/in the worst case, where iDk/NUL1andjD\\nk/NUL2:‚.lgn/to ﬁnd Aj,‚.lg2n/to ﬁnd Ai,‚.2i/D‚.n/to put yin its\\ncorrect place inarray Ai,and ‚.2j/D‚.n/todivide array Aj. Thefollowing\\nsequence of noperations, where n=3is a power of 2, yields an amortized cost\\nthat is no better: perform n=3INSERToperations, followed by n=3pairs of\\nDELETEand INSERT. It costs O.nlgn/to do the ﬁrst n=3INSERToperations.\\nThis creates a single full array. Each subsequent D ELETE/INSERTpair costs\\n‚.n/for the D ELETEto divide the full array and another ‚.n/for the I NSERT\\ntorecombine it. Thetotal isthen ‚.n2/, or‚.n/per operation.\\nSolution to Problem 17-4\\na.For RB-I NSERT, consider a complete red-black tree in which the colors alte r-\\nnate between levels. That is, the root is black, the children of the root are red,\\nthe grandchildren of the root are black, the great-grandchi ldren of the root are\\nred, and so on. When a node is inserted as a red child of one of th e red leaves,\\nthencase1of RB-I NSERT-FIXUPoccurs .lg.nC1//=2times, sothat thereare\\n\\x7f.lgn/color changes to ﬁx the colors of nodes on the path from the ins erted\\nnode to the root.\\nFor RB-D ELETE, consider a complete red-black tree in which all nodes are\\nblack. If a leaf is deleted, then the double blackness will be pushed all the way\\nuptotheroot,withacolorchangeateachlevel(case2ofRB-D ELETE-FIXUP),\\nfor atotal of \\x7f.lgn/color changes.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 350}),\n",
              " Document(page_content='Solutions forChapter 17: AmortizedAnalysis 17-21\\nb.All cases except for case 1 of RB-I NSERT-FIXUPand case 2 of RB-D ELETE-\\nFIXUPareterminating.\\nc.Case 1 of RB-I NSERT-FIXUPreduces the number of red nodes by 1. As Fig-\\nure 13.5 shows, node ´’s parent and uncle change from red to black, and ´’s\\ngrandparent changes from black tored. Hence, ˆ.T0/Dˆ.T //NUL1.\\nd.Lines 1–16 of RB-I NSERTcause one node insertion and a unit increase in po-\\ntential. The nonterminating case of RB-I NSERT-FIXUP(Case 1) makes three\\ncolor changes and decreases the potential by 1. The terminating cases of RB-\\nINSERT-FIXUP(cases 2 and 3) cause one rotation each and do not affect the\\npotential. (Althoughcase3makescolorchanges, thepotent ial doesnotchange.\\nAsFigure13.6shows,node ´’sparentchangesfromredtoblack,and ´’sgrand-\\nparent changes from black tored.)\\ne.The number of structural modiﬁcations and amount of potenti al change result-\\ning from lines 1–16 of RB-I NSERTand from the terminating cases of RB-\\nINSERT-FIXUPareO.1/, and so the amortized number of structural modiﬁca-\\ntions of these parts is O.1/. The nonterminating case of RB-I NSERT-FIXUP\\nmay repeat O.lgn/times, but its amortized number of structural modiﬁcations\\nis0, since by our assumption the unit decrease in the potential p ays for the\\nstructural modiﬁcations needed. Therefore, the amortized number of structural\\nmodiﬁcations performed by RB-I NSERTisO.1/.\\nf.From Figure 13.5, we see that case 1 of RB-I NSERT-FIXUPmakes the follow-\\ning changes tothe tree:\\n\\x0fChangesablacknodewithtworedchildren(node C)toarednode,resulting\\ninapotential change of /NUL2.\\n\\x0fChanges a red node (node Ain part (a) and node Bin part (b)) to a black\\nnode with one redchild, resulting inno potential change.\\n\\x0fChanges arednode (node D)toablack node withnoredchildren, resulting\\ninapotential change of 1.\\nThe total change inpotential is /NUL1, which pays for the structural modiﬁcations\\nperformed, andthustheamortized number of structural modi ﬁcations incase1\\n(the nonterminating case) is 0. The terminating cases of RB-I NSERT-FIXUP\\ncause O.1/structural changes. Because w.\\x17/is based solely on node col-\\nors and the number of color changes caused by terminating cas es isO.1/, the\\nchange in potential interminating cases is O.1/. Hence, theamortized number\\nof structural modiﬁcations in the terminating cases is O.1/. The overall amor-\\ntized number of structural modiﬁcations in RB-I NSERT, therefore, is O.1/.\\ng.Figure 13.7 shows that case 2 of RB-D ELETE-FIXUPmakes the following\\nchanges tothe tree:\\n\\x0fChanges ablack node withnoredchildren (node D) toarednode, resulting\\ninapotential change of /NUL1.\\n\\x0fIfBisred, then it loses ablack child, withno effect on potentia l.\\n\\x0fIfBis black, then it goes from having no red children to having on e red\\nchild, resulting ina potential change of /NUL1.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 351}),\n",
              " Document(page_content='17-22 Solutions for Chapter 17: AmortizedAnalysis\\nThe total change in potential is either /NUL1or/NUL2, depending on the color of B.\\nIn either case, one unit of potential pays for the structural modiﬁcations per-\\nformed, and thus the amortized number of structural modiﬁca tions in case 2\\n(the nonterminating case) is at most 0. The terminating cases of RB-D ELETE\\ncause O.1/structural changes. Because w.\\x17/is based solely on node col-\\nors and the number of color changes caused by terminating cas es isO.1/, the\\nchange inpotential in terminating cases is O.1/. Hence, the amortized number\\nof structural changes in the terminating cases is O.1/. The overall amortized\\nnumber of structural modiﬁcations in RB-D ELETE-FIXUP, therefore, is O.1/.\\nh.Since the amortized number structural modiﬁcation in each o peration is O.1/,\\nthe actual number of structural modiﬁcations for any sequen ce of mRB-\\nINSERTand RB-D ELETEoperations on an initially empty red-black tree\\nisO.m/in theworst case.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 352}),\n",
              " Document(page_content='Lecture Notes forChapter 21:\\nDataStructures for DisjointSets\\nChapter 21overview\\nDisjoint-set datastructures\\n\\x0fAlso known as“union ﬁnd.”\\n\\x0fMaintaincollection SDfS1; : : : ; S kgofdisjointdynamic(changingovertime)\\nsets.\\n\\x0fEach set is identiﬁed by a representative , which is somemember of the set.\\nDoesn’t matter which member istherepresentative, aslong a sifweaskfor the\\nrepresentative twice without modifying the set, we get the s ame answer both\\ntimes.\\n[We do not include notes for the proof of running time of the di sjoint-set forest\\nimplementation, which iscovered inSection 21.4.]\\nOperations\\n\\x0fMAKE-SET.x/: make anew set SiDfxg, and add SitoS.\\n\\x0fUNION .x; y/: ifx2Sx; y2Sy,then SDS/NULSx/NULSy[fSx[Syg.\\n\\x0fRepresentative ofnewsetisanymemberof Sx[Sy,oftentherepresentative\\nof one of SxandSy.\\n\\x0fDestroys SxandSy(since sets must be disjoint).\\n\\x0fFIND-SET.x/: return representative of set containing x.\\nAnalysis interms of:\\n\\x0fnD#of elementsD# of MAKE-SEToperations,\\n\\x0fmDtotal # of operations.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 353}),\n",
              " Document(page_content='21-2 Lecture Notes for Chapter 21: DataStructures for Disjo intSets\\nAnalysis\\n\\x0fSince M AKE-SETcounts toward total #of operations, m\\x15n.\\n\\x0fCan have at most n/NUL1UNIONoperations, since after n/NUL1UNIONs, only 1\\nset remains.\\n\\x0fAssumethat theﬁrst noperations are M AKE-SET(helpful foranalysis, usually\\nnot really necessary).\\nApplication\\nDynamic connected components.\\nFor a graph GD.V; E/, vertices u; \\x17are in same connected component if and\\nonly if there’s apath between them.\\n\\x0fConnected components partition vertices into equivalence classes.\\nCONNECTED -COMPONENTS .G/\\nforeach vertex \\x172G:V\\nMAKE-SET.\\x17/\\nforeach edge .u; \\x17/2G:E\\nifFIND-SET.u/¤FIND-SET.\\x17/\\nUNION .u; \\x17/\\nSAME-COMPONENT .u; \\x17/\\nifFIND-SET.u/==FIND-SET.\\x17/\\nreturn TRUE\\nelse return FALSE\\nNote\\nIf actually implementing connected components,\\n\\x0feach vertex needs ahandle to itsobject in thedisjoint-set d ata structure,\\n\\x0feach object inthe disjoint-set data structure needs ahandl e toits vertex.\\nLinked listrepresentation\\n\\x0fEachset isasingly linked list, represented by anobject wit hattributes\\n\\x0fhead: the ﬁrst element inthe list, assumed to bethe set’s represe ntative, and\\n\\x0ftail: the last element inthe list.\\nObjects may appear within thelist in anyorder.\\n\\x0fEachobject in thelist has attributes for\\n\\x0fthe set member,\\n\\x0fpointer tothe set object, and\\n\\x0fnext.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 354}),\n",
              " Document(page_content='Lecture Notes for Chapter 21: DataStructures for Disjoint S ets 21-3\\nMAKE-SET: create asingleton list.\\nFIND-SET: follow the pointer back to the list object, and then follow t hehead\\npointer tothe representative.\\nUNION: acouple of waysto doit.\\n1. UNION .x; y/: append y’s list onto end of x’s list. Use x’s tail pointer to ﬁnd\\nthe end.\\n\\x0fNeedto update the pointer back to the set object for every nod e ony’s list.\\n\\x0fIf appending alarge list onto asmall list, it can take awhile .\\nOperation #objects updated\\nUNION .x2; x1/ 1\\nUNION .x3; x2/ 2\\nUNION .x4; x3/ 3\\nUNION .x5; x4/ 4\\n::::::\\nUNION .xn; xn/NUL1/ n/NUL1\\n‚.n2/total\\nAmortized timeper operation D‚.n/.\\n2.Weighted-union heuristic: Always append the smaller list to the larger list.\\n(Break ties arbitrarily.)\\nA single union can still take \\x7f.n/time, e.g.,if both sets have n=2members.\\nTheorem\\nWith weighted union, a sequence of moperations on nelements takes\\nO.mCnlgn/time.\\nSketch of proof Each M AKE-SETand FIND-SETstill takes O.1/. How many\\ntimes can each object’s representative pointer be updated? It must be in the\\nsmaller set each time.\\ntimes updated size of resulting set\\n1\\x152\\n2\\x154\\n3\\x158\\n::::::\\nk\\x152k\\n::::::\\nlgn\\x15n\\nTherefore, each representative is updated \\x14lgntimes. (theorem)\\nSeemspretty good, but wecan do much better.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 355}),\n",
              " Document(page_content='21-4 Lecture Notes for Chapter 21: DataStructures for Disjo intSets\\nDisjoint-setforest\\nForest of trees.\\n\\x0f1tree per set. Root is representative.\\n\\x0fEachnode points only to itsparent.\\nc\\nh e\\nbf\\nd\\ngf\\nc\\nh e\\nbd\\ngUNION(e,g)\\n\\x0fMAKE-SET: make asingle-node tree.\\n\\x0fUNION: make one root achild of the other.\\n\\x0fFIND-SET: follow pointers to the root.\\nNot so good—could get alinear chain of nodes.\\nGreat heuristics\\n\\x0fUnion by rank: make the root of the smaller tree (fewer nodes) a child of the\\nroot of the larger tree.\\n\\x0fDon’t actually use size.\\n\\x0fUserank, which is anupper bound onheight of node.\\n\\x0fMake the root with the smaller rank into a child of the root wit h the larger\\nrank.\\n\\x0fPath compression: Find path Dnodes visited during F IND-SETon the trip to\\nthe root. Make all nodes on the ﬁndpath direct children of roo t.\\nabcd\\nd\\na b c\\nEach node has twoattributes, p(parent) and rank.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 356}),\n",
              " Document(page_content='Lecture Notes for Chapter 21: DataStructures for Disjoint S ets 21-5\\nMAKE-SET.x/\\nx:pDx\\nx:rankD0\\nUNION .x; y/\\nLINK.FIND-SET.x/;FIND-SET.y//\\nLINK.x; y/\\nifx:rank > y:rank\\ny:pDx\\nelsex:pDy\\n//If equal ranks, choose yasparent and increment its rank.\\nifx:rank==y:rank\\ny:rankDy:rankC1\\nFIND-SET.x/\\nifx¤x:p\\nx:pDFIND-SET.x:p/\\nreturn x:p\\nFIND-SETmakes apass upto ﬁndthe root, and apass down asrecursion unw inds\\ntoupdate each node on ﬁndpath to point directly toroot.\\nRunningtime\\nIf use both union by rank and path compression, O.m ˛.n// .\\nn ˛.n/\\n0–2 0\\n3 1\\n4–7 2\\n8–2047 3\\n2048– A4.1/4\\nWhat’s A4.1/? SeeSection 21.4, if you dare. It’s \\x1d1080\\x19#of atoms in observ-\\nable universe.\\nThis bound is tight—there exists a sequence of operations th at takes \\x7f.m ˛.n//\\ntime.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 357}),\n",
              " Document(page_content='Solutionsfor Chapter21:\\nDataStructures forDisjointSets\\nSolution to Exercise21.2-3\\nThis solutionisalsopostedpublicly\\nWe want to show that we can assign O.1/charges to M AKE-SETand FIND-SET\\nand an O.lgn/charge to U NIONsuch that the charges for a sequence of these\\noperations areenoughtocoverthecostofthesequence— O.mCnlgn/,according\\nto the theorem. When talking about the charge for each kind of operation, it is\\nhelpful toalso be able totalk about the number of each kind of operation.\\nConsider theusualsequence of mMAKE-SET, UNION,and FIND-SEToperations,\\nnof which are M AKE-SEToperations, and let l < nbe the number of U NION\\noperations. (Recall the discussion in Section 21.1 about th ere being at most n/NUL1\\nUNIONoperations.) Thenthereare nMAKE-SEToperations, lUNIONoperations,\\nandm/NULn/NULlFIND-SEToperations.\\nThe theorem didn’t separately name the number lof UNIONs; rather, it bounded\\nthe number by n. If you go through the proof of the theorem with lUNIONs, you\\ngetthetimebound O.m/NULlCllgl/DO.mCllgl/forthesequenceofoperations.\\nThatis,theactualtimetakenbythesequenceofoperations i satmost c.mCllgl/,\\nfor some constant c.\\nThus, wewant toassign operation charges such that\\n(MAKE-SETcharge)\\x01n\\nC(FIND-SETcharge)\\x01.m/NULn/NULl/\\nC(UNIONcharge)\\x01l\\n\\x15c.mCllgl/ ;\\nso that the amortized costs givean upper bound on the actual c osts.\\nThefollowing assignments work, where c0is someconstant\\x15c:\\n\\x0fMAKE-SET:c0\\n\\x0fFIND-SET:c0\\n\\x0fUNION:c0.lgnC1/\\nSubstituting into the above sum, weget\\nc0nCc0.m/NULn/NULl/Cc0.lgnC1/lDc0mCc0llgn\\nDc0.mCllgn/\\n> c.mCllgl/ :', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 358}),\n",
              " Document(page_content='Solutions forChapter 21: DataStructures for DisjointSets 21-7\\nSolutionto Exercise 21.2-5\\nAs the hint suggests, make the representative of each set be t he tail of its linked\\nlist. Except for the tail element, each element’s represent ative pointer points tothe\\ntail. The tail’s representative pointer points to the head. An element is the tail if\\nits next pointer is NIL. Now we can get to the tail in O.1/time: if x:next==NIL,\\nthentailDx, elsetailDx:rep. We can get to the head in O.1/time as well: if\\nx:next==NIL, thenheadDx:rep, elseheadDx:rep:rep. The set object needs\\nonlytostoreapointer tothetail,thoughapointertoanylis telementwouldsufﬁce.\\nSolutionto Exercise 21.2-6\\nThissolutionisalsopostedpublicly\\nLet’scall the two lists AandB, and suppose that the representative of the new list\\nwill be the representative of A. Rather than appending Bto the end of A, instead\\nsplice BintoAright after the ﬁrst element of A. We have to traverse Bto update\\npointers to the set object anyway, so we can just make the last element of Bpoint\\ntothe second element of A.\\nSolutionto Exercise 21.3-3\\nYou need to ﬁnd a sequence of moperations on nelements that takes \\x7f.mlgn/\\ntime. Start with nMAKE-SETs tocreate singleton sets fx1g;fx2g; : : : ;fxng. Next\\nperform the n/NUL1UNIONoperations shownbelowtocreateasingle setwhosetree\\nhas depth lg n.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 359}),\n",
              " Document(page_content='21-8 Solutions for Chapter 21: Data Structures forDisjoint Sets\\nUNION .x1; x2/ n=2 of these\\nUNION .x3; x4/\\nUNION .x5; x6/\\n:::\\nUNION .xn/NUL1; xn/\\nUNION .x2; x4/ n=4 of these\\nUNION .x6; x8/\\nUNION .x10; x12/\\n:::\\nUNION .xn/NUL2; xn/\\nUNION .x4; x8/ n=8 of these\\nUNION .x12; x16/\\nUNION .x20; x24/\\n:::\\nUNION .xn/NUL4; xn/\\n:::\\nUNION .xn=2; xn/ 1 of these\\nFinally, perform m/NUL2nC1FIND-SEToperations on the deepest element in the\\ntree. Each of these F IND-SEToperations takes \\x7f.lgn/time. Letting m\\x153n, we\\nhave more than m=3FIND-SEToperations, so that the total cost is \\x7f.mlgn/.\\nSolution to Exercise21.3-4\\nMaintain acircular, singly linked list of the nodes of each s et. Toprint, just follow\\nthe list until you get back to node x, printing each member of the list. The only\\nother operations that change are F IND-SET, which sets x:nextDx, and L INK,\\nwhich exchanges the pointers x:nextandy:next.\\nSolution to Exercise21.3-5\\nWith the path-compression heuristic, the sequence of mMAKE-SET, FIND-SET,\\nand LINKoperations, where all the L INKoperations take place before any of the\\nFIND-SEToperations, runs in O.m/time. The key observation is that once a\\nnode xappears onaﬁndpath, xwillbeeitherarootorachildofaroot atalltimes\\nthereafter.\\nWe use the accounting method to obtain the O.m/time bound. We charge a\\nMAKE-SEToperation two dollars. One dollar pays for the M AKE-SET, and one\\ndollar remains on the node xthat is created. The latter pays for the ﬁrst time that\\nxappears onaﬁnd path and isturned into achild of a root.\\nWe charge one dollar for a L INKoperation. This dollar pays for the actual linking\\nof one node to another.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 360}),\n",
              " Document(page_content='Solutions forChapter 21: DataStructures for DisjointSets 21-9\\nWe charge one dollar for a F IND-SET. This dollar pays for visiting the root and\\nits child, and for the path compression of these two nodes, du ring the F IND-SET.\\nAll other nodes on the ﬁnd path use their stored dollar to pay f or their visitation\\nand path compression. As mentioned, after the F IND-SET, all nodes on the ﬁnd\\npath become children of a root (except for the root itself), a nd so whenever they\\nare visited during asubsequent F IND-SET, the FIND-SEToperation itself will pay\\nfor them.\\nSince we charge each operation either one or two dollars, a se quence of mopera-\\ntions is charged at most 2mdollars, and sothe total timeis O.m/.\\nObserve that nothing inthe aboveargument requires union by rank. Therefore, we\\nget an O.m/timebound regardless of whether weuse union by rank.\\nSolutionto Exercise 21.4-4\\nClearly, each M AKE-SETand LINKoperation takes O.1/time. Because the rank\\nofanode isanupper bound onitsheight, each ﬁndpathhasleng thO.lgn/,which\\nin turn implies that each F IND-SETtakes O.lgn/time. Thus, any sequence of\\nmMAKE-SET, LINK, and F IND-SEToperations on nelements takes O.mlgn/\\ntime. It is easy to prove an analogue of Lemma 21.7 to show that if we convert a\\nsequence of m0MAKE-SET, UNION, and FIND-SEToperations into asequence of\\nmMAKE-SET, LINK,and FIND-SEToperations thattake O.mlgn/time,thenthe\\nsequence of m0MAKE-SET, UNION, and FIND-SEToperations takes O.m0lgn/\\ntime.\\nSolutionto Exercise 21.4-5\\nProfessor Dante is mistaken. Take the following scenario. L etnD16, and make\\n16 separate singleton sets using M AKE-SET. Then do 8 U NIONoperations to link\\nthesetsinto8pairs, whereeachpair hasarootwithrank0and achildwithrank1.\\nNowdo4 U NIONs tolink pairs of these trees, sothat there are4trees, each w itha\\nroot ofrank2,children oftheroot ofranks1and0,andanodeo frank0thatisthe\\nchild of the rank-1 node. Now link pairs of these trees togeth er, so that there are\\ntwo resulting trees, each with a root of rank 3 and each contai ning a path from a\\nleaf tothe root withranks 0,1, and 3. Finally, link these two trees together, sothat\\nthere is a path from a leaf to the root with ranks 0, 1, 3, and 4. L etxandybe the\\nnodesonthispathwithranks1and3,respectively. Since A1.1/D3,level .x/D1,\\nand since A0.3/D4, level .y/D0. Yet yfollows xonthe ﬁndpath.\\nSolutionto Exercise 21.4-6\\nFirst, ˛0.22047/NUL1/DminfkWAk.1/\\x152047gD3, and 22047/NUL1\\x1d1080.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 361}),\n",
              " Document(page_content='21-10 Solutions for Chapter 21: Data Structures forDisjoin t Sets\\nSecond, we need that 0\\x14level.x/\\x14˛0.n/for all nonroots xwithx:rank\\x151.\\nWith this deﬁnition of ˛0.n/, we have A˛0.n/.x:rank/\\x15A˛0.n/.1/\\x15lg.nC1/ >\\nlgn\\x15x:p:rank. The rest of the proof goes through with ˛0.n/replacing ˛.n/.\\nSolution to Problem 21-1\\na.Forthe input sequence\\n4; 8;E; 3;E; 9; 2; 6;E;E;E; 1; 7;E; 5 ;\\nthe values in the extracted array would be 4; 3; 2; 6; 8; 1 .\\nThe following table shows the situation after the ith iteration of the forloop\\nwhen we use O FF-LINE-MINIMUMon the same input. (For this input, nD9\\nandm—thenumber of extractions—is 6).\\ni K1K2 K3 K4 K5 K6 K7 extracted\\n123456\\n0f4; 8gf3gf9; 2; 6gfgfgf1; 7gf5g\\n1f4; 8gf3gf9; 2; 6gfgfg f5; 1; 7g 1\\n2f4; 8gf3gf9; 2; 6gfg f5; 1; 7g 2 1\\n3f4; 8g f9; 2; 6; 3gfg f5; 1; 7g 32 1\\n4 f9; 2; 6; 3; 4; 8gfg f5; 1; 7g 432 1\\n5 f9; 2; 6; 3; 4; 8gfg f5; 1; 7g 432 1\\n6 f9; 2; 6; 3; 4; 8gf5; 1; 7g 43261\\n7 f9; 2; 6; 3; 4; 8gf5; 1; 7g 43261\\n8 f5; 1; 7; 9; 2; 6; 3; 4; 8 g432681\\nBecause jDmC1in the iterations for iD5andiD7, no changes occur in\\nthese iterations.\\nb.We want to show that the array extracted returned by O FF-LINE-MINIMUMis\\ncorrect, meaning that for iD1; 2; : : : ; m ,extracted Œj \\x8dis the key returned by\\nthejth EXTRACT-MINcall.\\nWe start with nINSERToperations and mEXTRACT-MINoperations. The\\nsmallest of all the elements will be extracted in the ﬁrst E XTRACT-MINafter\\nitsinsertion. Soweﬁnd jsuchthat theminimum element isin Kj,and put the\\nminimum element in extracted Œj \\x8d, which corresponds to the E XTRACT-MIN\\nafter the minimum element insertion.\\nNow we reduce to a similar problem with n/NUL1INSERToperations and m/NUL1\\nEXTRACT-MINoperations in the following way: the I NSERToperations are\\nthe same but without the insertion of the smallest that was ex tracted, and the\\nEXTRACT-MINoperations are the same but without the extraction that ex-\\ntracted the smallest element.\\nConceptually, we unite I jand I jC1, removing the extraction between them and\\nalsoremoving theinsertion of theminimum element from I j[IjC1. Uniting I j\\nandI jC1isaccomplishedbyline6. Weneedtodeterminewhichsetis Kl,rather\\nthan just using KjC1unconditionally, because KjC1may have been destroyed\\nwhenit wasunited into ahigher-indexed set by aprevious exe cution of line 6.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 362}),\n",
              " Document(page_content='Solutions forChapter 21: DataStructures for DisjointSets 21-11\\nBecause we process extractions in increasing order of the mi nimum value\\nfound, the remaining iterations of the forloop correspond to solving the re-\\nduced problem.\\nThere are two other points worth making. First, if the smalle st remaining ele-\\nment had been inserted after the last E XTRACT-MIN(i.e., jDmC1), then\\nno changes occur, because this element is not extracted. Sec ond, there may be\\nsmaller elements within the Kjsets than the the one we are currently looking\\nfor. These elements do not affect the result, because they co rrespond to ele-\\nmentsthat werealready extracted, andtheir effect onthe al gorithm’s execution\\nis over.\\nc.To implement this algorithm, we place each element in a disjo int-set forest.\\nEachroot hasapointer toits Kiset, andeach Kisethasapointer totheroot of\\nthe tree representing it. All thevalid sets Kiare inalinked list.\\nBeforeO FF-LINE-MINIMUM , thereisinitializationthatbuildstheinitialsets Ki\\naccording to the I isequences.\\n\\x0fLine2(“determine jsuch that i2Kj”) turns into jDFIND-SET.i/.\\n\\x0fLine 5 (“let lbe the smallest value greater than jfor which set Klexists”)\\nturns into KlDKj:next.\\n\\x0fLine 6 (“ KlDKj[Kl, destroying Kj”) turns into lDLINK.j; l/and\\nremove Kjfrom the linked list.\\nToanalyzetherunningtime,wenotethatthereare nelementsandthatwehave\\nthe following disjoint-set operations:\\n\\x0fnMAKE-SEToperations\\n\\x0fat most n/NUL1UNIONoperations before starting\\n\\x0fnFIND-SEToperations\\n\\x0fat most nLINKoperations\\nThus the number mof overall operations is O.n/. The total running time is\\nO.m ˛.n//DO.n ˛.n// .\\n[The“tight bound”wordingthatthisquestionusesdoesnotr efertoan“asymp-\\ntotically tight”bound. Instead, thequestion ismerelyask ing foraboundthat is\\nnot too “loose.”]\\nSolutionto Problem 21-2\\na.Denote the number of nodes by n, and let nD.mC1/=3, so that mD\\n3n/NUL1. First, perform the noperations M AKE-TREE.\\x171/, MAKE-TREE.\\x172/,\\n..., MAKE-TREE.\\x17n/. Thenperform thesequence of n/NUL1GRAFToperations\\nGRAFT .\\x171; \\x172/, GRAFT .\\x172; \\x173/,..., GRAFT .\\x17n/NUL1; \\x17n/; this sequence produces\\na single disjoint-set tree that is a linear chain of nnodes with \\x17nat the root\\nand\\x171as the only leaf. Then perform F IND-DEPTH .\\x171/repeatedly, ntimes.\\nThetotal number of operations is nC.n/NUL1/CnD3n/NUL1Dm.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 363}),\n",
              " Document(page_content='21-12 Solutions for Chapter 21: Data Structures forDisjoin t Sets\\nEach M AKE-TREEand GRAFToperation takes O.1/time. Each F IND-DEPTH\\noperation has tofollow an n-node ﬁndpath, and soeachof the nFIND-DEPTH\\noperations takes ‚.n/time. The total time is n\\x01‚.n/C.2n/NUL1/\\x01O.1/D\\n‚.n2/D‚.m2/.\\nb.MAKE-TREEislike M AKE-SET, except that it also sets the dvalue to 0:\\nMAKE-TREE.\\x17/\\n\\x17:pD\\x17\\n\\x17:rankD0\\n\\x17:dD0\\nIt is correct to set \\x17:dto0, because the depth of the node in the single-node\\ndisjoint-set tree is 0, and the sum of the depths on the ﬁnd path for \\x17consists\\nonly of \\x17:d.\\nc.FIND-DEPTHwill call aprocedure F IND-ROOT:\\nFIND-ROOT.\\x17/\\nif\\x17:p¤\\x17:p:p\\nyD\\x17:p\\n\\x17:pDFIND-ROOT.y/\\n\\x17:dD\\x17:dCy:d\\nreturn \\x17:p\\nFIND-DEPTH .\\x17/\\nFIND-ROOT.\\x17///no need tosave thereturn value\\nif\\x17==\\x17:p\\nreturn \\x17:d\\nelse return \\x17:dC\\x17:p:d\\nFIND-ROOTperformspathcompressionandupdatespseudodistances alo ngthe\\nﬁndpathfrom \\x17. ItissimilartoF IND-SETonpage571,butwiththreechanges.\\nFirst, when \\x17is either the root or a child of a root (one of these conditions\\nholds if and only if \\x17:pD\\x17:p:p) in the disjoint-set forest, we don’t have to\\nrecurse; instead, we just return \\x17:p. Second, when we do recurse, we save\\nthe pointer \\x17:pinto a new variable y. Third, when we recurse, we update \\x17:d\\nby adding into it the dvalues of all nodes on the ﬁnd path that are no longer\\nproper ancestors of \\x17after path compression; these nodes are precisely the\\nproper ancestors of \\x17other than the root. Thus, as long as \\x17does not start out\\ntheFIND-ROOTcallaseithertherootorachildoftheroot,weadd y:dinto\\x17:d.\\nNote that y:dhas been updated prior to updating \\x17:d, ifyis also neither the\\nroot nor achild of the root.\\nFIND-DEPTHﬁrst calls F IND-ROOTto perform path compression and update\\npseudodistances. Afterward, the ﬁnd path from \\x17consists of either just \\x17(if\\x17\\nis a root) or just \\x17and\\x17:p(if\\x17is not a root, in which case it is a child of the\\nroot after path compression). In the former case, the depth o f\\x17is just \\x17:d, and\\ninthe latter case, thedepth is \\x17:dC\\x17:p:d.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 364}),\n",
              " Document(page_content='Solutions forChapter 21: DataStructures for DisjointSets 21-13\\nd.Our procedure for G RAFTisacombination of U NIONand LINK:\\nGRAFT .r; \\x17/\\nr0DFIND-ROOT.r/\\n\\x170DFIND-ROOT.\\x17/\\n´DFIND-DEPTH .\\x17/\\nifr0:rank > \\x170:rank\\n\\x170:pDr0\\nr0:dDr0:dC´C1\\n\\x170:dD\\x170:d/NULr0:d\\nelser0:pD\\x170\\nr0:dDr0:dC´C1/NUL\\x170:d\\nifr0:rank==\\x170:rank\\n\\x170:rankD\\x170:rankC1\\nThis procedure works as follows. First, we call F IND-ROOTonrand\\x17in\\norder to ﬁnd the roots r0and\\x170, respectively, of their trees in the disjoint-set\\nforest. As we saw in part (c), these F IND-ROOTcalls also perform path com-\\npression and update pseudodistances on the ﬁnd paths from rand\\x17. We then\\ncall FIND-DEPTH .\\x17/, saving the depth of \\x17in the variable ´. (Since we have\\njustcompressed \\x17’sﬁndpath,thiscallof F IND-DEPTHtakes O.1/time.) Next,\\nwe emulate the action of L INK, by making the root ( r0or\\x170) of smaller rank a\\nchild of the root of larger rank; in case of atie, wemake r0a child of \\x170.\\nIf\\x170has the smaller rank, then all nodes in r’s tree will have their depths in-\\ncreased bythe depth of \\x17plus1(because ristobecome achild of \\x17). Altering\\nthepsuedodistance oftherootofadisjoint-set treechange sthecomputeddepth\\nof all nodes in that tree, and so adding ´C1tor0:daccomplishes this update\\nfor all nodes in r’s disjoint-set tree. Since \\x170will become a child of r0in the\\ndisjoint-set forest, we have just increased the computed de pth of all nodes in\\nthe disjoint-set tree rooted at \\x170byr0:d. These computed depths should not\\nhave changed, however. Thus, we subtract off r0:dfrom \\x170:d, so that the sum\\n\\x170:dCr0:dafter making \\x170a child of r0equals \\x170:dbefore making \\x170a child\\nofr0.\\nOn the other hand, if r0has the smaller rank, or if the ranks are equal, then r0\\nbecomes a child of \\x170in the disjoint-set forest. In this case, \\x170remains a root\\nin the disjoint-set forest afterward, and we can leave \\x170:dalone. We have to\\nupdate r0:d, however, so that after making r0a child of \\x170, the depth of each\\nnode in r’sdisjoint-set treeisincreased by ´C1. Weadd ´C1tor0:d,but we\\nalso subtract out \\x170:d, since we have just made r0a child of \\x170. Finally, if the\\nranksof r0and\\x170areequal, weincrement therankof \\x170,asisdoneinthe L INK\\nprocedure.\\ne.The asymptotic running times of M AKE-TREE, FIND-DEPTH, and G RAFTare\\nequivalent tothose of M AKE-SET, FIND-SET, and U NION, respectively. Thus,\\na sequence of moperations, nof which are M AKE-TREEoperations, takes\\n‚.m ˛.n// timein the worst case.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 365}),\n",
              " Document(page_content='Lecture Notes forChapter 22:\\nElementaryGraph Algorithms\\nGraphrepresentation\\nGivengraph GD.V; E/. Inpseudocode, represent vertexsetby G:Vandedgeset\\nbyG:E.\\n\\x0fGmaybeeither directed or undirected.\\n\\x0fTwocommon ways torepresent graphs for algorithms:\\n1. Adjacency lists.\\n2. Adjacency matrix.\\nWhen expressing the running time of an algorithm, it’s often in terms of bothjVj\\nandjEj. In asymptotic notation—and onlyinasymptotic notation—we’ll drop the\\ncardinality. Example: O.VCE/.\\n[Theintroduction to Part VItalks moreabout this.]\\nAdjacencylists\\nArrayAdjofjVjlists, one per vertex.\\nVertex u’slisthasall vertices \\x17suchthat .u; \\x17/2E. (Worksfor bothdirected and\\nundirected graphs.)\\nIn pseudocode, denote the array as attribute G:Adj, so will see notation such as\\nG:AdjŒu\\x8d.\\nExample\\nForan undirected graph:\\n1 2\\n3\\n4 51\\n2\\n3\\n4\\n525\\n1\\n2\\n2\\n41 25 345Adj\\n43', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 366}),\n",
              " Document(page_content='22-2 Lecture Notes for Chapter 22: Elementary GraphAlgorit hms\\nIf edges have weights, can put theweights inthe lists.\\nWeight: wWE!R\\nWe’ll use weights later onfor spanning trees and shortest pa ths.\\nSpace: ‚.VCE/.\\nTime:tolist all vertices adjacent to u:‚.degree .u//.\\nTime:todetermine whether .u; \\x17/2E:O.degree .u//.\\nExample\\nFor adirected graph:\\n1 2\\n31\\n2\\n3\\n42\\n4\\n1 2\\n4Adj\\n3 4\\nSameasymptotic space and time.\\nAdjacency matrix\\njVj\\x02jVjmatrix AD.aij/\\naijD(\\n1if.i; j /2E ;\\n0otherwise :\\n1 0 0 1\\n0 1 1 1\\n1 0 1 0\\n1 1 0 1\\n1 0 1 00\\n1\\n0\\n0\\n11 2 3 4 5\\n1\\n2\\n3\\n4\\n51 0 0\\n0 0 1\\n1 0 0\\n0 1 10\\n0\\n1\\n01 2 3 4\\n1\\n2\\n3\\n4\\nSpace: ‚.V2/.\\nTime:tolist all vertices adjacent to u:‚.V /.\\nTime:todetermine whether .u; \\x17/2E:‚.1/.\\nCanstore weights instead of bits for weighted graph.\\nWe’ll use both representations in these lecture notes.\\nRepresenting graph attributes\\nGraphalgorithmsusuallyneedtomaintainattributesforve rticesand/oredges. Use\\nthe usual dot-notation: denote attribute dof vertex \\x17by\\x17:d.\\nUsethe dot-notation for edges, too: denote attribute fof edge .u; \\x17/by.u; \\x17/:f.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 367}),\n",
              " Document(page_content='Lecture Notes for Chapter 22: Elementary Graph Algorithms 2 2-3\\nImplementing graph attributes\\nNo one best way to implement. Depends on the programming lang uage, the algo-\\nrithm, and how the rest of the program interacts withthe grap h.\\nIf representing the graph with adjacency lists, can represe nt vertex attributes in\\nadditional arrays that parallel the Adjarray, e.g., dŒ1 : :jVj\\x8d, so that if vertices\\nadjacent to uareinAdjŒu\\x8d,store u:din array entry dŒu\\x8d.\\nBut can represent attributes in other ways. Example: repres ent vertex attributes as\\ninstance variables within asubclass of a Vertex class.\\nBreadth-ﬁrst search\\nInput:Graph GD.V; E/,eitherdirectedorundirected, and sourcevertex s2V.\\nOutput: \\x17:dDdistance (smallest #of edges) from sto\\x17,for all \\x172V.\\nIn book, also \\x17:\\x19such that .u; \\x17/islast edge on shortest path s;\\x17.\\n\\x0fuis\\x17’spredecessor .\\n\\x0fset of edgesf.\\x17:\\x19; \\x17/W\\x17¤sgformsatree.\\nLater, we’ll see a generalization of breadth-ﬁrst search, w ith edge weights. For\\nnow, we’ll keep it simple.\\n\\x0fCompute only \\x17:d,not\\x17:\\x19.[See book for \\x17:\\x19.]\\n\\x0fOmittingcolorsofvertices. [Usedinbook toreasonabout thealgorithm. We’ll\\nskip them here.]\\nIdea\\nSendawave out from s.\\n\\x0fFirst hits all vertices 1edge from s.\\n\\x0fFrom there, hits all vertices 2edges from s.\\n\\x0fEtc.\\nUseFIFOqueue Qto maintain wavefront.\\n\\x0f\\x172Qif and only if wavehas hit \\x17but has not come out of \\x17yet.\\nBFS.V; E; s/\\nforeachu2V/NULfsg\\nu:dD1\\ns:dD0\\nQD;\\nENQUEUE .Q; s/\\nwhile Q¤;\\nuDDEQUEUE .Q/\\nforeach\\x172G:AdjŒu\\x8d\\nif\\x17:d==1\\n\\x17:dDu:dC1\\nENQUEUE .Q; \\x17/', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 368}),\n",
              " Document(page_content='22-4 Lecture Notes for Chapter 22: Elementary GraphAlgorit hms\\nExample\\ndirected graph [undirected example inbook] .\\na\\nbs\\nec\\nig\\nhf0\\n1\\n321\\n2\\n3\\n33\\nCanshow that Qconsists of vertices with dvalues.\\ni i i : : : i i C1 iC1 : : : iC1\\n\\x0fOnly1 or 2values.\\n\\x0fIf 2, differ by 1and all smallest areﬁrst.\\nSinceeachvertexgetsaﬁnite dvalueatmost once, valuesassigned tovertices are\\nmonotonically increasing over time.\\nActual proof of correctness isa bit trickier. Seebook.\\nBFSmaynot reach all vertices.\\nTimeDO.VCE/.\\n\\x0fO.V /because every vertex enqueued at most once.\\n\\x0fO.E/because everyvertexdequeued atmostonceandweexamine .u; \\x17/only\\nwhen uis dequeued. Therefore, every edge examined at most once if d irected,\\nat most twiceif undirected.\\nDepth-ﬁrst search\\nInput: GD.V; E/, directed or undirected. Nosource vertex given!\\nOutput: 2timestamps on each vertex:\\n\\x0f\\x17:dDdiscovery time\\n\\x0f\\x17:fDﬁnishingtime\\nThese will beuseful for other algorithms later on.\\nCanalso compute \\x17:\\x19.[Seebook.]\\nWill methodically explore everyedge.\\n\\x0fStart over from different vertices asnecessary.\\nAssoon as wediscover avertex, explore from it.\\n\\x0fUnlike BFS,which puts a vertex onaqueue so that weexplore fr om it later.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 369}),\n",
              " Document(page_content='Lecture Notes for Chapter 22: Elementary Graph Algorithms 2 2-5\\nAsDFSprogresses, every vertex has a color:\\n\\x0fWHITEDundiscovered\\n\\x0fGRAYDdiscovered, but not ﬁnished (not done exploring from it)\\n\\x0fBLACKDﬁnished (have found everything reachable from it)\\nDiscovery and ﬁnishing times:\\n\\x0fUnique integers from 1to 2jVj.\\n\\x0fFor all \\x17,\\x17:d< \\x17:f.\\nInother words, 1\\x14\\x17:d< \\x17:f\\x142jVj.\\nPseudocode\\nUsesaglobal timestamp time.\\nDFS.G/\\nforeachu2G:V\\nu:colorDWHITE\\ntimeD0\\nforeachu2G:V\\nifu:color==WHITE\\nDFS-V ISIT.G; u/\\nDFS-V ISIT.G; u/\\ntimeDtimeC1\\nu:dDtime\\nu:colorDGRAY//discover u\\nforeach\\x172G:AdjŒu\\x8d//explore .u; \\x17/\\nif\\x17:color==WHITE\\nDFS-V ISIT.\\x17/\\nu:colorDBLACK\\ntimeDtimeC1\\nu:fDtime //ﬁnish u\\nExample\\n[Gothroughthisexample,addinginthe dandfvaluesasthey’recomputed. Show\\ncolors asthey change. Don’t put in the edge types yet.]\\n121\\n43118\\n651613\\n151472109TT\\nT\\nT\\nTTBF\\nC CC\\nCCCd f', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 370}),\n",
              " Document(page_content='22-6 Lecture Notes for Chapter 22: Elementary GraphAlgorit hms\\nTimeD‚.VCE/.\\n\\x0fSimilar to BFSanalysis.\\n\\x0f‚,not just O, since guaranteed to examine every vertex and edge.\\nDFS forms a depth-ﬁrst forest comprised of > 1depth-ﬁrst trees . Each tree is\\nmade of edges .u; \\x17/such that uis gray and \\x17is white when .u; \\x17/isexplored.\\nTheorem (Parenthesis theorem)\\n[Proof omitted.]\\nFor all u; \\x17,exactly one of the following holds:\\n1.u:d< u:f< \\x17:d< \\x17:for\\x17:d< \\x17:f< u:d< u:f(i.e.,theintervals Œu:d; u:f\\x8d\\nandŒ\\x17:d; \\x17:f\\x8dare disjoint) and neither of uand\\x17isadescendant of the other.\\n2.u:d< \\x17:d< \\x17:f< u:fand\\x17isadescendant of u.\\n3.\\x17:d< u:d< u:f< \\x17:fanduisadescendant of \\x17.\\nSou:d< \\x17:d< u:f< \\x17:f cannothappen.\\nLike parentheses:\\n\\x0fOK: ( ) [ ] ( [ ] ) [ ( ) ]\\n\\x0fNot OK: ( [ ) ] [ ( ] )\\nCorollary\\n\\x17isaproper descendant of uif and only if u:d< \\x17:d< \\x17:f< u:f.\\nTheorem (White-path theorem)\\n[Proof omitted.]\\n\\x17is a descendant of uif and only if at time u:d, there is a path u;\\x17consisting\\nof only white vertices. (Except for u, which was justcolored gray.)\\nClassiﬁcation of edges\\n\\x0fTreeedge: inthe depth-ﬁrst forest. Foundby exploring .u; \\x17/.\\n\\x0fBack edge: .u; \\x17/,where uisadescendant of \\x17.\\n\\x0fForward edge: .u; \\x17/,where \\x17isadescendant of u, but not atree edge.\\n\\x0fCross edge: any other edge. Can go between vertices in same depth-ﬁrst tr ee\\nor in different depth-ﬁrst trees.\\n[Now label the example from above withedge types.]\\nIn an undirected graph, there may be some ambiguity since .u; \\x17/and.\\x17; u/are\\nthe same edge. Classify by theﬁrst type above that matches.\\nTheorem\\n[Proof omitted.]\\nIn DFS of an undirected graph, we get only tree and back edges. No forward or\\ncross edges.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 371}),\n",
              " Document(page_content='Lecture Notes for Chapter 22: Elementary Graph Algorithms 2 2-7\\nTopologicalsort\\nDirected acyclic graph (dag)\\nAdirected graph with nocycles.\\nGoodfor modeling processes and structures that have a partial order:\\n\\x0fa > bandb > c)a > c.\\n\\x0fBut may have aandbsuch that neither a > bnorb > c.\\nCanalways makea total order (either a > borb > afor all a¤b)from apartial\\norder. In fact, that’s what atopological sort will do.\\nExample\\nDag of dependencies for putting on goalie equipment: [Leave on board, but show\\nwithout discovery and ﬁnishing times. Will put them inlater .]\\nshorts\\n17/22pantsT-shirt\\nleg padshosesocks\\n16/2325/26 15/24\\nskates18/21\\n19/20batting glove\\nchest pad\\nsweater\\nmask\\ncatch glove7/14\\n8/13\\n9/12\\n10/11\\n2/5\\nblocker3/41/6\\nLemma\\nAdirected graph Gisacyclic if and only if aDFSof Gyields no back edges.\\nProof): Show that back edge )cycle.\\nSuppose there isaback edge .u; \\x17/. Then \\x17is ancestor of uindepth-ﬁrst forest.\\nv\\nBT\\nT\\nT\\nu', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 372}),\n",
              " Document(page_content='22-8 Lecture Notes for Chapter 22: Elementary GraphAlgorit hms\\nTherefore, there isapath \\x17;u, so\\x17;u!\\x17isacycle.\\n(: Show that cycle)back edge.\\nSuppose Gcontains cycle c. Let \\x17betheﬁrstvertexdiscovered in c,andlet .u; \\x17/\\nbe the preceding edge in c. At time \\x17:d, vertices of cform a white path \\x17;u\\n(since \\x17istheﬁrstvertexdiscoveredin c). Bywhite-paththeorem, uisdescendant\\nof\\x17in depth-ﬁrst forest. Therefore, .u; \\x17/isaback edge. (lemma)\\nTopological sort of a dag: a linear ordering of vertices such that if .u; \\x17/2E,\\nthenuappears somewhere before \\x17. (Not like sorting numbers.)\\nTOPOLOGICAL -SORT.G/\\ncall DFS .G/tocompute ﬁnishing times \\x17:ffor all \\x172G:V\\noutput vertices inorder of decreasing ﬁnishing times\\nDon’t need tosort by ﬁnishing times.\\n\\x0fCan just output vertices as they’re ﬁnished and understand t hat we want the\\nreverseof this list.\\n\\x0fOr put them onto the frontof a linked list as they’re ﬁnished. When done, the\\nlist contains vertices intopologically sorted order.\\nTime\\n‚.VCE/.\\nDoexample. [Now writediscovery and ﬁnishing times in goalie equipment exam-\\nple.]\\nOrder:\\n26 socks\\n24 shorts\\n23 hose\\n22 pants\\n21 skates\\n20 legpads\\n14 t-shirt\\n13 chest pad\\n12 sweater\\n11 mask\\n6 batting glove\\n5 catch glove\\n4 blocker\\nCorrectness\\nJust need toshow if .u; \\x17/2E,then \\x17:f< u:f.\\nWhen weexplore .u; \\x17/, what are the colors of uand\\x17?\\n\\x0fuisgray.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 373}),\n",
              " Document(page_content='Lecture Notes for Chapter 22: Elementary Graph Algorithms 2 2-9\\n\\x0fIs\\x17gray, too?\\n\\x0fNo,because then \\x17would beancestor of u.\\n).u; \\x17/is aback edge.\\n)contradiction of previous lemma(dag has no back edges).\\n\\x0fIs\\x17white?\\n\\x0fThenbecomes descendant of u.\\nByparenthesis theorem, u:d< \\x17:d<\\x17:f< u:f.\\n\\x0fIs\\x17black?\\n\\x0fThen \\x17isalready ﬁnished.\\nSince we’re exploring .u; \\x17/, wehave not yet ﬁnished u.\\nTherefore, \\x17:f< u:f.\\nStrongly connected components\\nGivendirected graph GD.V; E/.\\nAstrongly connected component (SCC)ofGisamaximal set of vertices C\\x12V\\nsuch that for all u; \\x172C,both u;\\x17and\\x17;u.\\nExample\\n[Just show SCC’sat ﬁrst. DoDFSalittle later.]\\n14/19 15/16\\n17/18 13/203/4\\n2/51/12\\n10/116/9\\n7/8\\nAlgorithm uses GTDtranspose ofG.\\n\\x0fGTD.V; ET/,ETDf.u; \\x17/W.\\x17; u/2Eg.\\n\\x0fGTisGwithall edges reversed.\\nCancreate GTin‚.VCE/timeif using adjacency lists.\\nObservation\\nGandGThave thesameSCC’s. ( uand\\x17are reachable from each other in Gif\\nand only if reachable from each other in GT.)\\nComponentgraph\\n\\x0fGSCCD.VSCC; ESCC/.\\n\\x0fVSCChas one vertex for each SCCin G.\\n\\x0fESCChas anedge if there’s an edge between thecorresponding SCC’ sinG.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 374}),\n",
              " Document(page_content='22-10 Lecture Notes for Chapter 22: Elementary GraphAlgori thms\\nFor our example:\\nLemma\\nGSCCis a dag. More formally, let CandC0be distinct SCC’s in G, letu; \\x172C,\\nu0; \\x1702C0, and suppose there is a path u;u0inG. Then there cannot also be a\\npath\\x170;\\x17inG.\\nProofSuppose there is a path \\x170;\\x17inG. Then there are paths u;u0;\\x170\\nand\\x170;\\x17;uinG. Therefore, uand\\x170are reachable from each other, so they\\nare not inseparate SCC’s. (lemma)\\nSCC.G/\\ncall DFS .G/tocompute ﬁnishing times u:ffor all u\\ncompute GT\\ncall DFS .GT/,but in the mainloop, consider vertices in order of decreasi ngu:f\\n(as computed in ﬁrst DFS)\\noutput the vertices in each tree of thedepth-ﬁrst forest for med insecond DFS\\nas aseparate SCC\\nExample:\\n1. DoDFS\\n2.GT\\n3. DFS(roots blackened)\\nTime: ‚.VCE/.\\nHowcan this possibly work?\\nIdea\\nByconsidering vertices insecond DFSindecreasing order of ﬁnishing timesfrom\\nﬁrstDFS,wearevisitingverticesofthecomponent graphint opological sortorder.\\nToprove that it works, ﬁrst deal with 2notational issues:\\n\\x0fWill bediscussing u:dandu:f. These always refer to ﬁrstDFS.\\n\\x0fExtend notation for dandfto sets of vertices U\\x12V:\\n\\x0fd.U /Dmin u2Ufu:dg(earliest discovery time)\\n\\x0ff .U /Dmax u2Ufu:fg(latest ﬁnishing time)', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 375}),\n",
              " Document(page_content='Lecture Notes for Chapter 22: Elementary Graph Algorithms 2 2-11\\nLemma\\nLetCandC0be distinct SCC’sin GD.V; E/. Suppose there is an edge .u; \\x17/2\\nEsuch that u2Cand\\x172C0.\\nv uCC′\\nThen f .C / > f .C0/.\\nProofTwocases, depending on which SCC had the ﬁrst discovered ver tex during\\nthe ﬁrst DFS.\\n\\x0fIfd.C / < d.C0/, letxbe the ﬁrst vertex discovered in C. At time x:d, all\\nvertices in CandC0arewhite. Thus, there exist paths ofwhitevertices from x\\nto all vertices in CandC0.\\nBy the white-path theorem, all vertices in CandC0are descendants of xin\\ndepth-ﬁrst tree.\\nBythe parenthesis theorem, x:fDf .C / > f .C0/.\\n\\x0fIfd.C / > d.C0/, letybe the ﬁrst vertex discovered in C0. At time y:d, all\\nvertices in C0are white and there is a white path from yto each vertex in C0\\n)all vertices in C0become descendants of y. Again, y:fDf .C0/.\\nAt time y:d, all vertices in Care white.\\nBy earlier lemma, since there is an edge .u; \\x17/, wecannot have apath from C0\\ntoC.\\nSono vertex in Cis reachable from y.\\nTherefore, at time y:f,all vertices in Care still white.\\nTherefore, for all w2C,w:f> y:f,which implies that f .C / > f .C0/.\\n(lemma)\\nCorollary\\nLetCandC0be distinct SCC’s in GD.V; E/. Suppose there is an edge\\n.u; \\x17/2ET, where u2Cand\\x172C0. Then f .C / < f .C0/.\\nProof .u; \\x17/2ET).\\x17; u/2E. Since SCC’s of GandGTare the same,\\nf .C0/ > f .C / . (corollary)\\nCorollary\\nLetCandC0be distinct SCC’s in GD.V; E/, and suppose that f .C / > f .C0/.\\nThenthere cannot bean edge from CtoC0inGT.\\nProofIt’s the contrapositive of theprevious corollary.\\nNowwehave the intuition tounderstand whythe SCCprocedure works.\\nWhen we do the second DFS, on GT, start with SCC Csuch that f .C /is max-\\nimum. The second DFS starts from some x2C, and it visits all vertices in C.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 376}),\n",
              " Document(page_content='22-12 Lecture Notes for Chapter 22: Elementary GraphAlgori thms\\nCorollarysaysthatsince f .C / > f .C0/forall C0¤C,therearenoedgesfrom C\\ntoC0inGT.\\nTherefore, DFSwill visit onlyvertices in C.\\nWhichmeansthatthedepth-ﬁrst treerootedat xcontainsexactlytheverticesof C.\\nThenextrootchoseninthesecondDFSisinSCC C0suchthat f .C0/ismaximum\\nover all SCC’s other than C. DFS visits all vertices in C0, but the only edges out\\nofC0go to C,whichwe’ve already visited .\\nTherefore, the only tree edges will be tovertices in C0.\\nWecan continue theprocess.\\nEach timewechoose aroot for the second DFS,it can reach only\\n\\x0fvertices inits SCC—gettree edges to these,\\n\\x0fvertices inSCC’s already visited in second DFS—get notree edges tothese.\\nWeare visiting vertices of .GT/SCCinreverse of topologically sorted order.\\n[The book has aformal proof.]', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 377}),\n",
              " Document(page_content='SolutionsforChapter 22:\\nElementaryGraph Algorithms\\nSolutionto Exercise 22.1-6\\nWe start by observing that if aijD1, so that .i; j /2E, then vertex icannot\\nbe a universal sink, for it has an outgoing edge. Thus, if row icontains a 1, then\\nvertex icannot be a universal sink. This observation also means that if there is a\\nself-loop .i; i/, then vertex iis not a universal sink. Now suppose that aijD0,\\nso that .i; j /62E, and also that i¤j. Then vertex jcannot be a universal sink,\\nfor either its in-degree must be strictly less than jVj/NUL1or it has aself-loop. Thus\\nif column jcontains a 0in any position other than the diagonal entry .j; j /, then\\nvertex jcannot beauniversal sink.\\nUsing the above observations, the following procedure retu rnsTRUEif vertex k\\nis a universal sink, and FALSEotherwise. It takes as input a jVj\\x02jVjadjacency\\nmatrix AD.aij/.\\nIS-SINK.A; k/\\nletAbejVj\\x02jVj\\nforjD1tojVj//check for a 1inrow k\\nifakj==1\\nreturn FALSE\\nforiD1tojVj//check for an off-diagonal 0incolumn k\\nifaik==0andi¤k\\nreturn FALSE\\nreturn TRUE\\nBecause this procedure runs in O.V /time, we may call it only O.1/times in\\norder to achieve our O.V /-time bound for determining whether directed graph G\\ncontains a universal sink.\\nObserve also that a directed graph can have at most one univer sal sink. This prop-\\nerty holds because if vertex jis a universal sink, then we would have .i; j /2E\\nfor all i¤jand so noother vertex icould beauniversal sink.\\nThefollowing procedure takes an adjacency matrix Aas input and returns either a\\nmessage that there is no universal sink or a message containi ng the identity of the\\nuniversal sink. It works by eliminating all but one vertex as a potential universal\\nsinkandthenchecking theremaining candidate vertexbyasi ngle call to I S-SINK.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 378}),\n",
              " Document(page_content='22-14 Solutions for Chapter 22: Elementary GraphAlgorithm s\\nUNIVERSAL -SINK.A/\\nletAbejVj\\x02jVj\\niDjD1\\nwhile i\\x14jVjandj\\x14jVj\\nifaij==1\\niDiC1\\nelsejDjC1\\nifi >jVj\\nreturn“there is nouniversal sink”\\nelseifIS-SINK.A; i/==FALSE\\nreturn“there is nouniversal sink”\\nelse return i“is auniversal sink”\\nUNIVERSAL -SINKwalks through the adjacency matrix, starting at the upper le ft\\ncorner and always moving either right or down by one position , depending on\\nwhether the current entry aijit is examining is 0or1. It stops once either iorj\\nexceedsjVj.\\nTounderstand whyU NIVERSAL -SINKworks,weneedtoshowthatafterthe while\\nloop terminates, the only vertex that might be a universal si nk is vertex i. The call\\nto IS-SINKthen determines whether vertex iis indeed auniversal sink.\\nLet us ﬁx iandjto be values of these variables at the termination of the while\\nloop. We claim that every vertex ksuch that 1\\x14k < icannot be a universal\\nsink. Thatisbecausethewaythat iachieveditsﬁnalvalueatlooptermination was\\nby ﬁnding a 1in each row kfor which 1\\x14k < i. As we observed above, any\\nvertex kwhose row contains a 1cannot be auniversal sink.\\nIfi >jVjat loop termination, then we have eliminated all vertices fr om consid-\\neration, and so there is no universal sink. If, on the other ha nd,i\\x14jVjat loop\\ntermination, we need to show that every vertex ksuch that i < k\\x14jVjcannot\\nbe a universal sink. If i\\x14jVjat loop termination, then the whileloop terminated\\nbecause j >jVj. Thatmeansthatwefounda 0ineverycolumn. Recallourearlier\\nobservation that if column kcontains a 0inanoff-diagonal position, then vertex k\\ncannot be a universal sink. Since we found a 0in every column, we found a 0in\\nevery column ksuch that i < k\\x14jVj. Moreover, we never examined any matrix\\nentries in rows greater than i, and so wenever examined the diagonal entry in any\\ncolumn ksuch that i < k\\x14jVj. Therefore, all the 0s that wefound in columns k\\nsuch that i < k\\x14jVjwere off-diagonal. We conclude that every vertex ksuch\\nthati < k\\x14jVjcannot be auniversal sink.\\nThus, we have shown that every vertex less than iand every vertex greater than i\\ncannot beauniversal sink. Theonlyremaining possibility i sthat vertex imight be\\nauniversal sink, and the call to I S-SINKchecks whether it is.\\nTo see that U NIVERSAL -SINKruns in O.V /time, observe that either iorjis\\nincremented in each iteration of the whileloop. Thus, the whileloop makes at\\nmost 2jVj/NUL1iterations. Each iteration takes O.1/time, for a total whileloop\\ntime of O.V /and, combined with the O.V /-time call to I S-SINK, we get a total\\nrunning timeof O.V /.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 379}),\n",
              " Document(page_content='Solutions forChapter 22: Elementary Graph Algorithms 22-1 5\\nSolutionto Exercise 22.1-7\\nThissolutionisalsopostedpublicly\\nBBT.i; j /DX\\ne2EbiebT\\nejDX\\ne2Ebiebje\\n\\x0fIfiDj,then biebjeD1(it is1\\x011or./NUL1/\\x01./NUL1/) whenever eenters or leaves\\nvertex i, and 0otherwise.\\n\\x0fIfi¤j,then biebjeD/NUL1when eD.i; j /oreD.j; i/, and 0otherwise.\\nThus,\\nBBT.i; j /D(\\ndegree of iDin-degreeCout-degree if iDj ;\\n/NUL.#of edges connecting iandj /ifi¤j :\\nSolutionto Exercise 22.2-3\\nNote:Thisexercisechangedinthethirdprinting. Thissolutionr eﬂectsthechange.\\nThe BFS procedure cares only whether a vertex is white or not. A vertex \\x17must\\nbecomenon-whiteatthesametimethat \\x17:disassigned aﬁnitevaluesothatwedo\\nnot attempt to assign to \\x17:dagain, and so weneed to change vertex colors in lines\\n5 and 14. Once we have changed a vertex’s color to non-white, w e do not need to\\nchange it again.\\nSolutionto Exercise 22.2-5\\nThissolutionisalsopostedpublicly\\nThe correctness proof for the BFS algorithm shows that u:dDı.s; u/, and the\\nalgorithm doesn’t assume that the adjacency lists are inany particular order.\\nIn Figure 22.3, if tprecedes xinAdjŒw\\x8d, we can get the breadth-ﬁrst tree shown\\nin the ﬁgure. But if xprecedes tinAdjŒw\\x8danduprecedes yinAdjŒx\\x8d,we can get\\nedge .x; u/inthe breadth-ﬁrst tree.\\nSolutionto Exercise 22.2-6\\nTheedges in E\\x19are shaded in the following graph:\\nsuw\\nvx', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 380}),\n",
              " Document(page_content='22-16 Solutions for Chapter 22: Elementary GraphAlgorithm s\\nTo see that E\\x19cannot be a breadth-ﬁrst tree, let’s suppose that AdjŒs\\x8dcontains u\\nbefore \\x17. BFS adds edges .s; u/and.s; \\x17/to the breadth-ﬁrst tree. Since uis\\nenqueued before \\x17, BFS thenadds edges .u; w/and.u; x/. (Theorder of wandx\\ninAdjŒu\\x8ddoesn’t matter.) Symmetrically, if AdjŒs\\x8dcontains \\x17before u, then BFS\\nadds edges .s; \\x17/and.s; u/to the breadth-ﬁrst tree, \\x17is enqueued before u, and\\nBFS adds edges .\\x17; w/and.\\x17; x/. (Again, the order of wandxinAdjŒ\\x17\\x8ddoesn’t\\nmatter.) BFS willneverputbothedges .u; w/and.\\x17; x/intothebreadth-ﬁrst tree.\\nInfact,itwillalsoneverputbothedges .u; x/and.\\x17; w/intothebreadth-ﬁrst tree.\\nSolution to Exercise22.2-7\\nCreateagraph Gwhereeachvertexrepresents awrestler andeachedgerepres ents\\narivalry. Thegraph will contain nvertices and redges.\\nPerform as many BFS’s as needed to visit all vertices. Assign all wrestlers whose\\ndistance is even to be babyfaces and all wrestlers whose dist ance is odd to be\\nheels. Then check each edge to verify that it goes between a ba byface and a heel.\\nThis solution would take O.nCr/time for the BFS, O.n/timeto designate each\\nwrestler as a babyface or heel, and O.r/time to check edges, which is O.nCr/\\ntime overall.\\nSolution to Exercise22.3-4\\nNote:Thisexercisechangedinthethirdprinting. Thissolutionr eﬂectsthechange.\\nThe DFS and DFS-V ISITprocedures care only whether a vertex is white or not.\\nBy coloring vertex ugray when it is ﬁrst visited, in line 3 of DFS-V ISIT, we\\nensure that uwill not be visited again. Once we have changed a vertex’s col or to\\nnon-white, wedo not need tochange it again.\\nSolution to Exercise22.3-5\\na.Edge .u; \\x17/is a tree edge or forward edge if and only if \\x17is a descendant of u\\nin the depth-ﬁrst forest. (If .u; \\x17/is a back edge, then uis a descendant of \\x17,\\nandif .u; \\x17/isacross edge, then neither of uor\\x17isadescendant oftheother.)\\nByCorollary 22.8,therefore, .u; \\x17/isatreeedgeorforwardedgeifandonlyif\\nu:d< \\x17:d< \\x17:f< u:f.\\nb.First, suppose that .u; \\x17/is a back edge. A self-loop is by deﬁnition a back\\nedge. If .u; \\x17/is a self-loop, then clearly \\x17:dDu:d< u:fD\\x17:f. If.u; \\x17/\\nis not a self-loop, then uis a descendant of \\x17in the depth-ﬁrst forest, and by\\nCorollary 22.8, \\x17:d< u:d< u:f< \\x17:f.\\nNow,supposethat \\x17:d\\x14u:d< u:f\\x14\\x17:f. Ifuand\\x17arethesamevertex,then\\n\\x17:dDu:d< u:fD\\x17:f, and .u; \\x17/is a self-loop and hence a back edge. If u', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 381}),\n",
              " Document(page_content='Solutions forChapter 22: Elementary Graph Algorithms 22-1 7\\nand\\x17are distinct, then \\x17:d< u:d< u:f< \\x17:f. By the parenthesis theorem,\\ninterval Œu:d; u:f\\x8dis contained entirely within the interval Œ\\x17:d; \\x17:f\\x8d, and uis a\\ndescendant of \\x17inadepth-ﬁrst tree. Thus, .u; \\x17/is aback edge.\\nc.First,suppose that .u; \\x17/isacrossedge. Sinceneither unor\\x17isanancestor of\\ntheother,theparenthesistheoremsaysthattheintervals Œu:d; u:f\\x8dandŒ\\x17:d; \\x17:f\\x8d\\nare entirely disjoint. Thus, we must have either u:d< u:f< \\x17:d< \\x17:for\\n\\x17:d< \\x17:f< u:d< u:f. Weclaim that wecannot have u:d< \\x17:dif.u; \\x17/isa\\ncross edge. Why? If u:d< \\x17:d, then \\x17is white at time u:d. By the white-path\\ntheorem, \\x17is a descendant of u, which contradicts .u; \\x17/being a cross edge.\\nThus, wemust have \\x17:d< \\x17:f< u:d< u:f.\\nNowsuppose that \\x17:d< \\x17:f< u:d< u:f. Bytheparenthesis theorem, neither\\nunor\\x17is a descendant of the other, which means that .u; \\x17/must be a cross\\nedge.\\nSolutionto Exercise 22.3-8\\nLet usconsider the example graph and depth-ﬁrst search belo w.\\nd f\\nw1 6\\nu2 3\\n\\x174 5u vw\\nClearly, there is a path from uto\\x17inG. The bold edges are in the depth-ﬁrst\\nforest produced. We can see that u:d< \\x17:din the depth-ﬁrst search but \\x17is not a\\ndescendant of uinthe forest.\\nSolutionto Exercise 22.3-9\\nLet usconsider the example graph and depth-ﬁrst search belo w.\\nd f\\nw1 6\\nu2 3\\n\\x174 5u vw\\nClearly, there isapathfrom uto\\x17inG. Theboldedges of Gareinthedepth-ﬁrst\\nforest produced by the search. However, \\x17:d> u:fand theconjecture isfalse.\\nSolutionto Exercise 22.3-11\\nLet usconsider the example graph and depth-ﬁrst search belo w.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 382}),\n",
              " Document(page_content='22-18 Solutions for Chapter 22: Elementary GraphAlgorithm s\\nd f\\nw1 2\\nu3 4\\n\\x175 6w v u\\nClearly uhasbothincoming andoutgoing edges in Gbut adepth-ﬁrst search of G\\nproduced adepth-ﬁrst forest where uis inatree by itself.\\nSolution to Exercise22.3-12\\nThis solutionisalsopostedpublicly\\nThefollowingpseudocode modiﬁestheDFSandDFS-V ISITprocedurestoassign\\nvalues tothe ccattributes of vertices.\\nDFS.G/\\nforeach vertex u2G:V\\nu:colorDWHITE\\nu:\\x19DNIL\\ntimeD0\\ncounterD0\\nforeach vertex u2G:V\\nifu:color==WHITE\\ncounterDcounterC1\\nDFS-V ISIT.G; u;counter /\\nDFS-V ISIT.G; u;counter /\\nu:ccDcounter//label the vertex\\ntimeDtimeC1\\nu:dDtime\\nu:colorDGRAY\\nforeach\\x172G:AdjŒu\\x8d\\nif\\x17:color==WHITE\\n\\x17:\\x19Du\\nDFS-V ISIT.G; \\x17;counter /\\nu:colorDBLACK\\ntimeDtimeC1\\nu:fDtime\\nThis DFS increments acounter each time DFS-V ISITiscalled togrow anew tree\\nin the DFS forest. Every vertex visited (and added to the tree ) by DFS-V ISITis\\nlabeled with that same counter value. Thus u:ccD\\x17:ccif and only if uand\\x17are\\nvisitedinthesamecalltoDFS-V ISITfromDFS,andtheﬁnalvalueofthecounter\\nis the number of calls that were made to DFS-V ISITby DFS. Also, since every\\nvertex is visited eventually, every vertex islabeled.\\nThus all we need to show is that the vertices visited by each ca ll to DFS-V ISIT\\nfrom DFS are exactly the vertices in one connected component ofG.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 383}),\n",
              " Document(page_content='Solutions forChapter 22: Elementary Graph Algorithms 22-1 9\\n\\x0fAll vertices in a connected component are visited by one call to DFS-V ISIT\\nfrom DFS:\\nLetubethe ﬁrstvertex incomponent Cvisited by DFS-V ISIT. Sinceavertex\\nbecomes non-white only when it is visited, all vertices in Care white when\\nDFS-V ISITis called for u. Thus, by the white-path theorem, all vertices in C\\nbecome descendants of uin the forest, which means that all vertices in Care\\nvisited (by recursive calls to DFS-V ISIT) before DFS-V ISITreturns to DFS.\\n\\x0fAll vertices visited by one call to DFS-V ISITfrom DFS are in the same con-\\nnected component:\\nIftwoverticesarevisitedinthesamecalltoDFS-V ISITfrom DFS,theyarein\\nthe same connected component, because vertices are visited only by following\\npaths in G(by following edges found in adjacency lists, starting from some\\nvertex).\\nSolutionto Exercise 22.4-3\\nThissolutionisalsopostedpublicly\\nAn undirected graph is acyclic (i.e., a forest) if and only if a DFS yields no back\\nedges.\\n\\x0fIf there’s aback edge, there’s acycle.\\n\\x0fIf there’s no back edge, then by Theorem 22.10, there are only tree edges.\\nHence, the graph is acyclic.\\nThus, wecan run DFS: if weﬁndaback edge, there’s acycle.\\n\\x0fTime: O.V /. (Not O.VCE/!)\\nIf we ever seejVjdistinct edges, we must have seen a back edge because (by\\nTheorem B.2onp. 1174) in anacyclic (undirected) forest, jEj\\x14jVj/NUL1.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 384}),\n",
              " Document(page_content='22-20 Solutions for Chapter 22: Elementary GraphAlgorithm s\\nSolution to Exercise22.4-5\\nTOPOLOGICAL -SORT.G/\\n//Initialize in-degree,‚.V /time.\\nforeach vertex u2G:V\\nu:in-degreeD0\\n//Compute in-degree,‚.VCE/time.\\nforeach vertex u2G:V\\nforeach\\x172G:AdjŒu\\x8d\\n\\x17:in-degreeD\\x17:in-degreeC1\\n//Initialize Queue, ‚.V /time.\\nQD;\\nforeach vertex u2G:V\\nifu:in-degree==0\\nENQUEUE .Q; u/\\n//whileloop takes O.VCE/time.\\nwhile Q¤;\\nuDDEQUEUE .Q/\\noutput u\\n//forloop executes O.E/times total.\\nforeach\\x172G:AdjŒu\\x8d\\n\\x17:in-degreeD\\x17:in-degree/NUL1\\nif\\x17:in-degree==0\\nENQUEUE .Q; \\x17/\\n//Check for cycles, O.V /time.\\nforeach vertex u2G:V\\nifu:in-degree¤0\\nreport that there’s acycle\\n//Another waytocheck for cycles would be tocount the vertices\\n//that are output and report acycle if that number is <jVj.\\nToﬁndandoutput vertices ofin-degree 0,weﬁrstcompute all vertices’ in-degrees\\nby making a pass through all the edges (by scanning the adjace ncy lists of all the\\nvertices) and incrementing the in-degree of each vertex an e dge enters.\\n\\x0fComputing all in-degrees takes ‚.VCE/time (jVjadjacency lists accessed,\\njEjedges total found in those lists, ‚.1/work for each edge).\\nWe keep the vertices with in-degree 0 in a FIFO queue, so that t hey can be en-\\nqueued and dequeued in O.1/time. (The order in which vertices in the queue are\\nprocessed doesn’t matter, so anykind of FIFOqueue works.)\\n\\x0fInitializingthequeuetakesonepassovertheverticesdoin g‚.1/work,fortotal\\ntime‚.V /.\\nAs we process each vertex from the queue, we effectively remo ve its outgoing\\nedges from the graph by decrementing the in-degree of each ve rtex one of those\\nedgesenters,andweenqueueanyvertexwhosein-degreegoes to0. Wedonotneed\\nto actually remove the edges from the adjacency list, becaus e that adjacency list', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 385}),\n",
              " Document(page_content='Solutions forChapter 22: Elementary Graph Algorithms 22-2 1\\nwillnever beprocessed againbythealgorithm: Eachvertexi senqueued/dequeued\\natmostoncebecause itisenqueued onlyifitstarts outwithi n-degree 0orifitsin-\\ndegree becomes 0 after being decremented (and never increme nted) some number\\nof times.\\n\\x0fThe processing of a vertex from the queue happens O.V /times because no\\nvertex can be enqueued more than once. The per-vertex work (d equeue and\\noutput) takes O.1/time, for atotal of O.V /time.\\n\\x0fBecause the adjacency list of each vertex is scanned only whe n the vertex is\\ndequeued, the adjacency list of each vertex is scanned at mos t once. Since the\\nsumofthelengthsofalltheadjacencylistsis ‚.E/,atmost O.E/timeisspent\\nintotal scanning adjacency lists. Foreachedge inanadjace ncy list, ‚.1/work\\nis done, for atotal of O.E/time.\\nThusthe total time taken bythe algorithm is O.VCE/.\\nThealgorithm outputs vertices in theright order ( ubefore \\x17for every edge .u; \\x17/)\\nbecause \\x17will not be output until its in-degree becomes 0, which happe ns only\\nwhen every edge .u; \\x17/leading into \\x17has been “removed” due to the processing\\n(including output) of u.\\nIf there are no cycles, all vertices are output.\\n\\x0fProof: Assume that some vertex \\x170is not output. Vertex \\x170cannot start out\\nwith in-degree 0 (or it would be output), so there are edges in to\\x170. Since \\x170’s\\nin-degree never becomes 0, at least one edge .\\x171; \\x170/is never removed, which\\nmeans that at least one other vertex \\x171was not output. Similarly, \\x171not output\\nmeans that some vertex \\x172such that .\\x172; \\x171/2Ewas not output, and so on.\\nSince thenumber of vertices isﬁnite, this path ( \\x01\\x01\\x01! \\x172!\\x171!\\x170) isﬁnite,\\nsowemust have \\x17iD\\x17jfor some iandjinthis sequence, whichmeans there\\nis acycle.\\nIf there are cycles, not all vertices will be output, because some in-degrees never\\nbecome 0.\\n\\x0fProof: Assumethatavertexinacycleisoutput(itsin-degre ebecomes0). Let \\x17\\nbe the ﬁrst vertex in its cycle to be output, and let ube\\x17’s predecessor in the\\ncycle. In order for \\x17’s in-degree to become 0, the edge .u; \\x17/must have been\\n“removed,” which happens only when uis processed. But this cannot have\\nhappened, because \\x17is the ﬁrst vertex in its cycle to be processed. Thus no\\nvertices in cycles are output.\\nSolutionto Exercise 22.5-5\\nWehave atour disposal an O.VCE/-timealgorithm that computes strongly con-\\nnected components. Let us assume that the output of this algo rithm is a map-\\nping u:scc, giving the number of the strongly connected component cont aining\\nvertex u, for each vertex u. Without loss of generality, assume that u:sccis an\\ninteger inthe setf1; 2; : : : ;jVjg.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 386}),\n",
              " Document(page_content='22-22 Solutions for Chapter 22: Elementary GraphAlgorithm s\\nConstruct the multiset (a set that can contain the same objec t more than once)\\nTDfu:sccWu2Vg, and sort it by using counting sort. Since the values we are\\nsorting are integers in the range 1tojVj, the time to sort is O.V /. Go through the\\nsorted multiset Tandevery timeweﬁndanelement xthat isdistinct from theone\\nbefore it, add xtoVSCC. (Consider the ﬁrst element of the sorted set as “distinct\\nfrom the one before it.”) It takes O.V /time toconstruct VSCC.\\nConstruct theset of ordered pairs\\nSDf.x; y/Wthere is anedge .u; \\x17/2E; xDu:scc;andyD\\x17:sccg:\\nWecan easily construct this set in ‚.E/timeby going through all edges in Eand\\nlooking up u:sccand\\x17:sccfor each edge .u; \\x17/2E.\\nHavingconstructed S,removeall elementsoftheform .x; x/. Alternatively, when\\nweconstruct S, do not put an element in Swhen weﬁndan edge .u; \\x17/for which\\nu:sccD\\x17:scc.Snow has at mostjEjelements.\\nNowsort theelementsof Susingradixsort. Sortononecomponent atatime. The\\norder does not matter. In other words, we are performing two p asses of counting\\nsort. Thetimetodosois O.VCE/,sincethevalueswearesortingonareintegers\\nin the range 1tojVj.\\nFinally, go through the sorted set S, and every time we ﬁnd an element .x; y/\\nthat is distinct from the element before it (again consideri ng the ﬁrst element of\\nthe sorted set as distinct from the one before it), add .x; y/toESCC. Sorting and\\nthen adding .x; y/only if it is distinct from the element before it ensures that we\\nadd.x; y/at most once. It takes O.E/time to go through Sin this way, once S\\nhas been sorted.\\nThetotal timeis O.VCE/.\\nSolution to Exercise22.5-6\\nThe basic idea is to replace the edges within each SCC by one si mple, directed\\ncycle and then remove redundant edges between SCC’s. Since t here must be at\\nleast kedges within an SCC that has kvertices, a single directed cycle of kedges\\ngives the k-vertex SCCwiththe fewest possible edges.\\nThealgorithm works asfollows:\\n1. Identify all SCC’s of G. Time: ‚.VCE/, using the SCC algorithm in Sec-\\ntion 22.5.\\n2. Formthe component graph GSCC. Time: O.VCE/, byExercise 22.5-5.\\n3. Start with E0D;. Time: O.1/.\\n4. Foreach SCCof G, let the vertices in theSCCbe \\x171; \\x172; : : : ; \\x17 k,and addto E0\\nthe directed edges .\\x171; \\x172/; .\\x17 2; \\x173/; : : : ; .\\x17 k/NUL1; \\x17k/; .\\x17 k; \\x171/. These edges form\\na simple, directed cycle that includes all vertices of the SC C. Time for all\\nSCC’s: O.V /.\\n5. For each edge .u; \\x17/in the component graph GSCC, select any vertex xinu’s\\nSCC and any vertex yin\\x17’s SCC, and add the directed edge .x; y/toE0.\\nTime: O.E/.\\nThus, thetotal timeis ‚.VCE/.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 387}),\n",
              " Document(page_content='Solutions forChapter 22: Elementary Graph Algorithms 22-2 3\\nSolutionto Exercise 22.5-7\\nTodetermine whether GD.V; E/issemiconnected, do thefollowing:\\n1. Call S TRONGLY -CONNECTED -COMPONENTS .\\n2. Form the component graph. (By Exercise 22.5-5, you may ass ume that this\\ntakes O.VCE/time.)\\n3. Topologically sortthecomponent graph. (Recallthatit’ sadag.) Assumingthat\\nGcontains kSCC’s,thetopological sortgivesalinearordering h\\x171;\\x172;: : : ;\\x17 ki\\nof the vertices.\\n4. Verify that the sequence of vertices h\\x171; \\x172; : : : ; \\x17 kigiven by topological sort\\nforms a linear chain in the component graph. That is, verify t hat the edges\\n.\\x171; \\x172/; .\\x17 2; \\x173/; : : : ; .\\x17 k/NUL1; \\x17k/exist in the component graph. If the vertices\\nform a linear chain, then the original graph is semiconnecte d; otherwise it is\\nnot.\\nBecause we know that all vertices in each SCC are mutually rea chable from each\\nother, it sufﬁces to show that the component graph is semicon nected if and only if\\nit contains a linear chain. We must also show that if there’s a linear chain in the\\ncomponent graph, it’s the one returned by topological sort.\\nWe’ll ﬁrst show that if there’s a linear chain in the componen t graph, then it’s the\\none returned by topological sort. In fact, this is trivial. A topological sort has to\\nrespect everyedgeinthegraph. Soif there’s alinear chain, atopological sort must\\ngiveus the vertices inorder.\\nNow we’ll show that the component graph is semiconnected if a nd only if it con-\\ntains alinear chain.\\nFirst, suppose that the component graph contains a linear ch ain. Then for every\\npair of vertices u; \\x17in the component graph, there is a path between them. If u\\nprecedes \\x17inthelinear chain, thenthere’s apath u;\\x17. Otherwise, \\x17precedes u,\\nand there’s apath \\x17;u.\\nConversely, suppose that the component graph does not conta in a linear chain.\\nThen in the list returned by topological sort, there are two c onsecutive vertices \\x17i\\nand\\x17iC1,buttheedge .\\x17i; \\x17iC1/isnotinthecomponentgraph. Anyedgesoutof \\x17i\\nare to vertices \\x17j, where j > iC1, and so there is no path from \\x17ito\\x17iC1in the\\ncomponentgraph. Andsince \\x17iC1follows \\x17iinthetopologicalsort,therecannotbe\\nanypaths at all from \\x17iC1to\\x17i. Thus, the component graph is not semiconnected.\\nRunning timeof each step:\\n1.‚.VCE/.\\n2.O.VCE/.\\n3. Since the component graph has at most jVjvertices and at most jEjedges,\\nO.VCE/.\\n4. Also O.VCE/. We just check the adjacency list of each vertex \\x17iin the\\ncomponent graph to verify that there’s an edge .\\x17i; \\x17iC1/. We’ll go through\\neach adjacency list once.\\nThus, the total running timeis ‚.VCE/.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 388}),\n",
              " Document(page_content='22-24 Solutions for Chapter 22: Elementary GraphAlgorithm s\\nSolution to Problem 22-1\\nThis solutionisalsopostedpublicly\\na.1. Suppose .u; \\x17/is a back edge or a forward edge in a BFS of an undirected\\ngraph. Then one of uand\\x17, say u, is a proper ancestor of the other ( \\x17) in\\nthe breadth-ﬁrst tree. Since we explore all edges of ubefore exploring any\\nedges of anyof u’sdescendants, wemust explore theedge .u; \\x17/at thetime\\nweexplore u. But then .u; \\x17/must be atree edge.\\n2. In BFS, an edge .u; \\x17/is a tree edge when we set \\x17:\\x19Du. But we only\\ndo so when we set \\x17:dDu:dC1. Since neither u:dnor\\x17:dever changes\\nthereafter, wehave \\x17:dDu:dC1whenBFScompletes.\\n3. Consider a cross edge .u; \\x17/where, without loss of generality, uis visited\\nbefore \\x17. At the time we visit u, vertex \\x17must already be on the queue, for\\notherwise .u; \\x17/would be a tree edge. Because \\x17is on the queue, we have\\n\\x17:d\\x14u:dC1by Lemma 22.3. By Corollary 22.4, we have \\x17:d\\x15u:d.\\nThus, either \\x17:dDu:dor\\x17:dDu:dC1.\\nb.1. Suppose .u; \\x17/is a forward edge. Then we would have explored it while\\nvisiting u, and it would have been a tree edge.\\n2. Sameas for undirected graphs.\\n3. For any edge .u; \\x17/, whether or not it’s a cross edge, we cannot have\\n\\x17:d> u:dC1, since we visit \\x17at the latest when we explore edge .u; \\x17/.\\nThus, \\x17:d\\x14u:dC1.\\n4. Clearly, \\x17:d\\x150for all vertices \\x17. For a back edge .u; \\x17/,\\x17is an ancestor\\nofuin the breadth-ﬁrst tree, which means that \\x17:d\\x14u:d. (Note that since\\nself-loops areconsidered tobe back edges, wecould have uD\\x17.)\\nSolution to Problem 22-3\\na.An Euler tour is a single cycle that traverses each edge of Gexactly once, but\\nit might not be a simple cycle. An Euler tour can be decomposed into a set of\\nedge-disjoint simple cycles, however.\\nIfGhasanEulertour,therefore, wecanlookatthesimplecycles that,together,\\nform the tour. In each simple cycle, each vertex in the cycle h as one entering\\nedge and one leaving edge. In each simple cycle, therefore, e ach vertex \\x17has\\nin-degree .\\x17/Dout-degree .\\x17/, where the degrees are either 1(if\\x17is on the\\nsimple cycle) or 0(if\\x17is not on the simple cycle). Adding the in- and out-\\ndegrees over all edges proves that if Ghas an Euler tour, then in-degree .\\x17/D\\nout-degree .\\x17/for all vertices \\x17.\\nWeprovetheconverse—that ifin-degree .\\x17/Dout-degree .\\x17/forallvertices \\x17,\\nthenGhasanEulertour—intwodifferent ways. Oneproof isnoncons tructive,\\nand the other proof will help us design the algorithm for part (b).\\nFirst, we claim that if in-degree .\\x17/Dout-degree .\\x17/for all vertices \\x17, then we\\ncan pick any vertex ufor which in-degree .u/Dout-degree .u/\\x151and create', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 389}),\n",
              " Document(page_content='Solutions forChapter 22: Elementary Graph Algorithms 22-2 5\\nacycle(not necessarily simple) that contains u. Toprovethis claim,let usstart\\nby placing vertex uon the cycle, and choose any leaving edge of u, say .u; \\x17/.\\nNow we put \\x17on the cycle. Since in-degree .\\x17/Dout-degree .\\x17/\\x151, we can\\npicksomeleavingedgeof \\x17andcontinuevisitingedgesandvertices. Eachtime\\nwe pick an edge, we can remove it from further consideration. At each vertex\\nother than u, at the time we visit an entering edge, there must be an unvisi ted\\nleaving edge, since in-degree .\\x17/Dout-degree .\\x17/for all vertices \\x17. The only\\nvertex for which there might not be an unvisited leaving edge isu, since we\\nstarted the cycle by visiting one of u’s leaving edges. Since there’s always a\\nleaving edge we can visit from all vertices other than u, eventually the cycle\\nmust return to u, thus proving the claim.\\nThe nonconstructive proof proves the contrapositive—that ifGdoes not have\\nan Euler tour, then in-degree .\\x17/¤out-degree .\\x17/for some vertex \\x17—by con-\\ntradiction. Choose a graph GD.V; E/that does not have an Euler tour but\\nhas at least one edge and for which in-degree .\\x17/Dout-degree .\\x17/for all ver-\\ntices\\x17,andlet Ghavethefewestedgesofanysuchgraph. Bytheaboveclaim,\\nGcontains a cycle. Let Cbe a cycle of Gwith the greatest number of edges,\\nand let VCbe the set of vertices visited by cycle C. By our assumption, Cis\\nnot an Euler tour, and so the set of edges E0DE/NULCis nonempty. If we use\\nthe set Vof vertices and the set E0of edges, we get the graph G0D.V; E0/;\\nthis graph has in-degree .\\x17/Dout-degree .\\x17/for all vertices \\x17, since we have\\nremoved one entering edge and one leaving edge for each verte x on cycle C.\\nConsider any component G00D.V00; E00/ofG0, and observe that G00also has\\nin-degree .\\x17/Dout-degree .\\x17/for all vertices \\x17. Since E00\\x12E0¨E, it fol-\\nlows from how we chose GthatG00must have an Euler tour, say C0. Because\\ntheoriginal graph Gisconnected, theremustbesomevertex x2V00[VCand,\\nwithout loss of generality, consider xto be the ﬁrst and last vertex on both C\\nandC0. But then the cycle C00formed by ﬁrst traversing Cand then travers-\\ningC0is a cycle of Gwith more edges than C, contradicting our choice of C.\\nWeconclude that Cmust have been anEuler tour.\\nThe constructive proof uses the same ideas. Let us start at a v ertex uand, via\\nrandom traversal of edges, create acycle. Weknow that once w etake anyedge\\nentering a vertex \\x17¤u, we can ﬁnd an edge leaving \\x17that we have not yet\\ntaken. Eventually, weget back tovertex u,and ifthere arestill edges leaving u\\nthat we have not taken, we can continue the cycle. Eventually , we get back to\\nvertex uand there are no untaken edges leaving u. If we have visited every\\nedge in thegraph G, wearedone. Otherwise, since Gis connected, there must\\nbe some unvisited edge leaving a vertex, say \\x17, on the cycle. We can traverse\\na new cycle starting at \\x17, visiting only previously unvisited edges, and we can\\nsplice this cycle into the cycle we already know. That is, if t he original cycle\\nishu; : : : ; \\x17; w; : : : ; ui, and the new cycle is h\\x17; x; : : : ; \\x17i, then we can create\\nthe cyclehu; : : : ; \\x17; x; : : : ; \\x17; w; : : : ; u i. We continue this process of ﬁnding a\\nvertexwithanunvisited leavingedgeonavisitedcycle,vis itingacyclestarting\\nand ending at this vertex, and splicing in the newly visited c ycle, until wehave\\nvisited every edge.\\nb.Thealgorithm isbased onthe idea in the constructive proof a bove.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 390}),\n",
              " Document(page_content='22-26 Solutions for Chapter 22: Elementary GraphAlgorithm s\\nWe assume that Gis represented by adjacency lists, and we work with a copy\\nof the adjacency lists, so that as we visit each edge, we can re move it from\\nits adjacency list. The singly linked form of adjacency list will sufﬁce. The\\noutput of this algorithm is a doubly linked list Tof vertices which, read in list\\norder, will give an Euler tour. The algorithm constructs Tby ﬁnding cycles\\n(also represented by doubly linked lists) and splicing them intoT. By using\\ndoubly linked lists for cycles and the Euler tour, splicing a cycle into the Euler\\ntour takes constant time.\\nWe also maintain a singly linked list L, in which each list element consists of\\ntwoparts:\\n1. a vertex \\x17,and\\n2. a pointer tosome appearance of \\x17inT.\\nInitially, Lcontains one vertex, which maybeany vertex of G.\\nHereis thealgorithm:\\nEULER-TOUR.G/\\nTDempty list\\nLD.any vertex \\x172G:V;NIL/\\nwhile Lisnot empty\\nremove .\\x17;location-in-T/from L\\nCDVISIT.G; L; \\x17/\\niflocation-in-T==NIL\\nTDC\\nelsesplice CintoTjust before location-in-T\\nreturn T\\nVISIT.G; L; \\x17/\\nCDempty sequence of vertices\\nuD\\x17\\nwhileout-degree .u/ > 0\\nletwbethe ﬁrst vertex in G:AdjŒu\\x8d\\nremove wfrom G:AdjŒu\\x8d,decrementing out-degree .u/\\nadduonto the end of C\\nifout-degree .u/ > 0\\nadd.u; u’s location in C /toL\\nuDw\\nreturn C\\nThe use of NILin the initial assignment to Lensures that the ﬁrst cycle C\\nreturned by V ISITbecomes the current version of the Euler tour T. All cycles\\nreturned by V ISITthereafter are spliced into T. We assume that whenever an\\nempty cycle isreturned by V ISIT, splicing it into Tleaves Tunchanged.\\nEach time that E ULER-TOURremoves a vertex \\x17from the list L, it calls\\nVISIT.G; L; \\x17/ to ﬁnd a cycle C, possibly empty and possibly not simple, that\\nstarts and ends at \\x17; the cycle Cis represented by a list that starts with \\x17and\\nends withthelast vertexonthecycle before thecycle ends at \\x17. EULER-TOUR', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 391}),\n",
              " Document(page_content='Solutions forChapter 22: Elementary Graph Algorithms 22-2 7\\nthensplicesthiscycle CintotheEulertour Tjustbeforesomeappearance of \\x17\\ninT.\\nWhenV ISITisatavertex u,itlooksforsomevertex wsuchthattheedge .u; w/\\nhas not yet been visited. Removing wfromAdjŒu\\x8densures that we will never\\nvisit.u; w/again. V ISITadds uonto the cycle Cthat it constructs. If, after\\nremoving edge .u; w/, vertex ustill has any leaving edges, then u, along with\\nits location in C, is added to L. The cycle construction continues from w, and\\nit ceases once a vertex with no unvisited leaving edges is fou nd. Using the\\nargument from part (a), at that point, this vertex must close up a cycle. At that\\npoint, therefore, the cycle Cisreturned.\\nItispossiblethatavertex uhasunvisited leavingedgesatthetimeitisaddedto\\nlistLin VISIT, butthatbythetimethat uisremovedfrom LinEULER-TOUR,\\nall of its leaving edges have been visited. In this case, the whileloop of V ISIT\\nexecutes 0iterations, and V ISITreturns anempty cycle.\\nOnce the list Lis empty, every edge has been visited. The resulting cycle Tis\\nthen anEuler tour.\\nTo see that E ULER-TOURtakes O.E/time, observe that because we remove\\neach edge from its adjacency list as it is visited, no edge is v isited more than\\nonce. Sinceeachedgeisvisitedatsometime,thenumber ofti mesthat avertex\\nisaddedto L,andthusremovedfrom L,isatmostjEj. Thus,the whileloopin\\nEULER-TOURexecutesatmost Eiterations. The whileloopin V ISITexecutes\\none iteration per edge in the graph, and so it executes at most Eiterations as\\nwell. Since adding vertex uto the doubly linked list Ctakes constant timeand\\nsplicing CintoTtakes constant time, the entire algorithm takes O.E/time.\\nSolutionto Problem 22-4\\nCompute GTin the usual way, so that GTisGwith its edges reversed. Then do\\na depth-ﬁrst search on GT, but in the main loop of DFS, consider the vertices in\\norder of increasing values of L.\\x17/. If vertex uisinthedepth-ﬁrst tree withroot \\x17,\\nthen min .u/D\\x17. Clearly, this algorithm takes O.VCE/time.\\nTo show correctness, ﬁrst note that if uis in the depth-ﬁrst tree rooted at \\x17inGT,\\nthen there is a path \\x17;uinGT, and so there is a path u;\\x17inG. Thus, the\\nminimum vertex label of all vertices reachable from uis at most L.\\x17/, or in other\\nwords, L.\\x17/\\x15minfL.w/Ww2R.u/g.\\nNow suppose that L.\\x17/ > minfL.w/Ww2R.u/g, so that there is a vertex\\nw2R.u/such that L.w/ < L.\\x17/ . At the time \\x17:dthat we started the depth-\\nﬁrst search from \\x17, we would have already discovered w, so that w:d< \\x17:d.\\nBy the parenthesis theorem, either the intervals Œ\\x17:d; \\x17:f\\x8d, and Œw:d; w:f\\x8dare dis-\\njoint and neither \\x17norwis a descendant of the other, or we have the ordering\\nw:d< \\x17:d< \\x17:f< w:fand\\x17is a descendant of w. The latter case cannot\\noccur, since \\x17isaroot inthedepth-ﬁrst forest (whichmeansthat \\x17cannot beade-\\nscendant of any other vertex). In the former case, since w:d< \\x17:d, we must have\\nw:d< w:f< \\x17:d< \\x17:f. Inthiscase,since uisreachablefrom winGT,wewould', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 392}),\n",
              " Document(page_content='22-28 Solutions for Chapter 22: Elementary GraphAlgorithm s\\nhave discovered ubythetime w:f,sothat u:d< w:f. Sincewediscovered udur-\\ning a search that started at \\x17, we have \\x17:d\\x14u:d. Thus, \\x17:d\\x14u:d< w:f< \\x17:d,\\nwhich is acontradiction. Weconclude that no such vertex wcan exist.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 393}),\n",
              " Document(page_content='Lecture Notes forChapter 23:\\nMinimumSpanningTrees\\nChapter 23overview\\nProblem\\n\\x0fA townhas aset of houses and aset of roads.\\n\\x0fA road connects 2and only 2houses.\\n\\x0fA road connecting houses uand\\x17has arepair cost w.u; \\x17/.\\n\\x0fGoal:Repair enough (and no more) roads such that\\n1. everyone stays connected: can reach every house from all o ther houses, and\\n2. total repair cost isminimum.\\nModel asagraph:\\n\\x0fUndirected graph GD.V; E/.\\n\\x0fWeight w.u; \\x17/on each edge .u; \\x17/2E.\\n\\x0fFindT\\x12Esuch that\\n1.Tconnects all vertices ( Tis aspanningtree ), and\\n2.w.T /DX\\n.u;\\x17/ 2Tw.u; \\x17/isminimized.\\nAspanning tree whose weight isminimum over all spanning tre es is called a min-\\nimumspanningtree , orMST.\\nExample of such a graph [edges in MSTare shaded] :\\n10\\n1298 8\\n2\\n1195\\n6 17\\n3 3b\\na\\ncd\\nfeg\\nhi\\nIn this example, there is more than one MST. Replace edge .e; f /in the MST\\nby.c; e/. Get adifferent spanning tree withthe sameweight.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 394}),\n",
              " Document(page_content='23-2 Lecture Notes for Chapter 23: Minimum Spanning Trees\\nGrowing a minimum spanning tree\\nSomeproperties of anMST:\\n\\x0fIt hasjVj/NUL1edges.\\n\\x0fIt has no cycles.\\n\\x0fIt might not beunique.\\nBuildingupthesolution\\n\\x0fWewill build a set Aof edges.\\n\\x0fInitially, Ahas noedges.\\n\\x0fAsweadd edges to A, maintain aloop invariant:\\nLoop invariant: Ais asubset of some MST.\\n\\x0fAdd only edges that maintain the invariant. If Ais a subset of some MST, an\\nedge .u; \\x17/issafeforAif and only if A[f.u; \\x17/gis also a subset of some\\nMST. Sowewill add only safe edges.\\nGeneric MSTalgorithm\\nGENERIC-MST .G; w/\\nAD;\\nwhile Aisnot aspanning tree\\nﬁnd anedge .u; \\x17/that issafe for A\\nADA[f.u; \\x17/g\\nreturn A\\nUsethe loop invariant toshow that this generic algorithm wo rks.\\nInitialization: Theempty set trivially satisﬁes the loop invariant.\\nMaintenance: Since weadd only safe edges, Aremains asubset of some MST.\\nTermination: Alledges added to Aarein anMST,sowhen westop, Aisaspan-\\nning tree that is also anMST.\\nFindingasafe edge\\nHowdo weﬁndsafe edges?\\nLet’s look at the example. Edge .c; f /has the lowest weight of any edge in the\\ngraph. Is it safe for AD;?\\nIntuitively: Let S\\x1aVbe any set of vertices that includes cbut not f(so that\\nfis inV/NULS). In any MST, there has to be one edge (at least) that connects S\\nwithV/NULS. Why not choose the edge with minimum weight? (Which would be\\n.c; f /in this case.)\\nSomedeﬁnitions: Let S\\x1aVandA\\x12E.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 395}),\n",
              " Document(page_content='Lecture Notes for Chapter 23: Minimum Spanning Trees 23-3\\n\\x0fAcut.S; V/NULS/is apartition of vertices into disjoint sets VandS/NULV.\\n\\x0fEdge .u; \\x17/2Ecrossescut.S; V/NULS/ifone endpoint isin Sand theother is\\ninV/NULS.\\n\\x0fA cutrespects Aif and only if no edge in Acrosses thecut.\\n\\x0fAnedge isa lightedge crossing acut ifand onlyifitsweight isminimum over\\nall edges crossing the cut. For agiven cut, there can be > 1light edge crossing\\nit.\\nTheorem\\nLetAbeasubset of someMST, .S; V/NULS/be acut that respects A,and .u; \\x17/be\\nalight edge crossing .S; V/NULS/. Then .u; \\x17/issafe for A.\\nProofLetTbe anMSTthat includes A.\\nIfTcontains .u; \\x17/,done.\\nSonow assume that Tdoes not contain .u; \\x17/. We’ll construct adifferent MST T0\\nthat includes A[f.u; \\x17/g.\\nRecall: atreehas unique pathbetween eachpair of vertices. Since TisanMST,it\\ncontains a unique path pbetween uand\\x17. Path pmust cross the cut .S; V/NULS/\\nat least once. Let .x; y/be an edge of pthat crosses the cut. From how we\\nchose .u; \\x17/, must have w.u; \\x17/\\x14w.x; y/.\\nu\\nvyxS\\nV–S\\n[Except for the dashed edge .u; \\x17/, all edges shown are in T.Ais some subset of\\ntheedgesof T,butAcannot contain anyedgesthat crossthecut .S; V/NULS/,since\\nthis cut respects A. Shaded edges are thepath p.]\\nSincethe cut respects A,edge .x; y/isnot in A.\\nToform T0from T:\\n\\x0fRemove .x; y/. Breaks Tinto twocomponents.\\n\\x0fAdd.u; \\x17/. Reconnects.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 396}),\n",
              " Document(page_content='23-4 Lecture Notes for Chapter 23: Minimum Spanning Trees\\nSoT0DT/NULf.x; y/g[f.u; \\x17/g.\\nT0isaspanning tree.\\nw.T0/Dw.T //NULw.x; y/Cw.u; \\x17/\\n\\x14w.T / ;\\nsince w.u; \\x17/\\x14w.x; y/. Since T0is aspanning tree, w.T0/\\x14w.T /,and Tisan\\nMST,then T0must be anMST.\\nNeed toshow that .u; \\x17/issafe for A:\\n\\x0fA\\x12Tand.x; y/62A)A\\x12T0.\\n\\x0fA[f.u; \\x17/g\\x12T0.\\n\\x0fSince T0isan MST, .u; \\x17/is safe for A. (theorem)\\nSo, in G ENERIC-MST:\\n\\x0fAis a forest containing connected components. Initially, ea ch component is a\\nsingle vertex.\\n\\x0fAnysafe edge merges two of these components into one. Each co mponent is a\\ntree.\\n\\x0fSince an MST has exactly jVj/NUL1edges, the forloop iteratesjVj/NUL1times.\\nEquivalently,afteradding jVj/NUL1safeedges,we’redowntojustonecomponent.\\nCorollary\\nIfCD.VC; EC/is a connected component in the forest GAD.V; A/and.u; \\x17/\\nis a light edge connecting Cto some other component in GA(i.e.,.u; \\x17/is a light\\nedge crossing the cut .VC; V/NULVC/),then .u; \\x17/issafe for A.\\nProofSetSDVCin the theorem. (corollary)\\nThis idea naturally leads to the algorithm known as Kruskal’ s algorithm to solve\\nthe minimum-spanning-tree problem.\\nKruskal’s algorithm\\nGD.V; E/isa connected, undirected, weighted graph. wWE!R.\\n\\x0fStarts witheach vertex being its owncomponent.\\n\\x0fRepeatedly merges two components into one by choosing the li ght edge that\\nconnects them (i.e., the light edge crossing the cut between them).\\n\\x0fScans theset of edges in monotonically increasing order byw eight.\\n\\x0fUses a disjoint-set data structure to determine whether an e dge connects ver-\\ntices in different components.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 397}),\n",
              " Document(page_content='Lecture Notes for Chapter 23: Minimum Spanning Trees 23-5\\nKRUSKAL .G; w/\\nAD;\\nforeach vertex \\x172G:V\\nMAKE-SET.\\x17/\\nsort the edges of G:Einto nondecreasing order by weight w\\nforeach.u; \\x17/taken from the sorted list\\nifFIND-SET.u/¤FIND-SET.\\x17/\\nADA[f.u; \\x17/g\\nUNION .u; \\x17/\\nreturn A\\nRunthrough theabove example tosee how Kruskal’s algorithm works onit:\\n.c; f /Wsafe\\n.g; i/Wsafe\\n.e; f /Wsafe\\n.c; e/Wreject\\n.d; h/Wsafe\\n.f; h/Wsafe\\n.e; d/Wreject\\n.b; d/Wsafe\\n.d; g/Wsafe\\n.b; c/Wreject\\n.g; h/Wreject\\n.a; b/Wsafe\\nAtthispoint, wehaveonlyonecomponent, soallotheredgesw illberejected. [We\\ncould add a test to the main loop of KRUSKAL to stop oncejVj/NUL1edges have\\nbeen added to A.]\\nGet the shaded edges shown in the ﬁgure.\\nSuppose wehad examined .c; e/before .e; f /. Thenwould have found .c; e/safe\\nand would have rejected .e; f /.\\nAnalysis\\nInitialize A: O.1/\\nFirstforloop:jVjMAKE-SETs\\nSortE: O.ElgE/\\nSecondforloop: O.E/FIND-SETs and U NIONs\\n\\x0fAssuming the implementation of disjoint-set data structur e, already seen in\\nChapter 21, that uses union by rank and path compression:\\nO..VCE/ ˛.V //CO.ElgE/ :\\n\\x0fSince Gis connected,jEj\\x15jVj/NUL1)O.E ˛.V //CO.ElgE/.\\n\\x0f˛.jVj/DO.lgV /DO.lgE/.\\n\\x0fTherefore, total time is O.ElgE/.\\n\\x0fjEj\\x14jVj2)lgjEjDO.2lgV /DO.lgV /.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 398}),\n",
              " Document(page_content='23-6 Lecture Notes for Chapter 23: Minimum Spanning Trees\\n\\x0fTherefore, O.ElgV /time. (If edges are already sorted, O.E ˛.V // , which is\\nalmost linear.)\\nPrim’s algorithm\\n\\x0fBuilds one tree, so Aisalways a tree.\\n\\x0fStarts from an arbitrary “root” r.\\n\\x0fAt each step, ﬁnd a light edge crossing cut .VA; V/NULVA/, where VADvertices\\nthatAis incident on. Addthis edge to A.\\nlight edgeVA\\n[Edges of Aare shaded.]\\nHowto ﬁndthe light edge quickly?\\nUseapriority queue Q:\\n\\x0fEachobject is avertex in V/NULVA.\\n\\x0fKeyof \\x17is minimum weight of anyedge .u; \\x17/,where u2VA.\\n\\x0fThen the vertex returned by E XTRACT-MINis\\x17such that there exists u2VA\\nand.u; \\x17/islight edge crossing .VA; V/NULVA/.\\n\\x0fKeyof \\x17is1if\\x17isnot adjacent toany vertices in VA.\\nTheedges of Awill form arooted tree withroot r:\\n\\x0frisgiven asan input tothe algorithm, but it can be anyvertex.\\n\\x0fEach vertex knows its parent in the tree by the attribute \\x17:\\x19Dparent of \\x17.\\n\\x17:\\x19DNILif\\x17Dror\\x17has no parent.\\n\\x0fAsalgorithm progresses, ADf.\\x17; \\x17:\\x19/W\\x172V/NULfrg/NULQg.\\n\\x0fAttermination, VADV)QD;, soMSTis ADf.\\x17; \\x17:\\x19/W\\x172V/NULfrgg.\\n[Thepseudocodethatfollowsdiffersfromthebookinthatit explicitlycalls INSERT\\nandDECREASE -KEYto operate on Q.]', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 399}),\n",
              " Document(page_content='Lecture Notes for Chapter 23: Minimum Spanning Trees 23-7\\nPRIM.G; w; r/\\nQD;\\nforeachu2G:V\\nu:keyD1\\nu:\\x19DNIL\\nINSERT .Q; u/\\nDECREASE -KEY.Q; r; 0///r:keyD0\\nwhile Q¤;\\nuDEXTRACT-MIN.Q/\\nforeach\\x172G:AdjŒu\\x8d\\nif\\x172Qandw.u; \\x17/ < \\x17: key\\n\\x17:\\x19Du\\nDECREASE -KEY.Q; \\x17; w.u; \\x17//\\nDoexample from previous graph. [Let astudent pick the root.]\\nAnalysis\\nDepends onhow the priority queue is implemented:\\n\\x0fSuppose Qisa binary heap.\\nInitialize Qand ﬁrstforloop: O.VlgV /\\nDecrease key of r: O.lgV /\\nwhileloop: jVjEXTRACT-MINcalls)O.VlgV /\\n\\x14jEjDECREASE -KEYcalls)O.ElgV /\\nTotal: O.ElgV /\\n\\x0fSuppose wecould do D ECREASE -KEYinO.1/amortized time.\\nThen\\x14jEjDECREASE -KEYcalls take O.E/time altogether)total time\\nbecomes O.VlgVCE/.\\nIn fact, there is a way to do D ECREASE -KEYinO.1/amortized time: Fi-\\nbonacci heaps, in Chapter 19.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 400}),\n",
              " Document(page_content='Solutionsfor Chapter23:\\nMinimum SpanningTrees\\nSolution to Exercise23.1-1\\nThis solutionisalsopostedpublicly\\nTheorem 23.1 shows this.\\nLetAbethe empty set and Sbeany set containing ubut not \\x17.\\nSolution to Exercise23.1-4\\nThis solutionisalsopostedpublicly\\nAtrianglewhoseedgeweightsareallequalisagraphinwhich everyedgeisalight\\nedge crossing somecut. But thetriangle iscyclic, soitisno t aminimum spanning\\ntree.\\nSolution to Exercise23.1-6\\nThis solutionisalsopostedpublicly\\nSupposethatforeverycutof G,thereisauniquelightedgecrossingthecut. Letus\\nconsider twodistinct minimum spanning trees, TandT0,ofG. Because TandT0\\nare distinct, Tcontains some edge .u; \\x17/that is not in T0. If we remove .u; \\x17/\\nfrom T, then Tbecomes disconnected, resulting in a cut .S; V/NULS/. The edge\\n.u; \\x17/is a light edge crossing the cut .S; V/NULS/(by Exercise 23.1-3) and, by our\\nassumption, it’s the only light edge crossing this cut. Beca use.u; \\x17/is the only\\nlight edge crossing .S; V/NULS/and.u; \\x17/isnot in T0,each edge in T0that crosses\\n.S; V/NULS/must have weight strictly greater than w.u; \\x17/. As in the proof of\\nTheorem23.1,wecanidentifytheuniqueedge .x; y/inT0thatcrosses .S; V/NULS/\\nand lies on the cycle that results if we add .u; \\x17/toT0. By our assumption, we\\nknow that w.u; \\x17/ < w.x; y/ . Then, we can then remove .x; y/from T0and\\nreplace it by .u; \\x17/, giving a spanning tree with weight strictly less than w.T0/.\\nThus, T0was not a minimum spanning tree, contradicting the assumpti on that the\\ngraph had twounique minimum spanning trees.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 401}),\n",
              " Document(page_content='Solutions forChapter 23: Minimum Spanning Trees 23-9\\nHere’s acounterexample for theconverse:\\nxy\\nz1\\n1\\nHere, the graph is its own minimum spanning tree, and so the mi nimum spanning\\ntree is unique. Consider the cut .fxg;fy; ´g/. Both of the edges .x; y/and.x; ´/\\narelight edges crossing the cut, and they areboth light edge s.\\nSolutionto Exercise 23.1-10\\nLetw.T /DP\\n.x;y/ 2Tw.x; y/. We have w0.T /Dw.T //NULk. Consider any other\\nspanning tree T0, sothat w.T /\\x14w.T0/.\\nIf.x; y/62T0,then w0.T0/Dw.T0/\\x15w.T / > w0.T /.\\nIf.x; y/2T0,then w0.T0/Dw.T0//NULk\\x15w.T //NULkDw0.T /.\\nEither way, w0.T /\\x14w0.T0/, and so Tis a minimum spanning tree for weight\\nfunction w0.\\nSolutionto Exercise 23.2-4\\nWe know that Kruskal’s algorithm takes O.V /time for initialization, O.ElgE/\\ntime to sort the edges, and O.E ˛.V // time for the disjoint-set operations, for a\\ntotal running timeof O.VCElgECE ˛.V //DO.ElgE/.\\nIf we knew that all of the edge weights in the graph were intege rs in the range\\nfrom 1 tojVj, then we could sort the edges in O.VCE/time using counting\\nsort. Since the graph is connected, VDO.E/, and so the sorting time is reduced\\ntoO.E/. This would yield a total running time of O.VCECE ˛.V //D\\nO.E ˛.V // , again since VDO.E/, and since EDO.E ˛.V // . The time to\\nprocesstheedges, notthetimetosortthem,isnowthedomina nt term. Knowledge\\nabouttheweightswon’thelpspeedupanyotherpartofthealg orithm,sincenothing\\nbesides the sort uses the weight values.\\nIf the edge weights were integers in the range from 1 to Wfor some constant W,\\nthen we could again use counting sort to sort the edges more qu ickly. This time,\\nsortingwouldtake O.ECW /DO.E/time,since Wisaconstant. Asintheﬁrst\\npart, weget atotal running timeof O.E ˛.V // .', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 402}),\n",
              " Document(page_content='23-10 Solutions for Chapter 23: Minimum Spanning Trees\\nSolution to Exercise23.2-5\\nThe time taken by Prim’s algorithm is determined by the speed of the queue oper-\\nations. With the queue implemented as a Fibonacci heap, it ta kesO.ECVlgV /\\ntime.\\nSince the keys in the priority queue are edge weights, it migh t be possible to im-\\nplement thequeue evenmoreefﬁcientlywhentherearerestri ctions onthepossible\\nedge weights.\\nWecanimprove the running timeof Prim’salgorithm if Wisaconstant by imple-\\nmenting the queue as anarray QŒ0 : : WC1\\x8d(using the WC1slot for keyD1),\\nwhere each slot holds a doubly linked list of vertices with th at weight as their\\nkey. Then E XTRACT-MINtakes only O.W /DO.1/time (just scan for the ﬁrst\\nnonempty slot), and D ECREASE -KEYtakes only O.1/time (just remove the ver-\\ntex from the list it’s in and insert it at the front of the list i ndexed by the new key).\\nThisgivesatotalrunningtimeof O.E/,whichisthebestpossibleasymptotictime\\n(since \\x7f.E/edges must be processed).\\nHowever, if the range of edge weights is 1 to jVj, then E XTRACT-MINtakes\\n‚.V /time with this data structure. So the total time spent doing E XTRACT-MIN\\nis‚.V2/, slowing the algorithm to ‚.ECV2/D‚.V2/. In this case, it is better\\nto keep the Fibonacci-heap priority queue, which gave the ‚.ECVlgV /time.\\nOther data structures yield better running times:\\n\\x0fvanEmdeBoastrees(seeChapter20)giveanupperboundof O.ECVlglgV /\\ntimefor Prim’salgorithm.\\n\\x0fA redistributive heap (used in the single-source shortest- paths algorithm of\\nAhuja, Mehlhorn, Orlin, and Tarjan, and mentioned in the cha pter notes for\\nChapter 24) gives an upper bound of O/NUL\\nECVp\\nlgV\\x01\\nfor Prim’salgorithm.\\nSolution to Exercise23.2-7\\nWestart with thefollowing lemma.\\nLemma\\nLetTbe a minimum spanning tree of GD.V; E/, and consider a graph G0D\\n.V0; E0/for which Gis asubgraph, i.e., V\\x12V0andE\\x12E0. Let TDE/NULTbe\\ntheedges of Gthat arenot in T. Thenthereisaminimumspanning treeof G0that\\nincludes no edges in T.\\nProofByExercise23.2-1, thereisawaytoorder theedges of Esothat Kruskal’s\\nalgorithm, when run on G, produces the minimum spanning tree T. Wewill show\\nthat Kruskal’s algorithm, run on G0, produces a minimum spanning tree T0that\\nincludes noedges in T. Weassume that theedges in Eare considered in thesame\\nrelative order when Kruskal’s algorithm is run on Gand on G0. We ﬁrst state and\\nprove the following claim.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 403}),\n",
              " Document(page_content='Solutions forChapter 23: Minimum Spanning Trees 23-11\\nClaim\\nForanypairofvertices u; \\x172V,iftheseverticesareinthesamesetafterKruskal’s\\nalgorithm run on Gconsiders any edge .x; y/2E, then they are in the same set\\nafter Kruskal’s algorithm run on G0considers .x; y/.\\nProof of claim Let us order the edges of Eby nondecreasing weight as h.x1; y1/;\\n.x2; y2/; : : : ; .x k; yk/i,where kDjEj. Thissequence givestheorder inwhichthe\\nedgesof Eareconsidered byKruskal’salgorithm, whether itisrunon Goron G0.\\nWewilluseinduction, withtheinductive hypothesis that if uand\\x17areinthesame\\nset after Kruskal’s algorithm run on Gconsiders an edge .xi; yi/, then they are in\\nthe same set after Kruskal’s algorithm run on G0considers the same edge. Weuse\\ninduction on i.\\nBasis:For the basis, iD0. Kruskal’s algorithm run on Ghas not considered\\nany edges, and so all vertices are in different sets. The indu ctive hypothesis holds\\ntrivially.\\nInductivestep: WeassumethatanyverticesthatareinthesamesetafterKrus kal’s\\nalgorithm run on Ghas considered edges h.x1; y1/; .x 2; y2/; : : : ; .x i/NUL1; yi/NUL1/i\\nare in the same set after Kruskal’s algorithm run on G0has considered the same\\nedges. WhenKruskal’salgorithm runson G0,afteritconsiders .xi/NUL1; yi/NUL1/,itmay\\nconsidersomeedgesin E0/NULEbeforeconsidering .xi; yi/. Theedgesin E0/NULEmay\\ncause U NIONoperations to occur, but sets are never divided. Hence, any v ertices\\nthat are in the same set after Kruskal’s algorithm run on G0considers .xi/NUL1; yi/NUL1/\\narestill inthe sameset when .xi; yi/is considered.\\nWhen Kruskal’s algorithm run on Gconsiders .xi; yi/, either xiandyiare found\\ntobe inthe same set or they arenot.\\n\\x0fIf Kruskal’s algorithm run on Gﬁnds xiandyito be in the same set, then\\nno UNIONoperation occurs. The sets of vertices remain the same, and s o the\\ninductive hypothesis continues to hold after considering .xi; yi/.\\n\\x0fIf Kruskal’s algorithm run on Gﬁnds xiandyito be in different sets, then\\nthe operation U NION .xi; yi/will occur. Kruskal’s algorithm run on G0will\\nﬁnd that either xiandyiare in the same set or they are not. By the induc-\\ntive hypothesis, when edge .xi; yi/is considered, all vertices in xi’s set when\\nKruskal’s algorithm runs on Gare in xi’s set when Kruskal’s algorithm runs\\nonG0, and the same holds for yi. Regardless of whether Kruskal’s algorithm\\nrun on G0ﬁnds xiandyito already be in the same set, their sets are united af-\\nter considering .xi; yi/, and so the inductive hypothesis continues to hold after\\nconsidering .xi; yi/. (claim)\\nWith the claim in hand, we suppose that some edge .u; \\x17/2Tis placed into T0.\\nThat means that Kruskal’s algorithm run on Gfound uand\\x17to be in the same\\nset (since .u; \\x17/2T) but Kruskal’s algorithm run on G0found uand\\x17to be in\\ndifferentsets(since .u; \\x17/isplacedinto T0). Thisfactcontradictstheclaim,andwe\\nconcludethatnoedgein Tisplacedinto T0. Thus,byrunningKruskal’salgorithm\\nonGandG0,wedemonstrate that there exists aminimum spanning treeof G0that\\nincludes no edges in T. (lemma)\\nWe use this lemma as follows. Let G0D.V0; E0/be the graph GD.V; E/with\\ntheonenewvertexanditsincident edgesadded. Supposethat wehaveaminimum', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 404}),\n",
              " Document(page_content='23-12 Solutions for Chapter 23: Minimum Spanning Trees\\nspanning tree TforG. We compute a minimum spanning tree for G0by creating\\nthe graph G00D.V0; E00/, where E00consists of the edges of Tand the edges\\ninE0/NULE(i.e., the edges added to Gthat made G0), and then ﬁnding a minimum\\nspanning tree T0forG00. By the lemma, there is a minimum spanning tree for G0\\nthat includes noedges of E/NULT. Inother words, G0has aminimum spanning tree\\nthatincludes onlyedgesin TandE0/NULE;theseedgescompriseexactlytheset E00.\\nThus, the the minimum spanning tree T0ofG00is also a minimum spanning tree\\nofG0.\\nEventhough the proof of thelemmauses Kruskal’s algorithm, weare not required\\nto use this algorithm to ﬁnd T0. We can ﬁnd a minimum spanning tree by any\\nmeans we choose. Let us use Prim’s algorithm with a Fibonacci -heap priority\\nqueue. SincejV0jDjVjC1andjE00j\\x142jVj/NUL1(E00contains thejVj/NUL1\\nedges of Tand at mostjVjedges in E0/NULE), it takes O.V /time to construct G00,\\nand the run of Prim’s algorithm with a Fibonacci-heap priori ty queue takes time\\nO.E00CV0lgV0/DO.VlgV /. Thus, if we are given a minimum spanning tree\\nofG, wecancompute aminimum spanning tree of G0inO.VlgV /time.\\nSolution to Problem 23-1\\na.To see that the minimum spanning tree is unique, observe that since the graph\\nis connected and all edge weights are distinct, then there is a unique light edge\\ncrossing every cut. ByExercise 23.1-6, the minimum spannin g tree is unique.\\nTosee that the second-best minimum spanning tree need not be unique, here is\\naweighted, undirected graphwithauniqueminimumspanning treeofweight 7\\nand twosecond-best minimum spanning trees of weight 8:\\n1\\n2 43 5\\nminimum\\nspanning tree1\\n2 43 5\\nsecond-best\\nminimum\\nspanning tree1\\n2 43 5\\nsecond-best\\nminimum\\nspanning tree\\nb.Since any spanning tree has exactly jVj/NUL1edges, any second-best minimum\\nspanning tree must have at least one edge that is not in the (be st) minimum\\nspanning tree. If a second-best minimum spanning tree has ex actly one edge,\\nsay.x; y/,that isnot inthe minimum spanning tree, then it hasthe same set of\\nedgesastheminimumspanningtree,exceptthat .x; y/replacessomeedge,say\\n.u; \\x17/,oftheminimumspanningtree. Inthiscase, T0DT/NULf.u; \\x17/g[f.x; y/g,\\naswewished to show.\\nThus, all we need to show is that by replacing two or more edges of the min-\\nimum spanning tree, we cannot obtain a second-best minimum s panning tree.\\nLetTbe the minimum spanning tree of G, and suppose that there exists a\\nsecond-best minimum spanning tree T0that differs from Tby two or more', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 405}),\n",
              " Document(page_content='Solutions forChapter 23: Minimum Spanning Trees 23-13\\nedges. There are at least two edges in T/NULT0, and let .u; \\x17/be the edge in\\nT/NULT0with minimum weight. If we were to add .u; \\x17/toT0, we would get a\\ncycle c. This cycle contains some edge .x; y/inT0/NULT(since otherwise, T\\nwould contain acycle).\\nWe claim that w.x; y/ > w.u; \\x17/ . We prove this claim by contradiction,\\nso let us assume that w.x; y/ < w.u; \\x17/ . (Recall the assumption that\\nedge weights are distinct, so that we do not have to concern ou rselves with\\nw.x; y/Dw.u; \\x17/.) If we add .x; y/toT, we get a cycle c0, which contains\\nsomeedge .u0; \\x170/inT/NULT0(sinceotherwise, T0wouldcontainacycle). There-\\nfore, theset ofedges T00DT/NULf.u0; \\x170/g[f.x; y/gformsaspanning tree,and\\nwe must also have w.u0; \\x170/ < w.x; y/ , since otherwise T00would be a span-\\nning tree with weight less than w.T /. Thus, w.u0; \\x170/ < w.x; y/ < w.u; \\x17/ ,\\nwhichcontradictsourchoiceof .u; \\x17/astheedgein T/NULT0ofminimumweight.\\nSince the edges .u; \\x17/and.x; y/would be on a common cycle cif we were\\nto add .u; \\x17/toT0, the set of edges T0/NULf.x; y/g[f.u; \\x17/gis a spanning\\ntree, and its weight is less than w.T0/. Moreover, it differs from T(because\\nit differs from T0by only one edge). Thus, we have formed a spanning tree\\nwhose weight isless than w.T0/but is not T. Hence, T0wasnot asecond-best\\nminimum spanning tree.\\nc.Wecanﬁllin maxŒu; \\x17\\x8dforall u; \\x172VinO.V2/timebysimplydoingasearch\\nfrom each vertex u, having restricted the edges visited to those of the spannin g\\ntreeT. It doesn’t matter what kind of search we do: breadth-ﬁrst, d epth-ﬁrst,\\nor anyother kind.\\nWe’ll give pseudocode for both breadth-ﬁrst and depth-ﬁrst approaches. Each\\napproachdiffersfromthepseudocode giveninChapter22int hatwedon’tneed\\ntocompute dorfvalues,andwe’llusethe maxtableitselftorecordwhethera\\nvertex has been visited in a given search. In particular, maxŒu; \\x17\\x8dDNILif and\\nonlyif uD\\x17orwehavenotyetvisitedvertex \\x17inasearchfromvertex u. Note\\nalsothatsincewe’revisitingviaedgesinaspanningtreeof anundirectedgraph,\\nweare guaranteed that the search from each vertex u—whether breadth-ﬁrst or\\ndepth-ﬁrst—will visit all vertices. There will be no need to “restart” the search\\nasisdoneinthe DFS procedure ofSection 22.3. Ourpseudocod e assumesthat\\nthe adjacency list of each vertex consists only of edges inth e spanning tree T.\\nHere’s the breadth-ﬁrst search approach:', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 406}),\n",
              " Document(page_content='23-14 Solutions for Chapter 23: Minimum Spanning Trees\\nBFS-FILL-MAX.G; T; w/\\nletmaxbe anew table withan entry maxŒu; \\x17\\x8dfor each u; \\x172G:V\\nforeach vertex u2G:V\\nforeach vertex \\x172G:V\\nmaxŒu; \\x17\\x8dDNIL\\nQD;\\nENQUEUE .Q; u/\\nwhile Q¤;\\nxDDEQUEUE .Q/\\nforeach\\x172G:AdjŒx\\x8d\\nifmaxŒu; \\x17\\x8d==NILand\\x17¤u\\nifx==uorw.x; \\x17/ > maxŒu; x\\x8d\\nmaxŒu; \\x17\\x8dD.x; \\x17/\\nelsemaxŒu; \\x17\\x8dDmaxŒu; x\\x8d\\nENQUEUE .Q; \\x17/\\nreturnmax\\nHere’s the depth-ﬁrst search approach:\\nDFS-FILL-MAX.G; T; w/\\nletmaxbe anew table withan entry maxŒu; \\x17\\x8dfor each u; \\x172G:V\\nforeach vertex u2G:V\\nforeach vertex \\x172G:V\\nmaxŒu; \\x17\\x8dDNIL\\nDFS-FILL-MAX-VISIT.G; u; u;max/\\nreturnmax\\nDFS-FILL-MAX-VISIT.G; u; x;max/\\nforeach vertex \\x172G:AdjŒx\\x8d\\nifmaxŒu; \\x17\\x8d==NILand\\x17¤u\\nifx==uorw.x; \\x17/ > maxŒu; x\\x8d\\nmaxŒu; \\x17\\x8dD.x; \\x17/\\nelsemaxŒu; \\x17\\x8dDmaxŒu; x\\x8d\\nDFS-FILL-MAX-VISIT.G; u; \\x17;max/\\nFor either approach, we are ﬁlling in jVjrows of the maxtable. Since the\\nnumber of edges in the spanning tree is jVj/NUL1, each row takes O.V /time to\\nﬁll in. Thus, thetotal timeto ﬁllin the maxtable is O.V2/.\\nd.In part (b), we established that we can ﬁnd a second-best mini mum spanning\\ntree by replacing just one edge of the minimum spanning tree Tby some\\nedge .u; \\x17/not in T. As we know, if we create spanning tree T0by replacing\\nedge .x; y/2Tbyedge .u; \\x17/62T,then w.T0/Dw.T //NULw.x; y/Cw.u; \\x17/.\\nFor a given edge .u; \\x17/, the edge .x; y/2Tthat minimizes w.T0/is the edge\\nof maximum weight on the unique path between uand\\x17inT. If we have al-\\nreadycomputedthe maxtablefrompart(c)basedon T,thentheidentityofthis\\nedge isprecisely what isstored in maxŒu; \\x17\\x8d. Allwehave todoisdetermine an\\nedge .u; \\x17/62Tfor which w.maxŒu; \\x17\\x8d//NULw.u; \\x17/isminimum.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 407}),\n",
              " Document(page_content='Solutions forChapter 23: Minimum Spanning Trees 23-15\\nThus, our algorithm to ﬁnd a second-best minimum spanning tr ee goes as fol-\\nlows:\\n1. Computetheminimumspanningtree T. Time: O.ECVlgV /,usingPrim’s\\nalgorithmwithaFibonacci-heapimplementationoftheprio rityqueue. Since\\njEj<jVj2, this running time is O.V2/.\\n2. Given the minimum spanning tree T, compute the maxtable, as in part (c).\\nTime: O.V2/.\\n3. Find an edge .u; \\x17/62Tthat minimizes w.maxŒu; \\x17\\x8d//NULw.u; \\x17/. Time:\\nO.E/, which is O.V2/.\\n4. Havingfoundanedge .u; \\x17/instep3,return T0DT/NULfmaxŒu; \\x17\\x8dg[f.u; \\x17/g\\nasasecond-best minimum spanning tree.\\nThetotal timeis O.V2/.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 408}),\n",
              " Document(page_content='Lecture Notes forChapter 24:\\nSingle-SourceShortest Paths\\nShortest paths\\nHowtoﬁnd theshortest route between twopoints on amap.\\nInput:\\n\\x0fDirected graph GD.V; E/\\n\\x0fWeight function wWE!R\\nWeight of path pDh\\x170; \\x171; : : : ; \\x17 ki\\nDkX\\niD1w.\\x17 i/NUL1; \\x17i/\\nDsum of edge weights on path p :\\nShortest-path weight uto\\x17:\\nı.u; \\x17/D(\\nminn\\nw.p/Wup;\\x17o\\nif there exists apath u;\\x17 ;\\n1 otherwise :\\nShortest path uto\\x17is anypath psuch that w.p/Dı.u; \\x17/.\\nExample\\nshortest paths from s\\n[ıvalues appear inside vertices. Shaded edges show shortest p aths.]\\n653\\nst x\\ny z03 9\\n5 112\\n316\\n4 2 7\\n653\\nst x\\ny z03 9\\n5 112\\n316\\n42 7\\nThisexample shows that the shortest path might not beunique .\\nIt also shows that when we look at shortest paths from one vert ex to all other\\nvertices, theshortest paths areorganized as atree.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 409}),\n",
              " Document(page_content=\"24-2 Lecture Notes for Chapter 24: Single-Source Shortest P aths\\nCanthink of weights as representing anymeasure that\\n\\x0faccumulates linearly along apath, and\\n\\x0fwewant to minimize.\\nExamples: time, cost, penalties, loss.\\nGeneralization of breadth-ﬁrst search toweighted graphs.\\nVariants\\n\\x0fSingle-source: Find shortest paths from a given sourcevertex s2Vto every\\nvertex \\x172V.\\n\\x0fSingle-destination: Find shortest paths toagiven destination vertex.\\n\\x0fSingle-pair: Find shortest path from uto\\x17. No way known that’s better in\\nworst case than solving single-source.\\n\\x0fAll-pairs: Findshortest path from uto\\x17for all u; \\x172V. We’ll seealgorithms\\nfor all-pairs in thenext chapter.\\nNegative-weight edges\\nOK,aslong asno negative-weight cycles are reachable from t hesource.\\n\\x0fIf we have a negative-weight cycle, we can just keep going aro und it, and get\\nw.s; \\x17/D/NUL1for all \\x17on thecycle.\\n\\x0fBut OKif the negative-weight cycle is not reachable from the source.\\n\\x0fSomealgorithms work only if there are no negative-weight ed ges in the graph.\\nWe’ll be clear when they’re allowed and not allowed.\\nOptimalsubstructure\\nLemma\\nAnysubpath of ashortest path isa shortest path.\\nProofCut-and-paste.\\nu x y vpux pxy pyv\\nSuppose this path pisashortest path from uto\\x17.\\nThen ı.u; \\x17/Dw.p/Dw.p ux/Cw.p xy/Cw.p y\\x17/.\\nNowsuppose there exists ashorter path xp0xy;y.\\nThen w.p0\\nxy/ < w.p xy/.\\nConstruct p0:\\nu x y vpux p'xy pyv\", metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 410}),\n",
              " Document(page_content='Lecture Notes for Chapter 24: Single-Source Shortest Paths 24-3\\nThen\\nw.p0/Dw.p ux/Cw.p0\\nxy/Cw.p y\\x17/\\n< w.p ux/Cw.p xy/Cw.p y\\x17/\\nDw.p/ :\\nContradicts theassumption that pisashortest path. (lemma)\\nCycles\\nShortest paths can’t contain cycles:\\n\\x0fAlready ruled out negative-weight cycles.\\n\\x0fPositive-weight)wecan get ashorter path byomitting the cycle.\\n\\x0fZero-weight: no reason to use them )assume that our solutions won’t use\\nthem.\\nOutputof single-source shortest-path algorithm\\nForeach vertex \\x172V:\\n\\x0f\\x17:dDı.s; \\x17/.\\n\\x0fInitially, \\x17:dD1.\\n\\x0fReduces asalgorithms progress. But always maintain \\x17:d\\x15ı.s; \\x17/.\\n\\x0fCall\\x17:dashortest-path estimate .\\n\\x0f\\x17:\\x19Dpredecessor of \\x17on ashortest path from s.\\n\\x0fIf no predecessor, \\x17:\\x19DNIL.\\n\\x0f\\x19induces atree— shortest-path tree .\\n\\x0fWewon’t prove properties of \\x19in lecture—see text.\\nInitialization\\nAll theshortest-paths algorithms start with I NIT-SINGLE-SOURCE.\\nINIT-SINGLE-SOURCE .G; s/\\nforeach\\x172G:V\\n\\x17:dD1\\n\\x17:\\x19DNIL\\ns:dD0\\nRelaxing an edge .u; \\x17/\\nCan we improve the shortest-path estimate for \\x17by going through uand taking\\n.u; \\x17/?', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 411}),\n",
              " Document(page_content='24-4 Lecture Notes for Chapter 24: Single-Source Shortest P aths\\nRELAX .u; \\x17; w/\\nif\\x17:d> u:dCw.u; \\x17/\\n\\x17:dDu:dCw.u; \\x17/\\n\\x17:\\x19Du\\n3 3\\nRELAXu v\\n4 10\\n4 7RELAX4 6\\n4 6\\nFor all thesingle-source shortest-paths algorithms we’ll look at,\\n\\x0fstart by calling I NIT-SINGLE-SOURCE,\\n\\x0fthen relax edges.\\nThealgorithms differ in the order and how many timesthey rel ax each edge.\\nShortest-paths properties\\nBased on calling I NIT-SINGLE-SOURCEonce and then calling R ELAXzero or\\nmore times.\\nTriangle inequality\\nFor all .u; \\x17/2E, wehave ı.s; \\x17/\\x14ı.s; u/Cw.u; \\x17/.\\nProofWeight of shortest path s;\\x17is\\x14weight of any path s;\\x17. Path\\ns;u!\\x17is a path s;\\x17, and if we use a shortest path s;u, its weight is\\nı.s; u/Cw.u; \\x17/.\\nUpper-boundproperty\\nAlways have \\x17:d\\x15ı.s; \\x17/for all \\x17. Once \\x17:dDı.s; \\x17/, it never changes.\\nProofInitially true.\\nSuppose there exists avertex such that \\x17:d< ı.s; \\x17/.\\nWithout loss of generality, \\x17isﬁrst vertex for which this happens.\\nLetube thevertex that causes \\x17:dto change.\\nThen \\x17:dDu:dCw.u; \\x17/.\\nSo,\\n\\x17:d< ı.s; \\x17/\\n\\x14ı.s; u/Cw.u; \\x17/ (triangle inequality)\\n\\x14u:dCw.u; \\x17/ (\\x17is ﬁrst violation)\\n)\\x17:d< u:dCw.u; \\x17/ :', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 412}),\n",
              " Document(page_content='Lecture Notes for Chapter 24: Single-Source Shortest Paths 24-5\\nContradicts \\x17:dDu:dCw.u; \\x17/.\\nOnce \\x17:dreaches ı.s; \\x17/, it never goes lower. It never goes up, since relaxations\\nonly lower shortest-path estimates.\\nNo-path property\\nIfı.s; \\x17/D1, then \\x17:dD1always.\\nProof \\x17:d\\x15ı.s; \\x17/D1) \\x17:dD1.\\nConvergence property\\nIfs;u!\\x17is ashortest path, u:dDı.s; u/, and wecall R ELAX .u; \\x17; w/, then\\n\\x17:dDı.s; \\x17/afterward.\\nProofAfter relaxation:\\n\\x17:d\\x14u:dCw.u; \\x17/ (RELAXcode)\\nDı.s; u/Cw.u; \\x17/\\nDı.s; \\x17/ (lemma—optimal substructure)\\nSince \\x17:d\\x15ı.s; \\x17/, must have \\x17:dDı.s; \\x17/.\\nPathrelaxation property\\nLetpD h\\x170; \\x171; : : : ; \\x17 kibe a shortest path from sD\\x170to\\x17k. If we relax,\\nin order,.\\x170; \\x171/; .\\x17 1; \\x172/; : : : ; .\\x17 k/NUL1; \\x17k/, even intermixed with other relaxations,\\nthen\\x17k:dDı.s; \\x17 k/.\\nProofInduction to show that \\x17i:dDı.s; \\x17 i/after.\\x17i/NUL1; \\x17i/is relaxed.\\nBasis: iD0. Initially, \\x170:dD0Dı.s; \\x17 0/Dı.s; s/.\\nInductive step: Assume \\x17i/NUL1:dDı.s; \\x17 i/NUL1/. Relax .\\x17i/NUL1; \\x17i/. By convergence\\nproperty, \\x17i:dDı.s; \\x17 i/afterward and \\x17i:dnever changes.\\nThe Bellman-Fordalgorithm\\n\\x0fAllowsnegative-weight edges.\\n\\x0fComputes \\x17:dand\\x17:\\x19for all \\x172V.\\n\\x0fReturns TRUEifnonegative-weight cyclesreachable from s,FALSEotherwise.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 413}),\n",
              " Document(page_content='24-6 Lecture Notes for Chapter 24: Single-Source Shortest P aths\\nBELLMAN-FORD.G; w; s/\\nINIT-SINGLE-SOURCE .G; s/\\nforiD1tojG:Vj/NUL1\\nforeach edge .u; \\x17/2G:E\\nRELAX .u; \\x17; w/\\nforeach edge .u; \\x17/2G:E\\nif\\x17:d> u:dCw.u; \\x17/\\nreturn FALSE\\nreturn TRUE\\nCore:Thenested forloops relax all edges jVj/NUL1times.\\nTime: ‚.VE/.\\nExample\\nsr\\nx\\ny z0–1\\n1\\n2 –2–1\\n43\\n52\\n–321\\nValues you get on each pass and how quickly it converges depen ds on order of\\nrelaxation.\\nBut guaranteed to converge after jVj/NUL1passes, assuming no negative-weight\\ncycles.\\nProofUsepath-relaxation property.\\nLet\\x17be reachable from s, and let pDh\\x170; \\x171; : : : ; \\x17 kibe a shortest path from s\\nto\\x17, where \\x170Dsand\\x17kD\\x17. Since pis acyclic, it has\\x14jVj/NUL1edges, so\\nk\\x14jVj/NUL1.\\nEach iteration of the forloop relaxes all edges:\\n\\x0fFirst iteration relaxes .\\x170; \\x171/.\\n\\x0fSecond iteration relaxes .\\x171; \\x172/.\\n\\x0fkthiteration relaxes .\\x17k/NUL1; \\x17k/.\\nBythe path-relaxation property, \\x17:dD\\x17k:dDı.s; \\x17 k/Dı.s; \\x17/.\\nHowabout the TRUE/FALSEreturn value?\\n\\x0fSuppose there isno negative-weight cycle reachable from s.\\nAttermination, for all .u; \\x17/2E,\\n\\x17:dDı.s; \\x17/\\n\\x14ı.s; u/Cw.u; \\x17/ (triangle inequality)\\nDu:dCw.u; \\x17/ :\\nSo BELLMAN-FORDreturns TRUE.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 414}),\n",
              " Document(page_content='Lecture Notes for Chapter 24: Single-Source Shortest Paths 24-7\\n\\x0fNow suppose there exists negative-weight cycle cDh\\x170; \\x171; : : : ; \\x17 ki, where\\n\\x170D\\x17k,reachable from s.\\nThenkX\\niD1.\\x17i/NUL1; \\x17i/ < 0 :\\nSuppose (for contradiction) that B ELLMAN-FORDreturns TRUE.\\nThen \\x17i:d\\x14\\x17i/NUL1:dCw.\\x17 i/NUL1; \\x17i/foriD1; 2; : : : ; k .\\nSum around c:\\nkX\\niD1\\x17i:d\\x14kX\\niD1.\\x17i/NUL1:dCw.\\x17 i/NUL1; \\x17i//\\nDkX\\niD1\\x17i/NUL1:dCkX\\niD1w.\\x17 i/NUL1; \\x17i/\\nEach vertex appears once in each summationPk\\niD1\\x17i:dandPk\\niD1\\x17i/NUL1:d)\\n0\\x14kX\\niD1w.\\x17 i/NUL1; \\x17i/ :\\nContradicts cbeing anegative-weight cycle.\\nSingle-source shortestpaths ina directed acyclicgraph\\nSincea dag, we’re guaranteed no negative-weight cycles.\\nDAG-S HORTEST -PATHS .G; w; s/\\ntopologically sort the vertices\\nINIT-SINGLE-SOURCE .G; s/\\nforeach vertex u,taken in topologically sorted order\\nforeach vertex \\x172G:AdjŒu\\x8d\\nRELAX .u; \\x17; w/\\nExample\\ns txy z\\n26\\n2–2 –1\\n42 71\\n0 6 5 3\\nTime\\n‚.VCE/.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 415}),\n",
              " Document(page_content='24-8 Lecture Notes for Chapter 24: Single-Source Shortest P aths\\nCorrectness\\nBecause we process vertices in topologically sorted order, edges ofanypath must\\nbe relaxed inorder of appearance inthe path.\\n)Edges onany shortest path are relaxed inorder.\\n)Bypath-relaxation property, correct.\\nDijkstra’s algorithm\\nNonegative-weight edges.\\nEssentially aweighted version of breadth-ﬁrst search.\\n\\x0fInstead of aFIFOqueue, uses apriority queue.\\n\\x0fKeysareshortest-path weights ( \\x17:d).\\nHave twosets of vertices:\\n\\x0fSDvertices whose ﬁnal shortest-path weights are determined,\\n\\x0fQDpriority queueDV/NULS.\\nDIJKSTRA .G; w; s/\\nINIT-SINGLE-SOURCE .G; s/\\nSD;\\nQDG:V//i.e., insert all vertices into Q\\nwhile Q¤;\\nuDEXTRACT-MIN.Q/\\nSDS[fug\\nforeach vertex \\x172G:AdjŒu\\x8d\\nRELAX .u; \\x17; w/\\n\\x0fLooks a lot like Prim’s algorithm, but computing \\x17:d, and using shortest-path\\nweights as keys.\\n\\x0fDijkstra’salgorithmcanbeviewedasgreedy,sinceitalway schoosesthe“light-\\nest” (“closest”?) vertex in V/NULSto add to S.\\nExample\\nsx\\nyz2\\n3 410\\n108\\n56\\n5\\nOrder of adding to S:s; y; ´; x.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 416}),\n",
              " Document(page_content='Lecture Notes for Chapter 24: Single-Source Shortest Paths 24-9\\nCorrectness\\nLoop invariant: At the start of each iteration of the whileloop, \\x17:dD\\nı.s; \\x17/for all \\x172S.\\nInitialization: Initially, SD;,so trivially true.\\nTermination: At end, QD;) SDV)\\x17:dDı.s; \\x17/for all \\x172V.\\nMaintenance: Need to show that u:dDı.s; u/when uis added to Sin each\\niteration.\\nSupposethereexists usuchthat u:d¤ı.s; u/. Without lossofgenerality, let u\\nbe the ﬁrst vertex for which u:d¤ı.s; u/when uisadded to S.\\nObservations:\\n\\x0fu¤s, since s:dDı.s; s/D0.\\n\\x0fTherefore, s2S, soS¤;.\\n\\x0fThere must be some path s;u, since otherwise u:dDı.s; u/D1by\\nno-path property.\\nSo, there’s apath s;u.\\nThen there’s ashortest path sp;u.\\nJust before uis added to S, path pconnects avertexin S(i.e.,s) toavertex in\\nV/NULS(i.e.,u).\\nLetybe ﬁrst vertex along pthat’s in V/NULS, and let x2Sbey’s predecessor.\\nyp1Ss\\nxup2\\nDecompose pintosp1;x!yp2;u. (Could have xDsoryDu, so that p1\\norp2may have no edges.)\\nClaim\\ny:dDı.s; y/when uis added to S.\\nProof x2Sanduis the ﬁrst vertex such that u:d¤ı.s; u/when uis added\\ntoS)x:dDı.s; x/when xisadded to S. Relaxed .x; y/at that time, so by\\nthe convergence property, y:dDı.s; y/. (claim)\\nNowcan get acontradiction to u:d¤ı.s; u/:\\nyison shortest path s;u, and all edge weights are nonnegative\\n)ı.s; y/\\x14ı.s; u/)\\ny:dDı.s; y/\\n\\x14ı.s; u/\\n\\x14u:d(upper-bound property) .', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 417}),\n",
              " Document(page_content='24-10 Lecture Notes for Chapter 24: Single-Source Shortest Paths\\nAlso, both yanduwerein Qwhenwechose u,so\\nu:d\\x14y:d)u:dDy:d:\\nTherefore, y:dDı.s; y/Dı.s; u/Du:d.\\nContradicts assumption that u:d¤ı.s; u/. Hence, Dijkstra’s algorithm is cor-\\nrect.\\nAnalysis\\nLike Prim’salgorithm, depends onimplementation of priori ty queue.\\n\\x0fIf binary heap, each operation takes O.lgV /time)O.ElgV /.\\n\\x0fIf aFibonacci heap:\\n\\x0fEach EXTRACT-MINtakes O.1/amortized time.\\n\\x0fThere are O.V /other operations, taking O.lgV /amortized time each.\\n\\x0fTherefore, time is O.VlgVCE/.\\nDifference constraints\\nGiven aset of inequalities of the form xj/NULxi\\x14bk.\\n\\x0fx’sare variables, 1\\x14i; j\\x14n,\\n\\x0fb’sare constants, 1\\x14k\\x14m.\\nWant to ﬁnd a set of values for the x’s that satisfy all minequalities, or determine\\nthat no such values exist. Call such aset of values a feasible solution .\\nExample\\nx1/NULx2\\x145\\nx1/NULx3\\x146\\nx2/NULx4\\x14 /NUL 1\\nx3/NULx4\\x14 /NUL 2\\nx4/NULx1\\x14 /NUL 3\\nSolution: xD.0;/NUL4;/NUL5;/NUL3/\\nAlso: xD.5; 1; 0; 2/D[above solution]C5\\nLemma\\nIfxisafeasible solution, then so is xCdfor any constant d.\\nProof xisafeasible solution )xj/NULxi\\x14bkfor all i; j; k\\n).xjCd//NUL.xiCd/\\x14bk. (lemma)', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 418}),\n",
              " Document(page_content='Lecture Notes for Chapter 24: Single-Source Shortest Paths 24-11\\nConstraint graph\\nGD.V; E/,weighted, directed.\\n\\x0fVDf\\x170; \\x171; \\x172; : : : ; \\x17 ng: one vertex per variable C\\x170\\n\\x0fEDf.\\x17i; \\x17j/Wxj/NULxi\\x14bkisaconstraintg[f.\\x170; \\x171/; .\\x17 0; \\x172/; : : : ; .\\x17 0; \\x17n/g\\n\\x0fw.\\x17 0; \\x17j/D0for all j\\n\\x0fw.\\x17 i; \\x17j/Dbkifxj/NULxi\\x14bk\\nv0v2\\nv300 –4\\n–3 –5–3\\n6\\n–25\\n–1v1\\nv40\\n00\\n0\\nTheorem\\nGiven a system of difference constraints, let GD.V; E/be the corresponding\\nconstraint graph.\\n1. If Ghas nonegative-weight cycles, then\\nxD.ı.\\x17 0; \\x171/; ı.\\x17 0; \\x172/; : : : ; ı.\\x17 0; \\x17n//\\nis afeasible solution.\\n2. If Ghas anegative-weight cycle, then there is nofeasible solut ion.\\nProof\\n1. Show nonegative-weight cycles )feasible solution.\\nNeed toshow that xj/NULxi\\x14bkfor all constraints. Use\\nxjDı.\\x170; \\x17j/\\nxiDı.\\x170; \\x17i/\\nbkDw.\\x17 i; \\x17j/ :\\nBythe triangle inequality,\\nı.\\x170; \\x17j/\\x14ı.\\x170; \\x17i/Cw.\\x17 i; \\x17j/\\nxj\\x14xiCbk\\nxj/NULxi\\x14bk:\\nTherefore, feasible.\\n2. Show negative-weight cycles )no feasible solution.\\nWithout loss of generality, let a negative-weight cycle be cDh\\x171; \\x172; : : : ;\\n\\x17ki, where \\x171D\\x17k. (\\x170can’t be on c, since \\x170has no entering edges.) c\\ncorresponds to the constraints\\nx2/NULx1\\x14w.\\x17 1; \\x172/ ;\\nx3/NULx2\\x14w.\\x17 2; \\x173/ ;\\n:::\\nxk/NUL1/NULxk/NUL2\\x14w.\\x17 k/NUL2; \\x17k/NUL1/ ;\\nxk/NULxk/NUL1\\x14w.\\x17 k/NUL1; \\x17k/ :', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 419}),\n",
              " Document(page_content='24-12 Lecture Notes for Chapter 24: Single-Source Shortest Paths\\nIfxisa solution satisfying these inequalities, it must satisf y their sum.\\nSoadd them up.\\nEach xiis added once and subtracted once. ( \\x171D\\x17k)x1Dxk.)\\nWeget 0\\x14w.c/.\\nButw.c/ < 0 , since cisanegative-weight cycle.\\nContradiction)no such feasible solution xexists. (theorem)\\nHow toﬁndafeasible solution\\n1. Formconstraint graph.\\n\\x0fnC1vertices.\\n\\x0fmCnedges.\\n\\x0f‚.mCn/time.\\n2. Run B ELLMAN-FORDfrom \\x170.\\n\\x0fO..nC1/.mCn//DO.n2Cnm/time.\\n3. If B ELLMAN-FORDreturns FALSE)nofeasible solution.\\nIf BELLMAN-FORDreturns TRUE)setxiDı.\\x170; \\x17i/for all i.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 420}),\n",
              " Document(page_content='SolutionsforChapter 24:\\nSingle-SourceShortest Paths\\nSolutionto Exercise 24.1-3\\nThissolutionisalsopostedpublicly\\nIf the greatest number of edges on any shortest path from the s ource is m,then the\\npath-relaxation property tells us that after miterations of B ELLMAN-FORD, every\\nvertex \\x17hasachieveditsshortest-path weightin \\x17:d. Bytheupper-bound property,\\naftermiterations, no dvalueswilleverchange. Therefore,no dvalueswillchange\\ninthe .mC1/stiteration. Because wedonot know minadvance, wecannot make\\nthe algorithm iterate exactly mtimes and then terminate. But if we just make the\\nalgorithm stop when nothing changes any more, it will stop af termC1iterations.\\nBELLMAN-FORD-(M+1).G; w; s/\\nINITIALIZE -SINGLE-SOURCE .G; s/\\nchangesDTRUE\\nwhilechanges ==TRUE\\nchangesDFALSE\\nforeach edge .u; \\x17/2G:E\\nRELAX-M.u; \\x17; w/\\nRELAX-M.u; \\x17; w/\\nif\\x17:d> u:dCw.u; \\x17/\\n\\x17:dDu:dCw.u; \\x17/\\n\\x17:\\x19Du\\nchangesDTRUE\\nThe test for a negative-weight cycle (based on there being a dvalue that would\\nchange if another relaxation step was done) has been removed above, because this\\nversion of the algorithm will never get out of the whileloop unless all dvalues\\nstop changing.\\nSolutionto Exercise 24.2-3\\nInstead of modifying the D AG-SHORTEST -PATHSprocedure, we’ll modify the\\nstructure of the graph so that we can run D AG-SHORTEST -PATHSon it. In fact,', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 421}),\n",
              " Document(page_content='24-14 Solutions for Chapter 24: Single-Source Shortest Pat hs\\nwe’ll give two ways to transform a PERT chart GD.V; E/with weights on ver-\\ntices to a PERT chart G0D.V0; E0/with weights on edges. In each way, we’ll\\nhave thatjV0j\\x142jVjandjE0j\\x14jVjCjEj. We can then run on G0the same\\nalgorithm to ﬁnd a longest path through a dag as is given in Sec tion 24.2 of the\\ntext.\\nIn the ﬁrst way, we transform each vertex \\x172Vinto twovertices \\x170and\\x1700inV0.\\nAll edges in Ethat enter \\x17will enter \\x170inE0, and all edges in Ethat leave \\x17will\\nleave \\x1700inE0. In other words, if .u; \\x17/2E, then .u00; \\x170/2E0. All such edges\\nhave weight 0. Wealso put edges .\\x170; \\x1700/intoE0for all vertices \\x172V, and these\\nedgesaregiventheweight ofthecorresponding vertex \\x17inG. Thus,jV0jD2jVj,\\njE0jDjVjCjEj,andtheedge weight ofeach path in G0equals thevertex weight\\nof the corresponding path in G.\\nInthesecondway,weleaveverticesin Valone,butweaddonenewsourcevertex s\\ntoV0, so that V0DV[fsg. All edges of Eare in E0, and E0also includes an\\nedge .s; \\x17/for every vertex \\x172Vthat hasin-degree 0inG. Thus, the only vertex\\nwith in-degree 0inG0is the new source s. The weight of edge .u; \\x17/2E0is the\\nweight of vertex \\x17inG. In other words, the weight of each entering edge in G0is\\ntheweightofthevertexitentersin G. Ineffect,wehave“pushedback”theweight\\nof eachvertex ontotheedges that enter it. Here, jV0jDjVjC1,jE0j\\x14jVjCjEj\\n(since nomorethan jVjvertices havein-degree 0inG),andagaintheedgeweight\\nof each path in G0equals thevertex weight of the corresponding path in G.\\nSolution to Exercise24.3-3\\nThis solutionisalsopostedpublicly\\nYes, the algorithm still works. Let ube the leftover vertex that does not\\nget extracted from the priority queue Q. If uis not reachable from s, then\\nu:dDı.s; u/D1. If uis reachable from s, then there is a shortest path\\npDs;x!u. When the vertex xwas extracted, x:dDı.s; x/and then the\\nedge .x; u/wasrelaxed; thus, u:dDı.s; u/.\\nSolution to Exercise24.3-4\\n1. Verify that s:dD0ands:\\x19DNIL.\\n2. Verify that \\x17:dD\\x17:\\x19:Cw.\\x17:\\x19; \\x17/ for all \\x17¤s.\\n3. Verify that \\x17:dD1if and only if \\x17:ßDNILfor all \\x17¤s.\\n4. If any of the above veriﬁcation tests fail, declare the out put to be incorrect.\\nOtherwise, run one pass of Bellman-Ford, i.e., relax each ed ge.u; \\x17/2E\\none time. If any values of \\x17:dchange, then declare the output to be incorrect;\\notherwise, declare the output tobe correct.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 422}),\n",
              " Document(page_content='Solutions forChapter 24: Single-Source Shortest Paths 24- 15\\nSolutionto Exercise 24.3-5\\nLet the graph have vertices s; x; y; ´ and edges .s; x/; .x; y/; .y; ´/; .s; y/ , and\\nlet every edge have weight 0. Dijkstra’s algorithm could relax edges in the or-\\nder.s; y/; .s; x/; .y; ´/; .x; y/ . The graph has two shortest paths from sto´:\\nhs; x; y; ´iandhs; y; ´i, both with weight 0. The edges on the shortest path\\nhs; x; y; ´iare relaxed out of order, because .x; y/is relaxed after .y; ´/.\\nSolutionto Exercise 24.3-6\\nThissolutionisalsopostedpublicly\\nTo ﬁnd the most reliable path between sandt, run Dijkstra’s algorithm with edge\\nweights w.u; \\x17/D/NULlgr.u; \\x17/toﬁndshortestpathsfrom sinO.ECVlgV /time.\\nThe most reliable path is the shortest path from stot, and that path’s reliability is\\nthe product of the reliabilities of itsedges.\\nHere’s why this method works. Because the probabilities are independent, the\\nprobability that a path will not fail is the product of the pro babilities that its edges\\nwillnotfail. Wewanttoﬁndapath sp;tsuchthatQ\\n.u;\\x17/ 2pr.u; \\x17/ismaximized.\\nThisisequivalenttomaximizinglg .Q\\n.u;\\x17/ 2pr.u; \\x17//DP\\n.u;\\x17/ 2plgr.u; \\x17/,which\\nis in turn equivalent to minimizingP\\n.u;\\x17/ 2p/NULlgr.u; \\x17/. (Note: r.u; \\x17/can be 0,\\nand lg 0is undeﬁned. So in this algorithm, deﬁne lg 0D/NUL1.) Thus if we assign\\nweights w.u; \\x17/D/NULlgr.u; \\x17/, wehave ashortest-path problem.\\nSince lg 1= 0, lg x < 0for0 < x < 1 , and we have deﬁned lg 0D/NUL1, all the\\nweights warenonnegative, andwecanuseDijkstra’salgorithm toﬁndt heshortest\\npaths from sinO.ECVlgV /time.\\nAlternative solution\\nYoucan also workwith theoriginal probabilities by running amodiﬁed version of\\nDijkstra’salgorithm thatmaximizestheproduct ofreliabi lities alongapathinstead\\nof minimizing the sum of weights along apath.\\nInDijkstra’s algorithm, use the reliabilities asedge weig hts and substitute\\n\\x0fmax(and E XTRACT-MAX) for min(and E XTRACT-MIN) in relaxation andthe\\nqueue,\\n\\x0f\\x01forCinrelaxation,\\n\\x0f1(identityfor\\x01)for0(identityforC)and/NUL1(identityformin)for 1(identity\\nfor max).\\nForexample, wewould use the following instead of the usual R ELAXprocedure:\\nRELAX-RELIABILITY .u; \\x17; r/\\nif\\x17:d< u:d\\x01r.u; \\x17/\\n\\x17:dDu:d\\x01r.u; \\x17/\\n\\x17:\\x19Du', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 423}),\n",
              " Document(page_content='24-16 Solutions for Chapter 24: Single-Source Shortest Pat hs\\nThis algorithm is isomorphic to the one above: it performs th e same operations\\nexcept that it is working with the original probabilities in stead of the transformed\\nones.\\nSolution to Exercise24.3-8\\nObserve that if a shortest-path estimate is not 1, then it’s at most .jVj/NUL1/W.\\nWhy? In order to have \\x17:d<1, we must have relaxed an edge .u; \\x17/with\\nu:d<1. By induction, we can show that if we relax .u; \\x17/, then \\x17:dis at most\\nthe number of edges on apath from sto\\x17timesthe maximum edge weight. Since\\nany acyclic path has at most jVj/NUL1edges and the maximum edge weight is W,\\nwe see that \\x17:d\\x14.jVj/NUL1/W. Note also that \\x17:dmust also be an integer, unless\\nit is1.\\nWealsoobserve that inDijkstra’s algorithm, thevalues ret urned bythe E XTRACT-\\nMINcalls are monotonically increasing over time. Why? After we do our initial\\njVjINSERToperations, we never do another. The only other way that a key value\\ncan change is by a D ECREASE -KEYoperation. Since edge weights are nonneg-\\native, when we relax an edge .u; \\x17/, we have that u:d\\x14\\x17:d. Since uis the\\nminimum vertex that we just extracted, we know that any other vertex we extract\\nlater hasa keyvalue that isat least u:d.\\nWhenkeysareknowntobeintegersintherange0to kandthekeyvaluesextracted\\naremonotonically increasing overtime,wecanimplement am in-priority queueso\\nthat anysequence of mINSERT, EXTRACT-MIN, and DECREASE -KEYoperations\\ntakes O.mCk/time. Here’s how. We use an array, say AŒ0 : : k\\x8d, where AŒj \\x8dis\\na linked list of each element whose key is j. Think of AŒj \\x8das a bucket for all\\nelements with key j. We implement each bucket by a circular, doubly linked list\\nwith asentinel, so that wecan insert into or delete from each bucket in O.1/time.\\nWeperform the min-priority queue operations asfollows:\\n\\x0fINSERT: To insert an element with key j, just insert it into the linked list\\ninAŒj \\x8d. Time: O.1/per INSERT.\\n\\x0fEXTRACT-MIN: We maintain an index minof the value of the smallest key\\nextracted. Initially, minis0. To ﬁnd the smallest key, look in AŒmin\\x8dand, if\\nthis list is nonempty, use any element in it, removing the ele ment from the list\\nand returning it to the caller. Otherwise, we rely on the mono tonicity property\\nandincrement minuntilweeither ﬁndalist AŒmin\\x8dthatisnonempty(usingany\\nelementin AŒmin\\x8dasbefore) orwerunofftheendofthearray A(inwhichcase\\nthe min-priority queue isempty).\\nSince there are at most mINSERToperations, there are at most melements in\\nthemin-priority queue. Weincrement minat most ktimes, andweremoveand\\nreturn some element at most mtimes. Thus, the total time over all E XTRACT-\\nMINoperations is O.mCk/.\\n\\x0fDECREASE -KEY: To decrease the key of an element from jtoi, ﬁrst check\\nwhether i\\x14j, ﬂagging an error if not. Otherwise, we remove the element\\nfrom its list AŒj \\x8dinO.1/time and insert it into the list AŒi\\x8dinO.1/time.\\nTime: O.1/per DECREASE -KEY.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 424}),\n",
              " Document(page_content='Solutions forChapter 24: Single-Source Shortest Paths 24- 17\\nTo apply this kind of min-priority queue to Dijkstra’s algor ithm, we need to let\\nkD.jVj/NUL1/W,andwealsoneedaseparatelistforkeyswithvalue 1. Thenum-\\nber of operations misO.VCE/(since there arejVjINSERTandjVjEXTRACT-\\nMINoperationsandatmost jEjDECREASE -KEYoperations), andsothetotaltime\\nisO.VCECV W /DO.V WCE/.\\nSolutionto Exercise 24.3-9\\nFirst, observe that at any time, there are at most WC2distinct key values in the\\npriority queue. Why? A key value is either 1or it is not. Consider what happens\\nwhenever a key value \\x17:dbecomes ﬁnite. It must have occurred due to the relax-\\nationofanedge .u; \\x17/. Atthattime, uwasbeingplacedinto S,and u:d\\x14y:dfor\\nallvertices y2V/NULS. Afterrelaxing edge .u; \\x17/,wehave \\x17:d\\x14u:dCW. Since\\nanyothervertex y2V/NULSwithy:d<1alsohaditsestimatechangedbyarelax-\\nationofsomeedge xwithx:d\\x14u:d,wemusthave y:d\\x14x:dCW\\x14u:dCW.\\nThus, at the time that we are relaxing edges from a vertex u, wemust have, for all\\nvertices \\x172V/NULS, that u:d\\x14\\x17:d\\x14u:dCWor\\x17:dD1. Since shortest-path\\nestimates are integer values (except for 1), at any given moment wehave at most\\nWC2different ones: u:d; u:dC1; u:dC2; : : : ; u:dCWand1.\\nTherefore, we can maintain the min-priorty queue as a binary min-heap in which\\neachnodepointstoadoublylinkedlistofallverticeswitha givenkeyvalue. There\\nare at most WC2nodes in the heap, and so E XTRACT-MINruns in O.lgW /\\ntime. To perform D ECREASE -KEY, we need to be able to ﬁnd the heap node\\ncorresponding to a given key in O.lgW /time. We can do so in O.1/time as\\nfollows. First, keep a pointer infto the node containing all the 1keys. Second,\\nmaintain an array locŒ0 : : W \\x8d, wherelocŒi\\x8dpoints to the unique heap entry whose\\nkeyvalueiscongruent to i .mod .WC1//. Askeysmovearound intheheap, we\\ncan update this array in O.1/time per movement.\\nAlternatively, instead of using a binary min-heap, we could use a red-black tree.\\nNow INSERT, DELETE, MINIMUM, and SEARCH—from which we can construct\\nthe priority-queue operations—each run in O.lgW /time.\\nSolutionto Exercise 24.4-4\\nLetı.u/be the shortest-path weight from stou. Then wewant to ﬁnd ı.t/.\\nımust satisfy\\nı.s/D0\\nı.\\x17//NULı.u/\\x14w.u; \\x17/for all .u; \\x17/2E(Lemma24.10) ;\\nwhere w.u; \\x17/isthe weight of edge .u; \\x17/.\\nThus x\\x17Dı.\\x17/isasolution to\\nxsD0\\nx\\x17/NULxu\\x14w.u; \\x17/ :', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 425}),\n",
              " Document(page_content='24-18 Solutions for Chapter 24: Single-Source Shortest Pat hs\\nToturnthisintoasetofinequalities oftherequiredform,r eplace xsD0byxs\\x140\\nand/NULxs\\x140(i.e.,xs\\x150). Theconstraints are now\\nxs\\x140 ;\\n/NULxs\\x140 ;\\nx\\x17/NULxu\\x14w.u; \\x17/ ;\\nwhich still has x\\x17Dı.\\x17/asasolution.\\nHowever, ıisn’t the only solution to this set of inequalities. (For exa mple, if all\\nedge weights are nonnegative, all xiD0is a solution.) To force xtDı.t/as\\nrequired by the shortest-path problem, add the requirement to maximize (the ob-\\njective function) xt. This iscorrect because\\n\\x0fmax.xt/\\x15ı.t/because xtDı.t/ispart of onesolution totheset of inequali-\\nties,\\n\\x0fmax.xt/\\x14ı.t/can be demonstrated by a technique similar to the proof of\\nTheorem 24.9:\\nLetpbea shortest path from stot. Then bydeﬁnition,\\nı.t/DX\\n.u;\\x17/ 2pw.u; \\x17/ :\\nBut for each edge .u; \\x17/wehave the inequality x\\x17/NULxu\\x14w.u; \\x17/,so\\nı.t/DX\\n.u;\\x17/ 2pw.u; \\x17/\\x15X\\n.u;\\x17/ 2p.x\\x17/NULxu/Dxt/NULxs:\\nButxsD0, soxt\\x14ı.t/.\\nNote: Maximizing xtsubject to the above inequalities solves the single-pair\\nshortest-path problem when tisreachable from sandtherearenonegative-weight\\ncycles. But if there’s a negative-weight cycle, the inequal ities have no feasible so-\\nlution (as demonstrated in the proof of Theorem 24.9); and if tis not reachable\\nfrom s, then xtisunbounded.\\nSolution to Exercise24.4-7\\nThis solutionisalsopostedpublicly\\nObserve that after the ﬁrst pass, all dvalues are at most 0, and that relaxing\\nedges .\\x170; \\x17i/willneveragainchangea dvalue. Therefore,wecaneliminate \\x170by\\nrunning the Bellman-Ford algorithm on the constraint graph without the \\x170vertex\\nbut initializing all shortest path estimates to 0instead of1.\\nSolution to Exercise24.4-10\\nToallowforsingle-variableconstraints,weaddthevariab lex0andletitcorrespond\\nto the source vertex \\x170of the constraint graph. The idea is that, if there are no', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 426}),\n",
              " Document(page_content='Solutions forChapter 24: Single-Source Shortest Paths 24- 19\\nnegative-weight cycles containing \\x170, we will ﬁnd that ı.\\x170; \\x170/D0. In this case,\\nwe set x0D0, and so we can treat any single-variable constraint using xias if it\\nwerea2-variable constraint with x0as the other variable.\\nSpeciﬁcally, we treat the constraint xi\\x14bkas if it were xi/NULx0\\x14bk, and we\\naddtheedge .\\x170; \\x17i/withweight bktotheconstraint graph. Wetreattheconstraint\\n/NULxi\\x14bkas if it were x0/NULxi\\x14bk, and we add the edge .\\x17i; \\x170/with weight bk\\ntothe constraint graph.\\nOnce we ﬁnd shortest-path weights from \\x170, we set xiDı.\\x170; \\x17i/for all\\niD0; 1; : : : ; n ; that is, we do as before but also include x0as one of the vari-\\nables that we set to a shortest-path weight. Since \\x170is the source vertex, either\\nx0D0orx0< 0.\\nIfı.\\x170; \\x170/D0, so that x0D0, then setting xiDı.\\x170; \\x17i/for all iD0; 1; : : : ; n\\ngives a feasible solution for the system. The only new constr aints beyond those in\\nthe text are those involving x0. For constraints xi\\x14bk, we use xi/NULx0\\x14bk. By\\nthe triangle inequality, ı.\\x170; \\x17i/\\x14ı.\\x170; \\x170/Cw.\\x17 0; \\x17i/Dbk, and so xi\\x14bk.\\nFor constraints/NULxi\\x14bk, we use x0/NULxi\\x14bk. By the triangle inequality, 0D\\nı.\\x170; \\x170/\\x14ı.\\x170; \\x17i/Cw.\\x17 i; \\x170/; thus, 0\\x14xiCbkor, equivalently,/NULxi\\x14bk.\\nIfı.\\x170; \\x170/ < 0,sothat x0< 0,thenthereisanegative-weight cyclecontaining \\x170.\\nThe portion of the proof of Theorem 24.9 that deals with negat ive-weight cycles\\ncarries through but with \\x170on the negative-weight cycle, and we see that there is\\nnofeasible solution.\\nSolutionto Exercise 24.5-4\\nThissolutionisalsopostedpublicly\\nWhenever R ELAXsets\\x19for some vertex, it also reduces the vertex’s dvalue.\\nThusif s:\\x19gets set toa non- NILvalue, s:disreduced from its initial value of 0to\\nanegativenumber. But s:distheweightofsomepathfrom stos,whichisacycle\\nincluding s. Thus, there is anegative-weight cycle.\\nSolutionto Exercise 24.5-7\\nSuppose we have a shortest-paths tree G\\x19. Relax edges in G\\x19according to the\\norder in which a BFS would visit them. Then we are guaranteed t hat the edges\\nalong each shortest path are relaxed in order. By the path-re laxation property, we\\nwould then have \\x17:dDı.s; \\x17/for all \\x172V. Since G\\x19contains at mostjVj/NUL1\\nedges, weneed to relax only jVj/NUL1edges toget \\x17:dDı.s; \\x17/for all \\x172V.\\nSolutionto Exercise 24.5-8\\nSupposethat there isanegative-weight cycle cDh\\x170; \\x171; : : : ; \\x17 ki,where \\x170D\\x17k,\\nthatisreachable fromthesourcevertex s;thus, w.c/ < 0 . Without lossofgeneral-', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 427}),\n",
              " Document(page_content='24-20 Solutions for Chapter 24: Single-Source Shortest Pat hs\\nity,cissimple. Theremust beanacyclic path from stosome vertex of cthat uses\\nno other vertices in c. Without loss of generality let this vertex of cbe\\x170, and let\\nthis path from sto\\x170bepDhu0; u1; : : : ; u li, where u0DsandulD\\x170D\\x17k.\\n(It may be the case that ulDs, in which case path phas no edges.) After the call\\nto INITIALIZE -SINGLE-SOURCEsets\\x17:dD1for all \\x172V/NULfsg, perform the\\nfollowing sequence ofrelaxations. First,relaxeveryedge inpath p,inorder. Then\\nrelax every edge in cycle c, in order, and repeatedly relax the cycle. That is, we\\nrelax the edges .u0; u1/,.u1; u2/, ..., .ul/NUL1; \\x170/,.\\x170; \\x171/,.\\x171; \\x172/, ..., .\\x17k/NUL1; \\x170/,\\n.\\x170; \\x171/,.\\x171; \\x172/, ..., .\\x17k/NUL1; \\x170/,.\\x170; \\x171/,.\\x171; \\x172/,..., .\\x17k/NUL1; \\x170/, ....\\nWe claim that every edge relaxation in this sequence reduces a shortest-path es-\\ntimate. Clearly, the ﬁrst time we relax an edge .ui/NUL1; ui/or.\\x17j/NUL1; \\x17j/, for\\niD1; 2; : : : ; l andjD1; 2; : : : ; k/NUL1(note that we have not yet relaxed the last\\nedge of cycle c), we reduce ui:dor\\x17j:dfrom1to a ﬁnite value. Now consider\\nthe relaxation of any edge .\\x17j/NUL1; \\x17j/after this opening sequence of relaxations.\\nWe use induction on the number of edge relaxations to show tha t this relaxation\\nreduces \\x17j:d.\\nBasis:The next edge relaxed after the opening sequence is .\\x17k/NUL1; \\x17k/. Before\\nrelaxation, \\x17k:dDw.p/, and after relaxation, \\x17k:dDw.p/Cw.c/ < w.p/ ,\\nsince w.c/ < 0 .\\nInductive step: Consider the relaxation of edge .\\x17j/NUL1; \\x17j/. Since cis a sim-\\nple cycle, the last time \\x17j:dwas updated was by a relaxation of this same\\nedge. By the inductive hypothesis, \\x17j/NUL1:dhas just been reduced. Thus,\\n\\x17j/NUL1:dCw.\\x17 j/NUL1; \\x17j/ < \\x17 j:d,and so therelaxation will reduce the value of \\x17j:d.\\nSolution to Problem 24-1\\na.Assume for the purpose contradiction that Gfis not acyclic; thus Gfhas a\\ncycle. A cycle must have at least one edge .u; \\x17/in which uhas higher index\\nthan\\x17. This edge is not in Ef(by the deﬁnition of Ef), in contradition to the\\nassumption that Gfhasa cycle. Thus Gfisacyclic.\\nThe sequenceh\\x171; \\x172; : : : ; \\x17 jVjiis a topological sort for Gf, because from the\\ndeﬁnition of Efwe know that all edges are directed from smaller indices to\\nlarger indices.\\nTheproof for Ebissimilar.\\nb.For all vertices \\x172V, we know that either ı.s; \\x17/D1orı.s; \\x17/is ﬁnite.\\nIfı.s; \\x17/D 1, then \\x17:dwill be1. Thus, we need to consider only the\\ncase where \\x17:dis ﬁnite. There must be some shortest path from sto\\x17. Let\\npDh\\x170; \\x171; : : : ; \\x17 k/NUL1; \\x17kibe that path, where \\x170Dsand\\x17kD\\x17. Let us now\\nconsider how manytimes there isa change indirection in p,that is, asituation\\nin which .\\x17i/NUL1; \\x17i/2Efand.\\x17i; \\x17iC1/2Ebor vice versa. There can be at\\nmostjVj/NUL1edgesin p,sotherecanbeatmost jVj/NUL2changesindirection. Any\\nportion of the path where there is no change in direction is co mputed with the\\ncorrect dvalues in the ﬁrst or second half of a single pass once the vert ex that\\nbeginstheno-change-in-direction sequencehasthecorrec tdvalue,becausethe\\nedges are relaxed in the order of the direction of the sequenc e. Each change in', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 428}),\n",
              " Document(page_content='Solutions forChapter 24: Single-Source Shortest Paths 24- 21\\ndirection requires a half pass in the new direction of the pat h. The following\\ntable showsthe maximum number of passes needed depending on the parity of\\njVj/NUL1and the direction of the ﬁrst edge:\\njVj/NUL1ﬁrst edge direction passes\\neven forward .jVj/NUL1/=2\\neven backward .jVj/NUL1/=2C1\\nodd forward jVj=2\\nodd backward jVj=2\\nIn anycase, the maximum number of passes that wewill need is djVj=2e.\\nc.This scheme does not affect the asymptotic running time of th e algorithm be-\\ncause even though we perform only djVj=2epasses instead ofjVj/NUL1passes,\\nit is still O.V /passes. Each pass still takes ‚.E/time, so the running time\\nremains O.VE/.\\nSolutionto Problem 24-2\\na.Consider boxes with dimensions xD.x1; : : : ; x d/,yD.y1; : : : ; y d/, and\\n´D.´1; : : : ; ´ d/. Suppose there exists a permutation \\x19such that x\\x19.i/< y i\\nforiD1; : : : ; dand there exists a permutation \\x190such that y\\x190.i/< ´ ifor\\niD1; : : : ; d, so that xnests inside yandynests inside ´. Construct a\\npermutation \\x1900, where \\x1900.i/D\\x190.\\x19.i//. Then for iD1; : : : ; d, we have\\nx\\x1900.i/Dx\\x190.\\x19.i// < y \\x190.i/< ´ i,and so xnests inside ´.\\nb.Sort the dimensions of each box from longest to shortest. A bo xXwith\\nsorteddimensions .x1; x2; : : : ; x d/nestsinsideabox Ywithsorteddimensions\\n.y1; y2; : : : ; y d/if and only if xi< y iforiD1; 2; : : : ; d . The sorting can\\nbe done in O.dlgd/time, and the test for nesting can be done in O.d/time,\\nand so the algorithm runs in O.dlgd/time. This algorithm works because a\\nd-dimensional box can be oriented sothat every permutation o f itsdimensions\\nis possible. (Experiment witha 3-dimensional box if you are unsure of this).\\nc.Construct a dag GD.V; E/, where each vertex \\x17icorresponds to box Bi, and\\n.\\x17i; \\x17j/2Eifandonlyifbox Binestsinsidebox Bj. Graph Gisindeedadag,\\nbecause nesting is transitive and antireﬂexive (i.e., no bo x nests inside itself).\\nThetimetoconstruct thedagis O.dn2Cdnlgd/,fromcomparingeachofthe/NULn\\n2\\x01\\npairs of boxes after sorting the dimensions of each.\\nAddasupersource vertex sandasupersink vertex ttoG, andaddedges .s; \\x17 i/\\nfor all vertices \\x17iwith in-degree 0and.\\x17j; t/for all vertices \\x17jwith out-\\ndegree 0. Call theresulting dag G0. Thetime todo sois O.n/.\\nFind a longest path from stotinG0. (Section 24.2 discusses how to ﬁnd a\\nlongest path in a dag.) This path corresponds to a longest seq uence of nesting\\nboxes. Thetimetoﬁndalongest pathis O.n2/,since G0hasnC2verticesand\\nO.n2/edges.\\nOverall, this algorithm runs in O.dn2Cdnlgd/time.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 429}),\n",
              " Document(page_content='24-22 Solutions for Chapter 24: Single-Source Shortest Pat hs\\nSolution to Problem 24-3\\nThis solutionisalsopostedpublicly\\na.Wecan use the Bellman-Ford algorithm on a suitable weighted , directed graph\\nGD.V; E/, which we form as follows. There is one vertex in Vfor each\\ncurrency, and for each pair of currencies ciandcj, there are directed edges\\n.\\x17i; \\x17j/and.\\x17j; \\x17i/. (Thus,jVjDnandjEjDn.n/NUL1/.)\\nWeare looking for a cycle hi1; i2; i3; : : : ; i k; i1isuch that\\nRŒi 1; i2\\x8d\\x01RŒi 2; i3\\x8d\\x01\\x01\\x01RŒi k/NUL1; ik\\x8d\\x01RŒi k; i1\\x8d > 1 :\\nTaking logarithms of both sides of this inequality gives\\nlgRŒi 1; i2\\x8dClgRŒi 2; i3\\x8dC\\x01\\x01\\x01ClgRŒi k/NUL1; ik\\x8dClgRŒi k; i1\\x8d > 0 :\\nIf wenegate both sides, weget\\n./NULlgRŒi 1; i2\\x8d/C./NULlgRŒi 2; i3\\x8d/C\\x01\\x01\\x01C .lgRŒi k/NUL1; ik\\x8d/C./NULlgRŒi k; i1\\x8d/ < 0 ;\\nand so we want to determine whether Gcontains a negative-weight cycle with\\nthese edge weights.\\nWecan determine whether there exists a negative-weight cyc le inGby adding\\nan extra vertex \\x170with 0-weight edges .\\x170; \\x17i/for all \\x17i2V, running\\nBELLMAN-FORDfrom \\x170, and using the boolean result of B ELLMAN-FORD\\n(which is TRUEif there are no negative-weight cycles and FALSEif there is a\\nnegative-weightcycle)toguideouranswer. Thatis,weinve rtthebooleanresult\\nof BELLMAN-FORD.\\nThis method works because adding the new vertex \\x170with 0-weight edges\\nfrom \\x170to all other vertices cannot introduce any new cycles, yet it ensures\\nthat all negative-weight cycles are reachable from \\x170.\\nIt takes ‚.n2/time to create G, which has ‚.n2/edges. Then it takes O.n3/\\ntimeto run B ELLMAN-FORD. Thus, the total timeis O.n3/.\\nAnotherwaytodetermine whetheranegative-weight cycleex istsistocreate G\\nand,withoutadding \\x170anditsincidentedges,runeitheroftheall-pairsshortest -\\npathsalgorithms. Iftheresulting shortest-path distance matrixhasanynegative\\nvalues onthe diagonal, then there is anegative-weight cycl e.\\nb.Note: Thesolution tothis part also serves as asolution to Ex ercise 24.1-6.\\nAssuming that we ran B ELLMAN-FORDto solve part (a), we only need to ﬁnd\\ntheverticesofanegative-weightcycle. Wecandosoasfollo ws. Gothroughthe\\nedgesonceagain. Onceweﬁndanedge .u; \\x17/forwhich u:dCw.u; \\x17/ < \\x17: d,\\nthen we know that either vertex \\x17is on a negative-weight cycle or is reachable\\nfromone. Wecanﬁndavertexonthenegative-weight cyclebyt racingbackthe\\n\\x19values from \\x17, keeping track of which vertices we’ve visited until we reac h\\na vertex xthat we’ve visited before. Then we can trace back \\x19values from x\\nuntil weget backto x,andall vertices inbetween, alongwith x,will constitute\\nanegative-weight cycle. Wecan use the recursive method giv en by the P RINT-\\nPATHprocedure of Section 22.2, but stop it when it returns to vert exx.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 430}),\n",
              " Document(page_content='Solutions forChapter 24: Single-Source Shortest Paths 24- 23\\nTherunningtimeis O.n3/torunB ELLMAN-FORD, plus O.m/tocheckallthe\\nedges and O.n/to print the vertices of the cycle, for atotal of O.n3/time.\\nSolutionto Problem 24-4\\na.Since all weights are nonnegative, use Dijkstra’s algorith m. Implement the\\npriority queue as an array QŒ0 : :jEjC1\\x8d, where QŒi\\x8dis a list of vertices \\x17for\\nwhich \\x17:dDi. Initialize \\x17:dfor\\x17¤stojEjC1instead of to1, so that all\\nvertices have a place in Q. (Any initial \\x17:d> ı.s; \\x17/ works in the algorithm,\\nsince \\x17:ddecreases until it reaches ı.s; \\x17/.)\\nThejVjEXTRACT-MINs can be done in O.E/total time, and decreasing a\\ndvalue during relaxation can be done in O.1/time, for a total running time\\nofO.E/.\\n\\x0fWhen \\x17:ddecreases, just add \\x17tothe front of the list in QŒ\\x17:d\\x8d.\\n\\x0fEXTRACT-MINremoves the head of the list in the ﬁrst nonempty slot of Q.\\nTo do E XTRACT-MINwithout scanning all of Q, keep track of the small-\\nestifor which QŒi\\x8dis not empty. The key point is that when \\x17:ddecreases\\ndue to relaxation of edge .u; \\x17/,\\x17:dremains\\x15u:d, so it never moves to\\nan earlier slot of Qthan the one that had u, the previous minimum. Thus\\nEXTRACT-MINcan always scan upward inthe array, taking atotal of O.E/\\ntimefor all E XTRACT-MINs.\\nb.For all .u; \\x17/2E, we have w1.u; \\x17/2f0; 1g, soı1.s; \\x17/\\x14jVj/NUL1\\x14jEj.\\nUsepart (a) to get the O.E/time bound.\\nc.To show that wi.u; \\x17/D2wi/NUL1.u; \\x17/orwi.u; \\x17/D2wi/NUL1.u; \\x17/C1, observe\\nthatthe ibitsof wi.u; \\x17/consist ofthe i/NUL1bitsof wi/NUL1.u; \\x17/followedbyone\\nmore bit. If that low-order bit is 0, then wi.u; \\x17/D2wi/NUL1.u; \\x17/; if it is 1, then\\nwi.u; \\x17/D2wi/NUL1.u; \\x17/C1.\\nNotice the following twoproperties of shortest paths:\\n1. If all edge weights are multiplied by a factor of c, then all shortest-path\\nweights are multiplied by c.\\n2. If all edge weights are increased by at most c, then all shortest-path weights\\nare increased by at most c.jVj/NUL1/, since all shortest paths have at most\\njVj/NUL1edges.\\nThe lowest possible value for wi.u; \\x17/is2wi/NUL1.u; \\x17/, so by the ﬁrst observa-\\ntion, the lowest possible value for ıi.s; \\x17/is2ıi/NUL1.s; \\x17/.\\nThe highest possible value for wi.u; \\x17/is2wi/NUL1.u; \\x17/C1. Therefore, us-\\ning the two observations together, the highest possible val ue for ıi.s; \\x17/is\\n2ıi/NUL1.s; \\x17/CjVj/NUL1.\\nd.Wehave\\nywi.u; \\x17/Dwi.u; \\x17/C2ıi/NUL1.s; u//NUL2ıi/NUL1.s; \\x17/\\n\\x152wi/NUL1.u; \\x17/C2ıi/NUL1.s; u//NUL2ıi/NUL1.s; \\x17/\\n\\x150 :', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 431}),\n",
              " Document(page_content='24-24 Solutions for Chapter 24: Single-Source Shortest Pat hs\\nThe second line follows from part (c), and the third line foll ows from\\nLemma24.10: ıi/NUL1.s; \\x17/\\x14ıi/NUL1.s; u/Cwi/NUL1.u; \\x17/.\\ne.Observe that if wecompute ywi.p/for any path pWu;\\x17, the terms ıi/NUL1.s; t/\\ncancel for every intermediate vertex ton the path. Thus,\\nywi.p/Dwi.p/C2ıi/NUL1.s; u//NUL2ıi/NUL1.s; \\x17/ :\\n(Thisrelationshipwillbeshownindetailinequation(25.1 0)withintheproofof\\nLemma25.1.) The ıi/NUL1termsdependonlyon u,\\x17,and s,butnotonthepath p;\\ntherefore the same paths will be of minimum wiweight and of minimum ywi\\nweight between uand\\x17. Letting uDs, weget\\nyıi.s; \\x17/Dıi.s; \\x17/C2ıi/NUL1.s; s//NUL2ıi/NUL1.s; \\x17/\\nDıi.s; \\x17//NUL2ıi/NUL1.s; \\x17/ :\\nRewriting this result as ıi.s; \\x17/Dyıi.s; \\x17/C2ıi/NUL1.s; \\x17/and combining it with\\nıi.s; \\x17/\\x142ıi/NUL1.s; \\x17/CjVj/NUL1(frompart(c))givesus yıi.s; \\x17/\\x14jVj/NUL1\\x14jEj.\\nf.Tocompute ıi.s; \\x17/from ıi/NUL1.s; \\x17/for all \\x172VinO.E/time:\\n1. Compute theweights ywi.u; \\x17/inO.E/time, asshown in part (d).\\n2. By part (e),yıi.s; \\x17/\\x14jEj, so use part (a) to compute all yıi.s; \\x17/inO.E/\\ntime.\\n3. Compute all ıi.s; \\x17/fromyıi.s; \\x17/andıi/NUL1.s; \\x17/as shown in part (e), in\\nO.V /time.\\nTocompute all ı.s; \\x17/inO.ElgW /time:\\n1. Compute ı1.s; \\x17/for all \\x172V. Asshown inpart (b), this takes O.E/time.\\n2. For each iD2; 3; : : : ; k , compute all ıi.s; \\x17/from ıi/NUL1.s; \\x17/inO.E/\\ntime as shown above. This procedure computes ı.s; \\x17/Dık.u; \\x17/in time\\nO.Ek/DO.ElgW /.\\nSolution to Problem 24-6\\nObservethatabitonicsequencecanincrease, thendecrease , thenincrease,oritcan\\ndecrease, thenincrease, thendecrease. Thatis,therecanb eatmosttwochangesof\\ndirection in a bitonic sequence. Any sequence that increase s, then decreases, then\\nincreases, then decreases has abitonic sequence as asubseq uence.\\nNow, let us suppose that we had an even stronger condition tha n the bitonic prop-\\nerty given in the problem: for each vertex \\x172V, the weights of the edges along\\nany shortest path from sto\\x17are increasing. Then we could call I NITIALIZE -\\nSINGLE-SOURCEand then just relax all edges one time, going in increasing or der\\nof weight. Then the edges along every shortest path would be r elaxed in order\\nof their appearance on the path. (We rely on the uniqueness of edge weights to\\nensure that the ordering is correct.) The path-relaxation p roperty (Lemma 24.15)\\nwould guarantee that we would have computed correct shortes t paths from sto\\neach vertex.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 432}),\n",
              " Document(page_content='Solutions forChapter 24: Single-Source Shortest Paths 24- 25\\nIfweweakenthecondition sothat theweightsoftheedgesalo nganyshortest path\\nincrease and then decrease, we could relax all edges one time , in increasing order\\nofweight,andthenonemoretime,indecreasingorderofweig ht. Thatorder,along\\nwith uniqueness of edge weights, would ensure that we had rel axed the edges of\\neveryshortestpathinorder,andagainthepath-relaxation propertywouldguarantee\\nthat wewould have computed correct shortest paths.\\nTomakesurethatwehandleallbitonicsequences, wedoassug gested above. That\\nis,weperformfourpasses,relaxingeachedgeonceineachpa ss. Theﬁrstandthird\\npasses relax edges in increasing order of weight, and the sec ond and fourth passes\\nin decreasing order. Again, by the path-relaxation propert y and the uniqueness of\\nedge weights, wehave computed correct shortest paths.\\nThetotal timeis O.VCElgV /,asfollows. Thetimetosort jEjedges byweight\\nisO.ElgE/DO.ElgV /(sincejEjDO.V2/). INITIALIZE -SINGLE-SOURCE\\ntakes O.V /time. Each of the four passes takes O.E/time. Thus, the total time is\\nO.ElgVCVCE/DO.VCElgV /.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 433}),\n",
              " Document(page_content='Lecture Notes forChapter 25:\\nAll-PairsShortest Paths\\nChapter 25overview\\nGiven a directed graph GD.V; E/, weight function wWE!R,jVjDn.\\nAssumethat wecan number thevertices 1; 2; : : : ; n .\\nGoal: create an n\\x02nmatrix DD.dij/of shortest-path distances, so that\\ndijDı.i; j /for all vertices iandj.\\nCould run B ELLMAN-FORDonce from each vertex:\\n\\x0fO.V2E/—which is O.V4/if the graph is dense(ED‚.V2/).\\nIfnonegative-weight edges, couldrunDijkstra’s algorith m oncefromeachvertex:\\n\\x0fO.VElgV /withbinary heap— O.V3lgV /if dense,\\n\\x0fO.V2lgVCVE/withFibonacci heap— O.V3/if dense.\\nWe’ll see how to doin O.V3/in all cases, with nofancy data structure.\\nShortest paths and matrixmultiplication\\nAssume that Gis given as adjacency matrix of weights: WD.wij/, with vertices\\nnumbered 1ton.\\nwijD\\x80\\n0 ifiDj ;\\nweight of .i; j /ifi¤j,.i; j /2E ;\\n1 ifi¤j,.i; j /…E :\\nWon’t worry about predecessors—see book.\\nWill use dynamic programming at ﬁrst.\\nOptimal substructure\\nRecall: subpaths of shortest paths are shortest paths.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 434}),\n",
              " Document(page_content='25-2 Lecture Notes for Chapter 25: All-PairsShortest Paths\\nRecursive solution\\nLetl.m/\\nijDweight of shortest path i;jthat contains\\x14medges.\\n\\x0fmD0\\n)there isashortest path i;jwith\\x14medges if and only if iDj\\n)l.0/\\nijD(\\n0ifiDj ;\\n1ifi¤j :\\n\\x0fm\\x151\\n)l.m/\\nijDmin\\x10\\nl.m/NUL1/\\nijmin\\n1\\x14k\\x14n˚\\nl.m/NUL1/\\nikCwkj/TAB\\x11\\n(kranges over all possible\\npredecessors of j)\\nDmin\\n1\\x14k\\x14n˚\\nl.m/NUL1/\\nikCwkj/TAB\\n(since wjjD0for all j) .\\n\\x0fObserve that when mD1, must have l.1/\\nijDwij.\\nConceptually, when the path is restricted to at most 1 edge, t he weight of the\\nshortest path i;jmust be wij.\\nAndthe mathworks out, too:\\nl.1/\\nijDmin\\n1\\x14k\\x14n˚\\nl.0/\\nikCwkj/TAB\\nDl.0/\\niiCwij (l.0/\\niiisthe only non-1among l.0/\\nik)\\nDwij:\\nAll simple shortest paths contain \\x14n/NUL1edges\\n)ı.i; j /Dl.n/NUL1/\\nijDl.n/\\nijDl.nC1/\\nijD: : :\\nComputea solution bottom-up\\nCompute L.1/; L.2/; : : : ; L.n/NUL1/.\\nStart with L.1/DW,since l.1/\\nijDwij.\\nGofrom L.m/NUL1/toL.m/:\\nEXTEND .L; W; n/\\nletL0D/NUL\\nl0\\nij\\x01\\nbe anew n\\x02nmatrix\\nforiD1ton\\nforjD1ton\\nl0\\nijD1\\nforkD1ton\\nl0\\nijDmin.l0\\nij; likCwkj/\\nreturn L0\\nCompute each L.m/:\\nSLOW-APSP .W; n/\\nL.1/DW\\nformD2ton/NUL1\\nletL.m/be anew n\\x02nmatrix\\nL.m/DEXTEND .L.m/NUL1/; W; n/\\nreturn L.n/NUL1/', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 435}),\n",
              " Document(page_content='Lecture Notes for Chapter 25: All-PairsShortest Paths 25-3\\nTime\\n\\x0fEXTEND:‚.n3/.\\n\\x0fSLOW-APSP: ‚.n4/.\\nObservation\\nEXTENDis like matrix multiplication:\\nL!A\\nW!B\\nL0!C\\nmin! C\\nC ! \\x01\\n1 ! 0\\nletCbe an n\\x02nmatrix\\nforiD1ton\\nforjD1ton\\ncijD0\\nforkD1ton\\ncijDcijCaik\\x01bkj\\nreturn C\\nSo,wecan view E XTENDasjust like matrix multiplication!\\nWhydowecare?\\nBecause our goal is to compute L.n/NUL1/as fast as we can. Don’t need to compute\\nalltheintermediate L.1/; L.2/; L.3/; : : : ; L.n/NUL2/.\\nSupposewehadamatrix Aandwewantedtocompute An/NUL1(likecalling E XTEND\\nn/NUL1times).\\nCould compute A; A2; A4; A8; : : :\\nIfweknew AmDAn/NUL1for all m\\x15n/NUL1,could just ﬁnishwith Ar,where risthe\\nsmallest power of 2that’s\\x15n/NUL1. (rD2dlg.n/NUL1/e)\\nFASTER-APSP .W; n/\\nL.1/DW\\nmD1\\nwhile m < n/NUL1\\nletL.2m/bea new n\\x02nmatrix\\nL.2m/DEXTEND .L.m/; L.m/; n/\\nmD2m\\nreturn L.m/\\nOKtoovershoot, since products don’t change after L.n/NUL1/.\\nTime\\n‚.n3lgn/.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 436}),\n",
              " Document(page_content='25-4 Lecture Notes for Chapter 25: All-PairsShortest Paths\\nFloyd-Warshallalgorithm\\nA different dynamic-programming approach.\\nFor path pDh\\x171; \\x172; : : : ; \\x17 li,anintermediate vertex isany vertex of pother than\\n\\x171or\\x17l.\\nLetd.k/\\nijDshortest-path weight of any path i;jwith all intermediate vertices\\ninf1; 2; : : : ; kg.\\nConsider ashortest path ip;jwith all intermediate vertices in f1; 2; : : : ; kg:\\n\\x0fIfkis not an intermediate vertex, then all intermediate vertic es of pare in\\nf1; 2; : : : ; k/NUL1g.\\n\\x0fIfkisan intermediate vertex:\\ni k jp1 p2\\nall intermediate vertices in {1, 2, ..., k–1}\\nRecursive formulation\\nd.k/\\nijD(\\nwij ifkD0 ;\\nmin/NUL\\nd.k/NUL1/\\nij; d.k/NUL1/\\nikCd.k/NUL1/\\nkj\\x01\\nifk\\x151 :\\n(Have d.0/\\nijDwijbecause can’t have intermediate vertices )\\x14 1edge.)\\nWant D.n/D/NUL\\nd.n/\\nij\\x01\\n, since all vertices numbered \\x14n.\\nComputebottom-up\\nCompute in increasing order of k:\\nFLOYD-WARSHALL .W; n/\\nD.0/DW\\nforkD1ton\\nletD.k/D/NUL\\nd.k/\\nij\\x01\\nbe anew n\\x02nmatrix\\nforiD1ton\\nforjD1ton\\nd.k/\\nijDmin/NUL\\nd.k/NUL1/\\nij; d.k/NUL1/\\nikCd.k/NUL1/\\nkj\\x01\\nreturn D.n/\\nCandrop superscripts. (SeeExercise 25.2-4 in text.)\\nTime\\n‚.n3/.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 437}),\n",
              " Document(page_content='Lecture Notes for Chapter 25: All-PairsShortest Paths 25-5\\nTransitive closure\\nGiven GD.V; E/, directed.\\nCompute G\\x03D.V; E\\x03/.\\n\\x0fE\\x03Df.i; j /Wthere isapath i;jinGg.\\nCould assign weight of 1toeach edge, then run F LOYD-WARSHALL .\\n\\x0fIfdij< n,then there isapath i;j.\\n\\x0fOtherwise, dijD1and there isno path.\\nSimplerway\\nSubstitute other values and operators in F LOYD-WARSHALL .\\n\\x0fUseunweighted adjacency matrix\\n\\x0fmin!_(OR)\\n\\x0fC!^(AND)\\n\\x0ft.k/\\nijD(\\n1if there ispath i;jwith all intermediate vertices in f1; 2; : : : ; kg;\\n0otherwise :\\n\\x0ft.0/\\nijD(\\n0ifi¤jand.i; j /…E ;\\n1ifiDjor.i; j /2E :\\n\\x0ft.k/\\nijDt.k/NUL1/\\nij_/NUL\\nt.k/NUL1/\\nik^t.k/NUL1/\\nkj\\x01\\n.\\nTRANSITIVE -CLOSURE .G; n/\\nnDjG:Vj\\nletT.0/D/NUL\\nt.0/\\nij\\x01\\nbeanew n\\x02nmatrix\\nforiD1ton\\nforjD1ton\\nifi==jor.i; j /2G:E\\nt.0/\\nijD1\\nelset.0/\\nijD0\\nforkD1ton\\nletT.k/D/NUL\\nt.k/\\nij\\x01\\nbea new n\\x02nmatrix\\nforiD1ton\\nforjD1ton\\nt.k/\\nijDt.k/NUL1/\\nij_/NUL\\nt.k/NUL1/\\nik^t.k/NUL1/\\nkj\\x01\\nreturn T.n/\\nTime\\n‚.n3/,but simpler operations than F LOYD-WARSHALL .', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 438}),\n",
              " Document(page_content='25-6 Lecture Notes for Chapter 25: All-PairsShortest Paths\\nJohnson’s algorithm\\nIdea\\nIf thegraph issparse, it pays torun Dijkstra’s algorithm on ce from each vertex.\\nIf we use a Fibonacci heap for the priority queue, the running time is down\\ntoO.V2lgVCVE/, which is better than F LOYD-WARSHALL ’s‚.V3/time if\\nEDo.V2/.\\nBut Dijkstra’s algorithm requires that all edge weights be n onnegative.\\nDonald Johnson ﬁgured out how to make an equivalent graph tha tdoeshave all\\nedge weights\\x150.\\nReweighting\\nCompute anew weight function ywsuch that\\n1. Forall u; \\x172V,pisashortestpath u;\\x17using wifandonlyif pisashortest\\npathu;\\x17usingyw.\\n2. Forall .u; \\x17/2E;yw.u; \\x17/\\x150.\\nProperty (1)saysthat itsufﬁcestoﬁndshortest pathswith yw. Property (2)sayswe\\ncan do soby running Dijkstra’s algorithm from each vertex.\\nHowto come upwith yw?\\nLemmashowsit’s easy toget property (1):\\nLemma (Reweightingdoesn’t changeshortest paths)\\nGiven a directed, weighted graph GD.V; E/; wWE!R. Let hbe any function\\nsuch that hWV!R. For all .u; \\x17/2E, deﬁne\\nyw.u; \\x17/Dw.u; \\x17/Ch.u//NULh.\\x17/ :\\nLetpDh\\x170; \\x171; : : : ; \\x17 kibe anypath \\x170;\\x17k.\\nThen pisashortestpath \\x170;\\x17kwithwifandonlyif pisashortestpath \\x170;\\x17k\\nwithyw. (Formally, w.p/Dı.\\x170; \\x17k/if and only ifywDyı.\\x170; \\x17k/, whereyıis the\\nshortest-path weight with yw.)\\nAlso, Ghas anegative-weight cycle with wif and only if Ghasanegative-weight\\ncycle withyw.\\nProofFirst, we’ll show that yw.p/Dw.p/Ch.\\x17 0//NULh.\\x17 k/:\\nyw.p/DkX\\niD1yw.\\x17 i/NUL1; \\x17i/\\nDkX\\niD1.w.\\x17 i/NUL1; \\x17i/Ch.\\x17 i/NUL1//NULh.\\x17 i//\\nDkX\\niD1w.\\x17 i/NUL1; \\x17i/Ch.\\x17 0//NULh.\\x17 k/(sum telescopes)\\nDw.p/Ch.\\x17 0//NULh.\\x17 k/ :', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 439}),\n",
              " Document(page_content='Lecture Notes for Chapter 25: All-PairsShortest Paths 25-7\\nTherefore, any path \\x170p;\\x17khasyw.p/Dw.p/Ch.\\x17 0//NULh.\\x17 k/. Since h.\\x17 0/\\nandh.\\x17 k/don’t depend on the path from \\x170to\\x17k, if one path \\x170;\\x17kis shorter\\nthan another with w,it’s also shorter with yw.\\nNow show there exists a negative-weight cycle with wif and only if there exists a\\nnegative-weight cycle with yw:\\n\\x0fLet cycle cDh\\x170; \\x171; : : : ; \\x17 ki,where \\x170D\\x17k.\\n\\x0fThen\\nyw.c/Dw.c/Ch.\\x17 0//NULh.\\x17 k/\\nDw.c/ (since \\x170D\\x17k) .\\nTherefore, chas a negative-weight cycle with wif and only if it has a negative-\\nweight cycle withyw. (lemma)\\nSo, now to get property (2), we just need to come up with a funct ionhWV!R\\nsuch that when wecompute yw.u; \\x17/Dw.u; \\x17/Ch.u//NULh.\\x17/, it’s\\x150.\\nDowhat wedid for difference constraints:\\n\\x0fG0D.V0; E0/\\n\\x0fV0DV[fsg, where sisa newvertex.\\n\\x0fE0DE[f.s; \\x17/W\\x172Vg.\\n\\x0fw.s; \\x17/D0for all \\x172V.\\n\\x0fSincenoedgesenter s,G0hasthesamesetofcyclesas G. Inparticular, G0has\\nanegative-weight cycle if and only if Gdoes.\\nDeﬁne h.\\x17/Dı.s; \\x17/for all \\x172V.\\nClaim\\nyw.u; \\x17/Dw.u; \\x17/Ch.u//NULh.\\x17/\\x150.\\nProofBythe triangle inequality,\\nı.s; \\x17/\\x14ı.s; u/Cw.u; \\x17/\\nh.\\x17/\\x14h.u/Cw.u; \\x17/ :\\nTherefore, w.u; \\x17/Ch.u//NULh.\\x17/\\x150. (claim)', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 440}),\n",
              " Document(page_content='25-8 Lecture Notes for Chapter 25: All-PairsShortest Paths\\nJohnson’salgorithm\\nform G0\\nrun BELLMAN-FORDonG0to compute ı.s; \\x17/for all \\x172G0:V\\nifBELLMAN-FORDreturns FALSE\\nGhas anegative-weight cycle\\nelsecomputeyw.u; \\x17/Dw.u; \\x17/Cı.s; u//NULı.s; \\x17/for all .u; \\x17/2E\\nletDD.du\\x17/be anew n\\x02nmatrix\\nforeach vertex u2G:V\\nrunDijkstra’s algorithm from uusing weight function yw\\ntocomputeyı.u; \\x17/for all \\x172V\\nforeach vertex \\x172G:V\\n//Compute entry du\\x17inmatrix D.\\ndu\\x17Dyı.u; \\x17/Cı.s; \\x17//NULı.s; u/„ƒ‚ …\\nbecause if pisapath u;\\x17,thenyw.p/Dw.p/Ch.u//NULh.\\x17/\\nreturn D\\nTime\\n\\x0f‚.VCE/tocompute G0.\\n\\x0fO.VE/torun B ELLMAN-FORD.\\n\\x0f‚.E/to computeyw.\\n\\x0fO.V2lgVCVE/torunDijkstra’salgorithm jVjtimes(usingFibonacciheap).\\n\\x0f‚.V2/tocompute Dmatrix.\\nTotal: O.V2lgVCVE/.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 441}),\n",
              " Document(page_content='SolutionsforChapter 25:\\nAll-PairsShortest Paths\\nSolutionto Exercise 25.1-3\\nThissolutionisalsopostedpublicly\\nThematrix L.0/corresponds tothe identity matrix\\nID/NUL\\n1 0 0\\x01\\x01\\x010\\n0 1 0\\x01\\x01\\x010\\n0 0 1\\x01\\x01\\x010\\n:::::::::::::::\\n0 0 0\\x01\\x01\\x011\\x01\\nof regular matrix multiplication. Substitute 0(the identity forC) for1(the iden-\\ntity for min), and 1(the identity for\\x01) for0(the identity forC).\\nSolutionto Exercise 25.1-5\\nThissolutionisalsopostedpublicly\\nTheall-pairs shortest-paths algorithm in Section 25.1 com putes\\nL.n/NUL1/DWn/NUL1DL.0/\\x01Wn/NUL1;\\nwhere l.n/NUL1/\\nijDı.i; j /andL.0/is the identity matrix. That is, the entry in the\\nith row and jth column of the matrix “product” is the shortest-path dista nce from\\nvertex ito vertex j, and row iof the product is the solution to the single-source\\nshortest-paths problem for vertex i.\\nNotice that in a matrix “product” CDA\\x01B, theith row of Cis the ith row of A\\n“multiplied” by B. Sinceallwewantisthe ithrowof C,weneverneedmorethan\\ntheithrow of A.\\nThus the solution to the single-source shortest-paths from vertex iisL.0/\\ni\\x01Wn/NUL1,\\nwhere L.0/\\niis the ith row of L.0/—a vector whose ith entry is 0 and whose other\\nentries are1.\\nDoing the above “multiplications” starting from the left is essentially the same\\nas the B ELLMAN-FORDalgorithm. The vector corresponds to the dvalues in\\nBELLMAN-FORD—the shortest-path estimates from the source to each vertex .', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 442}),\n",
              " Document(page_content='25-10 Solutions for Chapter 25: All-PairsShortest Paths\\n\\x0fThevector is initially 0for the source and 1for all other vertices, the sameas\\nthe values set up for dby INITIALIZE -SINGLE-SOURCE.\\n\\x0fEach “multiplication” of the current vector by Wrelaxes all edges just as\\nBELLMAN-FORDdoes. Thatis,adistance estimate intherow,saythedistanc e\\nto\\x17,isupdated toasmaller estimate, ifany,formedbyadding so mew.u; \\x17/to\\nthe current estimate of the distance to u.\\n\\x0fTherelaxation/multiplication isdone n/NUL1times.\\nSolution to Exercise25.1-10\\nRun SLOW-ALL-PAIRS-SHORTEST -PATHSon the graph. Look at the diagonal el-\\nementsof L.m/. Returntheﬁrstvalueof mforwhichone(ormore)ofthediagonal\\nelements ( l.m/\\nii)isnegative. If mreaches nC1,thenstop anddeclare that there are\\nno negative-weight cycles.\\nLetthenumber ofedges inaminimum-length negative-weight cyclebe m\\x03,where\\nm\\x03D1if thegraph has no negative-weight cycles.\\nCorrectness\\nLet’s assume that for some value m\\x03\\x14nand some value of i, we ﬁnd that\\nl.m\\x03/\\nii < 0. Then the graph has a cycle with m\\x03edges that goes from vertex i\\ntoitself, and thiscycle has negativeweight (stored in l.m\\x03/\\nii). Thisistheminimum-\\nlengthnegative-weight cyclebecause S LOW-ALL-PAIRS-SHORTEST -PATHScom-\\nputesallpaths of 1edge, thenallpathsof 2edges, andsoon,andallcyclesshorter\\nthanm\\x03edgeswerecheckedbeforeanddidnothavenegativeweight. N owassume\\nthat for all m\\x14n, there is no negative l.m/\\niielement. Then, there is no negative-\\nweight cycle inthe graph, because all cycles have length at m ostn.\\nTime\\nO.n4/. Moreprecisely, ‚.n3\\x01min.n; m\\x03//.\\nFaster solution\\nRun FASTER-ALL-PAIRS-SHORTEST -PATHSon the graph until the ﬁrst time that\\nthe matrix L.m/has one or more negative values on the diagonal, or until we ha ve\\ncomputed L.m/for some m > n. If we ﬁnd any negative entries on the diagonal,\\nweknowthattheminimum-lengthnegative-weight cyclehasm orethan m=2edges\\nandatmost medges. Wejustneedtobinarysearchforthevalueof m\\x03intherange\\nm=2 < m\\x03\\x14m. The key observation is that on our way to computing L.m/, we\\ncomputed L.1/,L.2/,L.4/,L.8/, ..., L.m=2/, and these matrices sufﬁce to compute\\nevery matrix we’ll need. Here’s pseudocode:', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 443}),\n",
              " Document(page_content='Solutions forChapter 25: All-PairsShortest Paths 25-11\\nFIND-MIN-LENGTH-NEG-WEIGHT-CYCLE .W /\\nnDW:rows\\nL.1/DW\\nmD1\\nwhile m\\x14nand no diagonal entries of L.m/are negative\\nL.2m/DEXTEND-SHORTEST -PATHS .L.m/; L.m//\\nmD2m\\nifm > nand nodiagonal entries of L.m/arenegative\\nreturn“no negative-weight cycles”\\nelseif m\\x142\\nreturn m\\nelse\\nlowDm=2\\nhighDm\\ndDm=4\\nwhile d\\x151\\nsDlowCd\\nL.s/DEXTEND-SHORTEST -PATHS .L.low/; L.d//\\nifL.s/hasany negative entries on the diagonal\\nhighDs\\nelselowDs\\ndDd=2\\nreturnhigh\\nCorrectness\\nIf, after the ﬁrst whileloop, m > nand no diagonal entries of L.m/are negative,\\nthen there is no negative-weight cycle. Otherwise, if m\\x142, then either mD1or\\nmD2,and L.m/istheﬁrstmatrixwithanegativeentryonthediagonal. Thus ,the\\ncorrect value to return is m.\\nIfm > 2, then wemaintain an interval bracketed by the values lowandhigh, such\\nthat the correct value m\\x03is in the range low< m\\x03\\x14high. We use the following\\nloop invariant:\\nLoop invariant: Atthe start of each iteration of the “ while d\\x151” loop,\\n1.dD2pfor someinteger p\\x15/NUL1,\\n2.dD.high/NULlow/=2,\\n3.low< m\\x03\\x14high.\\nInitialization: Initially, mis an integer power of 2andm > 2. Since dDm=4,\\nwe have that dis an integer power of 2andd > 1=2, so that dD2pfor some\\ninteger p\\x150. We also have .high/NULlow/=2D.m/NUL.m=2//=2Dm=4Dd.\\nFinally, L.m/has a negative entry on the diagonal and L.m=2/does not. Since\\nlowDm=2andhighDm,wehave that low< m\\x03\\x14high.\\nMaintenance: We usehigh,low, and dto denote variable values in a given it-\\neration, and high0,low0, and d0to denote the same variable values in the next\\niteration. Thus, we wish to show that dD2pfor some integer p\\x15/NUL1im-\\nplies d0D2p0for some integer p0\\x15/NUL1, that dD.high/NULlow/=2implies\\nd0D.high0/NULlow0/=2, and that low< m\\x03\\x14highimplieslow0< m\\x03\\x14high0.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 444}),\n",
              " Document(page_content='25-12 Solutions for Chapter 25: All-PairsShortest Paths\\nTosee that d0D2p0, note that d0Dd=2, and so dD2p/NUL1. Thecondition that\\nd\\x151implies that p\\x150, and so p0\\x15/NUL1.\\nWithin each iteration, sis set tolowCd, and one of the following actions\\noccurs:\\n\\x0fIfL.s/has any negative entries on the diagonal, then high0is set to sand\\nd0is set to d=2. Upon entering the next iteration, .high0/NULlow0/=2D\\n.s/NULlow0/=2D..lowCd//NULlow/=2Dd=2Dd0. Since L.s/hasanegative\\ndiagonal entry, we know that m\\x03\\x14s. Because high0Dsandlow0Dlow,\\nwehave that low0< m\\x03\\x14high0.\\n\\x0fIfL.s/has no negative entries on the diagonal, then low0is set to s, and\\nd0is set to d=2. Upon entering the next iteration, .high0/NULlow0/=2D\\n.high0/NULs/=2D.high/NUL.lowCd//=2D.high/NULlow/=2/NULd=2Dd/NULd=2D\\nd=2Dd0. Since L.s/hasnonegativediagonalentries,weknowthat m\\x03> s.\\nBecauselow0Dsandhigh0Dhigh,wehave that low0< m\\x03\\x14high0.\\nTermination: At termination, d < 1. Since dD2pfor some integer p\\x15/NUL1,\\nwe must have pD/NUL1, so that dD1=2. By the second part of the loop\\ninvariant, if we multiply both sides by 2, we get that high/NULlowD2dD1.\\nBy the third part of the loop invariant, we know that low< m\\x03\\x14high. Since\\nhigh/NULlowD2dD1andm\\x03>low, the only possible value for m\\x03ishigh,\\nwhich the procedure returns.\\nTime\\nIf there isnonegative-weight cycle, theﬁrst whileloop iterates ‚.lgn/times, and\\nthe total time is ‚.n3lgn/.\\nNow suppose that there is a negative-weight cycle. We claim t hat each time we\\ncall EXTEND-SHORTEST -PATHS .L.low/; L.d//, we have already computed L.low/\\nandL.d/. Initially, since lowDm=2, we had already computed L.low/in the ﬁrst\\nwhileloop. Insucceedingiterationsofthesecond whileloop,theonlywaythat low\\nchangesiswhenitgetsthevalueof s,andwehavejustcomputed L.s/. Asfor L.d/,\\nobservethat dtakesonthevalues m=4; m=8; m=16; : : : ; 1 ,andagain,wecomputed\\nall of these Lmatrices in the ﬁrst whileloop. Thus, the claim is proven. Each of\\nthe twowhileloops iterates ‚.lgm\\x03/times. Since we have already computed the\\nparameterstoeachcallofE XTEND-SHORTEST -PATHS, eachiterationisdominated\\nby the ‚.n3/-time call to E XTEND-SHORTEST -PATHS. Thus, the total time is\\n‚.n3lgm\\x03/.\\nIn general, therefore, the running timeis ‚.n3lgmin .n; m\\x03//.\\nSpace\\nThesloweralgorithmneedstokeeponlythreematricesatany time,andsoitsspace\\nrequirement is ‚.n3/. This faster algorithm needs to maintain ‚.lgmin .n; m\\x03//\\nmatrices, and sothe space requirement increases to ‚.n3lgmin .n; m\\x03//.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 445}),\n",
              " Document(page_content='Solutions forChapter 25: All-PairsShortest Paths 25-13\\nSolutionto Exercise 25.2-4\\nThissolutionisalsopostedpublicly\\nWiththesuperscripts, thecomputationis d.k/\\nijDmin/NUL\\nd.k/NUL1/\\nij; d.k/NUL1/\\nikCd.k/NUL1/\\nkj\\x01\\n. If,\\nhaving dropped the superscripts, we were to compute and stor edikordkjbefore\\nusing these values tocompute dij, wemight be computing one of the following:\\nd.k/\\nijDmin/NUL\\nd.k/NUL1/\\nij; d.k/\\nikCd.k/NUL1/\\nkj\\x01\\n;\\nd.k/\\nijDmin/NUL\\nd.k/NUL1/\\nij; d.k/NUL1/\\nikCd.k/\\nkj\\x01\\n;\\nd.k/\\nijDmin/NUL\\nd.k/NUL1/\\nij; d.k/\\nikCd.k/\\nkj\\x01\\n:\\nInanyofthesescenarios, we’recomputingtheweightofasho rtest pathfrom itoj\\nwith all intermediate vertices in f1; 2; : : : ; kg. If we use d.k/\\nik, rather than d.k/NUL1/\\nik,\\nin the computation, then we’re using a subpath from itokwith all intermediate\\nvertices inf1; 2; : : : ; kg. But kcannot be an intermediate vertex on ashortest path\\nfrom itok, since otherwise there would be a cycle on this shortest path . Thus,\\nd.k/\\nikDd.k/NUL1/\\nik. A similar argument applies to show that d.k/\\nkjDd.k/NUL1/\\nkj. Hence, we\\ncan drop the superscripts inthe computation.\\nSolutionto Exercise 25.2-6\\nHereare twoways todetect negative-weight cycles:\\n1. Checkthemain-diagonal entriesoftheresultmatrixfora negativevalue. There\\nis anegative weight cycle if and only if d.n/\\nii< 0for some vertex i:\\n\\x0fd.n/\\niiisapathweightfrom itoitself; soifitisnegative,thereisapathfrom i\\ntoitself (i.e., acycle), withnegative weight.\\n\\x0fIf thereisanegative-weight cycle, consider theonewithth efewest vertices.\\n\\x0fIfithasjustonevertex,thensome wii< 0,sodiistartsoutnegative,and\\nsince dvaluesareneverincreased, itisalsonegativewhenthealgo rithm\\nterminates.\\n\\x0fIfithasatleast twovertices, let kbethehighest-numbered vertexinthe\\ncycle,andlet ibesomeothervertexinthecycle. d.k/NUL1/\\nikandd.k/NUL1/\\nkihave\\ncorrect shortest-path weights, because they are not based o n negative-\\nweight cycles. (Neither d.k/NUL1/\\niknord.k/NUL1/\\nkicaninclude kasanintermedi-\\nate vertex, and iandkare on the negative-weight cycle with the fewest\\nvertices.) Since i;k;iisa negative-weight cycle, the sum of those\\ntwo weights is negative, so d.k/\\niiwill be set to a negative value. Since d\\nvalues are never increased, it is also negative when the algo rithm termi-\\nnates.\\nIn fact, it sufﬁces to check whether d.n/NUL1/\\nii < 0for some vertex i. Here’s why.\\nA negative-weight cycle containing vertex ieither contains vertex nor it does\\nnot. Ifitdoesnot,thenclearly d.n/NUL1/\\nii < 0. Ifthenegative-weightcyclecontains', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 446}),\n",
              " Document(page_content='25-14 Solutions for Chapter 25: All-PairsShortest Paths\\nvertex n, then consider d.n/NUL1/\\nnn. This value must be negative, since the cycle,\\nstarting and ending at vertex n, does not include vertex nas an intermediate\\nvertex.\\n2. Alternatively, onecouldjustrunthenormal F LOYD-WARSHALL algorithm one\\nextra iteration tosee if any of the dvalues change. If there arenegative cycles,\\nthen some shortest-path cost will be cheaper. If there are no such cycles, then\\nnodvalues will change because the algorithm gives the correct s hortest paths.\\nSolution to Exercise25.3-4\\nThis solutionisalsopostedpublicly\\nIt changes shortest paths. Consider the following graph. VDfs; x; y; ´g, and\\nthere are 4 edges: w.s; x/D2,w.x; y/D2,w.s; y/D5, and w.s; ´/D/NUL10.\\nSo we’d add 10 to every weight to make yw. With w, the shortest path from stoy\\niss!x!y, with weight 4. With yw, the shortest path from stoyiss!y,\\nwithweight 15. (Thepath s!x!yhasweight 24.) Theproblem isthat byjust\\nadding the same amount to every edge, you penalize paths with more edges, even\\nif their weights are low.\\nSolution to Exercise25.3-6\\nIn this solution, weassume that 1/NUL1isundeﬁned; inparticular, it’snot 0.\\nLetGD.V; E/, where VDfs; ug,EDf.u; s/g, and w.u; s/D0. There\\nis only one edge, and it enters s. When we run Bellman-Ford from s, we get\\nh.s/Dı.s; s/D0andh.u/Dı.s; u/D 1. When we reweight, we get\\nyw.u; s/D0C1/NUL 0D1. We computeyı.u; s/D1, and so we compute\\ndusD1C 0/NUL1¤ 0. Since ı.u; s/D0,weget an incorrect answer.\\nIf the graph Gis strongly connected, then we get h.\\x17/Dı.s; \\x17/ <1for all\\nvertices \\x172V. Thus,thetriangleinequalitysaysthat h.\\x17/\\x14h.u/Cw.u; \\x17/forall\\nedges .u; \\x17/2E,andsoyw.u; \\x17/Dw.u; \\x17/Ch.u//NULh.\\x17/\\x150. Moreover,alledge\\nweightsyw.u; \\x17/usedinLemma25.1areﬁnite,andsothelemmaholds. Therefor e,\\nthe conditions we need in order to use Johnson’s algorithm ho ld: that reweighting\\ndoes not change shortest paths, and that all edge weights yw.u; \\x17/are nonnegative.\\nAgain relying on Gbeing strongly connected, we get that yı.u; \\x17/ <1for all\\nedges .u; \\x17/2E, which means that du\\x17Dyı.u; \\x17/Ch.\\x17//NULh.u/is ﬁnite and\\ncorrect.\\nSolution to Problem 25-1\\na.LetTD.tij/be thejVj\\x02jVjmatrix representing the transitive closure, such\\nthattijis 1if there is apath from itoj,and 0otherwise.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 447}),\n",
              " Document(page_content='Solutions forChapter 25: All-PairsShortest Paths 25-15\\nInitialize T(when there are no edges in G) as follows:\\ntijD(\\n1ifiDj ;\\n0otherwise :\\nWeupdate Tasfollows when an edge .u; \\x17/isadded to G:\\nTRANSITIVE -CLOSURE-UPDATE .T; u; \\x17/\\nletTbejVj\\x02jVj\\nforiD1tojVj\\nforjD1tojVj\\niftiu==1andt\\x17j==1\\ntijD1\\n\\x0fWith this procedure, the effect of adding edge .u; \\x17/is to create a path (via\\nthe new edge) from every vertex that could already reach uto every vertex\\nthat could already bereached from \\x17.\\n\\x0fNotethat theprocedure sets tu\\x17D1,because both tuuandt\\x17\\x17areinitialized\\nto1.\\n\\x0fThisprocedure takes ‚.V2/timebecause of the twonested loops.\\nb.Consider inserting the edge .\\x17jVj; \\x171/into the straight-line graph \\x171!\\x172!\\n\\x01\\x01\\x01! \\x17jVj.\\nBeforethisedgeisinserted, only jVj.jVjC1/=2entries in Tare1(theentries\\nonandabovethemaindiagonal). Aftertheedgeisinserted, t hegraphisacycle\\ninwhicheveryvertexcanreacheveryothervertex,soall jVj2entriesin Tare1.\\nHencejVj2/NUL.jVj.jVjC1/=2/D‚.V2/entriesmustbechangedin T,soany\\nalgorithm toupdate the transitive closure must take \\x7f.V2/timeon this graph.\\nc.The algorithm in part (a) would take ‚.V4/time to insert all possible ‚.V2/\\nedges, so we need a more efﬁcient algorithm in order for any se quence of in-\\nsertions totake only O.V3/total time.\\nToimprovethealgorithm, noticethattheloopover jispointless when ti\\x17D1.\\nThat is, if there is already a path i;\\x17, then adding the edge .u; \\x17/cannot\\nmakeanynew vertices reachable from i. Theloop toset tijto1forjsuch that\\nthere exists a path \\x17;jis just setting entries that are already 1. Eliminate\\nthis redundant processing as follows:\\nTRANSITIVE -CLOSURE-UPDATE .T; u; \\x17/\\nletTbejVj\\x02jVj\\nforiD1tojVj\\niftiu==1andti\\x17==0\\nforjD1tojVj\\nift\\x17j==1\\ntijD1\\nWe show that this procedure takes O.V3/time to update the transitive closure\\nfor any sequence of ninsertions:\\n\\x0fThere cannot bemore than jVj2edges in G, son\\x14jVj2.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 448}),\n",
              " Document(page_content='25-16 Solutions for Chapter 25: All-PairsShortest Paths\\n\\x0fSummedover ninsertions, thetimefortheouter forloopheader andthetest\\nfortiu==1andti\\x17==0isO.nV /DO.V3/.\\n\\x0fThe last three lines, which take ‚.V /time, are executed only O.V2/times\\nforninsertions. Toseewhy,notice that thelast threelinesaree xecutedonly\\nwhen ti\\x17equals 0, and in that case, the last line sets ti\\x17D1. Thus, the\\nnumber of 0entriesin Tisreducedbyatleast 1eachtimethelastthreelines\\nrun. Since there are only jVj2entries in T, these lines can run at most jVj2\\ntimes.\\n\\x0fHence, the total running timeover ninsertions is O.V3/.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 449}),\n",
              " Document(page_content='Lecture Notes forChapter 26:\\nMaximumFlow\\nChapter 26overview\\nNetwork ﬂow\\n[The third edition treats ﬂow networks differently from the ﬁrst two editions. The\\nconcept of net ﬂow is gone, except that we do discuss net ﬂow ac ross a cut. Skew\\nsymmetryisalso gone, asisimplicit summation notation. Th ethirdedition counts\\nﬂowsonedgesdirectly. Weﬁndthat although themathematics isnot quiteasslick\\nasintheﬁrsttwoeditions, theapproach inthethirdedition matchesintuition more\\nclosely, and therefore students tend topick it upmore quick ly.]\\nUseagraph tomodel material that ﬂowsthrough conduits.\\nEachedge represents one conduit, and has a capacity, which isan upper bound on\\ntheﬂowrateDunits/time.\\nCanthinkofedgesaspipesofdifferent sizes. Butﬂowsdon’t havetobeofliquids.\\nBook has an example where a ﬂow is how many trucks per day can sh ip hockey\\npucks between cities.\\nWant tocompute maxrate that wecan ship material from adesig natedsourcetoa\\ndesignated sink.\\nFlownetworks\\nGD.V; E/directed.\\nEachedge .u; \\x17/has acapacity c.u; \\x17/\\x150.\\nIf.u; \\x17/62E,then c.u; \\x17/D0.\\nIf.u; \\x17/2E,then reverse edge .\\x17; u/62E. (Canwork around this restriction.)\\nSourcevertex s,sinkvertex t, assume s;\\x17;tfor all \\x172V, so that each\\nvertex lies on apath from source to sink.\\nExample: [Edges arelabeled withcapacities.]', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 450}),\n",
              " Document(page_content='26-2 Lecture Notes for Chapter 26: Maximum Flow\\n3\\n2\\n32\\n3\\n1s t\\n2w\\nyx\\nz3 3\\nFlow\\nA function fWV\\x02V!Rsatisfying\\n\\x0fCapacity constraint: For all u; \\x172V; 0\\x14f .u; \\x17/\\x14c.u; \\x17/,\\n\\x0fFlowconservation: Forall u2V/NULfs; tg,X\\n\\x172Vf .\\x17; u/\\n„ƒ‚…\\nﬂowinto uDX\\n\\x172Vf .u; \\x17/\\n„ƒ‚…\\nﬂowout of u.\\nEquivalently,X\\n\\x172Vf .u; \\x17//NULX\\n\\x172Vf .\\x17; u/D0.\\n[Add ﬂows to previous example. Edges here are labeled as ﬂow/ capacity. Leave\\non board.]\\n2/3\\n1/2\\n2/32/2\\n1/3\\n1/1s t\\n2/2w\\nyx\\nz1/3 1/3\\n\\x0fNotethat all ﬂowsare \\x14capacities.\\n\\x0fVerify ﬂowconservation byadding upﬂowsat acouple of verti ces.\\n\\x0fNotethat all ﬂowsD0islegitimate.\\nValueof ﬂow fDjfj\\nDX\\n\\x172Vf .s; \\x17//NULX\\n\\x172Vf .\\x17; s/\\nDﬂowout of source/NULﬂowinto source :\\nIn the example above, value of ﬂow fDjfjD3.\\nMaximum-ﬂow problem\\nGiven G,s,t, and c,ﬁndaﬂowwhose value ismaximum.\\nAntiparallel edges\\nDeﬁnitionofﬂownetworkdoesnotallowboth .u; \\x17/and.\\x17; u/tobeedges. These\\nedges would be antiparallel .\\nWhat if wereally need antiparallel edges?', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 451}),\n",
              " Document(page_content='Lecture Notes for Chapter 26: Maximum Flow 26-3\\n\\x0fChoose one of them, say .u; \\x17/.\\n\\x0fCreate anew vertex \\x170.\\n\\x0fReplace .u; \\x17/bytwonewedges .u; \\x170/and.\\x170; \\x17/,with c.u; \\x170/Dc.\\x170; \\x17/D\\nc.u; \\x17/.\\n\\x0fGet an equivalent ﬂownetwork withno antiparallel edges.\\nCuts\\nAcut.S; T /ofﬂownetwork GD.V; E/isapartitionof VintoSandTDV/NULS\\nsuch that s2Sandt2T.\\n\\x0fSimilar to cut used in minimum spanning trees, except that he re the graph is\\ndirected, and werequire s2Sandt2T.\\nForﬂow f,thenet ﬂowacross cut .S; T /is\\nf .S; T /DX\\nu2SX\\n\\x172Tf .u; \\x17//NULX\\nu2SX\\n\\x172Tf .\\x17; u/ :\\nCapacity of cut .S; T /is\\nc.S; T /DX\\nu2SX\\n\\x172Tc.u; \\x17/ :\\nAminimumcut ofGisacut whose capacity isminimum over all cuts of G.\\nAsymmetry between net ﬂow across a cut and capacity of a cut: For capacity,\\ncount only capacities of edges going from StoT. Ignore edges going in the\\nreverse direction. For net ﬂow, count ﬂow on all edges across the cut: ﬂow on\\nedges going from StoTminus ﬂowon edges going from TtoS.\\nInprevious example, consider thecut SDfs; w; yg; TDfx; ´; tg.\\nf .S; T /Df .w; x/Cf .y; ´/„ƒ‚…\\nfrom StoT/NULf .x; y/„ƒ‚…\\nfrom TtoS\\nD2C2/NUL1\\nD3 :\\nc.S; T /Dc.w; x/Cc.y; ´/„ƒ‚…\\nfrom StoT\\nD2C3\\nD5 :\\nNowconsider thecut SDfs; w; x; yg; TDf´; tg.\\nf .S; T /Df .x; t/Cf .y; ´/„ƒ‚…\\nfrom StoT/NULf .´; x/„ƒ‚…\\nfrom TtoS\\nD2C2/NUL1\\nD3 :\\nc.S; T /Dc.x; t/Cc.y; ´/„ƒ‚…\\nfrom StoT\\nD3C3\\nD6 :\\nSameﬂow asprevious cut, higher capacity.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 452}),\n",
              " Document(page_content='26-4 Lecture Notes for Chapter 26: Maximum Flow\\nLemma\\nFor any cut .S; T /,f .S; T /Djfj.\\n(Net ﬂowacross thecut equals value of the ﬂow.)\\n[Leave on board.]\\n[This proof is much more involved than the proof in the ﬁrst tw o editions. You\\nmight want to omit it, or just give the intuition that no matte r where you cut the\\npipes inanetwork, you’ll see the sameﬂowvolume coming out o f theopenings.]\\nProofRewriteﬂowconservation: for any u2V/NULfs; tg,\\nX\\n\\x172Vf .u; \\x17//NULX\\n\\x172Vf .\\x17; u/D0 :\\nTake deﬁnition ofjfjand add in left-hand side of above equation, summed over\\nall vertices in S/NULfsg. Above equation applies to each vertex in S/NULfsg(since\\nt62Sand obviously s62S/NULfsg), sojust adding in lots of 0s:\\njfjDX\\n\\x172Vf .s; \\x17//NULX\\n\\x172Vf .\\x17; s/CX\\nu2S/NULfsg X\\n\\x172Vf .u; \\x17//NULX\\n\\x172Vf .\\x17; u/!\\n:\\nExpand right-hand summation and regroup terms:\\njfjDX\\n\\x172Vf .s; \\x17//NULX\\n\\x172Vf .\\x17; s/CX\\nu2S/NULfsgX\\n\\x172Vf .u; \\x17//NULX\\nu2S/NULfsgX\\n\\x172Vf .\\x17; u/\\nDX\\n\\x172V \\nf .s; \\x17/CX\\nu2S/NULfsgf .u; \\x17/!\\n/NULX\\n\\x172V \\nf .\\x17; s/CX\\nu2S/NULfsgf .\\x17; u/!\\nDX\\n\\x172VX\\nu2Sf .u; \\x17//NULX\\n\\x172VX\\nu2Sf .\\x17; u/ :\\nPartition VintoS[Tand split each summation over Vinto summations over S\\nandT:\\njfjDX\\n\\x172SX\\nu2Sf .u; \\x17/CX\\n\\x172TX\\nu2Sf .u; \\x17//NULX\\n\\x172SX\\nu2Sf .\\x17; u//NULX\\n\\x172TX\\nu2Sf .\\x17; u/\\nDX\\n\\x172TX\\nu2Sf .u; \\x17//NULX\\n\\x172TX\\nu2Sf .\\x17; u/\\nC X\\n\\x172SX\\nu2Sf .u; \\x17//NULX\\n\\x172SX\\nu2Sf .\\x17; u/!\\n:\\nSummations within parentheses are the same, since f .x; y/appears once in each\\nsummation, for any x; y2V. These summations cancel:\\njfjDX\\nu2SX\\n\\x172Tf .u; \\x17//NULX\\nu2SX\\n\\x172Tf .\\x17; u/\\nDf .S; T / : (lemma)\\nCorollary\\nThevalue of any ﬂow \\x14capacity of any cut.\\n[Leave on board.]', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 453}),\n",
              " Document(page_content='Lecture Notes for Chapter 26: Maximum Flow 26-5\\nProofLet.S; T /be anycut, fbeany ﬂow.\\njfjDf .S; T / (lemma)\\nDX\\nu2SX\\n\\x172Tf .u; \\x17//NULX\\nu2SX\\n\\x172Tf .\\x17; u/(deﬁnition of f .S; T /)\\n\\x14X\\nu2SX\\n\\x172Tf .u; \\x17/ (f .\\x17; u/\\x150)\\n\\x14X\\nu2SX\\n\\x172Tc.u; \\x17/ (capacity constraint)\\nDc.S; T / : (deﬁnition of c.S; T /)(corollary)\\nTherefore, maximum ﬂow \\x14capacity of minimum cut.\\nWill see alittle later that this is infact an equality.\\nThe Ford-Fulkersonmethod\\nResidualnetwork\\nGivenaﬂow fin network GD.V; E/.\\nConsider apair of vertices u; \\x172V.\\nHowmuch additional ﬂowcan wepush directly from uto\\x17?\\nThat’s the residual capacity ,\\ncf.u; \\x17/D\\x80\\nc.u; \\x17//NULf .u; \\x17/ if.u; \\x17/2E ;\\nf .\\x17; u/ if.\\x17; u/2E ;\\n0 otherwise (i.e., .u; \\x17/; .\\x17; u/62E):\\nTheresidual network isGfD.V; E f/, where\\nEfDf.u; \\x17/2V\\x02VWcf.u; \\x17/ > 0g:\\nEachedge of the residual network can admit apositive ﬂow.\\nForour example:\\n2\\n1\\n11\\n1\\n22\\n22\\n11 2112Gf\\ns tw\\nyx\\nz\\nEveryedge .u; \\x17/2Efcorresponds toanedge .u; \\x17/2Eor.\\x17; u/2E(orboth).\\nTherefore,jEfj\\x142jEj.\\nResidual networkissimilar toaﬂownetwork, except thatitm aycontain antiparal-\\nlel edges ( .u; \\x17/and.\\x17; u/). Can deﬁne a ﬂow in a residual network that satisﬁes\\nthe deﬁnition of aﬂow,but with respect tocapacities cfinGf.\\nGiven ﬂows finGandf0inGf, deﬁne .f\"f0/, theaugmentation offbyf0,\\nasafunction V\\x02V!R:', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 454}),\n",
              " Document(page_content='26-6 Lecture Notes for Chapter 26: Maximum Flow\\n.f\"f0/.u; \\x17/D(\\nf .u; \\x17/Cf0.u; \\x17//NULf0.\\x17; u/if.u; \\x17/2E ;\\n0 otherwise\\nfor all u; \\x172V.\\nIntuition: Increase the ﬂow on .u; \\x17/byf0.u; \\x17/but decrease it by f0.\\x17; u/be-\\ncause pushing ﬂow on the reverse edge in the residual network decreases the ﬂow\\nin the original network. Alsoknown as cancellation .\\nLemma\\nGiven a ﬂow network G, a ﬂow finG, and the residual network Gf, letf0be a\\nﬂowin Gf. Then f\"f0is aﬂowin Gwithvaluejf\"f0jDjfjCjf0j.\\n[See book for proof. It has a lot of summations in it. Probably not worth writing\\non the board.]\\nAugmentingpath\\nA simple path s;tinGf.\\n\\x0fAdmitsmore ﬂowalong each edge.\\n\\x0fLikeasequence of pipes through which wecansquirt more ﬂowf romstot.\\nHowmuch more ﬂowcan wepush from stotalong augmenting path p?\\ncf.p/Dminfcf.u; \\x17/W.u; \\x17/ison pg:\\nFor our example, consider theaugmenting path pDhs; w; y; ´; x; ti.\\nMinimum residual capacity is1.\\nAfter we push 1 additional unit along p:[Continue from Gleft on board from\\nbefore. Edge .y; w/hasf .y; w/D0,whichweomit,showingonly c.y; w/D3.]\\n3\\n1\\n1\\n32\\n21\\n11223GfG\\ns tw\\nyx\\nz3/3\\n1/2\\n3/32/2\\n2/3\\n1/1s t\\n2/2w\\nyx\\nz3 2/3\\nObserve that Gfnow has no augmenting path. Why? No edges cross the cut\\n.fs; wg;fx; y; ´; tg/intheforward direction in Gf. Sonopathcanget from stot.\\nClaim that the ﬂowshown in Gisamaximum ﬂow.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 455}),\n",
              " Document(page_content='Lecture Notes for Chapter 26: Maximum Flow 26-7\\nLemma\\nGivenﬂownetwork G,ﬂow finG,residual network Gf. Let pbeanaugmenting\\npath in Gf. Deﬁne fpWV\\x02V!R:\\nfp.u; \\x17/D(\\ncf.p/if.u; \\x17/ison p ;\\n0otherwise :\\nThen fpis aﬂowin Gfwith valuejfpjDcf.p/ > 0.\\nCorollary\\nGiven ﬂow network G, ﬂow finG, and an augmenting path pinGf, deﬁne fp\\nasin lemma. Then f\"fpis aﬂowin Gwithvaluejf\"fpjDjfjCjfpj>jfj.\\nTheorem (Max-ﬂow min-cuttheorem)\\nThefollowing are equivalent:\\n1.fis amaximum ﬂow.\\n2.Gfhas noaugmenting path.\\n3.jfjDc.S; T /for some cut .S; T /.\\nProof\\n(1))(2): Showthe contrapositive: if Gfhasan augmenting path, then fisnot a\\nmaximum ﬂow. If Gfhas augmenting path p, then by the above corollary, f\"fp\\nisaﬂowin GwithvaluejfjCjfpj>jfj, sothat fwasnot amaximum ﬂow.\\n(2))(3): Suppose Gfhas no augmenting path. Deﬁne\\nSDf\\x172VWthere exists apath s;\\x17inGfg;\\nTDV/NULS :\\nMust have t2T; otherwise there is anaugmenting path.\\nTherefore, .S; T /is acut.\\nConsider u2Sand\\x172T:\\n\\x0fIf.u; \\x17/2E, must have f .u; \\x17/Dc.u; \\x17/; otherwise, .u; \\x17/2Ef)\\x172S.\\n\\x0fIf.\\x17; u/2E, must have f .\\x17; u/D0; otherwise, cf.u; \\x17/Df .\\x17; u/ > 0)\\n.u; \\x17/2Ef)\\x172S.\\n\\x0fIf.u; \\x17/; .\\x17; u/62E,must have f .u; \\x17/Df .\\x17; u/D0.\\nThen,\\nf .S; T /DX\\nu2SX\\n\\x172Tf .u; \\x17//NULX\\n\\x172TX\\nu2Sf .\\x17; u/\\nDX\\nu2SX\\n\\x172Tc.u; \\x17//NULX\\n\\x172TX\\nu2S0\\nDc.S; T / :\\nBylemma,jfjDf .S; T /Dc.S; T /.\\n(3))(1): Bycorollary,jfj\\x14c.S; T /.\\nTherefore,jfjDc.S; T /)fisamaxﬂow. (theorem)', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 456}),\n",
              " Document(page_content='26-8 Lecture Notes for Chapter 26: Maximum Flow\\nFord-Fulkerson algorithm\\nKeepaugmentingﬂowalonganaugmentingpathuntilthereisn oaugmentingpath.\\nRepresent theﬂowattribute using the usual dot-notation, b ut onan edge: .u; \\x17/:f.\\nFORD-FULKERSON .G; s; t/\\nforall.u; \\x17/2G:E\\n.u; \\x17/:fD0\\nwhilethere isan augmenting path pinGf\\naugment fbycf.p/\\nAnalysis\\nIf capacities are all integer, then each augmenting path rai sesjfjby\\x151. If max\\nﬂowis f\\x03, then need\\x14jf\\x03jiterations)time is O.Ejf\\x03j/.\\n[Handwaving—see book for better explanation.]\\nNote that this running time is notpolynomial in input size. It depends on jf\\x03j,\\nwhich is not a function of jVjandjEj.\\nIf capacities arerational, can scale them tointegers.\\nIf irrational, F ORD-FULKERSON might never terminate!\\nEdmonds-Karpalgorithm\\nDo FORD-FULKERSON , but compute augmenting paths by BFSof Gf. Augment-\\ning paths are shortest paths s;tinGf,withall edge weights D1.\\nEdmonds-Karp runs in O.VE2/time.\\nToprove, need tolook at distances tovertices in Gf.\\nLetıf.u; \\x17/Dshortest path distance uto\\x17inGf, withunit edge weights.\\nLemma\\nFor all \\x172V/NULfs; tg,ıf.s; \\x17/increases monotonically with each ﬂowaugmenta-\\ntion.\\nProofSupposethereexists \\x172V/NULfs; tgsuchthatsomeﬂowaugmentationcauses\\nıf.s; \\x17/todecrease. Will derive acontradiction.\\nLetfbetheﬂowbefore theﬁrstaugmentation that causesashortes t-path distance\\nto decrease, f0bethe ﬂowafterward.\\nLet\\x17be a vertex with minimum ıf0.s; \\x17/whose distance was decreased by the\\naugmentation, so ıf0.s; \\x17/ < ı f.s; \\x17/.\\nLet a shortest path sto\\x17inGf0bes;u!\\x17, so.u; \\x17/2Ef0andıf0.s; \\x17/D\\nıf0.s; u/C1. (Or ıf0.s; u/Dıf0.s; \\x17//NUL1.)\\nSince ıf0.s; u/ < ı f0.s; \\x17/and how wechose \\x17,wehave ıf0.s; u/\\x15ıf.s; u/.\\nClaim\\n.u; \\x17/62Ef.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 457}),\n",
              " Document(page_content='Lecture Notes for Chapter 26: Maximum Flow 26-9\\nProofIf.u; \\x17/2Ef,then\\nıf.s; \\x17/\\x14ıf.s; u/C1(triangle inequality)\\n\\x14ıf0.s; u/C1\\nDıf0.s; \\x17/ ;\\ncontradicting ıf0.s; \\x17/ < ı f.s; \\x17/. (claim)\\nHowcan .u; \\x17/62Efand.u; \\x17/2Ef0?\\nTheaugmentation must increase ﬂow \\x17tou.\\nSinceEdmonds-Karp augments along shortest paths, theshor test path stouinGf\\nhas.\\x17; u/as itslast edge.\\nTherefore,\\nıf.s; \\x17/Dıf.s; u//NUL1\\n\\x14ıf0.s; u//NUL1\\nDıf0.s; \\x17//NUL2 ;\\ncontradicting ıf0.s; \\x17/ < ı f.s; \\x17/.\\nTherefore, \\x17cannot exist. (lemma)\\nTheorem\\nEdmonds-Karp performs O.VE/augmentations.\\nProofSuppose pis an augmenting path and cf.u; \\x17/Dcf.p/. Then call .u; \\x17/a\\ncriticaledge in Gf, and it disappears from the residual network after augmenti ng\\nalong p.\\n\\x151edge on anyaugmenting path iscritical.\\nWill show that each of the jEjedges can become critical \\x14jVj=2times.\\nConsider u; \\x172Vsuch that either .u; \\x17/2Eor.\\x17; u/2Eor both. Since\\naugmenting paths are shortest paths, when .u; \\x17/becomes critical ﬁrst time,\\nıf.s; \\x17/Dıf.s; u/C1.\\nAugmentﬂow,sothat .u; \\x17/disppearsfromtheresidualnetwork. Thisedgecannot\\nreappear in the residual network until ﬂow from uto\\x17decreases, which happens\\nonly if .\\x17; u/is on an augmenting path in Gf0:ıf0.s; u/Dıf0.s; \\x17/C1. (f0is\\nﬂowwhenthis occurs.)\\nBylemma, ıf.s; \\x17/\\x14ıf0.s; \\x17/)\\nıf0.s; u/Dıf0.s; \\x17/C1\\n\\x15ıf.s; \\x17/C1\\nDıf.s; u/C2 :\\nTherefore, from the time .u; \\x17/becomes critical to the next time, distance of u\\nfrom sincreases by\\x152. Initially, distance to uis\\x150, and augmenting path can’t\\nhave s,u, and tasintermediate vertices.\\nTherefore, until ubecomes unreachable from source, its distance is \\x14jVj/NUL2)\\nafter.u; \\x17/becomes critical the ﬁrst time, it can become critical \\x14.jVj/NUL2/=2D\\njVj=2/NUL1times more).u; \\x17/can become critical \\x14jVj=2times.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 458}),\n",
              " Document(page_content='26-10 Lecture Notes for Chapter 26: Maximum Flow\\nSince O.E/pairs of vertices can have an edge between them in residual ne twork,\\ntotal # of critical edges during execution of Edmonds-Karp i sO.VE/. Since each\\naugmenting pathhas \\x151critical edge,have O.VE/augmentations. (theorem)\\nUseBFStoﬁndeach augmenting path in O.E/time)O.VE2/time.\\nCanget better bounds.\\nPush-relabel algorithms in Sections 26.4–26.5 give O.V3/.\\nCando even better.\\nMaximum bipartite matching\\nExample of aproblem that canbe solved byturning it into aﬂow problem.\\nGD.V; E/(undirected) is bipartiteif we can partition VDL[Rsuch that all\\nedges in Ego between LandR.\\nL R\\nmatching maximum matchingL R\\nAmatching is a subset of edges M\\x12Esuch that for all \\x172V,\\x141edge of M\\nis incident on \\x17. (Vertex \\x17ismatched if an edge of Mis incident on it; otherwise\\nunmatched ).\\nMaximum matching : a matching of maximum cardinality. ( Mis a maximum\\nmatching ifjMj\\x15jM0jfor all matchings M0.)\\nProblem\\nGiven abipartite graph (with the partition), ﬁnd amaximum m atching.\\nApplication\\nMatching planes to routes.\\n\\x0fLDset of planes.\\n\\x0fRDset of routes.\\n\\x0f.u; \\x17/2Eif plane ucan ﬂyroute \\x17.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 459}),\n",
              " Document(page_content='Lecture Notes for Chapter 26: Maximum Flow 26-11\\n\\x0fWant maximum #of routes tobe served by planes.\\nGiven G, deﬁne ﬂownetwork G0D.V0; E0/.\\n\\x0fV0DV[fs; tg.\\n\\x0fE0Df.s; u/Wu2Lg[f.u; \\x17/W.u; \\x17/2Eg[f.\\x17; t/W\\x172Rg.\\n\\x0fc.u; \\x17/D1for all .u; \\x17/2E0.\\ns t\\nEachvertex in Vhas\\x151incident edge)jEj\\x15jVj=2.\\nTherefore,jEj\\x14jE0jDjEjCjVj\\x143jEj.\\nTherefore,jE0jD‚.E/.\\nFindamax ﬂowin G0. Book showsthat it will have integer values for all .u; \\x17/.\\nUseedges that carry ﬂowof 1inmatching.\\nBookproves that this method produces amaximum matching.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 460}),\n",
              " Document(page_content='Solutionsfor Chapter26:\\nMaximum Flow\\nSolution to Exercise26.1-1\\nWe will prove that for every ﬂow in GD.V; E/, we can construct a ﬂow in\\nG0D.V0; E0/that has thesame value as that of the ﬂowin G. Therequired result\\nfollows since a maximum ﬂow in Gis also a ﬂow. Let fbe a ﬂow in G. By\\nconstruction, V0DV[fxgandE0D.E/NULf.u; \\x17/g/[f.u; x/; .x; \\x17/g. Construct\\nf0inG0asfollows:\\nf0.y; ´/D(\\nf .y; ´/ if.y; ´/¤.u; x/and.y; ´/¤.x; \\x17/ ;\\nf .u; \\x17/ if.y; ´/D.u; x/or.y; ´/D.x; \\x17/ :\\nInformally, f0is the same as f, except that the ﬂow f .u; \\x17/now passes through\\nanintermediate vertex x. Thevertex xhasincomingﬂow(ifany)onlyfrom u,and\\nhas outgoing ﬂow(if any) only to vertex \\x17.\\nWe ﬁrst prove that f0satisﬁes the required properties of a ﬂow. It is obvious that\\nthe capacity constraint is satisﬁed for every edge in E0and that every vertex in\\nV0/NULfu; \\x17; xgobeys ﬂow conservation.\\nToshow that edges .u; x/and.x; \\x17/obey thecapacity constraint, wehave\\nf .u; x/Df .u; \\x17/\\x14c.u; \\x17/Dc.u; x/ ;\\nf .x; \\x17/Df .u; \\x17/\\x14c.u; \\x17/Dc.x; \\x17/ :\\nWenow prove ﬂowconservation for u. Assuming that u62fs; tg,wehaveX\\ny2V0f0.u; y/DX\\ny2V0/NULfxgf0.u; y/Cf0.u; x/\\nDX\\ny2V/NULf\\x17gf .u; y/Cf .u; \\x17/\\nDX\\ny2Vf .u; y/\\nDX\\ny2Vf .y; u/ (because fobeys ﬂowconservation)\\nDX\\ny2V0f0.y; u/ :\\nFor vertex \\x17,asymmetric argument proves ﬂowconservation.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 461}),\n",
              " Document(page_content='Solutions forChapter 26: Maximum Flow 26-13\\nForvertex x,wehaveX\\ny2V0f0.y; x/Df0.u; x/\\nDf0.x; \\x17/\\nDX\\ny2V0f0.x; y/ :\\nThus, f0is avalid ﬂowin G0.\\nWenow prove that the values of the ﬂow in both cases are equal. If the source sis\\nnot infu; \\x17g, the proof is trivial, since our construction assigns the sa me ﬂows to\\nincoming and outgoing edges of s. IfsDu, then\\njf0jDX\\ny2V0f0.u; y//NULX\\ny2V0f0.y; u/\\nDX\\ny2V0/NULfxgf0.u; y//NULX\\ny2V0f0.y; u/Cf0.u; x/\\nDX\\ny2V/NULf\\x17gf .u; y//NULX\\ny2Vf .y; u/Cf .u; \\x17/\\nDX\\ny2Vf .u; y//NULX\\ny2Vf .y; u/\\nDjfj:\\nThecase when sD\\x17issymmetric. Weconclude that f0isavalidﬂowin G0with\\njf0jDjfj.\\nSolutionto Exercise 26.1-3\\nWeshowthat,givenanyﬂow f0intheﬂownetwork GD.V; E/,wecanconstruct\\na ﬂow fas stated in the exercise. The result will follow when f0is a maximum\\nﬂow. The idea is that even if there is a path from sto the connected component\\nofu,noﬂowcanenter the component, since theﬂowhasno path tore acht. Thus,\\nall the ﬂowinside the component must be cyclic, which can be m ade zero without\\naffecting the net value of the ﬂow.\\nTwocasesarepossible: where uisnotconnectedto t,andwhere uisnotconnected\\ntos. Weonly analyze the former case. The analysis for the latter case is similar.\\nLetYbe the set of all vertices that have no path to t. Our roadmap will be to ﬁrst\\nprove that no ﬂow can leave Y. We use this result and ﬂow conservation to prove\\nthatnoﬂowcanenter Y. Weshallthenconstuct theﬂow f,whichhastherequired\\nproperties, and prove that jfjDjf0j.\\nThe ﬁrst step is to prove that there can be no ﬂow from a vertex y2Yto a vertex\\n\\x172V/NULY. That is, f0.y; \\x17/D0. This is so, because there are no edges .y; \\x17/\\ninE. If there were an edge .y; \\x17/2E, then there would be a path from ytot,\\nwhich contradicts how wedeﬁned the set Y.\\nWewill now prove that f0.\\x17; y/D0, too. Wewill do soby applying ﬂow conser-\\nvation to each vertex in Yand taking the sum over Y. By ﬂow conservation, we\\nhave', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 462}),\n",
              " Document(page_content='26-14 Solutions for Chapter 26: Maximum Flow\\nX\\ny2YX\\n\\x172Vf0.y; \\x17/DX\\ny2YX\\n\\x172Vf0.\\x17; y/ :\\nPartitioning VintoYandV/NULYgivesX\\ny2YX\\n\\x172V/NULYf0.y; \\x17/CX\\ny2YX\\n\\x172Yf0.y; \\x17/\\nDX\\ny2YX\\n\\x172V/NULYf0.\\x17; y/CX\\ny2YX\\n\\x172Yf0.\\x17; y/ : (\\x03)\\nBut wealso have\\nX\\ny2YX\\n\\x172Yf0.y; \\x17/DX\\ny2YX\\n\\x172Yf0.\\x17; y/ ;\\nsince the left-hand side is the same as the right-hand side, e xcept for a change of\\nvariable names \\x17andy. Wealso have\\nX\\ny2YX\\n\\x172V/NULYf0.y; \\x17/D0 ;\\nsince f0.y; \\x17/D0for each y2Yand\\x172V/NULY. Thus, equation (\\x03) simpliﬁes\\nto\\nX\\ny2YX\\n\\x172V/NULYf0.\\x17; y/D0 :\\nBecause the ﬂowfunction isnonnegative, f .\\x17; y/D0for each \\x172Vandy2Y.\\nWe conclude that there can be no ﬂow between any vertex in Yand any vertex\\ninV/NULY.\\nThesametechnique canshowthat ifthereisapathfrom utotbut not from stou,\\nand we deﬁne Zas the set of vertices that do not have have a path from stou,\\nthen there can be no ﬂow between any vertex in Zand any vertex in V/NULZ. Let\\nXDY[Z. Wethus have f0.\\x17; x/Df0.x; \\x17/D0ifx2Xand\\x1762X.\\nWeare now ready to construct ﬂow f:\\nf .u; \\x17/D(\\nf0.u; \\x17/ifu; \\x1762X ;\\n0 otherwise :\\nWe note that fsatisﬁes the requirements of the exercise. We now prove that f\\nalso satisﬁes the requirements of aﬂowfunction.\\nThe capacity constraint is satisﬁed, since whenever f .u; \\x17/Df0.u; \\x17/, we have\\nf .u; \\x17/Df0.u; \\x17/\\x14c.u; \\x17/and whenever f .u; \\x17/D0, wehave f .u; \\x17/D0\\x14\\nc.u; \\x17/.\\nFor ﬂow conservation, let xbe some vertex other than sort. Ifx2X, then from\\nthe construction of f, wehave\\nX\\n\\x172Vf .x; \\x17/DX\\n\\x172Vf .\\x17; x/D0 :', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 463}),\n",
              " Document(page_content='Solutions forChapter 26: Maximum Flow 26-15\\nOtherwise, if x62X, note that f .x; \\x17/Df0.x; \\x17/andf .\\x17; x/Df0.\\x17; x/for all\\nvertices \\x172V. Thus,X\\n\\x172Vf .x; \\x17/DX\\n\\x172Vf0.x; \\x17/\\nDX\\n\\x172Vf0.\\x17; x/(because f0obeys ﬂowconservation)\\nDX\\n\\x172Vf .\\x17; x/ :\\nFinally, we prove that the value of the ﬂow remains the same. S inces62X, we\\nhave f .s; \\x17/Df0.s; \\x17/andf .\\x17; x/Df0.\\x17; x/for all vertices \\x172V, and so\\njfjDX\\n\\x172Vf .s; \\x17//NULX\\n\\x172Vf .\\x17; s/\\nDX\\n\\x172Vf0.s; \\x17//NULX\\n\\x172Vf0.\\x17; s/\\nDjf0j:\\nSolutionto Exercise 26.1-4\\nTo see that the ﬂows form a convex set, we show that if f1andf2are ﬂows, then\\nsois˛f1C.1/NUL˛/f 2for all ˛such that 0\\x14˛\\x141.\\nFor the capacity constraint, ﬁrst observe that ˛\\x141implies that 1/NUL˛\\x150. Thus,\\nfor any u; \\x172V, wehave\\n˛f1.u; \\x17/C.1/NUL˛/f 2.u; \\x17/\\x150\\x01f1.u; \\x17/C0\\x01.1/NUL˛/f 2.u; \\x17/\\nD0 :\\nSince f1.u; \\x17/\\x14c.u; \\x17/andf2.u; \\x17/\\x14c.u; \\x17/,wealso have\\n˛f1.u; \\x17/C.1/NUL˛/f 2.u; \\x17/\\x14˛c.u; \\x17/C.1/NUL˛/c.u; \\x17/\\nD.˛C.1/NUL˛//c.u; \\x17/\\nDc.u; \\x17/ :\\nFor ﬂow conservation, observe that since f1andf2obey ﬂow conservation, we\\nhaveP\\n\\x172Vf1.\\x17; u/DP\\n\\x172Vf1.u; \\x17/andP\\n\\x172Vf1.\\x17; u/DP\\n\\x172Vf1.u; \\x17/for\\nanyu2V/NULfs; tg. Weneed toshow that\\nX\\n\\x172V.˛f 1.\\x17; u/C.1/NUL˛/f 2.\\x17; u//DX\\n\\x172V.˛f 1.u; \\x17/C.1/NUL˛/f 2.u; \\x17//\\nforany u2V/NULfs; tg. Wemultiplybothsidesoftheequality for f1by˛,multiply\\nboth sides of the equality for f2by1/NUL˛, and add the left-hand and right-hand\\nsides of the resulting equalities toget\\n˛X\\n\\x172Vf1.\\x17; u/C.1/NUL˛/X\\n\\x172Vf2.\\x17; u/D˛X\\n\\x172Vf1.u; \\x17/C.1/NUL˛/X\\n\\x172Vf2.u; \\x17/ :', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 464}),\n",
              " Document(page_content='26-16 Solutions for Chapter 26: Maximum Flow\\nObserving that\\n˛X\\n\\x172Vf1.\\x17; u/C.1/NUL˛/X\\n\\x172Vf2.\\x17; u/DX\\n\\x172V˛f1.\\x17; u/CX\\n\\x172V.1/NUL˛/f 2.\\x17; u/\\nDX\\n\\x172V.˛f 1.\\x17; u/C.1/NUL˛/f 2.\\x17; u//\\nand, likewise, that\\n˛X\\n\\x172Vf1.u; \\x17/C.1/NUL˛/X\\n\\x172Vf2.u; \\x17/DX\\n\\x172V.˛f 1.u; \\x17/C.1/NUL˛/f 2.u; \\x17//\\ncompletestheproofthatﬂowconservation holds,andthusth atﬂowsformaconvex\\nset.\\nSolution to Exercise26.1-6\\nCreate a vertex for each corner, and if there is a street betwe en corners uand\\x17,\\ncreate directed edges .u; \\x17/and.\\x17; u/. Set the capacity of each edge to 1. Let the\\nsource be corner onwhich the professor’s house sits, and let thesink bethe corner\\non which the school is located. We wish to ﬁnd a ﬂow of value 2that also has the\\nproperty that f .u; \\x17/is an integer for all vertices uand\\x17. Such a ﬂow represents\\ntwoedge-disjoint paths from the house tothe school.\\nSolution to Exercise26.1-7\\nWewillconstruct G0bysplittingeachvertex \\x17ofGintotwovertices \\x171; \\x172,joined\\nby an edge of capacity l.\\x17/. All incoming edges of \\x17are now incoming edges\\nto\\x171. Alloutgoing edges from \\x17are now outgoing edges from \\x172.\\nMore formally, construct G0D.V0; E0/with capacity function c0as follows. For\\nevery \\x172V, create two vertices \\x171; \\x172inV0. Add an edge .\\x171; \\x172/inE0with\\nc0.\\x171; \\x172/Dl.\\x17/. For every edge .u; \\x17/2E, create an edge .u2; \\x171/inE0with\\ncapacity c0.u2; \\x171/Dc.u; \\x17/. Make s1andt2asthenewsourceandtarget vertices\\ninG0. Clearly,jV0jD2jVjandjE0jDjEjCjVj.\\nLetfbeaﬂowin Gthatrespectsvertexcapacities. Createaﬂowfunction f0inG0\\nas follows. For each edge .u; \\x17/2G, letf0.u2; \\x171/Df .u; \\x17/. For each vertex\\nu2V/NULftg, letf0.u1; u2/DP\\n\\x172Vf .u; \\x17/. Let f0.t1; t2/DP\\n\\x172Vf .\\x17; t/.\\nWereadilyseethatthereisaone-to-onecorrespondence bet weenﬂowsthatrespect\\nvertex capacities in Gand ﬂows in G0. For the capacity constraint, every edge\\ninG0of the form .u2; \\x171/has a corresponding edge in Gwith a corresponding\\ncapacity and ﬂow and thus satisﬁes the capacity constraint. For edges in E0of\\nthe form .u1; u2/, the capacities reﬂect the vertex capacities in G. Therefore, for\\nu2V/NULfs; tg, we have f0.u1; u2/DP\\n\\x172Vf .u; \\x17/\\x14l.u/Dc0.u1; u2/. We\\nalso have f0.t1; t2/DP\\n\\x172Vf .\\x17; t/\\x14l.t/Dc0.t1; t2/. Note that this constraint\\nalso enforces the vertex capacities in G.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 465}),\n",
              " Document(page_content='Solutions forChapter 26: Maximum Flow 26-17\\nNow, we prove ﬂow conservation. By construction, every vert ex of the form u1\\ninG0has exactly one outgoing edge .u1; u2/, and every incoming edge to u1cor-\\nresponds to an incoming edge of u2G. Thus, for all vertices u2V/NULfs; tg, we\\nhave\\nincoming ﬂowto u1DX\\n\\x172V0f0.\\x17; u 1/\\nDX\\n\\x172Vf .\\x17; u/\\nDX\\n\\x172Vf .u; \\x17/ (because fobeys ﬂowconservation)\\nDf0.u1; u2/\\nDoutgoing ﬂowfrom u1:\\nFort1,wehave\\nincoming ﬂowDX\\n\\x172V0f0.\\x17; t 1/\\nDX\\n\\x172Vf .\\x17; t/\\nDf0.t1; t2/\\nDoutgoing ﬂow :\\nVerticesof theform u2haveexactly oneincoming edge .u1; u2/,andeveryoutgo-\\ning edge of u2corresponds to anoutgoing edge of u2G. Thus, for u2¤t2,\\nincoming ﬂowDf0.u1; u2/\\nDX\\n\\x172Vf .u; \\x17/\\nDX\\n\\x172V0f0.u2; \\x17/\\nDoutgoing ﬂow :\\nFinally, weprove that jf0jDjfj:\\njf0jDX\\n\\x172V0f0.s1; \\x17/\\nDf0.s1; s2/(because there are noother outgoing edges from s1)\\nDX\\n\\x172Vf .s; \\x17/\\nDjfj:\\nSolutionto Exercise 26.2-1\\nLemma\\n1. If \\x1762V1,then f .s; \\x17/D0.\\n2. If \\x1762V2,then f .\\x17; s/D0.\\n3. If \\x1762V1[V2,then f0.s; \\x17/D0.\\n4. If \\x1762V1[V2,then f0.\\x17; s/D0.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 466}),\n",
              " Document(page_content='26-18 Solutions for Chapter 26: Maximum Flow\\nProof\\n1. Let \\x1762V1be some vertex. From the deﬁnition of V1, there is no edge from s\\nto\\x17. Thus, f .s; \\x17/D0.\\n2. Let \\x1762V2be some vertex. From the deﬁnition of V2, there is no edge from \\x17\\ntos. Thus, f .\\x17; s/D0.\\n3. Let \\x1762V1[V2besomevertex. Fromthedeﬁnitionof V1andV2,neither .s; \\x17/\\nnor.\\x17; s/exists. Therefore, the third condition of the deﬁnition of r esidual\\ncapacity (equation (26.2)) applies, and cf.s; \\x17/D0. Thus, f0.s; \\x17/D0.\\n4. Let \\x1762V1[V2besomevertex. Byequation (26.2), wehave that cf.\\x17; s/D0\\nand thus f0.\\x17; s/D0. (lemma)\\nWeconcludethatthesummationsinequation(26.6)equalthe summationsinequa-\\ntion (26.7).\\nSolution to Exercise26.2-8\\nLetGfbetheresidual network just before aniteration of the whileloopof F ORD-\\nFULKERSON , and let Esbe the set of residual edges of Gfintos. We’ll show\\nthat the augmenting path pchosen by F ORD-FULKERSON does not include an\\nedge in Es. Thus, even if we redeﬁne Gfto disallow edges in Es, the path pstill\\nremainsanaugmentingpathintheredeﬁnednetwork. Since premainsunchanged,\\nan iteration of the whileloop of F ORD-FULKERSON updates the ﬂow in the same\\nway as before the redeﬁnition. Furthermore, by disallowing some edges, we do\\nnot introduce any newaugmenting paths. Thus, F ORD-FULKERSON still correctly\\ncomputes a maximum ﬂow.\\nNow, we prove that F ORD-FULKERSON never chooses an augmenting path pthat\\nincludes an edge .\\x17; s/2Es. Why? The path palways starts from s, and if p\\nincluded an edge .\\x17; s/, the vertex swould be repeated twice in the path. Thus, p\\nwould no longer be a simplepath. Since F ORD-FULKERSON chooses only simple\\npaths, pcannot include .\\x17; s/.\\nSolution to Exercise26.2-9\\nThe augmented ﬂow f\"f0satisﬁes the ﬂow conservation property but not the\\ncapacity constraint property.\\nFirst, we prove that f\"f0satisﬁes the ﬂow conservation property. We note that\\nif edge .u; \\x17/2E, then .\\x17; u/62Eandf0.\\x17; u/D0. Thus, we can rewrite the\\ndeﬁnition of ﬂowaugmentation (equation (26.4)), when appl ied to twoﬂows,as\\n.f\"f0/.u; \\x17/D(\\nf .u; \\x17/Cf0.u; \\x17/if.u; \\x17/2E ;\\n0 otherwise :\\nThedeﬁnition implies that thenewﬂowoneach edge issimply t hesum ofthe two\\nﬂows on that edge. We now prove that in f\"f0, the net incoming ﬂow for each', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 467}),\n",
              " Document(page_content='Solutions forChapter 26: Maximum Flow 26-19\\nvertex equals the net outgoing ﬂow. Let u62fs; tgbeany vertex of G. WehaveX\\n\\x172V.f\"f0/.\\x17; u/\\nDX\\n\\x172V.f .\\x17; u/Cf0.\\x17; u//\\nDX\\n\\x172Vf .\\x17; u/CX\\n\\x172Vf0.\\x17; u/\\nDX\\n\\x172Vf .u; \\x17/CX\\n\\x172Vf0.u; \\x17/(because f,f0obey ﬂowconservation)\\nDX\\n\\x172V.f .u; \\x17/Cf0.u; \\x17//\\nDX\\n\\x172V.f\"f0/.u; \\x17/ :\\nWeconclude that f\"f0satisﬁes ﬂowconservation.\\nWenow show that f\"f0need not satisfy the capacity constraint bygiving asim-\\nplecounterexample. Lettheﬂownetwork Ghavejust asource andatarget vertex,\\nwith a single edge .s; t/having c.s; t/D1. Deﬁne the ﬂows fandf0to have\\nf .s; t/Df0.s; t/D1. Then, wehave .f\"f0/.s; t/D2 > c.s; t/ . Weconclude\\nthatf\"f0need not satisfy the capacity constraint.\\nSolutionto Exercise 26.2-11\\nThissolutionisalsopostedpublicly\\nFor any two vertices uand\\x17inG, we can deﬁne a ﬂow network Gu\\x17consisting\\nof the directed version of GwithsDu,tD\\x17, and all edge capacities set to 1.\\n(Theﬂownetwork Gu\\x17hasVverticesand 2jEjedges, sothatithas O.V /vertices\\nandO.E/edges, as required. Wewant all capacities to be 1so that the n umber of\\nedges of Gcrossing a cut equals the capacity of the cut in Gu\\x17.) Let fu\\x17denote a\\nmaximum ﬂowin Gu\\x17.\\nWe claim that for any u2V, the edge connectivity kequals min\\n\\x172V/NULfugfjfu\\x17jg. We’ll\\nshowbelowthatthisclaimholds. Assumingthatitholds,wec anﬁnd kasfollows:\\nEDGE-CONNECTIVITY .G/\\nkD1\\nselect anyvertex u2G:V\\nforeach vertex \\x172G:V/NULfug\\nset upthe ﬂownetwork Gu\\x17asdescribed above\\nﬁndthe maximum ﬂow fu\\x17onGu\\x17\\nkDmin.k;jfu\\x17j/\\nreturn k\\nThe claim follows from the max-ﬂow min-cut theorem and how we chose capac-\\nities so that the capacity of a cut is the number of edges cross ing it. We prove', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 468}),\n",
              " Document(page_content='26-20 Solutions for Chapter 26: Maximum Flow\\nthatkDmin\\n\\x172V/NULfugfjfu\\x17jg, for any u2Vby showing separately that kis at least this\\nminimum and that kis at most this minimum.\\n\\x0fProof that k\\x15min\\n\\x172V/NULfugfjfu\\x17jg:\\nLetmDmin\\n\\x172V/NULfugfjfu\\x17jg. Suppose we remove only m/NUL1edges from G. For\\nany vertex \\x17, by the max-ﬂow min-cut theorem, uand\\x17are still connected.\\n(Themax ﬂowfrom uto\\x17is at least m,hence any cut separating ufrom \\x17has\\ncapacity at least m, which means at least medges cross any such cut. Thus at\\nleastoneedgeisleftcrossingthecutwhenweremove m/NUL1edges.) Thusevery\\nvertex is connected to u, which implies that the graph is still connected. So at\\nleastmedgesmustberemovedtodisconnect thegraph—i.e., k\\x15min\\n\\x172V/NULfugfjfu\\x17jg.\\n\\x0fProof that k\\x14min\\n\\x172V/NULfugfjfu\\x17jg:\\nConsider a vertex \\x17with the minimum jfu\\x17j. By the max-ﬂow min-cut the-\\norem, there is a cut of capacity jfu\\x17jseparating uand\\x17. Since all edge ca-\\npacities are 1, exactly jfu\\x17jedges cross this cut. If these edges are removed,\\nthere is no path from uto\\x17, and so our graph becomes disconnected. Hence\\nk\\x14min\\n\\x172V/NULfugfjfu\\x17jg.\\n\\x0fThus, the claim that kDmin\\n\\x172V/NULfugfjfu\\x17jg,for any u2Vis true.\\nSolution to Exercise26.2-12\\nTheideaoftheproofisthatif f .\\x17; s/D1,thentheremustexistacyclecontaining\\nthe edge .\\x17; s/and for which each edge carries one unit of ﬂow. If we reduce th e\\nﬂow on each edge in the cycle by one unit, we can reduce f .\\x17; s/to0without\\naffecting the value of the ﬂow.\\nGiven the ﬂow network Gand the ﬂow f, we say that vertex yisﬂow-connected\\nto vertex ´if there exists a path pfrom yto´such that each edge of phas a\\npositive ﬂow on it. Wealso deﬁne yto be ﬂow-connected to itself. In particular, s\\nis ﬂow-connected to s.\\nWestart by proving the following lemma:\\nLemma\\nLetGD.V; E/beaﬂownetworkand fbeaﬂowin G. Ifsisnotﬂow-connected\\nto\\x17,then f .\\x17; s/D0.\\nProofTheidea isthat since sisnot ﬂow-connected to \\x17,there cannot beany ﬂow\\nfrom sto\\x17. By using ﬂow conservation, we will prove that there cannot b e any\\nﬂowfrom \\x17toseither, and thus that f .\\x17; s/D0.\\nLetYbe the set of all vertices ysuch that sis ﬂow-connected to y. By applying\\nﬂowconservation to vertices in V/NULYand taking the sum, weobtain\\nX\\n´2V/NULYX\\nx2Vf .x; ´/DX\\n´2V/NULYX\\nx2Vf .´; x/ :', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 469}),\n",
              " Document(page_content='Solutions forChapter 26: Maximum Flow 26-21\\nPartitioning VintoYandV/NULYgivesX\\n´2V/NULYX\\nx2V/NULYf .x; ´/CX\\n´2V/NULYX\\nx2Yf .x; ´/\\nDX\\n´2V/NULYX\\nx2V/NULYf .´; x/CX\\n´2V/NULYX\\nx2Yf .´; x/ : (\\x8e)\\nBut wehaveX\\n´2V/NULYX\\nx2V/NULYf .x; ´/DX\\n´2V/NULYX\\nx2V/NULYf .´; x/ ;\\nsince the left-hand side is the same as the right-hand side, e xcept for a change of\\nvariable names xand´. Wealso haveX\\n´2V/NULYX\\nx2Yf .x; ´/D0 ;\\nsince the ﬂow from any vertex in Yto any vertex in V/NULYmust be 0. Thus,\\nequation ( \\x8e) simpliﬁes to\\nX\\n´2V/NULYX\\nx2Yf .´; x/D0 :\\nThe above equation implies that f .´; x/D0for each ´2V/NULYandx2Y. In\\nparticular, since \\x172V/NULYands2Y,wehave that f .\\x17; s/D0.\\nNow, weshow how to construct the required ﬂow f0. Bythe contrapositive of the\\nlemma, f .\\x17; s/ > 0 implies that sis ﬂow-connected to \\x17through some path p.\\nLet path p0be the path sp;\\x17!s. Path p0is a cycle that has positive ﬂow\\non each edge. Because we assume that all edge capacities are i ntegers, the ﬂow\\non each edge of p0is at least 1. If we subtract 1from each edge of the cycle to\\nobtain aﬂow f0,then f0still satisﬁes theproperties of aﬂownetwork and hasthe\\nsame value asjfj. Because edge .\\x17; s/is in the cycle, we have that f0.\\x17; s/D\\nf .\\x17; s//NUL1D0.\\nSolutionto Exercise 26.2-13\\nLet.S; T /and.X; Y /be two cuts in G(andG0). Let c0be the capacity function\\nofG0. Onewaytodeﬁne c0istoaddasmall amount ıtothecapacity of eachedge\\ninG. That is, if uand\\x17aretwo vertices, weset\\nc0.u; \\x17/Dc.u; \\x17/Cı :\\nThus, if c.S; T /Dc.X; Y /and.S; T /has fewer edges than .X; Y /, then\\nwe would have c0.S; T / < c0.X; Y /. We have to be careful and choose a\\nsmall ı, lest we change the relative ordering of two unequal capacit ies. That is,\\nifc.S; T / < c.X; Y / ,thennomattermanymoreedges .S; T /hasthan .X; Y /,we\\nstill need to have c0.S; T / < c0.X; Y /. With this deﬁnition of c0, a minimum cut\\ninG0will be aminimum cut in Gthat has the minimum number of edges.\\nHow should wechoose the value of ı? Let mbe the minimum difference between\\ncapacities of two unequal-capacity cuts in G. Choose ıDm=.2jEj/. For any\\ncut.S; T /,since the cut can have at most jEjedges, wecan bound c0.S; T /by', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 470}),\n",
              " Document(page_content='26-22 Solutions for Chapter 26: Maximum Flow\\nc.S; T /\\x14c0.S; T /\\x14c.S; T /CjEj\\x01ı :\\nLetc.S; T / < c.X; Y / . Weneed toprove that c0.S; T / < c0.X; Y /. Wehave\\nc0.S; T /\\x14c.S; T /CjEj\\x01ı\\nDc.S; T /Cm=2\\n< c.X; Y / (since c.X; Y //NULc.S; T /\\x15m)\\n\\x14c0.X; Y / :\\nBecause all capacities are integral, we can choose mD1, obtaining ıD1=2jEj.\\nTo avoid dealing with fractional values, we can scale all cap acities by 2jEjto\\nobtain\\nc0.u; \\x17/D2jEj\\x01c.u; \\x17/C1 :\\nSolution to Exercise26.3-3\\nThis solutionisalsopostedpublicly\\nBy deﬁnition, an augmenting path is a simple path s;tin the residual net-\\nwork G0\\nf. Since Ghas no edges between vertices in Land no edges between\\nvertices in R, neither does the ﬂow network G0and hence neither does G0\\nf. Also,\\ntheonlyedgesinvolving sortconnect stoLandRtot. Notethat although edges\\ninG0cango only from LtoR,edges in G0\\nfcan also go from RtoL.\\nThus anyaugmenting path must go\\ns!L!R!\\x01\\x01\\x01! L!R!t ;\\ncrossing back and forth between LandRat most as many times as it can do\\nso without using a vertex twice. It contains s,t, and equal numbers of dis-\\ntinct vertices from LandR—at most 2C2\\x01min.jLj;jRj/vertices in all. The\\nlength of an augmenting path (i.e., its number of edges) is th us bounded above by\\n2\\x01min.jLj;jRj/C1.\\nSolution to Exercise26.4-1\\nWe apply the deﬁnition of excess ﬂow (equation (26.14)) to th e initial preﬂow f\\ncreated by I NITIALIZE -PREFLOW (equation (26.15)) to obtain\\ne.s/DX\\n\\x172Vf .\\x17; s//NULX\\n\\x172Vf .s; \\x17/\\nD0/NULX\\n\\x172Vc.s; \\x17/\\nD /NULX\\n\\x172Vc.s; \\x17/ :\\nNow,\\n/NULjf\\x03jDX\\n\\x172Vf\\x03.\\x17; s//NULX\\n\\x172Vf\\x03.s; \\x17/', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 471}),\n",
              " Document(page_content='Solutions forChapter 26: Maximum Flow 26-23\\n\\x150/NULX\\n\\x172Vc.s; \\x17/ (since f\\x03.\\x17; s/\\x150andf\\x03.s; \\x17/\\x14c.s; \\x17/)\\nDe.s/ :\\nSolutionto Exercise 26.4-3\\nEach time we call R ELABEL .u/, we examine all edges .u; \\x17/2Ef. Since the\\nnumber of relabel operations is at most 2jVj/NUL1per vertex, edge .u; \\x17/will be\\nexamined during relabel operations at most 4jVj/NUL2DO.V /times (at most\\n2jVj/NUL1times during calls to R ELABEL .u/and at most 2jVj/NUL1times during\\ncalls to R ELABEL .\\x17/). Summing up over all the possible residual edges, of which\\nthereareatmost 2jEjDO.E/,weseethatthetotaltimespent relabeling vertices\\nisO.VE/.\\nSolutionto Exercise 26.4-4\\nWe can ﬁnd a minimum cut, given a maximum ﬂow found in GD.V; E/by a\\npush-relabel algorithm, in O.V /time. First, ﬁndaheight yhsuch that 0 <yh <jVj\\nand there is no vertex whose height equals yhat termination of the algorithm. We\\nneed consider only jVj/NUL2vertices, since s:hDjVjandt:hD0. Becauseyhcan\\nbeone of at mostjVj/NUL1possible values, weknow that for at least one number in\\n1; 2; : : : ;jVj/NUL1, there will be no vertex of that height. Hence, yhis well deﬁned,\\nand it is easy to ﬁnd in O.V /time by using a simple boolean array indexed by\\nheights 1; 2; : : : ;jVj/NUL1.\\nLetSD˚\\nu2VWu:h>yh/TAB\\nandTD˚\\n\\x172VW\\x17:h<yh/TAB\\n. Because we know that\\ns:hDjVj>yh, we have s2S, and because t:hD0 <yh, we have t2T, as\\nrequired for acut.\\nWe need to show that f .u; \\x17/Dc.u; \\x17/, i.e., that .u; \\x17/62Ef, for all u2Sand\\n\\x172T. Once we do that, wehave that f .S; T /Dc.S; T /, and by Corollary 26.5,\\n.S; T /isaminimum cut.\\nSuppose for thepurpose of contradiction that there exist ve rtices u2Sand\\x172T\\nsuch that .u; \\x17/2Ef. Because his always maintained as a height function\\n(Lemma 26.16), we have that u:h\\x14\\x17:hC1. But we also have \\x17:h<yh < u:h,\\nandbecause allvalues areinteger, \\x17:h\\x14u:h/NUL2. Thus,wehave u:h\\x14\\x17:hC1\\x14\\nu:h/NUL2C1Du:h/NUL1,whichgivesthecontradiction that u:height\\x14u:height/NUL1.\\nThus, .S; T /isaminimum cut.\\nSolutionto Exercise 26.4-7\\nIf we set s:hDjVj/NUL2, we have to change our deﬁnition of a height function to\\nallow s:hDjVj/NUL2, rather than s:hDjVj. The only change we need to make to', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 472}),\n",
              " Document(page_content='26-24 Solutions for Chapter 26: Maximum Flow\\ntheproof of correctness istoupdate theproof ofLemma26.17 . Theoriginal proof\\nderives the contradiction that s:h\\x14k <jVj, which is at odds with s:hDjVj.\\nWhen s:hDjVj/NUL2, there isno contradiction.\\nAs in the original proof, let us suppose that we have a simple a ugmenting path\\nh\\x170; \\x171; : : : ; \\x17 ki, where \\x170Dsand\\x17kDt, so that k <jVj. How could .s; \\x17 1/be\\naresidual edge? Ithadbeensaturated in I NITIALIZE -PREFLOW, whichmeansthat\\nwehad tohave pushed some ﬂowfrom \\x171tos. In order for that tohave happened,\\nwemust have had \\x171:hDs:hC1. If weset s:hDjVj/NUL2, then \\x171:hwasjVj/NUL1\\nat the time. Since then, \\x171:hdid not decrease, and so we have \\x171:h\\x15jVj/NUL1.\\nWorking backwards over our augmenting path, we have \\x17k/NULi:h\\x14t:hCifor\\niD0; 1; : : : ; k . As before, because the augmenting path is simple, k <jVj.\\nLetting iDk/NUL1, we have \\x171:h\\x14t:hCk/NUL1 < 0CjVj/NUL1. We now have\\nthe contradiction that \\x171:h\\x15jVj/NUL1and\\x171:h<jVj/NUL1, which shows that\\nLemma26.17 still holds.\\nNothing inthe analysis changes asymptotically.\\nSolution to Problem 26-2\\na.The idea is to use a maximum-ﬂow algorithm to ﬁnd a maximum bip artite\\nmatching that selects theedges touseinaminimumpath cover . Wemustshow\\nhow to formulate the max-ﬂow problem and how to construct the path cover\\nfromtheresulting matching, andwemustprovethat thealgor ithm indeed ﬁnds\\naminimum path cover.\\nDeﬁne G0assuggested, withdirected edges. Make G0intoaﬂownetworkwith\\nsource x0and sink y0by deﬁning all edge capacities to be 1. G0is the ﬂow\\nnetwork corresponding to a bipartite graph G00in which LDfx1; : : : ; x ng,\\nRDfy1; : : : ; y ng, and the edges are the (undirected version of the) subset\\nofE0that doesn’t involve x0ory0.\\nThe relationship of Gto the bipartite graph G00is that every vertex iinGis\\nrepresented by two vertices, xiandyi, inG00. Edge .i; j /inGcorresponds to\\nedge .xi; yj/inG00. That is, an edge .xi; yj/inG00means that an edge in G\\nleaves iand enters j. Vertex xitells us about edges leaving i, and yitells us\\nabout edges entering i.\\nThe edges in a bipartite matching in G00can be used in a path cover of G, for\\nthe following reasons:\\n\\x0fIn a bipartite matching, no vertex is used more than once. In a bipartite\\nmatching in G00, since no xiis used more than once, at most one edge in the\\nmatching leaves any vertex iinG. Similarly, since no yjis used more than\\nonce, at most one edge inthe matching enters any vertex jinG.\\n\\x0fIn a path cover, since no vertex appears in more than one path, at most one\\npath edge enters each vertex and at most one path edge leaves e ach vertex.\\nWe can construct a path cover Pfrom any bipartite matching M(not just a\\nmaximum matching) by moving from some xito its matching yj(if any), then\\nfrom xjto its matching yk,and so on, asfollows:', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 473}),\n",
              " Document(page_content='Solutions forChapter 26: Maximum Flow 26-25\\n1. Startanewpathcontaining avertex ithat hasnot yet been placed inapath.\\n2. Ifxiis unmatched, the path can’t goany farther; just add it to P.\\n3. Ifxiis matched to some yj, add jto the current path. If jhas already been\\nplaced in a path (i.e., though we’ve just entered jby processing yj, we’ve\\nalready built a path that leaves jby processing xj), combine this path with\\nthat one and go back tostep 1. Otherwise go tostep 2toprocess xj.\\nThis algorithm constructs apath cover, for the following re asons:\\n\\x0fEveryvertexisputintosomepath,becausewekeeppickingan unusedvertex\\nfrom which to start apath until there are no unused vertices.\\n\\x0fNo vertex is put into two paths, because every xiis matched to at most\\noneyj, and vice versa. That is, at most one candidate edge leaves ea ch\\nvertex, and at most one candidate edge enters each vertex. Wh en building a\\npath, westart orenter avertexandthenleaveit,building as ingle path. Ifwe\\never enter avertex that wasleft earlier, it must havebeen th estart of another\\npath,sincetherearenocycles,andwecombinethosepathsso thatthevertex\\nisentered and left on asingle path.\\nEveryedge in Misused in somepath because wevisit every xi,and weincor-\\nporate the single edge, if any, from each visited xi. Thus, there is a one-to-one\\ncorrespondence between edges in the matching and edges in th e constructed\\npath cover.\\nWe now show that the path cover Pconstructed above has the fewest possible\\npaths when thematching is maximum.\\nLetfbe the ﬂowcorresonding to thebipartite matching M.\\njVjDX\\np2P(#vertices in p) (every vertex is onexactly 1path)\\nDX\\np2P(1+ # edges in p)\\nDX\\np2P1CX\\np2P(# edges in p)\\nDjPjCjMj (by 1-to-1 correspondence)\\nDjPjCjfj (by Lemma26.9) .\\nThus,fortheﬁxedset Vinourgraph G,jPj(thenumberofpaths)isminimized\\nwhen the ﬂow fismaximized.\\nTheoverall algorithm isas follows:\\n\\x0fUse FORD-FULKERSON to ﬁnd a maximum ﬂow in G0and hence a maxi-\\nmum bipartite matching MinG00.\\n\\x0fConstruct the path cover as described above.\\nTime\\nO.VE/total:\\n\\x0fO.VCE/toset up G0,\\n\\x0fO.VE/toﬁndthe maximum bipartite matching,', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 474}),\n",
              " Document(page_content='26-26 Solutions for Chapter 26: Maximum Flow\\n\\x0fO.E/to trace the paths, because each edge 2Mis traversed only once and\\nthere are O.E/edges in M.\\nb.Thealgorithm does not work if there are cycles.\\nConsider a graph Gwith 4 vertices, consisting of a directed triangle and an\\nedge pointing tothe triangle:\\nEDf.1; 2/; .2; 3/; .3; 1/; .4; 1/ g\\nGcanbecoveredwithasinglepath: 4!1!2!3,butouralgorithm might\\nﬁndonly a2-path cover.\\nInthe bipartite graph G0,the edges .xi; yj/are\\n.x1; y2/; .x 2; y3/; .x 3; y1/; .x 4; y1/ :\\nThere are 4 edges from an xito ayj, but 2 of them lead to y1, so a maximum\\nbipartite matching can have only 3 edges (and the maximum ﬂow inG0has\\nvalue 3). In fact, there are 2 possible maximum matchings. It is always pos-\\nsible to match .x1; y2/and.x2; y3/, and then either .x3; y1/or.x4; y1/can be\\nchosen, but not both.\\nThe maximum ﬂow found by one of our max-ﬂow algorithms could ﬁ nd the\\nﬂow corresponding to either of these matchings, since both a re maximal. If\\nit ﬁnds the matching with edge .x3; x1/, then the matching would not con-\\ntain.x4; x1/; given that matching, our path algorithm is forced to produc e 2\\npaths, one of which contains just the vertex 4.\\nSolution to Problem 26-3\\na.Assume for the sake of contradiction that Ak62Tfor some Ak2Ri. Since\\nAk62T, we must have Ak2S. On the other hand, we have Ji2T. Thus,\\nthe edge .Ak; Ji/crosses the cut .S; T /. But c.A k; Ji/D1by construction,\\nwhich contradicts the assumption that .S; T /isaﬁnite-capacity cut.\\nb.Let us deﬁne a project-plan as a set of jobs to accept and experts to hire. Let\\nPbe a project-plan. We assume that Phas two attributes. The attribute P:J\\ndenotes the set of accepted jobs, and P:Adenotes the set of hired experts.\\nAvalidproject-plan is one in which wehave hired all experts that ar e required\\nby the accepted jobs. Speciﬁcally, let Pbe a valid project plan. If Ji2P:J,\\nthenAk2P:Afor each Ak2Ri. Note that Professor Gore might decide to\\nhire moreexperts than those that are actually required.\\nWedeﬁnethe revenueofaproject-plan asthetotalproﬁtfromtheacceptedjobs\\nminus the total cost of the hired experts. The problem asks us to ﬁnd a valid\\nproject plan with maximum revenue.\\nWe start by proving the following lemma, which establishes t he relationship\\nbetween the capacity of a cut in ﬂow network Gand the revenue of a valid\\nproject-plan.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 475}),\n",
              " Document(page_content='Solutions forChapter 26: Maximum Flow 26-27\\nLemma (Min-cutmax-revenue)\\nThere exists aﬁnite-capacity cut .S; T /ofGwithcapacity c.S; T /if and only\\nif there exists a valid project-plan withnet revenue/NULP\\nJi2Jpi\\x01\\n/NULc.S; T /.\\nProofLet.S; T /beaﬁnite-capacity cutof Gwithcapacity c.S; T /. Weprove\\none direction of thelemma byconstructing the required proj ect-plan.\\nConstruct the project-plan Pby including JiinP:Jif and only if Ji2T\\nand including AkinP:Aif and only if Ak2T. From part (a), Pis a valid\\nproject-plan, since, for every Ji2P:J,wehave Ak2P:Afor each Ak2Ri.\\nSince the capacity of the cut is ﬁnite, there cannot be any edg es of the\\nform .Ak; Ji/crossing the cut, where Ak2SandJi2T. All edges going\\nfrom a vertex in Sto a vertex in Tmust be either of the form .s; A k/or of the\\nform .Ji; t/. Let EAbe the set of edges of the form .s; A k/that cross the cut,\\nand let EJbe the set of edges of the form .Ji; t/that cross the cut, so that\\nc.S; T /DX\\n.s;A k/2EAc.s; A k/CX\\n.Ji;t/2EJc.J i; t/ :\\nConsider edges of the form .s; A k/. Wehave\\n.s; A k/2EAif and only if Ak2T\\nif and only if Ak2P:A:\\nByconstruction, c.s; A k/Dck. Takingsummationsover EAandover P:A,we\\nobtain\\nX\\n.s;A k/2EAc.s; A k/DX\\nAk2P:Ack:\\nSimilarly, consider edges of the form .Ji; t/. Wehave\\n.Ji; t/2EJif and only if Ji2S\\nif and only if Ji62T\\nif and only if Ji62P:J:\\nBy construction, c.J i; t/Dpi. Taking summations over EJand over P:J, we\\nobtain\\nX\\n.Ji;t/2EJc.J i; t/DX\\nJi62P:Jpi:\\nLet\\x17bethe net revenue of P. Then, wehave', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 476}),\n",
              " Document(page_content='26-28 Solutions for Chapter 26: Maximum Flow\\n\\x17DX\\nJi2P:Jpi/NULX\\nAk2P:Ack\\nD X\\nJi2Jpi/NULX\\nJi62P:Jpi!\\n/NULX\\nAk2P:Ack\\nDX\\nJi2Jpi/NUL X\\nJi62P:JpiCX\\nAk2P:Ack!\\nDX\\nJi2Jpi/NUL X\\n.Ji;t/2EJc.J i; t/CX\\n.s;A k/2EAc.s; A k/!\\nD X\\nJi2Jpi!\\n/NULc.S; T / :\\nNow, we prove the other direction of the lemma by constructin g the required\\ncut from avalid project-plan.\\nConstruct the cut .S; T /as follows. For every Ji2P:J, letJi2T. For every\\nAk2P:A,letAk2T.\\nFirst, we prove that the cut .S; T /is a ﬁnite-capacity cut. Since edges of the\\nform .Ak; Ji/arethe onlyinﬁnite-capacity edges, itsufﬁces toprove tha t there\\nare noedges .Ak; Ji/such that Ak2SandJi2T.\\nFor the purpose of contradiction, assume there is an edge .Ak; Ji/such that\\nAk2SandJi2T. By our constuction, we must have Ji2P:Jand\\nAk62P:A. But since the edge .Ak; Ji/exists, we have Ak2Ri. Since Pis a\\nvalid project-plan, wederive the contradiction that Akmust have been in P:A.\\nFrom here on, the analysis is the same as the previous directi on. In particular,\\nthe last equation from the previous analysis holds: the net r evenue \\x17equals/NULP\\nJi2Jpi\\x01\\n/NULc.S; T /.\\nWe conclude that the problem of ﬁnding a maximum-revenue pro ject-plan re-\\nducestotheproblem ofﬁnding aminimumcut in G. Let .S; T /beaminimum\\ncut. From thelemma, the maximum net revenue is given by\\n X\\nji2Jpi!\\n/NULc.S; T / :\\nc.Construct the ﬂow network Gas shown in the problem statement. Obtain a\\nminimum cut .S; T /by running any of the maximum-ﬂow algorithms (say,\\nEdmonds-Karp). Construct the project plan Pas follows: add JitoP:Jif and\\nonly if Ji2T. Add AktoP:Aif and only if Ak2T.\\nFirst, we note that the number of vertices in GisjVjDmCnC2, and the\\nnumber of edges in GisjEjDrCmCn. Constructing Gand recovering\\nthe project-plan from the minimum cut are clearly linear-ti me operations. The\\nrunning time of our algorithm is thus asymptotically the sam e as the running\\ntime of the algorithm used to ﬁnd the minimum cut. If we use Edm onds-Karp\\ntoﬁndthe minimum cut, the running timeis O.VE2/.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 477}),\n",
              " Document(page_content='Solutions forChapter 26: Maximum Flow 26-29\\nSolutionto Problem 26-4\\nThissolutionisalsopostedpublicly\\na.JustexecuteoneiterationoftheFord-Fulkersonalgorithm . Theedge .u; \\x17/inE\\nwith increased capacity ensures that the edge .u; \\x17/is in the residual network.\\nSolook for an augmenting path and update the ﬂowif apath is fo und.\\nTime\\nO.VCE/DO.E/if we ﬁnd the augmenting path with either depth-ﬁrst or\\nbreadth-ﬁrst search.\\nTo see that only one iteration is needed, consider separatel y the cases in which\\n.u; \\x17/isor isnot anedge that crosses aminimumcut. If .u; \\x17/doesnot crossa\\nminimum cut, then increasing its capacity does not change th e capacity of any\\nminimum cut, and hence the value of the maximum ﬂow does not ch ange. If\\n.u; \\x17/doescrossaminimumcut, thenincreasing itscapacity by1in creases the\\ncapacity of that minimum cut by 1, and hence possibly the valu e of the maxi-\\nmum ﬂow by 1. In this case, there is either no augmenting path ( in which case\\nthere wassome other minimum cut that .u; \\x17/does not cross), or the augment-\\ning path increases ﬂow by 1. No matter what, one iteration of F ord-Fulkerson\\nsufﬁces.\\nb.Letfbe the maximum ﬂowbefore reducing c.u; \\x17/.\\nIff .u; \\x17/D0, wedon’t need to doanything.\\nIff .u; \\x17/ > 0 , we will need to update the maximum ﬂow. Assume from now\\non that f .u; \\x17/ > 0 , which in turn implies that f .u; \\x17/\\x151.\\nDeﬁne f0.x; y/Df .x; y/forall x; y2V,exceptthat f0.u; \\x17/Df .u; \\x17//NUL1.\\nAlthough f0obeys all capacity contraints, evenafter c.u; \\x17/has been reduced,\\nit is not a legal ﬂow, as it violates ﬂow conservation at u(unless uDs) and \\x17\\n(unless \\x17Dt).f0has one more unit of ﬂow entering uthan leaving u, and it\\nhas one more unit of ﬂowleaving \\x17than entering \\x17.\\nThe idea is to try to reroute this unit of ﬂow so that it goes out ofuand into \\x17\\nviasomeother path. Ifthat isnot possible, wemustreduceth eﬂowfrom stou\\nand from \\x17totby one unit.\\nLook for anaugmenting path from uto\\x17(note:notfrom stot).\\n\\x0fIf there issuch apath, augment the ﬂowalong that path.\\n\\x0fIf there is no such path, reduce the ﬂow from stouby augmenting the ﬂow\\nfrom utos. That is, ﬁnd an augmenting path u;sand augment the\\nﬂow along that path. (There deﬁnitely is such a path, because there is ﬂow\\nfrom stou.) Similarly,reducetheﬂowfrom \\x17totbyﬁndinganaugmenting\\npatht;\\x17and augmenting theﬂowalong that path.\\nTime\\nO.VCE/DO.E/if weﬁndthe paths witheither DFSor BFS.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 478}),\n",
              " Document(page_content='26-30 Solutions for Chapter 26: Maximum Flow\\nSolution to Problem 26-5\\na.The capacity of a cut is deﬁned to be the sum of the capacities o f the edges\\ncrossing it. Since the number of such edges is at most jEj, and the capacity of\\neach edge is at most C,the capacity of anycut of Gisat most CjEj.\\nb.Thecapacityofanaugmentingpathistheminimumcapacityof anyedgeonthe\\npath,sowearelookingforanaugmentingpathwhoseedges allhavecapacityat\\nleastK. Doabreadth-ﬁrst search ordepth-ﬁrst-search asusual toﬁ ndthepath,\\nconsidering only edges withresidual capacity at least K. (Treat lower-capacity\\nedges as though they don’t exist.) This search takes O.VCE/DO.E/time.\\n(Note thatjVjDO.E/in aﬂownetwork.)\\nc.MAX-FLOW-BY-SCALINGusestheFord-Fulkersonmethod. Itrepeatedlyaug-\\nments the ﬂow along an augmenting path until there are no augm enting paths\\nwith capacity at least 1. Since all the capacities are integers, and the capacity\\nofanaugmenting pathispositive, whentherearenoaugmenti ng pathswithca-\\npacity at least 1, there must be no augmenting paths whatsoever in the residua l\\nnetwork. Thus, by the max-ﬂow min-cut theorem, M AX-FLOW-BY-SCALING\\nreturns amaximum ﬂow.\\nd.\\x0fThe ﬁrst time line 4 is executed, the capacity of any edge in Gfequals its\\ncapacity in G, and by part (a) the capacity of a minimum cut of Gis at\\nmost CjEj. Initially KD2blgCc, and so 2KD2\\x012blgCcD2blgCcC1>\\n2lgCDC. Thus, the capacity of a minimum cut of Gfis initially less than\\n2KjEj.\\n\\x0fThe other times line 4 is executed, Khas just been halved, and so the ca-\\npacity of a cut of Gfis at most 2KjEjat line 4 if and only if that capacity\\nwas at most KjEjwhen the whileloop of lines 5–6 last terminated. Thus,\\nwe want to show that when line 7 isreached, the capacity of ami nimum cut\\nofGfisat most KjEj.\\nLetGfbe the residual network when line 7 is reached. When we reach\\nline 7, Gfcontains no augmenting path withcapacity at least K. Therefore,\\na maximum ﬂow f0inGfhas valuejf0j< KjEj. Then, by the max-ﬂow\\nmin-cut theorem, aminimum cut in Gfhas capacity less than KjEj.\\ne.By part (d), when line 4 is reached, the capacity of a minimum c ut of Gfis\\nat most 2KjEj, and thus the maximum ﬂow in Gfis at most 2KjEj. The\\nfollowing lemma shows that the value of a maximum ﬂow in Gequals the\\nvalue of thecurrent ﬂow finGplus the value of amaximum ﬂowin Gf.\\nLemma\\nLetfbe a ﬂow in ﬂow network G, and f0be a maximum ﬂow in the residual\\nnetwork Gf. Then f\"f0isa maximum ﬂowin G.\\nProofBythemax-ﬂow min-cut theorem, jf0jDcf.S; T /for somecut .S; T /\\nofGf, which is also a cut of G. By Lemma 26.4, jfjDf .S; T /. By\\nLemma 26.1, f\"f0is a ﬂow in Gwith valuejf\"f0jDjfjCjf0j. We', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 479}),\n",
              " Document(page_content='Solutions forChapter 26: Maximum Flow 26-31\\nwill show thatjfjCjf0jDc.S; T /which, by themax-ﬂow min-cut theorem,\\nwill prove that f\"f0isamaximum ﬂowin G.\\nWehave\\njfjCjf0jDf .S; T /Ccf.S; T /\\nD X\\nu2SX\\n\\x172Tf .u; \\x17//NULX\\nu2SX\\n\\x172Tf .\\x17; u/!\\nCX\\nu2SX\\n\\x172Tcf.u; \\x17/\\nD X\\nu2S;\\x172Tf .u; \\x17//NULX\\nu2S;\\x172Tf .\\x17; u/!\\nC0\\nB@X\\nu2S;\\x172T;\\n.u;\\x17/ 2Ec.u; \\x17//NULX\\nu2S;\\x172T;\\n.u;\\x17/ 2Ef .u; \\x17/CX\\nu2S;\\x172T;\\n.\\x17;u/ 2Ef .\\x17; u/1\\nCA:\\nNoting that .u; \\x17/62Eimplies f .u; \\x17/D0,wehave that\\nX\\nu2S;\\x172Tf .u; \\x17/DX\\nu2S;\\x172T;\\n.u;\\x17/ 2Ef .u; \\x17/ :\\nSimilarly,\\nX\\nu2S;\\x172Tf .\\x17; u/DX\\nu2S;\\x172T;\\n.\\x17;u/ 2Ef .\\x17; u/ :\\nThus, the summations of f .u; \\x17/cancel each other out, as do the summations\\noff .\\x17; u/. Therefore,\\njfjCjf0jDX\\nu2S;\\x172T;\\n.u;\\x17/ 2Ec.u; \\x17/\\nDX\\nu2SX\\n\\x172Tc.u; \\x17/\\nDc.S; T / : (lemma)\\nBythislemma,weseethatthevalueofamaximumﬂowin Gisatmost 2KjEj\\nmorethanthevalueofthecurrent ﬂow finG. Everytimetheinner whileloop\\nﬁnds an augmenting path of capacity at least K, the ﬂow in Gincreases by at\\nleastK. Sincetheﬂowcannot increase bymorethan 2KjEj,theloopexecutes\\nat most .2KjEj/=KD2jEjtimes.\\nf.The time complexity is dominated by the whileloop of lines 4–7. (The lines\\noutside the loop take O.E/time.) The outer whileloop executes O.lgC /\\ntimes, since Kis initially O.C /and is halved on each iteration, until K < 1.\\nBypart (e),theinner whileloop executes O.E/timesfor eachvalueof K,and\\nbypart (b), eachiteration takes O.E/time. Thus, thetotal timeis O.E2lgC /.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 480}),\n",
              " Document(page_content='SolutionsforChapter 27:\\nMultithreadedAlgorithms\\nSolutionto Exercise 27.1-1\\nThere will be no change in the asymptotic work, span, or paral lelism of P-F IB\\neven if we were to spawn the recursive call to P-F IB.n/NUL2/. The serialization of\\nP-FIBunderconsiderationwouldyieldthesamerecurrenceasthat forFIB;wecan,\\ntherefore, calculate the work as T1.n/D‚.\\x1en/. Similarly, because the spawned\\ncalls to P-F IB.n/NUL1/and P-F IB.n/NUL2/can run in parallel, we can calculate the\\nspan in exactly the same way as in the text, T1.n/D‚.n/, resulting in ‚.\\x1en=n/\\nparallelism.\\nSolutionto Exercise 27.1-5\\nBy the work law for PD4, we have 80DT4\\x15T1=4, orT1\\x14320. By the span\\nlawfor PD64,wehave T1\\x14T64D10. Nowwewilluseinequality (27.5)from\\nExercise 27.1-3 toderive acontradiction. For PD10, wehave\\n42DT10\\n\\x14320/NULT1\\n10CT1\\nD32C9\\n10T1\\nor, equivalently,\\nT1\\x1510\\n9\\x0110\\n> 10 ;\\nwhich contradicts T1\\x1410.\\nTherefore, therunning times reported by the professor ares uspicious.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 481}),\n",
              " Document(page_content='27-2 Solutions for Chapter 27: Multithreaded Algorithms\\nSolution to Exercise27.1-6\\nFAST-MAT-VEC.A; x/\\nnDA:rows\\nletybeanew vector of length n\\nparallel for iD1ton\\nyiD0\\nparallel for iD1ton\\nyiDMAT-SUB-LOOP.A; x; i; 1; n/\\nreturn y\\nMAT-SUB-LOOP.A; x; i; j; j0/\\nifj==j0\\nreturn aijxj\\nelsemidDb.jCj0/=2c\\nlhalfDspawnMAT-SUB-LOOP.A; x; i; j; mid/\\nuhalfDMAT-SUB-LOOP.A; x; i;midC1; j0/\\nsync\\nreturnlhalfCuhalf\\nWe calculate the work T1.n/of FAST-MAT-VECby computing the running time\\nof its serialization, i.e., by replacing the parallel for loop by an ordinary forloop.\\nTherefore, wehave T1.n/Dn T0\\n1.n/,where T0\\n1.n/denotestheworkof M AT-SUB-\\nLOOPto compute a given output entry yi. The work of M AT-SUB-LOOPis given\\nby the recurrence\\nT0\\n1.n/D2T0\\n1.n=2/C‚.1/ :\\nBy applying case 1 of the master theorem, we have T0\\n1.n/D‚.n/. Therefore,\\nT1.n/D‚.n2/.\\nTocalculate thespan, weuse\\nT1.n/D‚.lgn/Cmax\\n1\\x14i\\x14niter 1.i/ :\\nNote that each iteration of the second parallel for loop calls procedure M AT-\\nSUB-LOOPwith the same parameters, except for the index i. Because M AT-SUB-\\nLOOPrecursively halves the space between its last two parameter s (1andn),does\\nconstant-time workinthebasecase, andspawnsoneoftherec ursivecallsinparal-\\nlel with the other, it has span ‚.lgn/. The procedure F AST-MAT-VEC, therefore,\\nhas aspan of ‚.lgn/and‚.n2=lgn/parallelism.\\nSolution to Exercise27.1-7\\nWe analyze the work of P-T RANSPOSE , as usual, by computing the running time\\nof its serialization, where we replace both the parallel for loops with simple for', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 482}),\n",
              " Document(page_content='Solutions forChapter 27: Multithreaded Algorithms 27-3\\nloops. Wecan compute the workof P-T RANSPOSE using the summation\\nT1.n/D‚ nX\\njD2.j/NUL1/!\\nD‚ n/NUL1X\\njD1j!\\nD‚.n2/ :\\nThespanofP-T RANSPOSE isdeterminedbythespanofthedoublynested parallel\\nforloops. Althoughthenumberofiterationsoftheinnerloopde pendsonthevalue\\nof the variable jof the outer loop, each iteration of the inner loop does const ant\\nwork. Let iter 1.j /denote the span of the jth iteration of the outer loop and\\niter0\\n1.i/denote the span of the ith iteration of the inner loop. Wecharacterize the\\nspanT1.n/of P-TRANSPOSE as\\nT1.n/D‚.lgn/Cmax\\n2\\x14j\\x14niter 1.j / :\\nThemaximum occurs when jDn, and inthis case,\\niter 1.n/D‚.lgn/Cmax\\n1\\x14i\\x14n/NUL1iter0\\n1.i/ :\\nAs we noted, each iteration of the inner loop does constant wo rk, and therefore\\niter0\\n1.i/D‚.1/for all i. Thus, wehave\\nT1.n/D‚.lgn/C‚.lgn/C‚.1/\\nD‚.lgn/ :\\nSince the work P-T RANSPOSE is‚.n2/and its span is ‚.lgn/, the parallelism of\\nP-TRANSPOSE is‚.n2=lgn/.\\nSolutionto Exercise 27.1-8\\nIfweweretoreplacetheinner parallelfor loopofP-T RANSPOSE withanordinary\\nforloop, the work would still remain ‚.n2/. The span, however, would increase\\nto‚.n/because the last iteration of the parallel for loop, which dominates the\\nspan of the computation, would lead to .n/NUL1/iterations of the inner, serial for\\nloop. Theparallelism, therefore, would reduce to ‚.n2/=‚.n/D‚.n/.\\nSolutionto Exercise 27.1-9\\nBased on the values of work and span given for the two versions of the chess\\nprogram, wesolve for Pusing\\n2048\\nPC1D1024\\nPC8 :\\nThesolution gives Pbetween 146and147.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 483}),\n",
              " Document(page_content='27-4 Solutions for Chapter 27: Multithreaded Algorithms\\nSolution to Exercise27.2-3\\nP-FAST-MATRIX-MULTIPLY .A; B/\\nnDA:rows\\nletCbea new n\\x02nmatrix\\nparallel for iD1ton\\nparallel for jD1ton\\ncijDMATRIX-MULT-SUBLOOP .A; B; i; j; 1; n/\\nreturn C\\nMATRIX-MULT-SUBLOOP .A; B; i; j; k; k0/\\nifk==k0\\nreturn aikbkj\\nelsemidDb.kCk0/=2c\\nlhalfDspawnMATRIX-MULT-SUBLOOP .A; B; i; j; k; mid/\\nuhalfDMATRIX-MULT-SUBLOOP .A; B; i; j; midC1; k0/\\nsync\\nreturnlhalfCuhalf\\nWe calculate the work T1.n/of P-FAST-MATRIX-MULTIPLY by computing the\\nrunningtimeofitsserialization, i.e.,byreplacingthe parallelfor loopsbyordinary\\nforloops. Therefore, we have T1.n/Dn2T0\\n1.n/, where T0\\n1.n/denotes the work\\nof MATRIX-MULT-SUBLOOP to compute a given output entry cij. The work of\\nMATRIX-MULT-SUBLOOP isgiven by therecurrence\\nT0\\n1.n/D2T0\\n1.n=2/C‚.1/ :\\nBy applying case 1 of the master theorem, we have T0\\n1.n/D‚.n/. Therefore,\\nT1.n/D‚.n3/.\\nTocalculate thespan, weuse\\nT1.n/D‚.lgn/Cmax\\n1\\x14i\\x14niter 1.i/ :\\nNote that each iteration of the outer parallel for loop does the same amount of\\nwork: it calls the inner parallel for loop. Similarly, each iteration of the inner\\nparallel for loop calls procedure M ATRIX-MULT-SUBLOOP with the same pa-\\nrameters, except for the indices iandj. Because M ATRIX-MULT-SUBLOOP re-\\ncursivelyhalvesthespacebetweenitslasttwoparameters( 1andn),doesconstant-\\ntimeworkinthebasecase,andspawnsoneoftherecursivecal lsinparallelwiththe\\nother,ithasspan ‚.lgn/. Sinceeachiterationoftheinner parallelfor loop,which\\nhasniterations, hasspan ‚.lgn/,theinner parallel for loophasspan ‚.lgn/. By\\nsimilar logic, the outer parallel for loop, and hence procedure P-F AST-MATRIX-\\nMULTIPLY, has span ‚.lgn/and‚.n3=lgn/parallelism.\\nSolution to Exercise27.2-4\\nWe can efﬁciently multiply a p\\x02qmatrix by a q\\x02rmatrix in parallel by using\\nthe solution to Exercise 27.2-3 as a base. We will replace the upper limits of the', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 484}),\n",
              " Document(page_content='Solutions forChapter 27: Multithreaded Algorithms 27-5\\nnestedparallel for loops with pandrrespectively and we will pass qas the last\\nargument tothecall of M ATRIX-MULT-SUBLOOP. Wepresent thepseudocode for\\namultithreaded algorithmformultiplyinga p\\x02qmatrixbya q\\x02rmatrixinproce-\\ndure P-G EN-MATRIX-MULTIPLY below. Because the pseudocode for procedure\\nMATRIX-MULT-SUBLOOP(whichP-G EN-MATRIX-MULTIPLY calls)remainsthe\\nsameas waspresented inthe solution toExercise 27.2-3, wed onot repeat it here.\\nP-GEN-MATRIX-MULTIPLY .A; B/\\npDA:rows\\nqDA:columns\\nrDB:columns\\nletCbe anew p\\x02rmatrix\\nparallel for iD1top\\nparallel for jD1tor\\ncijDMATRIX-MULT-SUBLOOP .A; B; i; j; 1; q/\\nreturn C\\nTocalculatetheworkforP-G EN-MATRIX-MULTIPLY, wereplacethe parallelfor\\nloops with ordinary forloops. As before, we can calculate the work of M ATRIX-\\nMULT-SUBLOOP to be ‚.q/(because the input size to the procedure is qhere).\\nTherefore, thework of P-G EN-MATRIX-MULTIPLY isT1D‚.pqr/.\\nWecan analyze the span of P-G EN-MATRIX-MULTIPLY aswedid inthe solution\\nto Exercise 27.2-3, but we must take into account the differe nt number of loop\\niterations. Eachofthe piterations oftheouter parallel for loopexecutestheinner\\nparallel for loop, and each of the riterations of the inner parallel for loop calls\\nMATRIX-MULT-SUBLOOP, whose span is given by ‚.lgq/. We know that, in\\ngeneral, the span of a parallel for loop with niterations, where the ith iteration\\nhas spaniter 1.i/is given by\\nT1D‚.lgn/Cmax\\n1\\x14i\\x14niter 1.i/ :\\nBased on the above observations, we can calculate the span of P-GEN-MATRIX-\\nMULTIPLY as\\nT1D‚.lgp/C‚.lgr/C‚.lgq/\\nD‚.lg.pqr// :\\nThe parallelism of the procedure is, therefore, given by ‚.pqr=lg.pqr//. To\\ncheck whether this analysis is consistent with Exercise 27. 2-3, we note that if\\npDqDrDn, then the parallelism of P-G EN-MATRIX-MULTIPLY would\\nbe‚.n3=lgn3/D‚.n3=lgn/.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 485}),\n",
              " Document(page_content='27-6 Solutions for Chapter 27: Multithreaded Algorithms\\nSolution to Exercise27.2-5\\nP-MATRIX-TRANSPOSE -RECURSIVE .A; r; c; s/\\n//Transpose the s\\x02ssubmatrix starting at arc.\\nifs==1\\nreturn\\nelses0Dbs=2c\\nspawnP-MATRIX-TRANSPOSE -RECURSIVE .A; r; c; s0/\\nspawnP-MATRIX-TRANSPOSE -RECURSIVE .A; rCs0; cCs0; s/NULs0/\\nP-MATRIX-TRANSPOSE -SWAP.A; r; cCs0; rCs0; c; s0; s/NULs0/\\nsync\\nP-MATRIX-TRANSPOSE -SWAP.A; r 1; c1; r2; c2; s1; s2/\\n//Transpose the s1\\x02s2submatrix starting at ar1c1withthe s2\\x02s1submatrix\\n//starting at ar2c2.\\nifs1< s 2\\nP-MATRIX-TRANSPOSE -SWAP.A; r 2; c2; r1; c1; s2; s1/\\nelseif s1==1//since s1\\x15s2, must have that s2equals 1\\nexchange ar1;c1withar2;c2\\nelses0Dbs1=2c\\nspawnP-MATRIX-TRANSPOSE -SWAP.A; r 2; c2; r1; c1; s2; s0/\\nP-MATRIX-TRANSPOSE -SWAP.A; r 2; c2Cs0; r1Cs0; c1; s2; s1/NULs0/\\nsync\\nIn order to transpose an n\\x02nmatrix A, we call P-M ATRIX-TRANSPOSE -\\nRECURSIVE (A; 1; 1; n).\\nLet us ﬁrst calculate the work and span of P-M ATRIX-TRANSPOSE -SWAPso that\\nwe can plug in these values into the work and span calculation s of P-M ATRIX-\\nTRANSPOSE -RECURSIVE . The work T0\\n1.N /of P-M ATRIX-TRANSPOSE -SWAP\\nonan N-element matrixistherunning timeof itsserialization. We have therecur-\\nrence\\nT0\\n1.N /D2T0\\n1.N=2/C‚.1/\\nD‚.N / :\\nThespan T0\\n1.N /issimilarly described bythe recurrence\\nT0\\n1.N /DT0\\n1.N=2/C‚.1/\\nD‚.lgN / :\\nInorder tocalculate theworkof P-M ATRIX-TRANSPOSE -RECURSIVE , wecalcu-\\nlate the running time of its serialization. Let T1.N /be the work of the algorithm\\non an N-element matrix, where NDn2, and assume for simplicity that nis an\\nexact power of 2. Because the procedure makes two recursive calls with squar e\\nsubmatrices of sizes n=2\\x02n=2DN=4and because it does ‚.n2/D‚.N /work\\nto swap all the elements of the other two submatrices of size n=2\\x02n=2, its work\\nis given bythe recurrence\\nT1.N /D2T1.N=4/C‚.N /\\nD‚.N / :', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 486}),\n",
              " Document(page_content='Solutions forChapter 27: Multithreaded Algorithms 27-7\\nThe two parallel recursive calls in P-M ATRIX-TRANSPOSE -RECURSIVE execute\\nonmatricesof size n=2\\x02n=2. Thespanof theprocedure isgivenbymaximum of\\nthe span of one of these two recursive calls and the ‚.lgN /span of P-M ATRIX-\\nTRANSPOSE -SWAP, plus ‚.1/. Since the recurrence\\nT1.N /DT1.N=4/C‚.1/\\nhas the solution T1.N /D‚.lgN /by case 2 of Theorem 4.1, the span of the\\nrecursive call is asymptotically the same as the span of P-M ATRIX-TRANSPOSE -\\nSWAP, and hence the span of P-M ATRIX-TRANSPOSE -RECURSIVE is‚.lgN /.\\nThus, P-M ATRIX-TRANSPOSE -RECURSIVE has parallelism ‚.N=lgN /D\\n‚.n2=lgn/.\\nSolutionto Exercise 27.2-6\\nP-FLOYD-WARSHALL .W /\\nnDW:rows\\nparallel for iD1ton\\nparallel for jD1ton\\ndijDwij\\nforkD1ton\\nparallel for iD1ton\\nparallel for jD1ton\\ndijDmin.dij; dikCdkj/\\nreturn D\\nByExercise 25.2-4, wecan compute all the dijvalues inparallel.\\nTheworkofP-F LOYD-WARSHALL isthesameastherunningtimeofitsserializa-\\ntion, which wecomputed as ‚.n3/in Section 25.2. Thespan of the doubly nested\\nparallel for loops, which do constant work inside, is ‚.lgn/. Note, however, that\\nthesecond set of doubly nested parallel for loops isexecuted within each of the n\\niterations of the outermost serial forloop. Therefore, P-F LOYD-WARSHALL has\\nspan‚.nlgn/and‚.n2=lgn/parallelism.\\nSolutionto Problem 27-1\\na.Similar to M AT-VEC-MAIN-LOOP, the required procedure, which we name\\nNESTED-SUM-ARRAYS, will take parameters iandjto specify the range of\\nthe array that is being computed in parallel. In order to perf orm the pairwise\\naddition of two n-element arrays AandBand store the result into array C, we\\ncall NESTED-SUM-ARRAYS(A,B,C,1,A:length).', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 487}),\n",
              " Document(page_content='27-8 Solutions for Chapter 27: Multithreaded Algorithms\\nNESTED-SUM-ARRAYS .A; B; C; i; j /\\nifi==j\\nC Œi\\x8dDAŒi\\x8dCBŒi\\x8d\\nelsekDb.iCj /=2cspawnNESTED-SUM-ARRAYS .A; B; C; i; k/\\nNESTED-SUM-ARRAYS .A; B; C; kC1; j /\\nsync\\nThework of N ESTED-SUM-ARRAYSisgiven by the recurrence\\nT1.n/D2T1.n=2/C‚.1/\\nD‚.n/ ;\\nby case 1 of the master theorem. The span of the procedure is gi ven by the\\nrecurrence\\nT1.n/DT1.n=2/C‚.1/\\nD‚.lgn/ ;\\nbycase2ofthemastertheorem. Therefore, theabovealgorit hm has ‚.n=lgn/\\nparallelism.\\nb.Because A DD-SUBARRAY is serial, wecan calculate both itswork and span to\\nbe‚.j/NULiC1/,whichbasedonthearguments fromthecallinS UM-ARRAYS0\\nis‚.grain-size/, for all but the last call (which is O.grain-size/).\\nIfgrain-sizeD1, the procedure S UM-ARRAYS0calculates rto be n, and each\\nofthe niterationsoftheserial forloopspawnsA DD-SUBARRAY withthesame\\nvalue, kC1, for the last two arguments. For example, when kD0, the last\\ntwoarguments to A DD-SUBARRAY are1,when kD1,thelast twoarguments\\nare2, and so on. That is, in each call to A DD-SUBARRAY , itsforloop iterates\\nonce and calculates a single value in the array C. Whengrain-sizeD1, the\\nforloop in S UM-ARRAYS0iterates ntimes and each iteration takes ‚.1/time,\\nresulting in ‚.n/work.\\nAlthough the forloop in S UM-ARRAYS0looks serial, note that each iteration\\nspawnsthecalltoA DD-SUBARRAY andtheprocedurewaitsforallitsspawned\\nchildrenattheendofthe forloop. Thatis,allloopiterations of S UM-ARRAYS0\\nexecute in parallel. Therefore, one might be tempted to say t hat the span of\\nSUM-ARRAYS0isequaltothespanofasinglecalltoA DD-SUBARRAY plusthe\\nconstant workdone bytheﬁrstthreelines in S UM-ARRAYS0,giving ‚.1/span\\nand‚.n/parallelism. Thiscalculationofspanandparallelismwoul dbewrong,\\nhowever, because there are rspawns of A DD-SUBARRAY in SUM-ARRAYS0,\\nwhere ris not a constant. Hence, we must add a ‚.r/term to the span of\\nSUM-ARRAYS0in order to account for the overhead of spawning rcalls to\\nADD-SUBARRAY .\\nBased on the above discussion, the span of S UM-ARRAYS0is‚.r/C\\n‚.grain-size/C‚.1/. When grain-sizeD1, we get rDn; therefore,\\nSUM-ARRAYS0has‚.n/span and ‚.1/parallelism.\\nc.Forageneral grain-size,eachiterationofthe forloopinS UM-ARRAYS0except\\nfor the last results in grain-sizeiterations of the forloop in A DD-SUBARRAY .\\nIn the last iteration of S UM-ARRAYS0, theforloop in A DD-SUBARRAY iter-\\natesnmodgrain-sizetimes. Therefore, we can claim that the span of A DD-\\nSUBARRAY is‚.max.grain-size; nmodgrain-size//D‚.grain-size/.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 488}),\n",
              " Document(page_content='Solutions forChapter 27: Multithreaded Algorithms 27-9\\nSUM-ARRAYS0achievesmaximumparallelism whenitsspan,givenby ‚.r/C\\n‚.grain-size/C‚.1/, is minimum. Since rDdn=grain-sizee, the minimum\\noccurs when r\\x19grain-size,i.e., when grain-size\\x19pn.\\nSolutionto Problem 27-2\\na.We initialize the output matrix Cusing doubly nested parallel for loops and\\nthen call P-M ATRIX-MULTIPLY-RECURSIVE0,deﬁned below.\\nP-MATRIX-MULTIPLY-LESS-MEM.C; A; B/\\nnDA:rows\\nparallel for iD1ton\\nparallel for jD1ton\\ncijD0\\nP-MATRIX-MULTIPLY-RECURSIVE0.C; A; B/\\nP-MATRIX-MULTIPLY-RECURSIVE0.C; A; B/\\nnDA:rows\\nifn==1\\nc11Dc11Ca11b11\\nelsepartition A,B, and Cinton=2\\x02n=2submatrices\\nA11; A12; A21; A22;B11; B12; B21; B22; and C11; C12; C21; C22\\nspawnP-MATRIX-MULTIPLY-RECURSIVE0.C11; A11; B11/\\nspawnP-MATRIX-MULTIPLY-RECURSIVE0.C12; A11; B12/\\nspawnP-MATRIX-MULTIPLY-RECURSIVE0.C21; A21; B11/\\nP-MATRIX-MULTIPLY-RECURSIVE0.C22; A21; B12/\\nsync\\nspawnP-MATRIX-MULTIPLY-RECURSIVE0.C11; A12; B21/\\nspawnP-MATRIX-MULTIPLY-RECURSIVE0.C12; A12; B22/\\nspawnP-MATRIX-MULTIPLY-RECURSIVE0.C21; A22; B21/\\nP-MATRIX-MULTIPLY-RECURSIVE0.C22; A22; B22/\\nsync\\nb.The procedure P-M ATRIX-MULTIPLY-LESS-MEMperforms ‚.n2/work in\\nthe doubly nested parallel for loops, and then it calls the procedure\\nP-MATRIX-MULTIPLY-RECURSIVE0. The recurrence for the work M0\\n1.n/of\\nP-MATRIX-MULTIPLY-RECURSIVE0is8M0\\n1.n=2/C‚.1/, which gives us\\nM0\\n1.n/D‚.n3/. Therefore, T1.n/D‚.n3/.\\nThe span of the doubly nested parallel for loops that initialize the out-\\nput array Cis‚.lgn/. In P-M ATRIX-MULTIPLY-RECURSIVE0, there are\\ntwo groups of spawned recursive calls; therefore, the span M0\\n1.n/of\\nP-MATRIX-MULTIPLY-RECURSIVE0is given by the recurrence M0\\n1.n/D\\n2M0\\n1.n=2/C‚.1/, which gives us M0\\n1.n/D‚.n/. Because the span ‚.n/\\nof P-M ATRIX-MULTIPLY-RECURSIVE0dominates, wehave T1.n/D‚.n/.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 489}),\n",
              " Document(page_content='27-10 Solutions for Chapter 27: Multithreaded Algorithms\\nc.The parallelism of P-M ATRIX-MULTIPLY-LESS-MEMis‚.n3=n/D‚.n2/.\\nIgnoringtheconstantsinthe ‚-notation, theparallelism formultiplying 1000\\x02\\n1000matrices is 10002D106, which is only a factor of 10less than that\\nof P-M ATRIX-MULTIPLY-RECURSIVE . Although the parallelism of the new\\nprocedure is much less than that of P-M ATRIX-MULTIPLY-RECURSIVE , the\\nalgorithm still scales well for alarge number of processors .\\nSolution to Problem 27-4\\na.Hereis amultithreaded ˝-reduction algorithm:\\nP-REDUCE .x; i; j /\\nifi==j\\nreturn xŒi\\x8d\\nelsemidDb.iCj /=2c\\nlhDspawnP-REDUCE .x; i;mid/\\nrhDP-REDUCE .x;midC1; j /\\nsync\\nreturnlh˝rh\\nIfwedenotethelength j/NULiC1ofthesubarray xŒi : : j \\x8dbyn,thentheworkfor\\nthe above algorithm is given by the recurrence T1.n/D2T1.n=2/C‚.1/D\\n‚.n/. Because one of the recursive calls to P-R EDUCEis spawned and the\\nproceduredoesconstantworkfollowingtherecursivecalls andinthebasecase,\\nthe span isgiven by the recurrence T1.n/DT1.n=2/C‚.1/D‚.lgn/.\\nb.The work and span of P-S CAN-1-AUXdominate the work and span of P-\\nSCAN-1. We can calculate the work of P-S CAN-1-AUXby replacing the par-\\nallel forloop with an ordinary forloop and noting that in each iteration, the\\nrunning time of P-R EDUCEwill be equal to ‚.l/. Since P-S CAN-1 calls P-\\nSCAN-1-AUXwith 1andnas the last two arguments, the running time of\\nP-SCAN-1, and hence itswork, is ‚.1C2C\\x01\\x01\\x01C n/D‚.n2/.\\nAs we noted earlier, the parallel for loop in P-S CAN-1-AUXundergoes nit-\\nerations; therefore, the span of P-S CAN-1-AUXis given by ‚.lgn/for the\\nrecursive splitting of the loop iterations plus the span of t he iteration that has\\nmaximum span. Among the loop iterations, the call to P-R EDUCEin the last\\niteration (when lDn) has the maximum span, equal to ‚.lgn/. Thus, P-\\nSCAN-1 has ‚.lgn/span and ‚.n2=lgn/parallelism.\\nc.In P-SCAN-2-AUX, before the parallel for loop in lines 7 and 8 executes,\\nthe following invariant is satisﬁed: yŒl\\x8dDxŒi\\x8d˝xŒiC1\\x8d˝\\x01\\x01\\x01˝ xŒl\\x8dfor\\nlDi; iC1; : : : ; kandyŒl\\x8dDxŒkC1\\x8d˝xŒkC2\\x8d˝\\x01\\x01\\x01˝ xŒl\\x8dforlD\\nkC1; kC2; : : : ; j. Theparallelfor loopneednotupdate yŒi\\x8d; : : : ; yŒk\\x8d ,since\\nthey have the correct values after the call to P-S CAN-2-AUX.x; y; i; k/ . For\\nlDkC1; kC2; : : : ; j, theparallel for loop sets\\nyŒl\\x8dDyŒk\\x8d˝yŒl\\x8d\\nDxŒi\\x8d˝\\x01\\x01\\x01˝ xŒk\\x8d˝xŒkC1\\x8d˝\\x01\\x01\\x01˝ xŒl\\x8d', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 490}),\n",
              " Document(page_content='Solutions forChapter 27: Multithreaded Algorithms 27-11\\nDxŒi\\x8d˝\\x01\\x01\\x01˝ xŒl\\x8d ;\\nas desired. We can run this loop in parallel because the lth iteration depends\\nonly on the values of yŒk\\x8d, which is the same in all iterations, and yŒl\\x8d. There-\\nfore, when the call to P-S CAN-2-AUXfrom P-S CAN-2 returns, array yrepre-\\nsents the˝-preﬁx computation of array x.\\nBecause the work and span of P-S CAN-2-AUXdominate the work and span\\nof P-SCAN-2, we will concentrate on calculating these values for P-S CAN-2-\\nAUXworking on an array of size n. The work PS2A 1.n/of P-SCAN-2-AUX\\nis given by the recurrence PS2A 1.n/D2PS2A 1.n=2/C‚.n/, which equals\\n‚.nlgn/bycase2ofthemastertheorem. Thespan PS2A 1.n/ofP-SCAN-2-\\nAUXisgivenbytherecurrence PS2A 1.n/DPS2A 1.n=2/C‚.lgn/,which\\nequals ‚.lg2n/per Exercise 4.6-2. That is, the work, span, and parallelism of\\nP-SCAN-2 are ‚.nlgn/,‚.lg2n/, and ‚.n=lgn/,respectively.\\nd.The missing expression in line 8 of P-S CAN-UPistŒk\\x8d˝right. The missing\\nexpressions inlines5and6of P-S CAN-DOWNare\\x17and\\x17˝tŒk\\x8d,respectively.\\nAs suggested in the hint, we will prove that the value \\x17passed to\\nP-SCAN-DOWN.\\x17; x; t; y; i; j / satisﬁes \\x17DxŒ1\\x8d˝xŒ2\\x8d˝\\x01\\x01\\x01˝ xŒi/NUL1\\x8d,\\nsothat thevalue \\x17˝xŒi\\x8dstored into yŒi\\x8dinthebasecaseof P-S CAN-DOWNis\\ncorrect.\\nInorder tocomputethearguments that arepassed to P-S CAN-DOWN, wemust\\nﬁrst understand what tŒk\\x8dholds as aresult of the call to P-S CAN-UP. Acall to\\nP-SCAN-UP.x; t; i; j / returns xŒi\\x8d˝\\x01\\x01\\x01˝ xŒj \\x8d;because tŒk\\x8dstores thereturn\\nvalue of P-S CAN-UP.x; t; i; k/ , wecan say that tŒk\\x8dDxŒi\\x8d˝\\x01\\x01\\x01˝ xŒk\\x8d.\\nThe value \\x17DxŒ1\\x8dwhen P-S CAN-DOWN.xŒ1\\x8d; x; t; y; 2; n/ is called from\\nP-SCAN-3 clearly satisiﬁes \\x17DxŒ1\\x8d˝\\x01\\x01\\x01˝ xŒi/NUL1\\x8d. Let us suppose that\\n\\x17DxŒ1\\x8d˝xŒ2\\x8d˝\\x01\\x01\\x01˝ xŒi/NUL1\\x8din a call of P-S CAN-DOWN.\\x17; x; t; y; i; j / .\\nTherefore, \\x17meets the required condition in the ﬁrst recursive call, wit hi\\nandkas the last two arguments, in P-S CAN-DOWN. If we can prove that\\nthevalue \\x17˝tŒk\\x8dpassedtothesecondrecursivecallinP-S CAN-DOWNequals\\nxŒ1\\x8d˝xŒ2\\x8d˝\\x01\\x01\\x01˝ xŒk\\x8d,wewouldhaveprovedtherequiredconditionon \\x17for\\nall calls to P-S CAN-DOWN. Earlier, we proved that tŒk\\x8dDxŒi\\x8d˝\\x01\\x01\\x01˝ xŒk\\x8d;\\ntherefore,\\n\\x17˝tŒk\\x8dDxŒ1\\x8d˝xŒ2\\x8d˝\\x01\\x01\\x01˝ xŒi/NUL1\\x8d˝xŒi\\x8d˝\\x01\\x01\\x01 xŒk\\x8d\\nDxŒ1\\x8d˝xŒ2\\x8d˝\\x01\\x01\\x01˝ xŒk\\x8d :\\nThus,thevalue \\x17passedtoP-S CAN-DOWN.\\x17; x; t; y; i; j / satisﬁes \\x17DxŒ1\\x8d˝\\nxŒ2\\x8d˝\\x01\\x01\\x01˝ xŒi/NUL1\\x8d.\\ne.LetPSU 1.n/andPSU 1.n/denote the work and span of P-S CAN-UPand\\nletPSD 1.n/andPSD 1.n/denote the work and span of P-S CAN-DOWN.\\nThen the expressions T1.n/DPSU 1.n/CPSD 1.n/C‚.1/andT1.n/D\\nPSU 1.n/CPSD 1.n/C‚.1/characterize theworkand spanof P-S CAN-3.\\nThework PSU 1.n/of P-SCAN-UPis given bythe recurrence\\nPSU 1.n/D2PSU 1.n=2/C‚.1/ ;\\nand its span is deﬁned by therecurrence', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 491}),\n",
              " Document(page_content='27-12 Solutions for Chapter 27: Multithreaded Algorithms\\nPSU 1.n/DPSU 1.n=2/C‚.1/ :\\nUsingthemaster theorem tosolve theserecurrences, weget PSU 1.n/D‚.n/\\nandPSU 1.n/D‚.lgn/.\\nSimilarly, the recurrences\\nPSD 1.n/D2PSD 1.n=2/C‚.1/ ; (\\x03)\\nPSD 1.n/DPSD 1.n=2/C‚.1/ (\\x8e)\\ndeﬁnetheworkandspanof P-S CAN-DOWN,andtheyevaluate to PSD 1.n/D\\n‚.n/andPSD 1.n/D‚.lgn/.\\nApplyingtheresultsfortheworkandspanofP-S CAN-UPandP-S CAN-DOWN\\nobtained above in the expressions for the work and span of P-S CAN-3, we\\ngetT1.n/D‚.n/andT1.n/D‚.lgn/. Hence, P-S CAN-3 has ‚.n=lgn/\\nparallelism. P-S CAN-3 performs less work than P-S CAN-1, but with the same\\nspan, and it has the same parallelism as P-S CAN-2 with less work and a lower\\nspan.\\nSolution to Problem 27-5\\na.In this part of the problem, we will assume that nis an exact power of 2, so\\nthat inarecursive step, whenwedividethe n\\x02nmatrix Aintofour n=2\\x02n=2\\nmatrices, we will be guaranteed that n=2is an integer, for all n\\x152. We\\nmakethisassumptionsimplytoavoidintroducing bn=2canddn=2etermsinthe\\npseudocode and the analysis that follow. In the pseudocode b elow, we assume\\nthatwehaveaprocedure B ASE-CASEavailabletous,whichcalculatesthebase\\ncase of the stencil.\\nSIMPLE-STENCIL .A; i; j; n/\\nifn==1\\nAŒi; j \\x8dDBASE-CASE.A; i; j /\\nelse//Calculate submatrix A11.\\nSIMPLE-STENCIL .A; i; j; n=2/\\n//Calculate submatrices A12andA21in parallel.\\nspawnSIMPLE-STENCIL .A; i; jCn=2; n=2/\\nSIMPLE-STENCIL .A; iCn=2; j; n=2/\\nsync\\n//Calculate submatrix A22.\\nSIMPLE-STENCIL .A; iCn=2; jCn=2; n=2/\\nTo perform a simple stencil calculation on an n\\x02nmatrix A, we call\\nSIMPLE-STENCIL .A; 1; 1; n/ . The recurrence for the work is T1.n/D\\n4T1.n=2/C‚.1/D‚.n2/. Of thefour recursive calls inthe algorithm above,\\nonly two run in parallel. Therefore, the recurrence for the s pan is T1.n/D\\n3T1.n=2/C‚.1/D‚.nlg3/, and theparallelism is ‚.n2/NULlg3/\\x19‚.n0:415/.\\nb.Similar to S IMPLE-STENCILof the previous part, we present P-S TENCIL-3,\\nwhichdivides Aintoninesubmatrices, eachofsize n=3\\x02n=3,andsolvesthem', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 492}),\n",
              " Document(page_content='Solutions forChapter 27: Multithreaded Algorithms 27-13\\nrecursively. To perform a stencil calculation on an n\\x02nmatrix A, we call\\nP-STENCIL-3.A; 1; 1; n/ .\\nP-STENCIL-3.A; i; j; n/\\nifn==1\\nAŒi; j \\x8dDBASE-CASE.A; i; j /\\nelse//Group 1: compute submatrix A11.\\nP-STENCIL-3.A; i; j; n=3/\\n//Group 2: compute submatrices A12andA21.\\nspawnP-STENCIL-3.A; i; jCn=3; n=3/\\nP-STENCIL-3.A; iCn=3; j; n=3/\\nsync\\n//Group 3: compute submatrices A13,A22, and A31.\\nspawnP-STENCIL-3.A; i; jC2n=3; n=3/\\nspawnP-STENCIL-3.A; iCn=3; jCn=3; n=3/\\nP-STENCIL-3.A; iC2n=3; j; n=3/\\nsync\\n//Group 4: compute submatrices A23andA32.\\nspawnP-STENCIL-3.A; iCn=3; jC2n=3; n=3/\\nP-STENCIL-3.A; iC2n=3; jCn=3; n=3/\\nsync\\n//Group 5: compute submatrix A33.\\nP-STENCIL-3.A; iC2n=3; jC2n=3; n=3/\\nFrom the pseudocode, we can informally say that we can solve t he nine sub-\\nproblems in ﬁvegroups, as shownin the following matrix:/NUL\\n1 2 3\\n2 3 4\\n3 4 5\\x01\\n:\\nEach entry in the above matrix speciﬁes the group of the corre sponding n=3\\x02\\nn=3submatrix of A; we can compute in parallel the entries of all submatrices\\nthat fall in the same group. In general, for iD2; 3; 4; 5, we can calculate\\ngroup iafter completing the computation of group i/NUL1.\\nThe recurrence for the work is T1.n/D9T1.n=3/C‚.1/D‚.n2/. The\\nrecurrence forthespanis T1.n/D5T1.n=3/C‚.1/D‚.nlog35/. Therefore,\\nthe parallelism is ‚.n2/NULlog35/\\x19‚.n0:535/.\\nc.Similartothepreviouspart, wecansolvethe b2subproblems in 2b/NUL1groups: \\x04\\n1 2 3 \\x01\\x01\\x01 b/NUL2 b/NUL1 b\\n2 3 4 \\x01\\x01\\x01 b/NUL1 b bC1\\n3 4 5 \\x01\\x01\\x01 b bC1 bC2\\n:::::::::::::::::::::\\nb/NUL2 b/NUL1 b\\x01\\x01\\x012b/NUL5 2b/NUL4 2b/NUL3\\nb/NUL1 b bC1\\x01\\x01\\x012b/NUL4 2b/NUL3 2b/NUL2\\nb bC1 bC2\\x01\\x01\\x012b/NUL3 2b/NUL2 2b/NUL1˘\\n:', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 493}),\n",
              " Document(page_content='27-14 Solutions for Chapter 27: Multithreaded Algorithms\\nThe recurrence for the work is T1.n/Db2T1.n=b/C‚.1/D‚.n2/. The\\nrecurrenceforthespanis T1.n/D.2b/NUL1/T 1.n=b/C‚.1/D‚.nlogb.2b/NUL1//.\\nTheparallelism is ‚.n2/NULlogb.2b/NUL1//.\\nAsthe hint suggests, inorder toshow that the parallelism mu st be o.n/for any\\nchoice of b\\x152,weneed toshow that 2/NULlogb.2b/NUL1/, which istheexponent\\nofnin the parallelism, is strictly less than 1for any choice of b\\x152. Since\\nb\\x152,weknowthat 2b/NUL1 > b,whichimpliesthatlogb.2b/NUL1/ >logbbD1.\\nHence, 2/NULlogb.2b/NUL1/ < 2/NUL1D1.\\nd.Theideabehind achieving ‚.n=lgn/parallelism issimilar tothat presented in\\nthe previous part, except without recursive division. We wi ll compute AŒ1; 1\\x8d\\nserially, which will enable us tocompute entries AŒ1; 2\\x8dandAŒ2; 1\\x8din parallel,\\nafter which we can compute entries AŒ1; 3\\x8d,AŒ2; 2\\x8dandAŒ3; 1\\x8din parallel, and\\nsoon. Here isthe pseudocode:\\nP-STENCIL .A/\\nnDA:rows\\n//Calculate all entries on the antidiagonal and above it.\\nforiD1ton\\nparallel for jD1toi\\nAŒi/NULjC1; j \\x8dDBASE-CASE.A; i/NULjC1; j /\\n//Calculate all entries below the antidiagonal.\\nforiD2ton\\nparallel for jDiton\\nAŒnCi/NULj; j \\x8dDBASE-CASE.A; nCi/NULj; j /\\nFor each value of index iof the ﬁrst serial forloop, the inner loop iterates i\\ntimes, doing constant work in each iteration. Because index iranges from 1\\ntonin the ﬁrst forloop, we require ‚.1C2C\\x01\\x01\\x01C n/D‚.n2/work to\\ncalculate all entries on the antidiagonal and above it. For e ach value of index i\\nof the second serial forloop, the inner loop iterates n/NULiC1times, doing\\nconstant work in each iteration. Because index iranges from 2tonin the\\nsecondforloop, we require ‚..n/NUL1/C.n/NUL2/C\\x01\\x01\\x01C 1/D‚.n2/work\\nto calculate all entries on the antidiagonal and above it. Th erefore, the work of\\nP-STENCILisT1.n/D‚.n2/.\\nNote that both forloops in P-S TENCIL, which execute parallel for loops\\nwithin, are serial. Therefore, in order to calculate the spa n of P-S TENCIL,\\nwemust addthespansof allthe parallel for loops. Giventhat any parallel for\\nloop in P-S TENCILdoes constant workin each iteration, thespan of a parallel\\nforloop with n0iterations is ‚.lgn0/. Hence,\\nT1.n/D‚..lg1Clg2C\\x01\\x01\\x01Clgn/C.lg.n/NUL1/C\\x01\\x01\\x01C 1//\\nD‚.lg.nŠ/Clg.n/NUL1/Š/\\nD‚.nlgn/ ;\\ngiving us ‚.n=lgn/parallelism.', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 494}),\n",
              " Document(page_content='Index\\nThis index covers exercises and problems from the textbook t hat are solved in this\\nmanual. Theﬁrst page inthe manual that has thesolution isli sted here.\\nExercise 2.2-2, 2-17\\nExercise 2.2-4, 2-17\\nExercise 2.3-3, 2-17\\nExercise 2.3-4, 2-18\\nExercise 2.3-5, 2-18\\nExercise 2.3-6, 2-19\\nExercise 2.3-7, 2-19\\nExercise 3.1-1, 3-7\\nExercise 3.1-2, 3-7\\nExercise 3.1-3, 3-8\\nExercise 3.1-4, 3-8\\nExercise 3.1-8, 3-8\\nExercise 3.2-4, 3-9\\nExercise 3.2-5, 3-9\\nExercise 3.2-6, 3-10\\nExercise 3.2-7, 3-10\\nExercise 4.1-1, 4-17\\nExercise 4.1-2, 4-17\\nExercise 4.1-4, 4-17\\nExercise 4.1-5, 4-18\\nExercise 4.2-2, 4-19\\nExercise 4.2-4, 4-19\\nExercise 4.3-1, 4-20\\nExercise 4.3-7, 4-20\\nExercise 4.4-6, 4-21\\nExercise 4.4-9, 4-21\\nExercise 4.5-2, 4-22\\nExercise 5.1-3, 5-9\\nExercise 5.2-1, 5-10\\nExercise 5.2-2, 5-10\\nExercise 5.2-4, 5-11\\nExercise 5.2-5, 5-12\\nExercise 5.3-1, 5-13\\nExercise 5.3-2, 5-13Exercise 5.3-3, 5-13\\nExercise 5.3-4, 5-14\\nExercise 5.3-7, 5-14\\nExercise 5.4-6, 5-16\\nExercise 6.1-1, 6-10\\nExercise 6.1-2, 6-10\\nExercise 6.1-3, 6-10\\nExercise 6.2-6, 6-11\\nExercise 6.3-3, 6-11\\nExercise 6.4-1, 6-14\\nExercise 6.5-2, 6-15\\nExercise 6.5-6, 6-15\\nExercise 7.2-3, 7-9\\nExercise 7.2-5, 7-9\\nExercise 7.3-1, 7-10\\nExercise 7.4-2, 7-10\\nExercise 8.1-3, 8-10\\nExercise 8.1-4, 8-10\\nExercise 8.2-2, 8-11\\nExercise 8.2-3, 8-11\\nExercise 8.2-4, 8-11\\nExercise 8.3-2, 8-12\\nExercise 8.3-3, 8-12\\nExercise 8.3-4, 8-13\\nExercise 8.4-2, 8-13\\nExercise 9.1-1, 9-10\\nExercise 9.3-1, 9-10\\nExercise 9.3-3, 9-11\\nExercise 9.3-5, 9-12\\nExercise 9.3-8, 9-13\\nExercise 9.3-9, 9-14\\nExercise 11.1-4, 11-16\\nExercise 11.2-1, 11-17\\nExercise 11.2-4, 11-17', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 495}),\n",
              " Document(page_content='I-2 Index\\nExercise 11.2-6, 11-18\\nExercise 11.3-3, 11-19\\nExercise 11.3-5, 11-20\\nExercise 12.1-2, 12-15\\nExercise 12.2-5, 12-15\\nExercise 12.2-7, 12-16\\nExercise 12.3-3, 12-17\\nExercise 12.4-1, 12-12\\nExercise 12.4-2, 12-17\\nExercise 12.4-3, 12-9\\nExercise 12.4-4, 12-18\\nExercise 13.1-3, 13-13\\nExercise 13.1-4, 13-13\\nExercise 13.1-5, 13-13\\nExercise 13.2-4, 13-14\\nExercise 13.3-3, 13-14\\nExercise 13.3-4, 13-15\\nExercise 13.4-6, 13-16\\nExercise 13.4-7, 13-16\\nExercise 14.1-5, 14-9\\nExercise 14.1-6, 14-9\\nExercise 14.1-7, 14-9\\nExercise 14.2-2, 14-10\\nExercise 14.3-3, 14-13\\nExercise 14.3-6, 14-14\\nExercise 14.3-7, 14-15\\nExercise 15.1-1, 15-21\\nExercise 15.1-2, 15-21\\nExercise 15.1-3, 15-22\\nExercise 15.1-4, 15-22\\nExercise 15.1-5, 15-23\\nExercise 15.2-4, 15-23\\nExercise 15.2-5, 15-24\\nExercise 15.3-1, 15-25\\nExercise 15.3-5, 15-26\\nExercise 15.3-6, 15-27\\nExercise 15.4-4, 15-28\\nExercise 16.1-1, 16-9\\nExercise 16.1-2, 16-10\\nExercise 16.1-3, 16-11\\nExercise 16.1-4, 16-11\\nExercise 16.1-5, 16-13\\nExercise 16.2-2, 16-14\\nExercise 16.2-4, 16-16\\nExercise 16.2-6, 16-16\\nExercise 16.2-7, 16-17\\nExercise 16.3-1, 16-17\\nExercise 16.4-2, 16-17Exercise 16.4-3, 16-18\\nExercise 17.1-3, 17-14\\nExercise 17.2-1, 17-15\\nExercise 17.2-2, 17-15\\nExercise 17.2-3, 17-16\\nExercise 17.3-3, 17-17\\nExercise 21.2-3, 21-6\\nExercise 21.2-5, 21-7\\nExercise 21.2-6, 21-7\\nExercise 21.3-3, 21-7\\nExercise 21.3-4, 21-8\\nExercise 21.3-5, 21-8\\nExercise 21.4-4, 21-9\\nExercise 21.4-5, 21-9\\nExercise 21.4-6, 21-9\\nExercise 22.1-6, 22-13\\nExercise 22.1-7, 22-15\\nExercise 22.2-3, 22-15\\nExercise 22.2-5, 22-15\\nExercise 22.2-6, 22-15\\nExercise 22.2-7, 22-16\\nExercise 22.3-4, 22-16\\nExercise 22.3-5, 22-16\\nExercise 22.3-8, 22-17\\nExercise 22.3-9, 22-17\\nExercise 22.3-11, 22-17\\nExercise 22.3-12, 22-18\\nExercise 22.4-3, 22-19\\nExercise 22.4-5, 22-20\\nExercise 22.5-5, 22-21\\nExercise 22.5-6, 22-22\\nExercise 22.5-7, 22-23\\nExercise 23.1-1, 23-8\\nExercise 23.1-4, 23-8\\nExercise 23.1-6, 23-8\\nExercise 23.1-10, 23-9\\nExercise 23.2-4, 23-9\\nExercise 23.2-5, 23-10\\nExercise 23.2-7, 23-10\\nExercise 24.1-3, 24-13\\nExercise 24.2-3, 24-13\\nExercise 24.3-3, 24-14\\nExercise 24.3-4, 24-14\\nExercise 24.3-5, 24-15\\nExercise 24.3-6, 24-15\\nExercise 24.3-8, 24-16\\nExercise 24.3-9, 24-17\\nExercise 24.4-4, 24-17', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 496}),\n",
              " Document(page_content='Index I-3\\nExercise 24.4-7, 24-18\\nExercise 24.4-10, 24-18\\nExercise 24.5-4, 24-19\\nExercise 24.5-7, 24-19\\nExercise 24.5-8, 24-19\\nExercise 25.1-3, 25-9\\nExercise 25.1-5, 25-9\\nExercise 25.1-10, 25-10\\nExercise 25.2-4, 25-13\\nExercise 25.2-6, 25-13\\nExercise 25.3-4, 25-14\\nExercise 25.3-6, 25-14\\nExercise 26.1-1, 26-12\\nExercise 26.1-3, 26-13\\nExercise 26.1-4, 26-15\\nExercise 26.1-6, 26-16\\nExercise 26.1-7, 26-16\\nExercise 26.2-1, 26-17\\nExercise 26.2-8, 26-18\\nExercise 26.2-9, 26-18\\nExercise 26.2-11, 26-19\\nExercise 26.2-12, 26-20\\nExercise 26.2-13, 26-21\\nExercise 26.3-3, 26-22\\nExercise 26.4-1, 26-22\\nExercise 26.4-3, 26-23\\nExercise 26.4-4, 26-23\\nExercise 26.4-7, 26-23\\nExercise 27.1-1, 27-1\\nExercise 27.1-5, 27-1\\nExercise 27.1-6, 27-2\\nExercise 27.1-7, 27-2\\nExercise 27.1-8, 27-3\\nExercise 27.1-9, 27-3\\nExercise 27.2-3, 27-4\\nExercise 27.2-4, 27-4\\nExercise 27.2-5, 27-6\\nExercise 27.2-6, 27-7\\nProblem 2-1, 2-20\\nProblem 2-2, 2-21\\nProblem 2-4, 2-22\\nProblem 3-3, 3-10\\nProblem 4-1, 4-22\\nProblem 4-3, 4-24\\nProblem 5-1, 5-17\\nProblem 6-1, 6-15\\nProblem 6-2, 6-16Problem 7-2, 7-11\\nProblem 7-4, 7-12\\nProblem 8-1, 8-13\\nProblem 8-3, 8-16\\nProblem 8-4, 8-17\\nProblem 8-7, 8-20\\nProblem 9-1, 9-15\\nProblem 9-2, 9-16\\nProblem 9-3, 9-19\\nProblem 9-4, 9-21\\nProblem 11-1, 11-21\\nProblem 11-2, 11-22\\nProblem 11-3, 11-24\\nProblem 12-2, 12-19\\nProblem 12-3, 12-20\\nProblem 13-1, 13-16\\nProblem 14-1, 14-15\\nProblem 14-2, 14-17\\nProblem 15-1, 15-29\\nProblem 15-2, 15-31\\nProblem 15-3, 15-34\\nProblem 15-4, 15-36\\nProblem 15-5, 15-39\\nProblem 15-8, 15-42\\nProblem 15-9, 15-45\\nProblem 15-11, 15-47\\nProblem 15-12, 15-50\\nProblem 16-1, 16-20\\nProblem 16-5, 16-23\\nProblem 17-2, 17-19\\nProblem 17-4, 17-20\\nProblem 21-1, 21-10\\nProblem 21-2, 21-11\\nProblem 22-1, 22-24\\nProblem 22-3, 22-24\\nProblem 22-4, 22-27\\nProblem 23-1, 23-12\\nProblem 24-1, 24-20\\nProblem 24-2, 24-21\\nProblem 24-3, 24-22\\nProblem 24-4, 24-23\\nProblem 24-6, 24-24\\nProblem 25-1, 25-14\\nProblem 26-2, 26-24\\nProblem 26-3, 26-26\\nProblem 26-4, 26-29\\nProblem 26-5, 26-30\\nProblem 27-1, 27-7', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 497}),\n",
              " Document(page_content='I-4 Index\\nProblem 27-2, 27-9\\nProblem 27-4, 27-10\\nProblem 27-5, 27-12', metadata={'source': '/content/19908___Introduction to Algorithms.pdf', 'page': 498})]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loader2 = PyPDFLoader(\"/content/Introduction.to.Algorithms.4th.Edition.pdf\")"
      ],
      "metadata": {
        "id": "PsIu_lhSy3wd",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1702083387670,
          "user_tz": 360,
          "elapsed": 5,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = loader2.load()"
      ],
      "metadata": {
        "id": "60dqImNxy-D3",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1702083591609,
          "user_tz": 360,
          "elapsed": 203943,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JCunfJGg0FX-",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1702083592198,
          "user_tz": 360,
          "elapsed": 592,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "003137d9-b956-4cf0-a3a3-cffdecc3c0b7"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content='', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 0}),\n",
              " Document(page_content='Introduction to Algorithms\\nFourth Edition', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 1}),\n",
              " Document(page_content='Thomas H. Cormen\\nCharles E. Leiserson\\nRonald L. Rivest\\nClifford Stein\\nIntroduction to Algorithms\\nFourth Edition\\nThe MIT Press\\nCambridge, Massachusetts London, England', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 2}),\n",
              " Document(page_content='© 2022 Massachusetts Institute of Technology\\nAll rights reserved. No part of this book may be reproduced in any form or by any electronic or\\nmechanical means (including photocopying, recording, or information storage and retrieval)\\nwithout permission in writing from the publisher.\\nThe MIT Press would like to thank the anonymous peer reviewers who provided comments on\\ndrafts of this book. The generous work of academic experts is essential for establishing the\\nauthority and quality of our publications. We acknowledge with gratitude the contributions of\\nthese otherwise uncredited readers.\\nNames: Cormen, Thomas H., author. | Leiserson, Charles Eric, author. | Rivest, Ronald L.,\\nauthor. | Stein, Clifford, author.\\nTitle: Introduction to algorithms / Thomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest,\\nClifford Stein.\\nDescription: Fourth edition. | Cambridge, Massachusetts : The MIT Press, [2022] | Includes\\nbibliographical references and index.\\nIdentiﬁers: LCCN 2021037260 | ISBN 9780262367509\\nSubjects: LCSH: Computer programming. | Computer algorithms.\\nClassiﬁcation: LCC QA76.6 .C662 2022 | DDC 005.13--dc23\\nLC record available at http://lccn.loc.gov/2021037260\\n10 9 8 7 6 5 4 3 2 1\\nd_r0', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 3}),\n",
              " Document(page_content='Contents\\nCopyright\\nPreface\\nI\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0Foundations\\nIntroduction\\n1\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0T he Role of Algorithms in Computing\\n1.1\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0A lgorithms\\n1.2\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0A lgorithms as a technology\\n2\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0G etting Started\\n2.1\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0I nsertion sort\\n2.2\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0A nalyzing algorithms\\n2.3\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0D esigning algorithms\\n3\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0C haracterizing Running Times\\n3.1\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0O-notation, Ω-notation, and Θ -notation\\n3.2\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0A symptotic notation: formal deﬁnitions\\n3.3\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0S tandard notations and common functions\\n4\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0D ivide-and-Conquer\\n4.1\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0M ultiplying square matrices\\n4.2\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0S trassen’s algorithm for matrix multiplication\\n4.3\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0T he substitution method for solving recurrences\\n4.4\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0T he recursion-tree method for solving\\nrecurrences\\n4.5\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0T he master method for solving recurrences', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 4}),\n",
              " Document(page_content='\\xa0 ★\\xa0\\xa0\\xa0\\xa0\\xa04.6\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0P roof of the continuous master theorem\\n\\xa0 ★\\xa0\\xa0\\xa0\\xa0\\xa04.7\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0A kra-Bazzi recurrences\\n5\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0P robabilistic Analysis and Random ized Algorithms\\n5.1\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0T he hiring problem\\n5.2\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0I ndicator random variables\\n5.3\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0R andomized algorithms\\n\\xa0 ★\\xa0\\xa0\\xa0\\xa0\\xa05.4\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0P robabilistic analysis and further uses of\\nindicator random variables\\nII\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0Sorting and Order Statistics\\nIntroduction\\n6\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0H eapsort\\n6.1\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0H eaps\\n6.2\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0M aintaining the heap property\\n6.3\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0B uilding a heap\\n6.4\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0T he heapsort algorithm\\n6.5\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0P riority queues\\n7\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0Q uicksort\\n7.1\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0D escription of quicksort\\n7.2\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0P erformance of quicksort\\n7.3\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0A  randomized version of quicksort\\n7.4\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0A nalysis of quicksort\\n8\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0S orting in Linear Time\\n8.1\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0L ower bounds for sorting\\n8.2\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0C ounting sort\\n8.3\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0R adix sort\\n8.4\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0B ucket sort\\n9\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0M edians and Order Statistics\\n9.1\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0M inimum and maximum\\n9.2\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0S election in expected linear time\\n9.3\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0S election in worst-case linear time', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 5}),\n",
              " Document(page_content='III\\xa0\\xa0\\xa0\\xa0\\xa0Data Structures\\nIntroduction\\n10\\xa0\\xa0\\xa0\\xa0Elementary Data Structures\\n10.1\\xa0\\xa0\\xa0\\xa0Simple array-based data structures: arrays,\\nmatrices, stacks, queues\\n10.2\\xa0\\xa0\\xa0\\xa0Linked lists\\n10.3\\xa0\\xa0\\xa0\\xa0Representing rooted trees\\n11\\xa0\\xa0\\xa0\\xa0Hash Tables\\n11.1\\xa0\\xa0\\xa0\\xa0Direct-address tables\\n11.2\\xa0\\xa0\\xa0\\xa0Hash tables\\n11.3\\xa0\\xa0\\xa0\\xa0Hash functions\\n11.4\\xa0\\xa0\\xa0\\xa0Open addressing\\n11.5\\xa0\\xa0\\xa0\\xa0Practical considerations\\n12\\xa0\\xa0\\xa0\\xa0Binary Search Trees\\n12.1\\xa0\\xa0\\xa0\\xa0W hat is a binary search tree?\\n12.2\\xa0\\xa0\\xa0\\xa0Querying a binary search tree\\n12.3\\xa0\\xa0\\xa0\\xa0Insertion and deletion\\n13\\xa0\\xa0\\xa0\\xa0Red-Black Trees\\n13.1\\xa0\\xa0\\xa0\\xa0Properties of red-black trees\\n13.2\\xa0\\xa0\\xa0\\xa0Rotations\\n13.3\\xa0\\xa0\\xa0\\xa0Insertion\\n13.4\\xa0\\xa0\\xa0\\xa0Deletion\\nIV\\xa0\\xa0\\xa0\\xa0\\xa0Advanced Design and Analysis Techniques\\nIntroduction\\n14\\xa0\\xa0\\xa0\\xa0Dynamic Programming\\n14.1\\xa0\\xa0\\xa0\\xa0Rod cutting\\n14.2\\xa0\\xa0\\xa0\\xa0M atrix-chain multiplication\\n14.3\\xa0\\xa0\\xa0\\xa0Elements of dynamic programming\\n14.4\\xa0\\xa0\\xa0\\xa0Longest common subsequence\\n14.5\\xa0\\xa0\\xa0\\xa0Optimal binary search trees\\n15\\xa0\\xa0\\xa0\\xa0Greedy Algorithms', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 6}),\n",
              " Document(page_content='15.1\\xa0\\xa0\\xa0\\xa0An activity-selection problem\\n15.2\\xa0\\xa0\\xa0\\xa0Elements of the greedy strategy\\n15.3\\xa0\\xa0\\xa0\\xa0Huffman codes\\n15.4\\xa0\\xa0\\xa0\\xa0Ofﬂine caching\\n16\\xa0\\xa0\\xa0\\xa0Amortized Analysis\\n16.1\\xa0\\xa0\\xa0\\xa0Aggregate analysis\\n16.2\\xa0\\xa0\\xa0\\xa0The accounting method\\n16.3\\xa0\\xa0\\xa0\\xa0The potential method\\n16.4\\xa0\\xa0\\xa0\\xa0Dynamic tables\\nV\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0Advanced Data Structures\\nIntroduction\\n17\\xa0\\xa0\\xa0\\xa0Augmenting Data Structures\\n17.1\\xa0\\xa0\\xa0\\xa0Dynamic order statistics\\n17.2\\xa0\\xa0\\xa0\\xa0How to augment a data structure\\n17.3\\xa0\\xa0\\xa0\\xa0Interval trees\\n18\\xa0\\xa0\\xa0\\xa0B-Trees\\n18.1\\xa0\\xa0\\xa0\\xa0Deﬁnition of B-trees\\n18.2\\xa0\\xa0\\xa0\\xa0Basic operations on B-trees\\n18.3\\xa0\\xa0\\xa0\\xa0Deleting a key from a B-tree\\n19\\xa0\\xa0\\xa0\\xa0Data Structures for Disjoint Sets\\n19.1\\xa0\\xa0\\xa0\\xa0Disjoint-set operations\\n19.2\\xa0\\xa0\\xa0\\xa0Linked-list representation of disjoint sets\\n19.3\\xa0\\xa0\\xa0\\xa0Disjoint-set forests\\n\\xa0 ★\\xa0\\xa0\\xa0\\xa0\\xa019.4\\xa0\\xa0\\xa0\\xa0Analysis of union by rank with path\\ncompression\\nVI\\xa0\\xa0\\xa0\\xa0\\xa0Graph Algorithms\\nIntroduction\\n20\\xa0\\xa0\\xa0\\xa0Elementary Graph Algorithms\\n20.1\\xa0\\xa0\\xa0\\xa0Representations of graphs\\n20.2\\xa0\\xa0\\xa0\\xa0Breadth-ﬁrst search', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 7}),\n",
              " Document(page_content='20.3\\xa0\\xa0\\xa0\\xa0Depth-ﬁrst search\\n20.4\\xa0\\xa0\\xa0\\xa0Topological sort\\n20.5\\xa0\\xa0\\xa0\\xa0Strongly connected components\\n21\\xa0\\xa0\\xa0\\xa0M inimum Spanning Trees\\n21.1\\xa0\\xa0\\xa0\\xa0Growing a minimum spanning tree\\n21.2\\xa0\\xa0\\xa0\\xa0The algorithms of Kruskal and Prim\\n22\\xa0\\xa0\\xa0\\xa0Single-Source Shortest Paths\\n22.1\\xa0\\xa0\\xa0\\xa0The Bellman-Ford algorithm\\n22.2\\xa0\\xa0\\xa0\\xa0Single-source shortest paths in directed acyclic\\ngraphs\\n22.3\\xa0\\xa0\\xa0\\xa0Dijkstra’s algorithm\\n22.4\\xa0\\xa0\\xa0\\xa0Difference constraints and shortest paths\\n22.5\\xa0\\xa0\\xa0\\xa0Proofs of shortest-paths properties\\n23\\xa0\\xa0\\xa0\\xa0All-Pairs Shortest Paths\\n23.1\\xa0\\xa0\\xa0\\xa0Shortest paths and matrix multiplication\\n23.2\\xa0\\xa0\\xa0\\xa0The Floyd-Warshall algorithm\\n23.3\\xa0\\xa0\\xa0\\xa0Johnson’s algorithm for sparse graphs\\n24\\xa0\\xa0\\xa0\\xa0M aximum Flow\\n24.1\\xa0\\xa0\\xa0\\xa0Flow networks\\n24.2\\xa0\\xa0\\xa0\\xa0The Ford-Fulkerson method\\n24.3\\xa0\\xa0\\xa0\\xa0M aximum bipartite matching\\n25\\xa0\\xa0\\xa0\\xa0M atchings in Bipartite Graphs\\n25.1\\xa0\\xa0\\xa0\\xa0M aximum bipartite matching (revisited)\\n25.2\\xa0\\xa0\\xa0\\xa0The stable-marriage problem\\n25.3\\xa0\\xa0\\xa0\\xa0The Hungarian algorithm for the assignment\\nproblem\\nVII\\xa0\\xa0\\xa0Selected Topics\\nIntroduction\\n26\\xa0\\xa0\\xa0\\xa0Parallel Algorithms\\n26.1\\xa0\\xa0\\xa0\\xa0The basics of fork-join parallelism\\n26.2\\xa0\\xa0\\xa0\\xa0Parallel matrix multiplication\\n26.3\\xa0\\xa0\\xa0\\xa0Parallel merge sort', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 8}),\n",
              " Document(page_content='27\\xa0\\xa0\\xa0\\xa0Online Algorithms\\n27.1\\xa0\\xa0\\xa0\\xa0W aiting for an elevator\\n27.2\\xa0\\xa0\\xa0\\xa0M aintaining a search list\\n27.3\\xa0\\xa0\\xa0\\xa0Online caching\\n28\\xa0\\xa0\\xa0\\xa0M atrix Operations\\n28.1\\xa0\\xa0\\xa0\\xa0Solving systems of linear equations\\n28.2\\xa0\\xa0\\xa0\\xa0Inverting matrices\\n28.3\\xa0\\xa0\\xa0\\xa0Symmetric positive-deﬁnite matrices and least-\\nsquares approximation\\n29\\xa0\\xa0\\xa0\\xa0Linear Programming\\n29.1\\xa0\\xa0\\xa0\\xa0Linear programming formulations and\\nalgorithms\\n29.2\\xa0\\xa0\\xa0\\xa0Formulating problems as linear programs\\n29.3\\xa0\\xa0\\xa0\\xa0Duality\\n30\\xa0\\xa0\\xa0\\xa0Polynomials and the FFT\\n30.1\\xa0\\xa0\\xa0\\xa0Representing polynomials\\n30.2\\xa0\\xa0\\xa0\\xa0The DFT and FFT\\n30.3\\xa0\\xa0\\xa0\\xa0FFT circuits\\n31\\xa0\\xa0\\xa0\\xa0Number-Theoretic Algorithms\\n31.1\\xa0\\xa0\\xa0\\xa0Elementary number-theoretic notions\\n31.2\\xa0\\xa0\\xa0\\xa0Greatest common divisor\\n31.3\\xa0\\xa0\\xa0\\xa0M odular arithmetic\\n31.4\\xa0\\xa0\\xa0\\xa0Solving modular linear equations\\n31.5\\xa0\\xa0\\xa0\\xa0The Chinese remainder theorem\\n31.6\\xa0\\xa0\\xa0\\xa0Powers of an element\\n31.7\\xa0\\xa0\\xa0\\xa0The RSA public-key cryptosystem\\n\\xa0 ★\\xa0\\xa0\\xa0\\xa0\\xa031.8\\xa0\\xa0\\xa0\\xa0Primality testing\\n32\\xa0\\xa0\\xa0\\xa0String Matching\\n32.1\\xa0\\xa0\\xa0\\xa0The naive string-matching algorithm\\n32.2\\xa0\\xa0\\xa0\\xa0The Rabin-Karp algorithm\\n32.3\\xa0\\xa0\\xa0\\xa0String matching with ﬁnite automata\\n\\xa0 ★\\xa0\\xa0\\xa0\\xa0\\xa032.4\\xa0\\xa0\\xa0\\xa0The Knuth-Morris-Pratt algorithm\\n32.5\\xa0\\xa0\\xa0\\xa0Sufﬁx arrays', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 9}),\n",
              " Document(page_content='33\\xa0\\xa0\\xa0\\xa0M achine-Learning Algorithms\\n33.1\\xa0\\xa0\\xa0\\xa0Clustering\\n33.2\\xa0\\xa0\\xa0\\xa0M ultiplicative-weights algorithms\\n33.3\\xa0\\xa0\\xa0\\xa0Gradient descent\\n34\\xa0\\xa0\\xa0\\xa0NP-Completeness\\n34.1\\xa0\\xa0\\xa0\\xa0Polynomial time\\n34.2\\xa0\\xa0\\xa0\\xa0Polynomial-time veriﬁcation\\n34.3\\xa0\\xa0\\xa0\\xa0NP-completeness and reducibility\\n34.4\\xa0\\xa0\\xa0\\xa0NP-completeness proofs\\n34.5\\xa0\\xa0\\xa0\\xa0NP-complete problems\\n35\\xa0\\xa0\\xa0\\xa0Approximation Algorithms\\n35.1\\xa0\\xa0\\xa0\\xa0The vertex-cover problem\\n35.2\\xa0\\xa0\\xa0\\xa0The traveling-salesperson problem\\n35.3\\xa0\\xa0\\xa0\\xa0The set-covering problem\\n35.4\\xa0\\xa0\\xa0\\xa0Randomization and linear programming\\n35.5\\xa0\\xa0\\xa0\\xa0The subset-sum problem\\nVIII\\xa0\\xa0Appendix: Mathematical Background\\nIntroduction\\nA\\xa0\\xa0\\xa0\\xa0\\xa0Summations\\nA.1\\xa0\\xa0\\xa0\\xa0\\xa0Summation formulas and properties\\nA.2 Bounding summations\\nB\\xa0\\xa0\\xa0\\xa0\\xa0Sets, Etc.\\nB.1\\xa0\\xa0\\xa0\\xa0\\xa0Sets\\nB.2\\xa0\\xa0\\xa0\\xa0\\xa0R elations\\nB.3\\xa0\\xa0\\xa0\\xa0\\xa0F unctions\\nB.4\\xa0\\xa0\\xa0\\xa0\\xa0G raphs\\nB.5\\xa0\\xa0\\xa0\\xa0\\xa0Trees\\nC\\xa0\\xa0\\xa0\\xa0\\xa0C ounting and Probability\\nC.1\\xa0\\xa0\\xa0\\xa0\\xa0C ounting\\nC.2\\xa0\\xa0\\xa0\\xa0\\xa0Probability\\nC.3\\xa0\\xa0\\xa0\\xa0\\xa0D iscrete random variables\\nC.4\\xa0\\xa0\\xa0\\xa0\\xa0T he geometric and binomial distributions', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 10}),\n",
              " Document(page_content='★\\xa0\\xa0\\xa0\\xa0\\xa0C .5\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0T he tails of the binomial distribution\\nD\\xa0\\xa0\\xa0\\xa0\\xa0M atrices\\nD.1\\xa0\\xa0\\xa0\\xa0\\xa0M atrices and matrix operations\\nD.2\\xa0\\xa0\\xa0\\xa0\\xa0B asic matrix properties\\nBibliography\\nIndex', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 11}),\n",
              " Document(page_content='Preface\\nNot so long ago, anyone who had heard the word “algorithm” was\\nalmost certainly a computer scientist or mathematician. With\\ncomputers having become prevalent in our modern lives, however, the\\nterm is no longer esoteric. If you look around your home, you’ll ﬁnd\\nalgorithms running in the most mundane places: your microwave oven,\\nyour washing machine, and, of course, your computer. You ask\\nalgorithms to make recommendations to you: what music you might\\nlike or what route to take when driving. Our society, for better or for\\nworse, asks algorithms to suggest sentences for convicted criminals. You\\neven rely on algorithms to keep you alive, or at least not to kill you: the\\ncontrol systems in your car or in medical equipment.1 The word\\n“algorithm” appears somewhere in the news seemingly every day.\\nTherefore, it behooves you to understand algorithms not just as a\\nstudent or practitioner of computer science, but as a citizen of the\\nworld. Once you understand algorithms, you can educate others about\\nwhat algorithms are, how they operate, and what their limitations are.\\nThis book provides a comprehensive introduction to the modern\\nstudy of computer algorithms. It presents many algorithms and covers\\nthem in considerable depth, yet makes their design accessible to all\\nlevels of readers. All the analyses are laid out, some simple, some more\\ninvolved. We have tried to keep explanations clear without sacriﬁcing\\ndepth of coverage or mathematical rigor.\\nEach chapter presents an algorithm, a design technique, an\\napplication area, or a related topic. Algorithms are described in English\\nand in a pseudocode designed to be readable by anyone who has done a', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 12}),\n",
              " Document(page_content='little programming. The book contains 231 ﬁgures—many with multiple\\nparts—illustrating how the algorithms work. Since we emphasize\\nefﬁciency as a design criterion, we include careful analyses of the\\nrunning times of the algorithms.\\nThe text is intended primarily for use in undergraduate or graduate\\ncourses in algorithms or data structures. Because it discusses\\nengineering issues in algorithm design, as well as mathematical aspects,\\nit is equally well suited for self-study by technical professionals.\\nIn this, the fourth edition, we have once again updated the entire\\nbook. The changes cover a broad spectrum, including new chapters and\\nsections, color illustrations, and what we hope you’ll ﬁnd to be a more\\nengaging writing style.\\nTo the teacher\\nWe have designed this book to be both versatile and complete. You\\nshould ﬁnd it useful for a variety of courses, from an undergraduate\\ncourse in data structures up through a graduate course in algorithms.\\nBecause we have provided considerably more material than can ﬁt in a\\ntypical one-term course, you can select the material that best supports\\nthe course you wish to teach.\\nYou should ﬁnd it easy to organize your course around just the\\nchapters you need. We have made chapters relatively self-contained, so\\nthat you need not worry about an unexpected and unnecessary\\ndependence of one chapter on another. Whereas in an undergraduate\\ncourse, you might use only some sections from a chapter, in a graduate\\ncourse, you might cover the entire chapter.\\nWe have included 931 exercises and 162 problems. Each section ends\\nwith exercises, and each chapter ends with problems. The exercises are\\ngenerally short questions that test basic mastery of the material. Some\\nare simple self-check thought exercises, but many are substantial and\\nsuitable as assigned homework. The problems include more elaborate\\ncase studies which often introduce new material. They often consist of\\nseveral parts that lead the student through the steps required to arrive at\\na solution.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 13}),\n",
              " Document(page_content='As with the third edition of this book, we have made publicly\\navailable solutions to some, but by no means all, of the problems and\\nexercises. You can ﬁnd these solutions on our website,\\nhttp://mitpress.mit.edu/algorithms/. You will want to check this site to\\nsee whether it contains the solution to an exercise or problem that you\\nplan to assign. Since the set of solutions that we post might grow over\\ntime, we recommend that you check the site each time you teach the\\ncourse.\\nWe have starred ( ★) the sections and exercises that are more suitable\\nfor graduate students than for undergraduates. A starred section is not\\nnecessarily more difﬁcult than an unstarred one, but it may require an\\nunderstanding of more advanced mathematics. Likewise, starred\\nexercises may require an advanced background or more than average\\ncreativity.\\nTo the student\\nWe hope that this textbook provides you with an enjoyable introduction\\nto the ﬁeld of algorithms. We have attempted to make every algorithm\\naccessible and interesting. To help you when you encounter unfamiliar\\nor difﬁcult algorithms, we describe each one in a step-by-step manner.\\nWe also provide careful explanations of the mathematics needed to\\nunderstand the analysis of the algorithms and supporting ﬁgures to help\\nyou visualize what is going on.\\nSince this book is large, your class will probably cover only a portion\\nof its material. Although we hope that you will ﬁnd this book helpful to\\nyou as a course textbook now, we have also tried to make it\\ncomprehensive enough to warrant space on your future professional\\nbookshelf.\\nWhat are the prerequisites for reading this book?\\nYou need some programming experience. In particular, you\\nshould understand recursive procedures and simple data\\nstructures, such as arrays and linked lists (although Section 10.2\\ncovers linked lists and a variant that you may ﬁnd new).', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 14}),\n",
              " Document(page_content='You should have some facility with mathematical proofs, and\\nespecially proofs by mathematical induction. A few portions of\\nthe book rely on some knowledge of elementary calculus.\\nAlthough this book uses mathematics throughout, Part I and\\nAppendices A–D teach you all the mathematical techniques you\\nwill need.\\nOur website, http://mitpress.mit.edu/algorithms/, links to solutions\\nfor some of the problems and exercises. Feel free to check your solutions\\nagainst ours. We ask, however, that you not send your solutions to us.\\nTo the professional\\nThe wide range of topics in this book makes it an excellent handbook\\non algorithms. Because each chapter is relatively self-contained, you can\\nfocus on the topics most relevant to you.\\nSince most of the algorithms we discuss have great practical utility,\\nwe address implementation concerns and other engineering issues. We\\noften provide practical alternatives to the few algorithms that are\\nprimarily of theoretical interest.\\nIf you wish to implement any of the algorithms, you should ﬁnd the\\ntranslation of our pseudocode into your favorite programming language\\nto be a fairly straightforward task. We have designed the pseudocode to\\npresent each algorithm clearly and succinctly. Consequently, we do not\\naddress error handling and other software-engineering issues that\\nrequire speciﬁc assumptions about your programming environment. We\\nattempt to present each algorithm simply and directly without allowing\\nthe idiosyncrasies of a particular programming language to obscure its\\nessence. If you are used to 0-origin arrays, you might ﬁnd our frequent\\npractice of indexing arrays from 1 a minor stumbling block. You can\\nalways either subtract 1 from our indices or just overallocate the array\\nand leave position 0 unused.\\nWe understand that if you are using this book outside of a course,\\nthen you might be unable to check your solutions to problems and\\nexercises against solutions provided by an instructor. Our website,\\nhttp://mitpress.mit.edu/algorithms/, links to solutions for some of the', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 15}),\n",
              " Document(page_content='problems and exercises so that you can check your work. Please do not\\nsend your solutions to us.\\nTo our colleagues\\nWe have supplied an extensive bibliography and pointers to the current\\nliterature. Each chapter ends with a set of chapter notes that give\\nhistorical details and references. The chapter notes do not provide a\\ncomplete reference to the whole ﬁeld of algorithms, however. Though it\\nmay be hard to believe for a book of this size, space constraints\\nprevented us from including many interesting algorithms.\\nDespite myriad requests from students for solutions to problems and\\nexercises, we have adopted the policy of not citing references for them,\\nremoving the temptation for students to look up a solution rather than\\nto discover it themselves.\\nChanges for the fourth edition\\nAs we said about the changes for the second and third editions,\\ndepending on how you look at it, the book changed either not much or\\nquite a bit. A quick look at the table of contents shows that most of the\\nthird-edition chapters and sections appear in the fourth edition. We\\nremoved three chapters and several sections, but we have added three\\nnew chapters and several new sections apart from these new chapters.\\nWe kept the hybrid organization from the ﬁrst three editions. Rather\\nthan organizing chapters only by problem domains or only according to\\ntechniques, this book incorporates elements of both. It contains\\ntechnique-based chapters on divide-and-conquer, dynamic\\nprogramming, greedy algorithms, amortized analysis, augmenting data\\nstructures, NP-completeness, and approximation algorithms. But it also\\nhas entire parts on sorting, on data structures for dynamic sets, and on\\nalgorithms for graph problems. We ﬁnd that although you need to know\\nhow to apply techniques for designing and analyzing algorithms,\\nproblems seldom announce to you which techniques are most amenable\\nto solving them.\\nSome of the changes in the fourth edition apply generally across the\\nbook, and some are speciﬁc to particular chapters or sections. Here is a', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 16}),\n",
              " Document(page_content='summary of the most signiﬁcant general changes:\\nWe added 140 new exercises and 22 new problems. We also\\nimproved many of the old exercises and problems, often as the\\nresult of reader feedback. (Thanks to all readers who made\\nsuggestions.)\\nWe have color! With designers from the MIT Press, we selected a\\nlimited palette, devised to convey information and to be pleasing\\nto the eye. (We are delighted to display red-black trees in—ge t this\\n—red and black!) To enhance readability, deﬁned terms,\\npseudocode comments, and page numbers in the index are in\\ncolor.\\nPseudocode procedures appear on a tan background to make\\nthem easier to spot, and they do not necessarily appear on the\\npage of their ﬁrst reference. When they don’t, the text directs you\\nto the relevant page. In the same vein, nonlocal references to\\nnumbered equations, theorems, lemmas, and corollaries include\\nthe page number.\\nWe removed topics that were rarely taught. We dropped in their\\nentirety the chapters on Fibonacci heaps, van Emde Boas trees,\\nand computational geometry. In addition, the following material\\nwas excised: the maximum-subarray problem, implementing\\npointers and objects, perfect hashing, randomly built binary\\nsearch trees, matroids, push-relabel algorithms for maximum ﬂow,\\nthe iterative fast Fourier transform method, the details of the\\nsimplex algorithm for linear programming, and integer\\nfactorization. You can ﬁnd all the removed material on our\\nwebsite, http://mitpress.mit.edu/algorithms/.\\nWe reviewed the entire book and rewrote sentences, paragraphs,\\nand sections to make the writing clearer, more personal, and\\ngender neutral. For example, the “traveling-salesman problem” in\\nthe previous editions is now called the “traveling-salesperson\\nproblem.” We believe that it is critically important for engineering\\nand science, including our own ﬁeld of computer science, to be\\nwelcoming to everyone. (The one place that stumped us is in', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 17}),\n",
              " Document(page_content='Chapter 13, which requires a term for a parent’s sibling. Because\\nthe English language has no such gender-neutral term, we\\nregretfully stuck with “uncle.”)\\nThe chapter notes, bibliography, and index were updated,\\nreﬂecting the dramatic growth of the ﬁeld of algorithms since the\\nthird edition.\\nWe corrected errors, posting most corrections on our website of\\nthird-edition errata. Those that were reported while we were in\\nfull swing preparing this edition were not posted, but were\\ncorrected in this edition. (Thanks again to all readers who helped\\nus identify issues.)\\nThe speciﬁc changes for the fourth edition include the following:\\nWe renamed Chapter 3 and added a section giving an overview of\\nasymptotic notation before delving into the formal deﬁnitions.\\nChapter 4 underwent substantial changes to improve its\\nmathematical foundation and make it more robust and intuitive.\\nThe notion of an algorithmic recurrence was introduced, and the\\ntopic of ignoring ﬂoors and ceilings in recurrences was addressed\\nmore rigorously. The second case of the master theorem\\nincorporates polylogarithmic factors, and a rigorous proof of a\\n“continuous” version of the master theorem is now provided. We\\nalso present the powerful and general Akra-Bazzi method\\n(without proof).\\nThe deterministic order-statistic algorithm in Chapter 9 is slightly\\ndifferent, and the analyses of both the randomized and\\ndeterministic order-statistic algorithms have been revamped.\\nIn addition to stacks and queues, Section 10.1 discusses ways to\\nstore arrays and matrices.\\nChapter 11 on hash tables includes a modern treatment of hash\\nfunctions. It also emphasizes linear probing as an efﬁcient method\\nfor resolving collisions when the underlying hardware implements\\ncaching to favor local searches.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 18}),\n",
              " Document(page_content='To replace the sections on matroids in Chapter 15, we converted a\\nproblem in the third edition about ofﬂine caching into a full\\nsection.\\nSection 16.4 now contains a more intuitive explanation of the\\npotential functions to analyze table doubling and halving.\\nChapter 17 on augmenting data structures was relocated from\\nPart III to Part V, reﬂecting our view that this technique goes\\nbeyond basic material.\\nChapter 25 is a new chapter about matchings in bipartite graphs.\\nIt presents algorithms to ﬁnd a matching of maximum cardinality,\\nto solve the stable-marriage problem, and to ﬁnd a maximum-\\nweight matching (known as the “assignment problem”).\\nChapter 26, on task-parallel computing, has been updated with\\nmodern terminology, including the name of the chapter.\\nChapter 27, which covers online algorithms, is another new\\nchapter. In an online algorithm, the input arrives over time, rather\\nthan being available in its entirety at the start of the algorithm.\\nThe chapter describes several examples of online algorithms,\\nincluding determining how long to wait for an elevator before\\ntaking the stairs, maintaining a linked list via the move-to-front\\nheuristic, and evaluating replacement policies for caches.\\nIn Chapter 29, we removed the detailed presentation of the\\nsimplex algorithm, as it was math heavy without really conveying\\nmany algorithmic ideas. The chapter now focuses on the key\\naspect of how to model problems as linear programs, along with\\nthe essential duality property of linear programming.\\nSection 32.5 adds to the chapter on string matching the simple,\\nyet powerful, structure of sufﬁx arrays.\\nChapter 33, on machine learning, is the third new chapter. It\\nintroduces several basic methods used in machine learning:\\nclustering to group similar items together, weighted-majority\\nalgorithms, and gradient descent to ﬁnd the minimizer of a\\nfunction.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 19}),\n",
              " Document(page_content='Section 34.5.6 summarizes strategies for polynomial-time\\nreductions to show that problems are NP-hard.\\nThe proof of the approximation algorithm for the set-covering\\nproblem in Section 35.3 has been revised.\\nWebsite\\nYou can use our website, http://mitpress.mit.edu/algorithms/, to obtain\\nsupplementary information and to communicate with us. The website\\nlinks to a list of known errors, material from the third edition that is not\\nincluded in the fourth edition, solutions to selected exercises and\\nproblems, Python implementations of many of the algorithms in this\\nbook, a list explaining the corny professor jokes (of course), as well as\\nother content, which we may add to. The website also tells you how to\\nreport errors or make suggestions.\\nHow we produced this book\\nLike the previous three editions, the fourth edition was produced in\\nLATEX 2 ε. We used the Times font with mathematics typeset using the\\nMathTime Professional II fonts. As in all previous editions, we\\ncompiled the index using Windex, a C program that we wrote, and\\nproduced the bibliography using BIBTEX. The PDF ﬁles for this book\\nwere created on a MacBook Pro running macOS 10.14.\\nOur plea to Apple in the preface of the third edition to update\\nMacDraw Pro for macOS 10 went for naught, and so we continued to\\ndraw illustrations on pre-Intel Macs running MacDraw Pro under the\\nClassic environment of older versions of macOS 10. Many of the\\nmathematical expressions appearing in illustrations were laid in with the\\npsfrag package for LATEX 2 ε.\\nAcknowledgments for the fourth edition\\nWe have been working with the MIT Press since we started writing the\\nﬁrst edition in 1987, collaborating with several directors, editors, and\\nproduction staff. Throughout our association with the MIT Press, their', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 20}),\n",
              " Document(page_content='support has always been outstanding. Special thanks to our editors\\nMarie Lee, who put up with us for far too long, and Elizabeth Swayze,\\nwho pushed us over the ﬁnish line. Thanks also to Director Amy Brand\\nand to Alex Hoopes.\\nAs in the third edition, we were geographically distributed while\\nproducing the fourth edition, working in the Dartmouth College\\nDepartment of Computer Science; the MIT Computer Science and\\nArtiﬁcial Intelligence Laboratory and the MIT Department of\\nElectrical Engineering and Computer Science; and the Columbia\\nUniversity Department of Industrial Engineering and Operations\\nResearch, Department of Computer Science, and Data Science Institute.\\nDuring the COVID-19 pandemic, we worked largely from home. We\\nthank our respective universities and colleagues for providing such\\nsupportive and stimulating environments. As we complete this book,\\nthose of us who are not retired are eager to return to our respective\\nuniversities now that the pandemic seems to be abating.\\nJulie Sussman, P.P.A., came to our rescue once again with her\\ntechnical copy-editing under tremendous time pressure. If not for Julie,\\nthis book would be riddled with errors (or, let’s say, many more errors\\nthan it has) and would be far less readable. Julie, we will be forever\\nindebted to you. Errors that remain are the responsibility of the authors\\n(and probably were inserted after Julie read the material).\\nDozens of errors in previous editions were corrected in the process of\\ncreating this edition. We thank our readers—too many to list them all—\\nwho have reported errors and suggested improvements over the years.\\nWe received considerable help in preparing some of the new material\\nin this edition. Neville Campbell (unafﬁliated), Bill Kuszmaul of MIT,\\nand Chee Yap of NYU provided valuable advice regarding the\\ntreatment of recurrences in Chapter 4. Yan Gu of the University of\\nCalifornia, Riverside, provided feedback on parallel algorithms in\\nChapter 26. Rob Shapire of Microsoft Research altered our approach to\\nthe material on machine learning with his detailed comments on\\nChapter 33. Qi Qi of MIT helped with the analysis of the Monty Hall\\nproblem (Problem C-1).\\nMolly Seaman and Mary Reilly of the MIT Press helped us select the\\ncolor palette in the illustrations, and Wojciech Jarosz of Dartmouth', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 21}),\n",
              " Document(page_content='College suggested design improvements to our newly colored ﬁgures.\\nYichen (Annie) Ke and Linda Xiao, who have since graduated from\\nDartmouth, aided in colorizing the illustrations, and Linda also\\nproduced many of the Python implementations that are available on the\\nbook’s website.\\nFinally, we thank our wives—Wendy Leiserson, Gail Rivest, Rebecca\\nIvry, and the late Nicole Cormen—an d our families. The patience and\\nencouragement of those who love us made this project possible. We\\naffectionately dedicate this book to them.\\n\\xa0\\nTHOMAS H. CORMEN Lebanon, New Hampshire\\nCHARLES E. LEISERSON Cambridge, Massachusetts\\nRONALD L. RIVEST Cambridge, Massachusetts\\nCLIFFORD STEIN New York, New York\\nJune, 2021\\n1 To understand many of the ways in which algorithms inﬂuence our daily lives, see the book by\\nFry [162].', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 22}),\n",
              " Document(page_content='Part I\\xa0\\xa0\\xa0\\xa0Foundations', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 23}),\n",
              " Document(page_content='\\xa0\\n\\xa0\\nIntroduction\\nWhen you design and analyze algorithms, you need to be able to\\ndescribe how they operate and how to design them. You also need some\\nmathematical tools to show that your algorithms do the right thing and\\ndo it efﬁciently. This part will get you started. Later parts of this book\\nwill build upon this base.\\nChapter 1 provides an overview of algorithms and their place in\\nmodern computing systems. This chapter deﬁnes what an algorithm is\\nand lists some examples. It also makes a case for considering algorithms\\nas a technology, alongside technologies such as fast hardware, graphical\\nuser interfaces, object-oriented systems, and networks.\\nIn Chapter 2, we see our ﬁrst algorithms, which solve the problem of\\nsorting a sequence of n numbers. They are written in a pseudocode\\nwhich, although not directly translatable to any conventional\\nprogramming language, conveys the structure of the algorithm clearly\\nenough that you should be able to implement it in the language of your\\nchoice. The sorting algorithms we examine are insertion sort, which uses\\nan incremental approach, and merge sort, which uses a recursive\\ntechnique known as “divide-and-conquer.” Although the time each\\nrequires increases with the value of n, the rate of increase differs\\nbetween the two algorithms. We determine these running times in\\nChapter 2, and we develop a useful “asymptotic” notation to express\\nthem.\\nChapter 3 precisely deﬁnes asymptotic notation. We’ll use\\nasymptotic notation to bound the growth of functions—most often,', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 24}),\n",
              " Document(page_content='functions that describe the running time of algorithms—from above and\\nbelow. The chapter starts by informally deﬁning the most commonly\\nused asymptotic notations and giving an example of how to apply them.\\nIt then formally deﬁnes ﬁve asymptotic notations and presents\\nconventions for how to put them together. The rest of Chapter 3 is\\nprimarily a presentation of mathematical notation, more to ensure that\\nyour use of notation matches that in this book than to teach you new\\nmathematical concepts.\\nChapter 4 delves further into the divide-and-conquer method\\nintroduced in Chapter 2. It provides two additional examples of divide-\\nand-conquer algorithms for multiplying square matrices, including\\nStrassen’s surprising method. Chapter 4 contains methods for solving\\nrecurrences, which are useful for describing the running times of\\nrecursive algorithms. In the substitution method, you guess an answer\\nand prove it correct. Recursion trees provide one way to generate a\\nguess. Chapter 4 also presents the powerful technique of the “master\\nmethod,” which you can often use to solve recurrences that arise from\\ndivide-and-conquer algorithms. Although the chapter provides a proof\\nof a foundational theorem on which the master theorem depends, you\\nshould feel free to employ the master method without delving into the\\nproof. Chapter 4 concludes with some advanced topics.\\nChapter 5 introduces probabilistic analysis and randomized\\nalgorithms. You typically use probabilistic analysis to determine the\\nrunning time of an algorithm in cases in which, due to the presence of\\nan inherent probability distribution, the running time may differ on\\ndifferent inputs of the same size. In some cases, you might assume that\\nthe inputs conform to a known probability distribution, so that you are\\naveraging the running time over all possible inputs. In other cases, the\\nprobability distribution comes not from the inputs but from random\\nchoices made during the course of the algorithm. An algorithm whose\\nbehavior is determined not only by its input but by the values produced\\nby a random-number generator is a randomized algorithm. You can use\\nrandomized algorithms to enforce a probability distribution on the\\ninputs—thereby ensuring that no particular input always causes poor\\nperformance—or even to bound the error rate of algorithms that are\\nallowed to produce incorrect results on a limited basis.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 25}),\n",
              " Document(page_content='Appendices A–D contain other mathematical material that you will\\nﬁnd helpful as you read this book. You might have seen much of the\\nmaterial in the appendix chapters before having read this book\\n(although the speciﬁc deﬁnitions and notational conventions we use\\nmay differ in some cases from what you have seen in the past), and so\\nyou should think of the appendices as reference material. On the other\\nhand, you probably have not already seen most of the material in Part I.\\nAll the chapters in Part I and the appendices are written with a tutorial\\nﬂavor.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 26}),\n",
              " Document(page_content='1\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0The Role of Algorithms in Computing\\nWhat are algorithms? Why is the study of algorithms worthwhile? What\\nis the role of algorithms relative to other technologies used in\\ncomputers? This chapter will answer these questions.\\n1.1\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0Algorithms\\nInformally, an algorithm is any well-deﬁned computational procedure\\nthat takes some value, or set of values, as input and produces some\\nvalue, or set of values, as output in a ﬁnite amount of time. An\\nalgorithm is thus a sequence of computational steps that transform the\\ninput into the output.\\nYou can also view an algorithm as a tool for solving a well-speciﬁed\\ncomputational problem. The statement of the problem speciﬁes in\\ngeneral terms the desired input/output relationship for problem\\ninstances, typically of arbitrarily large size. The algorithm describes a\\nspeciﬁc computational procedure for achieving that input/output\\nrelationship for all problem instances.\\nAs an example, suppose that you need to sort a sequence of numbers\\ninto monotonically increasing order. This problem arises frequently in\\npractice and provides fertile ground for introducing many standard\\ndesign techniques and analysis tools. Here is how we formally deﬁne the\\nsorting problem:\\nInput: A sequence of n numbers 〈a1, a2, … , an〉.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 27}),\n",
              " Document(page_content='Output: A permutation (reordering) \\n  of the input sequence\\nsuch that \\n .\\nThus, given the input sequence 〈31, 41, 59, 26, 41, 58〉, a correct sorting\\nalgorithm returns as output the sequence 〈26, 31, 41, 41, 58, 59〉. Such\\nan input sequence is called an instance of the sorting problem. In\\ngeneral, an instance of a problem1 consists of the input (satisfying\\nwhatever constraints are imposed in the problem statement) needed to\\ncompute a solution to the problem.\\nBecause many programs use it as an intermediate step, sorting is a\\nfundamental operation in computer science. As a result, you have a\\nlarge number of good sorting algorithms at your disposal. Which\\nalgorithm is best for a given application depends on—am ong other\\nfactors—the number of items to be sorted, the extent to which the items\\nare already somewhat sorted, possible restrictions on the item values,\\nthe architecture of the computer, and the kind of storage devices to be\\nused: main memory, disks, or even—archaically—tapes.\\nAn algorithm for a computational problem is correct if, for every\\nproblem instance provided as input, it halts—ﬁnishes its computing in\\nﬁnite time—and outputs the correct solution to the problem instance. A\\ncorrect algorithm solves the given computational problem. An incorrect\\nalgorithm might not halt at all on some input instances, or it might halt\\nwith an incorrect answer. Contrary to what you might expect, incorrect\\nalgorithms can sometimes be useful, if you can control their error rate.\\nWe’ll see an example of an algorithm with a controllable error rate in\\nChapter 31 when we study algorithms for ﬁnding large prime numbers.\\nOrdinarily, however, we’ll concern ourselves only with correct\\nalgorithms.\\nAn algorithm can be speciﬁed in English, as a computer program, or\\neven as a hardware design. The only requirement is that the speciﬁcation\\nmust provide a precise description of the computational procedure to be\\nfollowed.\\nWhat kinds of problems are solved by al gorithms?', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 28}),\n",
              " Document(page_content='Sorting is by no means the only computational problem for which\\nalgorithms have been developed. (You probably suspected as much\\nwhen you saw the size of this book.) Practical applications of algorithms\\nare ubiquitous and include the following examples:\\nThe Human Genome Project has made great progress toward the\\ngoals of identifying all the roughly 30,000 genes in human DNA,\\ndetermining the sequences of the roughly 3 billion chemical base\\npairs that make up human DNA, storing this information in\\ndatabases, and developing tools for data analysis. Each of these\\nsteps requires sophisticated algorithms. Although the solutions to\\nthe various problems involved are beyond the scope of this book,\\nmany methods to solve these biological problems use ideas\\npresented here, enabling scientists to accomplish tasks while using\\nresources efﬁciently. Dynamic programming, as in Chapter 14, is\\nan important technique for solving several of these biological\\nproblems, particularly ones that involve determining similarity\\nbetween DNA sequences. The savings realized are in time, both\\nhuman and machine, and in money, as more information can be\\nextracted by laboratory techniques.\\nThe internet enables people all around the world to quickly access\\nand retrieve large amounts of information. With the aid of clever\\nalgorithms, sites on the internet are able to manage and\\nmanipulate this large volume of data. Examples of problems that\\nmake essential use of algorithms include ﬁnding good routes on\\nwhich the data travels (techniques for solving such problems\\nappear in Chapter 22), and using a search engine to quickly ﬁnd\\npages on which particular information resides (related techniques\\nare in Chapters 11 and 32).\\nElectronic commerce enables goods and services to be negotiated\\nand exchanged electronically, and it depends on the privacy of\\npersonal information such as credit card numbers, passwords, and\\nbank statements. The core technologies used in electronic\\ncommerce include public-key cryptography and digital signatures', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 29}),\n",
              " Document(page_content='(covered in Chapter 31), which are based on numerical algorithms\\nand number theory.\\nManufacturing and other commercial enterprises often need to\\nallocate scarce resources in the most beneﬁcial way. An oil\\ncompany might wish to know where to place its wells in order to\\nmaximize its expected proﬁt. A political candidate might want to\\ndetermine where to spend money buying campaign advertising in\\norder to maximize the chances of winning an election. An airline\\nmight wish to assign crews to ﬂights in the least expensive way\\npossible, making sure that each ﬂight is covered and that\\ngovernment regulations regarding crew scheduling are met. An\\ninternet service provider might wish to determine where to place\\nadditional resources in order to serve its customers more\\neffectively. All of these are examples of problems that can be\\nsolved by modeling them as linear programs, which Chapter 29\\nexplores.\\nAlthough some of the details of these examples are beyond the scope\\nof this book, we do give underlying techniques that apply to these\\nproblems and problem areas. We also show how to solve many speciﬁc\\nproblems, including the following:\\nYou have a road map on which the distance between each pair of\\nadjacent intersections is marked, and you wish to determine the\\nshortest route from one intersection to another. The number of\\npossible routes can be huge, even if you disallow routes that cross\\nover themselves. How can you choose which of all possible routes\\nis the shortest? You can start by modeling the road map (which is\\nitself a model of the actual roads) as a graph (which we will meet\\nin Part VI and Appendix B). In this graph, you wish to ﬁnd the\\nshortest path from one vertex to another. Chapter 22 shows how\\nto solve this problem efﬁciently.\\nGiven a mechanical design in terms of a library of parts, where\\neach part may include instances of other parts, list the parts in\\norder so that each part appears before any part that uses it. If the\\ndesign comprises n parts, then there are n! possible orders, where', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 30}),\n",
              " Document(page_content='n! denotes the factorial function. Because the factorial function\\ngrows faster than even an exponential function, you cannot\\nfeasibly generate each possible order and then verify that, within\\nthat order, each part appears before the parts using it (unless you\\nhave only a few parts). This problem is an instance of topological\\nsorting, and Chapter 20 shows how to solve this problem\\nefﬁciently.\\nA doctor needs to determine whether an image represents a\\ncancerous tumor or a benign one. The doctor has available images\\nof many other tumors, some of which are known to be cancerous\\nand some of which are known to be benign. A cancerous tumor is\\nlikely to be more similar to other cancerous tumors than to\\nbenign tumors, and a benign tumor is more likely to be similar to\\nother benign tumors. By using a clustering algorithm, as in\\nChapter 33, the doctor can identify which outcome is more likely.\\nYou need to compress a large ﬁle containing text so that it\\noccupies less space. Many ways to do so are known, including\\n“LZW compression,” which looks for repeating character\\nsequences. Chapter 15 studies a different approach, “Huffman\\ncoding,” which encodes characters by bit sequences of various\\nlengths, with characters occurring more frequently encoded by\\nshorter bit sequences.\\nThese lists are far from exhaustive (as you again have probably\\nsurmised from this book’s heft), but they exhibit two characteristics\\ncommon to many interesting algorithmic problems:\\n1. They have many candidate solutions, the overwhelming majority\\nof which do not solve the problem at hand. Finding one that\\ndoes, or one that is “best,” without explicitly examining each\\npossible solution, can present quite a challenge.\\n2. They have practical applications. Of the problems in the above\\nlist, ﬁnding the shortest path provides the easiest examples. A\\ntransportation ﬁrm, such as a trucking or railroad company, has\\na ﬁnancial interest in ﬁnding shortest paths through a road or', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 31}),\n",
              " Document(page_content='rail network because taking shorter paths results in lower labor\\nand fuel costs. Or a routing node on the internet might need to\\nﬁnd the shortest path through the network in order to route a\\nmessage quickly. Or a person wishing to drive from New York to\\nBoston might want to ﬁnd driving directions using a navigation\\napp.\\nNot every problem solved by algorithms has an easily identiﬁed set\\nof candidate solutions. For example, given a set of numerical values\\nrepresenting samples of a signal taken at regular time intervals, the\\ndiscrete Fourier transform converts the time domain to the frequency\\ndomain. That is, it approximates the signal as a weighted sum of\\nsinusoids, producing the strength of various frequencies which, when\\nsummed, approximate the sampled signal. In addition to lying at the\\nheart of signal processing, discrete Fourier transforms have applications\\nin data compression and multiplying large polynomials and integers.\\nChapter 30 gives an efﬁcient algorithm, the fast Fourier transform\\n(commonly called the FFT), for this problem. The chapter also sketches\\nout the design of a hardware FFT circuit.\\nData structures\\nThis book also presents several data structures. A data structure is a way\\nto store and organize data in order to facilitate access and\\nmodiﬁcations. Using the appropriate data structure or structures is an\\nimportant part of algorithm design. No single data structure works well\\nfor all purposes, and so you should know the strengths and limitations\\nof several of them.\\nTechnique\\nAlthough you can use this book as a “cookbook” for algorithms, you\\nmight someday encounter a problem for which you cannot readily ﬁnd a\\npublished algorithm (many of the exercises and problems in this book,\\nfor example). This book will teach you techniques of algorithm design\\nand analysis so that you can develop algorithms on your own, show that\\nthey give the correct answer, and analyze their efﬁciency. Different', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 32}),\n",
              " Document(page_content='chapters address different aspects of algorithmic problem solving. Some\\nchapters address speciﬁc problems, such as ﬁnding medians and order\\nstatistics in Chapter 9, computing minimum spanning trees in Chapter\\n21, and determining a maximum ﬂow in a network in Chapter 24. Other\\nchapters introduce techniques, such as divide-and-conquer in Chapters\\n2 and 4, dynamic programming in Chapter 14, and amortized analysis\\nin Chapter 16.\\nHard problems\\nMost of this book is about efﬁcient algorithms. Our usual measure of\\nefﬁciency is speed: how long does an algorithm take to produce its\\nresult? There are some problems, however, for which we know of no\\nalgorithm that runs in a reasonable amount of time. Chapter 34 studies\\nan interesting subset of these problems, which are known as NP-\\ncomplete.\\nWhy are NP-complete problems interesting? First, although no\\nefﬁcient algorithm for an NP-complete problem has ever been found,\\nnobody has ever proven that an efﬁcient algorithm for one cannot exist.\\nIn other words, no one knows whether efﬁcient algorithms exist for NP-\\ncomplete problems. Second, the set of NP-complete problems has the\\nremarkable property that if an efﬁcient algorithm exists for any one of\\nthem, then efﬁcient algorithms exist for all of them. This relationship\\namong the NP-complete problems makes the lack of efﬁcient solutions\\nall the more tantalizing. Third, several NP-complete problems are\\nsimilar, but not identical, to problems for which we do know of efﬁcient\\nalgorithms. Computer scientists are intrigued by how a small change to\\nthe problem statement can cause a big change to the efﬁciency of the\\nbest known algorithm.\\nYou should know about NP-complete problems because some of\\nthem arise surprisingly often in real applications. If you are called upon\\nto produce an efﬁcient algorithm for an NP-complete problem, you are\\nlikely to spend a lot of time in a fruitless search. If, instead, you can\\nshow that the problem is NP-complete, you can spend your time\\ndeveloping an efﬁcient approximation algorithm, that is, an algorithm\\nthat gives a good, but not necessarily the best possible, solution.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 33}),\n",
              " Document(page_content='As a concrete example, consider a delivery company with a central\\ndepot. Each day, it loads up delivery trucks at the depot and sends them\\naround to deliver goods to several addresses. At the end of the day, each\\ntruck must end up back at the depot so that it is ready to be loaded for\\nthe next day. To reduce costs, the company wants to select an order of\\ndelivery stops that yields the lowest overall distance traveled by each\\ntruck. This problem is the well-known “traveling-salesperson problem,”\\nand it is NP-complete.2 It has no known efﬁcient algorithm. Under\\ncertain assumptions, however, we know of efﬁcient algorithms that\\ncompute overall distances close to the smallest possible. Chapter 35\\ndiscusses such “approximation algorithms.”\\nAlternative computing models\\nFor many years, we could count on processor clock speeds increasing at\\na steady rate. Physical limitations present a fundamental roadblock to\\never-increasing clock speeds, however: because power density increases\\nsuperlinearly with clock speed, chips run the risk of melting once their\\nclock speeds become high enough. In order to perform more\\ncomputations per second, therefore, chips are being designed to contain\\nnot just one but several processing “cores.” We can liken these multicore\\ncomputers to several sequential computers on a single chip. In other\\nwords, they are a type of “parallel computer.” In order to elicit the best\\nperformance from multicore computers, we need to design algorithms\\nwith parallelism in mind. Chapter 26 presents a model for “task-\\nparallel” algorithms, which take advantage of multiple processing cores.\\nThis model has advantages from both theoretical and practical\\nstandpoints, and many modern parallel-programming platforms\\nembrace something similar to this model of parallelism.\\nMost of the examples in this book assume that all of the input data\\nare available when an algorithm begins running. Much of the work in\\nalgorithm design makes the same assumption. For many important real-\\nworld examples, however, the input actually arrives over time, and the\\nalgorithm must decide how to proceed without knowing what data will\\narrive in the future. In a data center, jobs are constantly arriving and\\ndeparting, and a scheduling algorithm must decide when and where to', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 34}),\n",
              " Document(page_content='run a job, without knowing what jobs will be arriving in the future.\\nTrafﬁc must be routed in the internet based on the current state, without\\nknowing about where trafﬁc will arrive in the future. Hospital\\nemergency rooms make triage decisions about which patients to treat\\nﬁrst without knowing when other patients will be arriving in the future\\nand what treatments they will need. Algorithms that receive their input\\nover time, rather than having all the input present at the start, are online\\nalgorithms, which Chapter 27 examines.\\nExercises\\n1.1-1\\nDescribe your own real-world example that requires sorting. Describe\\none that requires ﬁnding the shortest distance between two points.\\n1.1-2\\nOther than speed, what other measures of efﬁciency might you need to\\nconsider in a real-world setting?\\n1.1-3\\nSelect a data structure that you have seen, and discuss its strengths and\\nlimitations.\\n1.1-4\\nHow are the shortest-path and traveling-salesperson problems given\\nabove similar? How are they different?\\n1.1-5\\nSuggest a real-world problem in which only the best solution will do.\\nThen come up with one in which “approximately” the best solution is\\ngood enough.\\n1.1-6\\nDescribe a real-world problem in which sometimes the entire input is\\navailable before you need to solve the problem, but other times the input\\nis not entirely available in advance and arrives over time.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 35}),\n",
              " Document(page_content='1.2\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0Algorithms as a technology\\nIf computers were inﬁnitely fast and computer memory were free, would\\nyou have any reason to study algorithms? The answer is yes, if for no\\nother reason than that you would still like to be certain that your\\nsolution method terminates and does so with the correct answer.\\nIf computers were inﬁnitely fast, any correct method for solving a\\nproblem would do. You would probably want your implementation to\\nbe within the bounds of good software engineering practice (for\\nexample, your implementation should be well designed and\\ndocumented), but you would most often use whichever method was the\\neasiest to implement.\\nOf course, computers may be fast, but they are not inﬁnitely fast.\\nComputing time is therefore a bounded resource, which makes it\\nprecious. Although the saying goes, “Time is money,” time is even more\\nvaluable than money: you can get back money after you spend it, but\\nonce time is spent, you can never get it back. Memory may be\\ninexpensive, but it is neither inﬁnite nor free. You should choose\\nalgorithms that use the resources of time and space efﬁciently.\\nEfﬁciency\\nDifferent algorithms devised to solve the same problem often differ\\ndramatically in their efﬁciency. These differences can be much more\\nsigniﬁcant than differences due to hardware and software.\\nAs an example, Chapter 2 introduces two algorithms for sorting. The\\nﬁrst, known as insertion sort, takes time roughly equal to c1n2 to sort n\\nitems, where c1 is a constant that does not depend on n. That is, it takes\\ntime roughly proportional to n2. The second, merge sort, takes time\\nroughly equal to c2n lg n, where lg n stands for log2\\xa0n and c2 is another\\nconstant that also does not depend on n. Insertion sort typically has a\\nsmaller constant factor than merge sort, so that c1 < c2. We’ll see that\\nthe constant factors can have far less of an impact on the running time\\nthan the dependence on the input size n. Let’s write insertion sort’s\\nrunning time as c1n · n and merge sort’s running time as c2n · lg n. Then', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 36}),\n",
              " Document(page_content='we see that where insertion sort has a factor of n in its running time,\\nmerge sort has a factor of lg n, which is much smaller. For example,\\nwhen n is 1000, lg n is approximately 10, and when n is 1,000,000, lg n is\\napproximately only 20. Although insertion sort usually runs faster than\\nmerge sort for small input sizes, once the input size n becomes large\\nenough, merge sort’s advantage of lg n versus n more than compensates\\nfor the difference in constant factors. No matter how much smaller c1 is\\nthan c2, there is always a crossover point beyond which merge sort is\\nfaster.\\nFor a concrete example, let us pit a faster computer (computer A)\\nrunning insertion sort against a slower computer (computer B) running\\nmerge sort. They each must sort an array of 10 million numbers.\\n(Although 10 million numbers might seem like a lot, if the numbers are\\neight-byte integers, then the input occupies about 80 megabytes, which\\nﬁts in the memory of even an inexpensive laptop computer many times\\nover.) Suppose that computer A executes 10 billion instructions per\\nsecond (faster than any single sequential computer at the time of this\\nwriting) and computer B executes only 10 million instructions per\\nsecond (much slower than most contemporary computers), so that\\ncomputer A is 1000 times faster than computer B in raw computing\\npower. To make the difference even more dramatic, suppose that the\\nworld’s craftiest programmer codes insertion sort in machine language\\nfor computer A, and the resulting code requires 2n2 instructions to sort\\nn numbers. Suppose further that just an average programmer\\nimplements merge sort, using a high-level language with an inefﬁcient\\ncompiler, with the resulting code taking 50 n lg n instructions. To sort 10\\nmillion numbers, computer A takes\\nwhile computer B takes\\n', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 37}),\n",
              " Document(page_content='By using an algorithm whose running time grows more slowly, even\\nwith a poor compiler, computer B runs more than 17 times faster than\\ncomputer A! The advantage of merge sort is even more pronounced\\nwhen sorting 100 million numbers: where insertion sort takes more than\\n23 days, merge sort takes under four hours. Although 100 million might\\nseem like a large number, there are more than 100 million web searches\\nevery half hour, more than 100 million emails sent every minute, and\\nsome of the smallest galaxies (known as ultra-compact dwarf galaxies)\\ncontain about 100 million stars. In general, as the problem size\\nincreases, so does the relative advantage of merge sort.\\nAlgorithms and other technologies\\nThe example above shows that you should consider algorithms, like\\ncomputer hardware, as a technology. Total system performance depends\\non choosing efﬁcient algorithms as much as on choosing fast hardware.\\nJust as rapid advances are being made in other computer technologies,\\nthey are being made in algorithms as well.\\nYou might wonder whether algorithms are truly that important on\\ncontemporary computers in light of other advanced technologies, such\\nas\\nadvanced computer architectures and fabrication technologies,\\neasy-to-use, intuitive, graphical user interfaces (GUIs),\\nobject-oriented systems,\\nintegrated web technologies,\\nfast networking, both wired and wireless,\\nmachine learning,\\nand mobile devices.\\nThe answer is yes. Although some applications do not explicitly require\\nalgorithmic content at the application level (such as some simple, web-\\nbased applications), many do. For example, consider a web-based\\nservice that determines how to travel from one location to another. Its\\nimplementation would rely on fast hardware, a graphical user interface,', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 38}),\n",
              " Document(page_content='wide-area networking, and also possibly on object orientation. It would\\nalso require algorithms for operations such as ﬁnding routes (probably\\nusing a shortest-path algorithm), rendering maps, and interpolating\\naddresses.\\nMoreover, even an application that does not require algorithmic\\ncontent at the application level relies heavily upon algorithms. Does the\\napplication rely on fast hardware? The hardware design used\\nalgorithms. Does the application rely on graphical user interfaces? The\\ndesign of any GUI relies on algorithms. Does the application rely on\\nnetworking? Routing in networks relies heavily on algorithms. Was the\\napplication written in a language other than machine code? Then it was\\nprocessed by a compiler, interpreter, or assembler, all of which make\\nextensive use of algorithms. Algorithms are at the core of most\\ntechnologies used in contemporary computers.\\nMachine learning can be thought of as a method for performing\\nalgorithmic tasks without explicitly designing an algorithm, but instead\\ninferring patterns from data and thereby automatically learning a\\nsolution. At ﬁrst glance, machine learning, which automates the process\\nof algorithmic design, may seem to make learning about algorithms\\nobsolete. The opposite is true, however. Machine learning is itself a\\ncollection of algorithms, just under a different name. Furthermore, it\\ncurrently seems that the successes of machine learning are mainly for\\nproblems for which we, as humans, do not really understand what the\\nright algorithm is. Prominent examples include computer vision and\\nautomatic language translation. For algorithmic problems that humans\\nunderstand well, such as most of the problems in this book, efﬁcient\\nalgorithms designed to solve a speciﬁc problem are typically more\\nsuccessful than machine-learning approaches.\\nData science is an interdisciplinary ﬁeld with the goal of extracting\\nknowledge and insights from structured and unstructured data. Data\\nscience uses methods from statistics, computer science, and\\noptimization. The design and analysis of algorithms is fundamental to\\nthe ﬁeld. The core techniques of data science, which overlap signiﬁcantly\\nwith those in machine learning, include many of the algorithms in this\\nbook.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 39}),\n",
              " Document(page_content='Furthermore, with the ever-increasing capacities of computers, we\\nuse them to solve larger problems than ever before. As we saw in the\\nabove comparison between insertion sort and merge sort, it is at larger\\nproblem sizes that the differences in efﬁciency between algorithms\\nbecome particularly prominent.\\nHaving a solid base of algorithmic knowledge and technique is one\\ncharacteristic that deﬁnes the truly skilled programmer. With modern\\ncomputing technology, you can accomplish some tasks without\\nknowing much about algorithms, but with a good background in\\nalgorithms, you can do much, much more.\\nExercises\\n1.2-1\\nGive an example of an application that requires algorithmic content at\\nthe application level, and discuss the function of the algorithms\\ninvolved.\\n1.2-2\\nSuppose that for inputs of size n on a particular computer, insertion sort\\nruns in 8n2 steps and merge sort runs in 64 n lg n steps. For which\\nvalues of n does insertion sort beat merge sort?\\n1.2-3\\nWhat is the smallest value of n such that an algorithm whose running\\ntime is 100n2 runs faster than an algorithm whose running time is 2n on\\nthe same machine?\\nProblems\\n1-1\\xa0\\xa0\\xa0\\xa0\\xa0C omparison of running times\\nFor each function f (n) and time t in the following table, determine the\\nlargest size n of a problem that can be solved in time t, assuming that\\nthe algorithm to solve the problem takes f (n) microseconds.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 40}),\n",
              " Document(page_content='Chapter notes\\nThere are many excellent texts on the general topic of algorithms,\\nincluding those by Aho, Hopcroft, and Ullman [5, 6], Dasgupta,\\nPapadimitriou, and Vazirani [107], Edmonds [133], Erickson [135],\\nGoodrich and Tamassia [195, 196], Kleinberg and Tardos [257], Knuth\\n[259, 260, 261, 262, 263], Levitin [298], Louridas [305], Mehlhorn and\\nSanders [325], Mitzenmacher and Upfal [331], Neapolitan [342],\\nRoughgarden [385, 386, 387, 388], Sanders, Mehlhorn, Dietzfelbinger,\\nand Dementiev [393], Sedgewick and Wayne [402], Skiena [414], Soltys-\\nKulinicz [419], Wilf [455], and Williamson and Shmoys [459]. Some of\\nthe more practical aspects of algorithm design are discussed by Bentley\\n[49, 50, 51], Bhargava [54], Kochenderfer and Wheeler [268], and\\nMcGeoch [321]. Surveys of the ﬁeld of algorithms can also be found in\\nbooks by Atallah and Blanton [27, 28] and Mehta and Sahhi [326]. For\\nless technical material, see the books by Christian and Grifﬁths [92],\\nCormen [104], Erwig [136], MacCormick [307], and Vöcking et al. [448].\\nOverviews of the algorithms used in computational biology can be\\nfound in books by Jones and Pevzner [240], Elloumi and Zomaya [134],\\nand Marchisio [315].', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 41}),\n",
              " Document(page_content='1 Sometimes, when the problem context is known, problem instances are themselves simply\\ncalled “problems.”\\n2 To be precise, only decision problems—those with a “yes/no” answer—can be NP-complete.\\nThe decision version of the traveling salesperson problem asks whether there exists an order of\\nstops whose distance totals at most a given amount.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 42}),\n",
              " Document(page_content='2\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0Getting Started\\nThis chapter will familiarize you with the framework we’ll use\\nthroughout the book to think about the design and analysis of\\nalgorithms. It is self-contained, but it does include several references to\\nmaterial that will be introduced in Chapters 3 and 4. (It also contains\\nseveral summations, which Appendix A shows how to solve.)\\nWe’ll begin by examining the insertion sort algorithm to solve the\\nsorting problem introduced in Chapter 1. We’ll specify algorithms using\\na pseudocode that should be understandable to you if you have done\\ncomputer programming. We’ll see why insertion sort correctly sorts and\\nanalyze its running time. The analysis introduces a notation that\\ndescribes how running time increases with the number of items to be\\nsorted. Following a discussion of insertion sort, we’ll use a method\\ncalled divide-and-conquer to develop a sorting algorithm called merge\\nsort. We’ll end with an analysis of merge sort’s running time.\\n2.1\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0Insertion sort\\nOur ﬁrst algorithm, insertion sort, solves the sorting problem introduced\\nin Chapter 1:\\nInput: A sequence of n numbers 〈a1, a2, … , an〉.\\nOutput: A permutation (reordering) \\n  of the input sequence\\nsuch that \\n .', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 43}),\n",
              " Document(page_content='The numbers to be sorted are also known as the keys. Although the\\nproblem is conceptually about sorting a sequence, the input comes in\\nthe form of an array with n elements. When we want to sort numbers,\\nit’s often because they are the keys associated with other data, which we\\ncall satellite data. Together, a key and satellite data form a record. For\\nexample, consider a spreadsheet containing student records with many\\nassociated pieces of data such as age, grade-point average, and number\\nof courses taken. Any one of these quantities could be a key, but when\\nthe spreadsheet sorts, it moves the associated record (the satellite data)\\nwith the key. When describing a sorting algorithm, we focus on the keys,\\nbut it is important to remember that there usually is associated satellite\\ndata.\\nIn this book, we’ll typically describe algorithms as procedures\\nwritten in a pseudocode that is similar in many respects to C, C++, Java,\\nPython,1 or JavaScript. (Apologies if we’ve omitted your favorite\\nprogramming language. We can’t list them all.) If you have been\\nintroduced to any of these languages, you should have little trouble\\nunderstanding algorithms “coded” in pseudocode. What separates\\npseudocode from real code is that in pseudocode, we employ whatever\\nexpressive method is most clear and concise to specify a given\\nalgorithm. Sometimes the clearest method is English, so do not be\\nsurprised if you come across an English phrase or sentence embedded\\nwithin a section that looks more like real code. Another difference\\nbetween pseudocode and real code is that pseudocode often ignores\\naspects of software engineering—s uch as data abstraction, modularity,\\nand error handling—in order to convey the essence of the algorithm\\nmore concisely.\\nWe start with insertion sort, which is an efﬁcient algorithm for\\nsorting a small number of elements. Insertion sort works the way you\\nmight sort a hand of playing cards. Start with an empty left hand and\\nthe cards in a pile on the table. Pick up the ﬁrst card in the pile and hold\\nit with your left hand. Then, with your right hand, remove one card at a\\ntime from the pile, and insert it into the correct position in your left\\nhand. As Figure 2.1 illustrates, you ﬁnd the correct position for a card\\nby comparing it with each of the cards already in your left hand,', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 44}),\n",
              " Document(page_content='starting at the right and moving left. As soon as you see a card in your\\nleft hand whose value is less than or equal to the card you’re holding in\\nyour right hand, insert the card that you’re holding in your right hand\\njust to the right of this card in your left hand. If all the cards in your left\\nhand have values greater than the card in your right hand, then place\\nthis card as the leftmost card in your left hand. At all times, the cards\\nheld in your left hand are sorted, and these cards were originally the top\\ncards of the pile on the table.\\nThe pseudocode for insertion sort is given as the procedure\\nINSERTION-SORT on the facing page. It takes two parameters: an\\narray A containing the values to be sorted and the number n of values of\\nsort. The values occupy positions A[1] through A[n] of the array, which\\nwe denote by A[1 : n]. When the INSERTION-SORT procedure is\\nﬁnished, array A[1 : n] contains the original values, but in sorted order.\\nFigure 2.1 Sorting a hand of cards using insertion sort.\\nINSERTION-SORT(A, n)\\n1for\\xa0i = 2 to\\xa0n\\n2 key = A[i]\\n3 // Insert A[i] into the sorted subarray A[1 : i – 1].\\n4 j = i – 1\\n5 while\\xa0j > 0 and A[j] > key', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 45}),\n",
              " Document(page_content='6 A[j + 1] = A[j]\\n7 j = j – 1\\n8 A[j + 1] = key\\nLoop invariants and the correctness of insertion sort\\nFigure 2.2 shows how this algorithm works for an array A that starts\\nout with the sequence 〈5, 2, 4, 6, 1, 3〉. The index i indicates the “current\\ncard” being inserted into the hand. At the beginning of each iteration of\\nthe for loop, which is indexed by i, the subarray (a contiguous portion of\\nthe array) consisting of elements A[1 : i – 1] (that is, A[1] through A[i –\\n1]) constitutes the currently sorted hand, and the remaining subarray\\nA[i + 1 : n] (elements A[i + 1] through A[n]) corresponds to the pile of\\ncards still on the table. In fact, elements A[1 : i – 1] are the elements\\noriginally in positions 1 through i – 1, but now in sorted order. We state\\nthese properties of A[1 : i – 1] formally as a loop invariant:\\nFigure 2.2 The operation of INSERTION-SORT(A, n), where A initially contains the sequence\\n〈5, 2, 4, 6, 1, 3〉 and n = 6. Array indices appear above the rectangles, and values stored in the\\narray positions appear within the rectangles. (a)–(e) The iterations of the for loop of lines 1–8. In\\neach iteration, the blue rectangle holds the key taken from A[i], which is compared with the\\nvalues in tan rectangles to its left in the test of line 5. Orange arrows show array values moved\\none position to the right in line 6, and blue arrows indicate where the key moves to in line 8. (f)\\nThe ﬁnal sorted array.\\nAt the start of each iteration of the for loop of lines 1–8, the\\nsubarray A[1 : i – 1] consists of the elements originally in A[1 : i\\n– 1], but in sorted order.\\nLoop invariants help us understand why an algorithm is correct.\\nWhen you’re using a loop invariant, you need to show three things:', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 46}),\n",
              " Document(page_content='Initialization: It is true prior to the ﬁrst iteration of the loop.\\nMaintenance: If it is true before an iteration of the loop, it remains true\\nbefore the next iteration.\\nTermination: The loop terminates, and when it terminates, the invariant\\n—usually along with the reason that the loop terminated—gives us a\\nuseful property that helps show that the algorithm is correct.\\nWhen the ﬁrst two properties hold, the loop invariant is true prior to\\nevery iteration of the loop. (Of course, you are free to use established\\nfacts other than the loop invariant itself to prove that the loop invariant\\nremains true before each iteration.) A loop-invariant proof is a form of\\nmathematical induction, where to prove that a property holds, you\\nprove a base case and an inductive step. Here, showing that the\\ninvariant holds before the ﬁrst iteration corresponds to the base case,\\nand showing that the invariant holds from iteration to iteration\\ncorresponds to the inductive step.\\nThe third property is perhaps the most important one, since you are\\nusing the loop invariant to show correctness. Typically, you use the loop\\ninvariant along with the condition that caused the loop to terminate.\\nMathematical induction typically applies the inductive step inﬁnitely,\\nbut in a loop invariant the “induction” stops when the loop terminates.\\nLet’s see how these properties hold for insertion sort.\\nInitialization: We start by showing that the loop invariant holds before\\nthe ﬁrst loop iteration, when i = 2.2 The subarray A[1 : i – 1] consists\\nof just the single element A[1], which is in fact the original element in\\nA[1]. Moreover, this subarray is sorted (after all, how could a subarray\\nwith just one value not be sorted?), which shows that the loop\\ninvariant holds prior to the ﬁrst iteration of the loop.\\nMaintenance: Next, we tackle the second property: showing that each\\niteration maintains the loop invariant. Informally, the body of the for\\nloop works by moving the values in A[i – 1], A[i – 2], A[i – 3], and so\\non by one position to the right until it ﬁnds the proper position for\\nA[i] (lines 4–7), at which point it inserts the value of A[i] (line 8). The\\nsubarray A[1 : i] then consists of the elements originally in A[1 : i], but', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 47}),\n",
              " Document(page_content='in sorted order. Incrementing\\xa0i (increasing its value by 1) for the next\\niteration of the for loop then preserves the loop invariant.\\nA more formal treatment of the second property would require us to\\nstate and show a loop invariant for the while loop of lines 5–7. Let’s\\nnot get bogged down in such formalism just yet. Instead, we’ll rely on\\nour informal analysis to show that the second property holds for the\\nouter loop.\\nTermination: Finally, we examine loop termination. The loop variable i\\nstarts at 2 and increases by 1 in each iteration. Once i’s value exceeds n\\nin line 1, the loop terminates. That is, the loop terminates once i\\nequals n + 1. Substituting n + 1 for i in the wording of the loop\\ninvariant yields that the subarray A[1 : n] consists of the elements\\noriginally in A[1 : n], but in sorted order. Hence, the algorithm is\\ncorrect.\\nThis method of loop invariants is used to show correctness in various\\nplaces throughout this book.\\nPseudocode conventions\\nWe use the following conventions in our pseudocode.\\nIndentation indicates block structure. For example, the body of\\nthe for loop that begins on line 1 consists of lines 2–8, and the\\nbody of the while loop that begins on line 5 contains lines 6–7 but\\nnot line 8. Our indentation style applies to if-else statements3 as\\nwell. Using indentation instead of textual indicators of block\\nstructure, such as begin and end statements or curly braces,\\nreduces clutter while preserving, or even enhancing, clarity.4\\nThe looping constructs while, for, and repeat-until and the if-else\\nconditional construct have interpretations similar to those in C,\\nC++, Java, Python, and JavaScript.5 In this book, the loop\\ncounter retains its value after the loop is exited, unlike some\\nsituations that arise in C++ and Java. Thus, immediately after a\\nfor loop, the loop counter’s value is the value that ﬁrst exceeded', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 48}),\n",
              " Document(page_content='the for loop bound.6 We used this property in our correctness\\nargument for insertion sort. The for loop header in line 1 is for\\xa0i =\\n2 to\\xa0n, and so when this loop terminates, i equals n + 1. We use the\\nkeyword to when a for loop increments its loop counter in each\\niteration, and we use the keyword downto when a for loop\\ndecrements its loop counter (reduces its value by 1 in each\\niteration). When the loop counter changes by an amount greater\\nthan 1, the amount of change follows the optional keyword by.\\nThe symbol “//” indicates that the remainder of the line is a\\ncomment.\\nVariables (such as i, j, and key) are local to the given procedure.\\nWe won’t use global variables without explicit indication.\\nWe access array elements by specifying the array name followed\\nby the index in square brackets. For example, A[i] indicates the ith\\nelement of the array A.\\nAlthough many programming languages enforce 0-origin indexing\\nfor arrays (0 is the smallest valid index), we choose whichever\\nindexing scheme is clearest for human readers to understand.\\nBecause people usually start counting at 1, not 0, most—but not\\nall—of the arrays in this book use 1-origin indexing. To be clear\\nabout whether a particular algorithm assumes 0-origin or 1-origin\\nindexing, we’ll specify the bounds of the arrays explicitly. If you\\nare implementing an algorithm that we specify using 1-origin\\nindexing, but you’re writing in a programming language that\\nenforces 0-origin indexing (such as C, C++, Java, Python, or\\nJavaScript), then give yourself credit for being able to adjust. You\\ncan either always subtract 1 from each index or allocate each\\narray with one extra position and just ignore position 0.\\nThe notation “:” denotes a subarray. Thus, A[i : j] indicates the\\nsubarray of A consisting of the elements A[i], A[i + 1], … , A[j].7\\nWe also use this notation to indicate the bounds of an array, as we\\ndid earlier when discussing the array A[1 : n].', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 49}),\n",
              " Document(page_content='We typically organize compound data into objects, which are\\ncomposed of attributes. We access a particular attribute using the\\nsyntax found in many object-oriented programming languages:\\nthe object name, followed by a dot, followed by the attribute\\nname. For example, if an object x has attribute f, we denote this\\nattribute by x.f.\\nWe treat a variable representing an array or object as a pointer\\n(known as a reference in some programming languages) to the\\ndata representing the array or object. For all attributes f of an\\nobject x, setting y = x causes y.f to equal x.f. Moreover, if we now\\nset x.f = 3, then afterward not only does x.f equal 3, but y.f equals\\n3 as well. In other words, x and y point to the same object after\\nthe assignment y = x. This way of treating arrays and objects is\\nconsistent with most contemporary programming languages.\\nOur attribute notation can “cascade.” For example, suppose that\\nthe attribute f is itself a pointer to some type of object that has an\\nattribute g. Then the notation x.f.g is implicitly parenthesized as\\n(x.f).g. In other words, if we had assigned y = x.f, then x.f.g is the\\nsame as y.g.\\nSometimes a pointer refers to no object at all. In this case, we give\\nit the special value NIL.\\nWe pass parameters to a procedure by value: the called procedure\\nreceives its own copy of the parameters, and if it assigns a value to\\na parameter, the change is not seen by the calling procedure. When\\nobjects are passed, the pointer to the data representing the object\\nis copied, but the object’s attributes are not. For example, if x is a\\nparameter of a called procedure, the assignment x = y within the\\ncalled procedure is not visible to the calling procedure. The\\nassignment x.f = 3, however, is visible if the calling procedure has\\na pointer to the same object as x. Similarly, arrays are passed by\\npointer, so that a pointer to the array is passed, rather than the\\nentire array, and changes to individual array elements are visible\\nto the calling procedure. Again, most contemporary programming\\nlanguages work this way.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 50}),\n",
              " Document(page_content='A return statement immediately transfers control back to the\\npoint of call in the calling procedure. Most return statements also\\ntake a value to pass back to the caller. Our pseudocode differs\\nfrom many programming languages in that we allow multiple\\nvalues to be returned in a single return statement without having\\nto create objects to package them together.8\\nThe boolean operators “and” and “or” are short circuiting. That\\nis, evaluate the expression “x and y” by ﬁrst evaluating x. If x\\nevaluates to FALSE, then the entire expression cannot evaluate to\\nTRUE, and therefore y is not evaluated. If, on the other hand, x\\nevaluates to TRUE, y must be evaluated to determine the value of\\nthe entire expression. Similarly, in the expression “x or y” the\\nexpression y is evaluated only if x evaluates to FALSE. Short-\\ncircuiting operators allow us to write boolean expressions such as\\n“x ≠ NIL and x.f = y” without worrying about what happens\\nupon evaluating x.f when x is NIL.\\nThe keyword error indicates that an error occurred because\\nconditions were wrong for the procedure to have been called, and\\nthe procedure immediately terminates. The calling procedure is\\nresponsible for handling the error, and so we do not specify what\\naction to take.\\nExercises\\n2.1-1\\nUsing Figure 2.2 as a model, illustrate the operation of INSERTION-\\nSORT on an array initially containing the sequence 〈31, 41, 59, 26, 41,\\n58〉.\\n2.1-2\\nConsider the procedure SUM-ARRAY on the facing page. It computes\\nthe sum of the n numbers in array A[1 : n]. State a loop invariant for this\\nprocedure, and use its initialization, maintenance, and termination\\nproperties to show that the SUM-ARRAY procedure returns the sum of\\nthe numbers in A[1 : n].', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 51}),\n",
              " Document(page_content='SUM-ARRAY(A, n)\\n1sum = 0\\n2for\\xa0i = 1 to\\xa0n\\n3 sum = sum + A[i]\\n4return\\xa0sum\\n2.1-3\\nRewrite the INSERTION-SORT procedure to sort into monotonically\\ndecreasing instead of monotonically increasing order.\\n2.1-4\\nConsider the searching problem:\\nInput: A sequence of n numbers 〈a1, a2, … , an〉 stored in array A[1 : n]\\nand a value x.\\nOutput: An index i such that x equals A[i] or the special value NIL if x\\ndoes not appear in A.\\nWrite pseudocode for linear search, which scans through the array\\nfrom beginning to end, looking for x. Using a loop invariant, prove that\\nyour algorithm is correct. Make sure that your loop invariant fulﬁlls the\\nthree necessary properties.\\n2.1-5\\nConsider the problem of adding two n-bit binary integers a and b,\\nstored in two n-element arrays A[0 : n – 1] and B[0 : n – 1], where each\\nelement is either 0 or 1, \\n , and \\n . The\\nsum c = a + b of the two integers should be stored in binary form in an\\n(n + 1)-element array C [0 : n], where \\n . Write a procedure\\nADD-BINARY-INTEGERS that takes as input arrays A and B, along\\nwith the length n, and returns array C holding the sum.\\n2.2\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0Analyzing algorithms', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 52}),\n",
              " Document(page_content='Analyzing an algorithm has come to mean predicting the resources that\\nthe algorithm requires. You might consider resources such as memory,\\ncommunication bandwidth, or energy consumption. Most often,\\nhowever, you’ll want to measure computational time. If you analyze\\nseveral candidate algorithms for a problem, you can identify the most\\nefﬁcient one. There might be more than just one viable candidate, but\\nyou can often rule out several inferior algorithms in the process.\\nBefore you can analyze an algorithm, you need a model of the\\ntechnology that it runs on, including the resources of that technology\\nand a way to express their costs. Most of this book assumes a generic\\none-processor, random-access machine (RAM) model of computation\\nas the implementation technology, with the understanding that\\nalgorithms are implemented as computer programs. In the RAM model,\\ninstructions execute one after another, with no concurrent operations.\\nThe RAM model assumes that each instruction takes the same amount\\nof time as any other instruction and that each data access—using the\\nvalue of a variable or storing into a variable—takes the same amount of\\ntime as any other data access. In other words, in the RAM model each\\ninstruction or data access takes a constant amount of time—even\\nindexing into an array.9\\nStrictly speaking, we should precisely deﬁne the instructions of the\\nRAM model and their costs. To do so, however, would be tedious and\\nyield little insight into algorithm design and analysis. Yet we must be\\ncareful not to abuse the RAM model. For example, what if a RAM had\\nan instruction that sorts? Then you could sort in just one step. Such a\\nRAM would be unrealistic, since such instructions do not appear in real\\ncomputers. Our guide, therefore, is how real computers are designed.\\nThe RAM model contains instructions commonly found in real\\ncomputers: arithmetic (such as add, subtract, multiply, divide,\\nremainder, ﬂoor, ceiling), data movement (load, store, copy), and\\ncontrol (conditional and unconditional branch, subroutine call and\\nreturn).\\nThe data types in the RAM model are integer, ﬂoating point (for\\nstoring real-number approximations), and character. Real computers do\\nnot usually have a separate data type for the boolean values TRUE and', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 53}),\n",
              " Document(page_content='FALSE. Instead, they often test whether an integer value is 0 (FALSE)\\nor nonzero (TRUE), as in C. Although we typically do not concern\\nourselves with precision for ﬂoating-point values in this book (many\\nnumbers cannot be represented exactly in ﬂoating point), precision is\\ncrucial for most applications. We also assume that each word of data\\nhas a limit on the number of bits. For example, when working with\\ninputs of size n, we typically assume that integers are represented by c\\nlog2\\xa0n bits for some constant c ≥ 1. We require c ≥ 1 so that each word\\ncan hold the value of n, enabling us to index the individual input\\nelements, and we restrict c to be a constant so that the word size does\\nnot grow arbitrarily. (If the word size could grow arbitrarily, we could\\nstore huge amounts of data in one word and operate on it all in\\nconstant time—an unrealistic scenario.)\\nReal computers contain instructions not listed above, and such\\ninstructions represent a gray area in the RAM model. For example, is\\nexponentiation a constant-time instruction? In the general case, no: to\\ncompute xn when x and n are general integers typically takes time\\nlogarithmic in n (see equation (31.34) on page 934), and you must worry\\nabout whether the result ﬁts into a computer word. If n is an exact\\npower of 2, however, exponentiation can usually be viewed as a\\nconstant-time operation. Many computers have a “shift left”\\ninstruction, which in constant time shifts the bits of an integer by n\\npositions to the left. In most computers, shifting the bits of an integer\\nby 1 position to the left is equivalent to multiplying by 2, so that shifting\\nthe bits by n positions to the left is equivalent to multiplying by 2n.\\nTherefore, such computers can compute 2n in 1 constant-time\\ninstruction by shifting the integer 1 by n positions to the left, as long as\\nn is no more than the number of bits in a computer word. We’ll try to\\navoid such gray areas in the RAM model and treat computing 2n and\\nmultiplying by 2n as constant-time operations when the result is small\\nenough to ﬁt in a computer word.\\nThe RAM model does not account for the memory hierarchy that is\\ncommon in contemporary computers. It models neither caches nor\\nvirtual memory. Several other computational models attempt to', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 54}),\n",
              " Document(page_content='account for memory-hierarchy effects, which are sometimes signiﬁcant\\nin real programs on real machines. Section 11.5 and a handful of\\nproblems in this book examine memory-hierarchy effects, but for the\\nmost part, the analyses in this book do not consider them. Models that\\ninclude the memory hierarchy are quite a bit more complex than the\\nRAM model, and so they can be difﬁcult to work with. Moreover,\\nRAM-model analyses are usually excellent predictors of performance\\non actual machines.\\nAlthough it is often straightforward to analyze an algorithm in the\\nRAM model, sometimes it can be quite a challenge. You might need to\\nemploy mathematical tools such as combinatorics, probability theory,\\nalgebraic dexterity, and the ability to identify the most signiﬁcant terms\\nin a formula. Because an algorithm might behave differently for each\\npossible input, we need a means for summarizing that behavior in\\nsimple, easily understood formulas.\\nAnalysis of insertion sort\\nHow long does the INSERTION-SORT procedure take? One way to tell\\nwould be for you to run it on your computer and time how long it takes\\nto run. Of course, you’d ﬁrst have to implement it in a real programming\\nlanguage, since you cannot run our pseudocode directly. What would\\nsuch a timing test tell you? You would ﬁnd out how long insertion sort\\ntakes to run on your particular computer, on that particular input,\\nunder the particular implementation that you created, with the\\nparticular compiler or interpreter that you ran, with the particular\\nlibraries that you linked in, and with the particular background tasks\\nthat were running on your computer concurrently with your timing test\\n(such as checking for incoming information over a network). If you run\\ninsertion sort again on your computer with the same input, you might\\neven get a different timing result. From running just one\\nimplementation of insertion sort on just one computer and on just one\\ninput, what would you be able to determine about insertion sort’s\\nrunning time if you were to give it a different input, if you were to run it\\non a different computer, or if you were to implement it in a different', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 55}),\n",
              " Document(page_content='programming language? Not much. We need a way to predict, given a\\nnew input, how long insertion sort will take.\\nInstead of timing a run, or even several runs, of insertion sort, we\\ncan determine how long it takes by analyzing the algorithm itself. We’ll\\nexamine how many times it executes each line of pseudocode and how\\nlong each line of pseudocode takes to run. We’ll ﬁrst come up with a\\nprecise but complicated formula for the running time. Then, we’ll distill\\nthe important part of the formula using a convenient notation that can\\nhelp us compare the running times of different algorithms for the same\\nproblem.\\nHow do we analyze insertion sort? First, let’s acknowledge that the\\nrunning time depends on the input. You shouldn’t be terribly surprised\\nthat sorting a thousand numbers takes longer than sorting three\\nnumbers. Moreover, insertion sort can take different amounts of time to\\nsort two input arrays of the same size, depending on how nearly sorted\\nthey already are. Even though the running time can depend on many\\nfeatures of the input, we’ll focus on the one that has been shown to have\\nthe greatest effect, namely the size of the input, and describe the\\nrunning time of a program as a function of the size of its input. To do\\nso, we need to deﬁne the terms “running time” and “input size” more\\ncarefully. We also need to be clear about whether we are discussing the\\nrunning time for an input that elicits the worst-case behavior, the best-\\ncase behavior, or some other case.\\nThe best notion for input size depends on the problem being studied.\\nFor many problems, such as sorting or computing discrete Fourier\\ntransforms, the most natural measure is the number of items in the input\\n—for example, the number n of items being sorted. For many other\\nproblems, such as multiplying two integers, the best measure of input\\nsize is the total number of bits needed to represent the input in ordinary\\nbinary notation. Sometimes it is more appropriate to describe the size of\\nthe input with more than just one number. For example, if the input to\\nan algorithm is a graph, we usually characterize the input size by both\\nthe number of vertices and the number of edges in the graph. We’ll\\nindicate which input size measure is being used with each problem we\\nstudy.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 56}),\n",
              " Document(page_content='The running time of an algorithm on a particular input is the number\\nof instructions and data accesses executed. How we account for these\\ncosts should be independent of any particular computer, but within the\\nframework of the RAM model. For the moment, let us adopt the\\nfollowing view. A constant amount of time is required to execute each\\nline of our pseudocode. One line might take more or less time than\\nanother line, but we’ll assume that each execution of the kth line takes\\nck time, where ck is a constant. This viewpoint is in keeping with the\\nRAM model, and it also reﬂects how the pseudocode would be\\nimplemented on most actual computers.10\\nLet’s analyze the INSERTION-SORT procedure. As promised, we’ll\\nstart by devising a precise formula that uses the input size and all the\\nstatement costs ck. This formula turns out to be messy, however. We’ll\\nthen switch to a simpler notation that is more concise and easier to use.\\nThis simpler notation makes clear how to compare the running times of\\nalgorithms, especially as the size of the input increases.\\nTo analyze the INSERTION-SORT procedure, let’s view it on the\\nfollowing page with the time cost of each statement and the number of\\ntimes each statement is executed. For each i = 2, 3, … , n, let ti denote\\nthe number of times the while loop test in line 5 is executed for that\\nvalue of i. When a for or while loop exits in the usual way—because the\\ntest in the loop header comes up FALSE—the test is executed one time\\nmore than the loop body. Because comments are not executable\\nstatements, assume that they take no time.\\nThe running time of the algorithm is the sum of running times for\\neach statement executed. A statement that takes ck steps to execute and\\nexecutes m times contributes ckm to the total running time.11 We\\nusually denote the running time of an algorithm on an input of size n by\\nT (n). To compute T (n), the running time of INSERTION-SORT on an\\ninput of n values, we sum the products of the cost and times columns,\\nobtaining\\nINSERTION-SORT(A, n) costtimes', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 57}),\n",
              " Document(page_content='1for\\xa0i = 2 to\\xa0n c1n\\n2 key = A[i] c2n – 1\\n3 // Insert A[i] into the sorted subarray A[1 : i – 1].0n – 1\\n4 j = i – 1 c4n – 1\\n5 while\\xa0j > 0 and A[j] > key c5\\n6 A[j + 1] = A[j] c6\\n7 j = j – 1 c7\\n8 A[j + 1] = key c8n – 1\\nEven for inputs of a given size, an algorithm’s running time may\\ndepend on which input of that size is given. For example, in\\nINSERTION-SORT, the best case occurs when the array is already\\nsorted. In this case, each time that line 5 executes, the value of key—the\\nvalue originally in A[i]—is already greater than or equal to all values in\\nA[1 : i – 1], so that the while loop of lines 5–7 always exits upon the ﬁrst\\ntest in line 5. Therefore, we have that ti = 1 for i = 2, 3, … , n, and the\\nbest-case running time is given by\\nWe can express this running time as an + b for constants a and b that\\ndepend on the statement costs ck (where a = c1 + c2 + c4 + c5 + c8 and\\nb = c2 + c4 + c5 + c8). The running time is thus a linear function of n.\\nThe worst case arises when the array is in reverse sorted order—that\\nis, it starts out in decreasing order. The procedure must compare each\\nelement A[i] with each element in the entire sorted subarray A[1 : i – 1],\\nand so ti = i for i = 2, 3, … , n. (The procedure ﬁnds that A[j] > key', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 58}),\n",
              " Document(page_content='every time in line 5, and the while loop exits only when j reaches 0.)\\nNoting that\\nand\\nwe ﬁnd that in the worst case, the running time of INSERTION-SORT\\nis\\nWe can express this worst-case running time as an2 + bn + c for\\nconstants a, b, and c that again depend on the statement costs ck (now,\\na = c5/2 + c6/2 + c7/2, b = c1 + c2 + c4 + c5/2 – c6/2 – c7/2 + c8, and c\\n= –(c2 + c4 + c5 + c8)). The running time is thus a quadratic function of\\nn.\\nTypically, as in insertion sort, the running time of an algorithm is\\nﬁxed for a given input, although we’ll also see some interesting\\n“randomized” algorithms whose behavior can vary even for a ﬁxed\\ninput.\\nWorst-case and average-case analysis', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 59}),\n",
              " Document(page_content='Our analysis of insertion sort looked at both the best case, in which the\\ninput array was already sorted, and the worst case, in which the input\\narray was reverse sorted. For the remainder of this book, though, we’ll\\nusually (but not always) concentrate on ﬁnding only the worst-case\\nrunning time, that is, the longest running time for any input of size n.\\nWhy? Here are three reasons:\\nThe worst-case running time of an algorithm gives an upper\\nbound on the running time for any input. If you know it, then you\\nhave a guarantee that the algorithm never takes any longer. You\\nneed not make some educated guess about the running time and\\nhope that it never gets much worse. This feature is especially\\nimportant for real-time computing, in which operations must\\ncomplete by a deadline.\\nFor some algorithms, the worst case occurs fairly often. For\\nexample, in searching a database for a particular piece of\\ninformation, the searching algorithm’s worst case often occurs\\nwhen the information is not present in the database. In some\\napplications, searches for absent information may be frequent.\\nThe “average case” is often roughly as bad as the worst case.\\nSuppose that you run insertion sort on an array of n randomly\\nchosen numbers. How long does it take to determine where in\\nsubarray A[1 : i – 1] to insert element A[i]? On average, half the\\nelements in A[1 : i – 1] are less than A[i], and half the elements are\\ngreater. On average, therefore, A[i] is compared with just half of\\nthe subarray A[1 : i – 1], and so ti is about i/2. The resulting\\naverage-case running time turns out to be a quadratic function of\\nthe input size, just like the worst-case running time.\\nIn some particular cases, we’ll be interested in the average-case\\nrunning time of an algorithm. We’ll see the technique of probabilistic\\nanalysis applied to various algorithms throughout this book. The scope\\nof average-case analysis is limited, because it may not be apparent what\\nconstitutes an “average” input for a particular problem. Often, we’ll\\nassume that all inputs of a given size are equally likely. In practice, this\\nassumption may be violated, but we can sometimes use a randomized', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 60}),\n",
              " Document(page_content='algorithm, which makes random choices, to allow a probabilistic\\nanalysis and yield an expected running time. We explore randomized\\nalgorithms more in Chapter 5 and in several other subsequent chapters.\\nOrder of growth\\nIn order to ease our analysis of the INSERTION-SORT procedure, we\\nused some simplifying abstractions. First, we ignored the actual cost of\\neach statement, using the constants ck to represent these costs. Still, the\\nbest-case and worst-case running times in equations (2.1) and (2.2) are\\nrather unwieldy. The constants in these expressions give us more detail\\nthan we really need. That’s why we also expressed the best-case running\\ntime as an + b for constants a and b that depend on the statement costs\\nck and why we expressed the worst-case running time as an2 + bn + c\\nfor constants a, b, and c that depend on the statement costs. We thus\\nignored not only the actual statement costs, but also the abstract costs\\nck.\\nLet’s now make one more simplifying abstraction: it is the rate of\\ngrowth, or order of growth, of the running time that really interests us.\\nWe therefore consider only the leading term of a formula (e.g., an2),\\nsince the lower-order terms are relatively insigniﬁcant for large values of\\nn. We also ignore the leading term’s constant coefﬁcient, since constant\\nfactors are less signiﬁcant than the rate of growth in determining\\ncomputational efﬁciency for large inputs. For insertion sort’s worst-case\\nrunning time, when we ignore the lower-order terms and the leading\\nterm’s constant coefﬁcient, only the factor of n2 from the leading term\\nremains. That factor, n2, is by far the most important part of the\\nrunning time. For example, suppose that an algorithm implemented on\\na particular machine takes n2/100 + 100n + 17 microseconds on an\\ninput of size n. Although the coefﬁcients of 1/100 for the n2 term and\\n100 for the n term differ by four orders of magnitude, the n2/100 term\\ndominates the 100n term once n exceeds 10,000. Although 10,000 might\\nseem large, it is smaller than the population of an average town. Many\\nreal-world problems have much larger input sizes.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 61}),\n",
              " Document(page_content='To highlight the order of growth of the running time, we have a\\nspecial notation that uses the Greek letter Θ  (theta). We write that\\ninsertion sort has a worst-case running time of Θ (n2) (pronounced\\n“theta of n-squared” or just “theta n-squared”). We also write that\\ninsertion sort has a best-case running time of Θ (n) (“theta of n” or\\n“theta n”). For now, think of Θ -notation as saying “roughly\\nproportional when n is large,” so that Θ (n2) means “roughly\\nproportional to n2 when n is large” and Θ (n) means “roughly\\nproportional to n when n is large” We’ll use Θ -notation informally in\\nthis chapter and deﬁne it precisely in Chapter 3.\\nWe usually consider one algorithm to be more efﬁcient than another\\nif its worst-case running time has a lower order of growth. Due to\\nconstant factors and lower-order terms, an algorithm whose running\\ntime has a higher order of growth might take less time for small inputs\\nthan an algorithm whose running time has a lower order of growth. But\\non large enough inputs, an algorithm whose worst-case running time is\\nΘ(n2), for example, takes less time in the worst case than an algorithm\\nwhose worst-case running time is Θ (n3). Regardless of the constants\\nhidden by the Θ -notation, there is always some number, say n0, such\\nthat for all input sizes n ≥ n0, the Θ (n2) algorithm beats the Θ (n3)\\nalgorithm in the worst case.\\nExercises\\n2.2-1\\nExpress the function n3/1000 + 100n2 – 100n + 3 in terms of Θ -\\nnotation.\\n2.2-2\\nConsider sorting n numbers stored in array A[1 : n] by ﬁrst ﬁnding the\\nsmallest element of A[1 : n] and exchanging it with the element in A[1].\\nThen ﬁnd the smallest element of A[2 : n], and exchange it with A[2].\\nThen ﬁnd the smallest element of A[3 : n], and exchange it with A[3].\\nContinue in this manner for the ﬁrst n – 1 elements of A. Write', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 62}),\n",
              " Document(page_content='pseudocode for this algorithm, which is known as selection sort. What\\nloop invariant does this algorithm maintain? Why does it need to run\\nfor only the ﬁrst n – 1 elements, rather than for all n elements? Give the\\nworst-case running time of selection sort in Θ -notation. Is the best-case\\nrunning time any better?\\n2.2-3\\nConsider linear search again (see Exercise 2.1-4). How many elements of\\nthe input array need to be checked on the average, assuming that the\\nelement being searched for is equally likely to be any element in the\\narray? How about in the worst case? Using Θ -notation, give the average-\\ncase and worst-case running times of linear search. Justify your answers.\\n2.2-4\\nHow can you modify any sorting algorithm to have a good best-case\\nrunning time?\\n2.3\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0Designing algorithms\\nYou can choose from a wide range of algorithm design techniques.\\nInsertion sort uses the incremental method: for each element A[i], insert\\nit into its proper place in the subarray A[1 : i], having already sorted the\\nsubarray A[1 : i – 1].\\nThis section examines another design method, known as “divide-\\nand-conquer,” which we explore in more detail in Chapter 4. We’ll use\\ndivide-and-conquer to design a sorting algorithm whose worst-case\\nrunning time is much less than that of insertion sort. One advantage of\\nusing an algorithm that follows the divide-and-conquer method is that\\nanalyzing its running time is often straightforward, using techniques\\nthat we’ll explore in Chapter 4.\\n2.3.1\\xa0\\xa0\\xa0\\xa0The divide-and-conquer method\\nMany useful algorithms are recursive in structure: to solve a given\\nproblem, they recurse (call themselves) one or more times to handle\\nclosely related subproblems. These algorithms typically follow the', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 63}),\n",
              " Document(page_content='divide-and-conquer method: they break the problem into several\\nsubproblems that are similar to the original problem but smaller in size,\\nsolve the subproblems recursively, and then combine these solutions to\\ncreate a solution to the original problem.\\nIn the divide-and-conquer method, if the problem is small enough—\\nthe base case—you just solve it directly without recursing. Otherwise—\\nthe recursive case—you perform three characteristic steps:\\nDivide the problem into one or more subproblems that are smaller\\ninstances of the same problem.\\nConquer the subproblems by solving them recursively.\\nCombine the subproblem solutions to form a solution to the original\\nproblem.\\nThe merge sort algorithm closely follows the divide-and-conquer\\nmethod. In each step, it sorts a subarray A[p : r], starting with the entire\\narray A[1 : n] and recursing down to smaller and smaller subarrays.\\nHere is how merge sort operates:\\nDivide the subarray A[p : r] to be sorted into two adjacent subarrays,\\neach of half the size. To do so, compute the midpoint q of A[p : r]\\n(taking the average of p and r), and divide A[p : r] into subarrays A[p :\\nq] and A[q + 1 : r].\\nConquer by sorting each of the two subarrays A[p : q] and A[q + 1 : r]\\nrecursively using merge sort.\\nCombine by merging the two sorted subarrays A[p : q] and A[q + 1 : r]\\nback into A[p : r], producing the sorted answer.\\nThe recursion “bottoms out”—it reaches the base case—when the\\nsubarray A[p : r] to be sorted has just 1 element, that is, when p equals r.\\nAs we noted in the initialization argument for INSERTION-SORT’s\\nloop invariant, a subarray comprising just a single element is always\\nsorted.\\nThe key operation of the merge sort algorithm occurs in the\\n“combine” step, which merges two adjacent, sorted subarrays. The\\nmerge operation is performed by the auxiliary procedure MERGE(A, p,', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 64}),\n",
              " Document(page_content='q, r) on the following page, where A is an array and p, q, and r are\\nindices into the array such that p ≤ q < r. The procedure assumes that\\nthe adjacent subarrays A[p : q] and A[q + 1 : r] were already recursively\\nsorted. It merges the two sorted subarrays to form a single sorted\\nsubarray that replaces the current subarray A[p : r].\\nTo understand how the MERGE procedure works, let’s return to our\\ncard-playing motif. Suppose that you have two piles of cards face up on\\na table. Each pile is sorted, with the smallest-value cards on top. You\\nwish to merge the two piles into a single sorted output pile, which is to\\nbe face down on the table. The basic step consists of choosing the\\nsmaller of the two cards on top of the face-up piles, removing it from its\\npile—which exposes a new top card—an d placing this card face down\\nonto the output pile. Repeat this step until one input pile is empty, at\\nwhich time you can just take the remaining input pile and ﬂip over the\\nentire pile, placing it face down onto the output pile.\\nLet’s think about how long it takes to merge two sorted piles of\\ncards. Each basic step takes constant time, since you are comparing just\\nthe two top cards. If the two sorted piles that you start with each have\\nn/2 cards, then the number of basic steps is at least n/2 (since in\\nwhichever pile was emptied, every card was found to be smaller than\\nsome card from the other pile) and at most n (actually, at most n – 1,\\nsince after n – 1 basic steps, one of the piles must be empty). With each\\nbasic step taking constant time and the total number of basic steps\\nbeing between n/2 and n, we can say that merging takes time roughly\\nproportional to n. That is, merging takes Θ (n) time.\\nIn detail, the MERGE procedure works as follows. It copies the two\\nsubarrays A[p : q] and A[q + 1 : r] into temporary arrays L and R (“left”\\nand “right”), and then it merges the values in L and R back into A[p : r].\\nLines 1 and 2 compute the lengths nL and nR of the subarrays A[p : q]\\nand A[q + 1 : r], respectively. Then line 3 creates arrays L[0 : nL – 1] and\\nR[0 : nR – 1] with respective lengths nL and nR.12 The for loop of lines\\n4–5 copies the subarray A[p : q] into L, and the for loop of lines 6–7\\ncopies the subarray A[q + 1 : r] into R.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 65}),\n",
              " Document(page_content='MERGE(A, p, q, r)\\n\\xa0\\xa01nL = q – p + 1 // length of A[p : q]\\n\\xa0\\xa02nR = r – q // length of A[q + 1 : r]\\n\\xa0\\xa03let L[0 : nL – 1] and R[0 : nR – 1] be new arrays\\n\\xa0\\xa04for\\xa0i = 0 to\\xa0nL – 1// copy A[p : q] into L[0 : nL – 1]\\n\\xa0\\xa05L[i] = A[p + i]\\n\\xa0\\xa06for\\xa0j = 0 to\\xa0nR – 1// copy A[q + 1 : r] into R[0 : nR – 1]\\n\\xa0\\xa07R[j] = A[q + j + 1]\\n\\xa0\\xa08i = 0 //\\xa0i indexes the smallest remaining element in L\\n\\xa0\\xa09j = 0 //\\xa0j indexes the smallest remaining element in R\\n10k = p //\\xa0k indexes the location in A to ﬁll\\n11// As long as each of the arrays L and R contains an unmerged\\nelement,\\n//\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0c opy the smallest unmerged element back into A[p : r].\\n12while\\xa0i < nL and j < nR\\n13if\\xa0L[i] ≤ R[j]\\n14 A[k] = L[i]\\n15 i = i + 1\\n16else\\xa0A[k] = R[j]\\n17 j = j + 1\\n18k = k + 1\\n19// Having gone through one of L and R entirely, copy the\\n//\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0r emainder of the other to the end of A[p : r].\\n20while\\xa0i < nL\\n21A[k] = L[i]\\n22i = i + 1\\n23k = k + 1\\n24while\\xa0j < nR\\n25A[k] = R[j]\\n26j = j + 1\\n27k = k + 1', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 66}),\n",
              " Document(page_content='Lines 8–18, illustrated in Figure 2.3, perform the basic steps. The\\nwhile loop of lines 12–18 repeatedly identiﬁes the smallest value in L\\nand R that has yet to be copied back into A[p : r] and copies it back in.\\nAs the comments indicate, the index k gives the position of A that is\\nbeing ﬁlled in, and the indices i and j give the positions in L and R,\\nrespectively, of the smallest remaining values. Eventually, either all of L\\nor all of R is copied back into A[p : r], and this loop terminates. If the\\nloop terminates because all of R has been copied back, that is, because j\\nequals nR, then i is still less than nL, so that some of L has yet to be\\ncopied back, and these values are the greatest in both L and R. In this\\ncase, the while loop of lines 20–23 copies these remaining values of L\\ninto the last few positions of A[p : r]. Because j equals nR, the while loop\\nof lines 24–27 iterates 0 times. If instead the while loop of lines 12–18\\nterminates because i equals nL, then all of L has already been copied\\nback into A[p : r], and the while loop of lines 24–27 copies the remaining\\nvalues of R back into the end of A[p : r].\\n', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 67}),\n",
              " Document(page_content='Figure 2.3 The operation of the while loop in lines 8–18 in the call MERGE(A, 9, 12, 16), when\\nthe subarray A[9 : 16] contains the values 〈2, 4, 6, 7, 1, 2, 3, 5〉. After allocating and copying into\\nthe arrays L and R, the array L contains 〈2, 4, 6, 7〉, and the array R contains 〈1, 2, 3, 5〉. Tan\\npositions in A contain their ﬁnal values, and tan positions in L and R contain values that have\\nyet to be copied back into A. Taken together, the tan positions always comprise the values\\noriginally in A[9 : 16]. Blue positions in A contain values that will be copied over, and dark\\npositions in L and R contain values that have already been copied back into A. (a)–(g) The\\narrays A, L, and R, and their respective indices k, i, and j prior to each iteration of the loop of\\nlines 12–18. At the point in part (g), all values in R have been copied back into A (indicated by j\\nequaling the length of R), and so the while loop in lines 12–18 terminates. (h) The arrays and\\nindices at termination. The while loops of lines 20–23 and 24–27 copied back into A the\\nremaining values in L and R, which are the largest values originally in A[9 : 16]. Here, lines 20–\\n23 copied L[2 : 3] into A[15 : 16], and because all values in R had already been copied back into\\nA, the while loop of lines 24–27 iterated 0 times. At this point, the subarray in A[9 : 16] is sorted.\\nTo see that the MERGE procedure runs in Θ (n) time, where n = r – p\\n+ 1,13 observe that each of lines 1–3 and 8–10 takes constant time, and\\nthe for loops of lines 4–7 take Θ (nL + nR) = Θ (n) time.14 To account for\\nthe three while loops of lines 12–18, 20–23, and 24–27,  observe that each\\niteration of these loops copies exactly one value from L or R back into\\nA and that every value is copied back into A exactly once. Therefore,\\nthese three loops together make a total of n iterations. Since each\\niteration of each of the three loops takes constant time, the total time\\nspent in these three loops is Θ (n).\\nWe can now use the MERGE procedure as a subroutine in the merge\\nsort algorithm. The procedure MERGE-SORT(A, p, r) on the facing\\npage sorts the elements in the subarray A[p : r]. If p equals r, the\\nsubarray has just 1 element and is therefore already sorted. Otherwise,\\nwe must have p < r, and MERGE-SORT runs the divide, conquer, and\\ncombine steps. The divide step simply computes an index q that\\npartitions A[p : r] into two adjacent subarrays: A[p : q], containing ⌈n/2 ⌉', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 68}),\n",
              " Document(page_content='elements, and A[q + 1 : r], containing ⌊n/2 ⌋ elements.15 The initial call\\nMERGE-SORT(A, 1, n) sorts the entire array A[1 : n].\\nFigure 2.4 illustrates the operation of the procedure for n = 8,\\nshowing also the sequence of divide and merge steps. The algorithm\\nrecursively divides the array down to 1-element subarrays. The combine\\nsteps merge pairs of 1-element subarrays to form sorted subarrays of\\nlength 2, merges those to form sorted subarrays of length 4, and merges\\nthose to form the ﬁnal sorted subarray of length 8. If n is not an exact\\npower of 2, then some divide steps create subarrays whose lengths differ\\nby 1. (For example, when dividing a subarray of length 7, one subarray\\nhas length 4 and the other has length 3.) Regardless of the lengths of the\\ntwo subarrays being merged, the time to merge a total of n items is Θ (n).\\nMERGE-SORT(A, p, r)\\n1if\\xa0p ≥ r // zero or one element?\\n2 return\\n3q = ⌊(p + r)/2 ⌋ // midpoint of A[p : r]\\n4MERGE-SORT(A, p, q) // recursively sort A[p : q]\\n5MERGE-SORT(A, q + 1, r) // recursively sort A[q + 1 : r]\\n6// Merge A[p : q] and A[q + 1 : r] into A[p : r].\\n7MERGE(A, p, q, r)\\n2.3.2\\xa0\\xa0\\xa0\\xa0Analyzing divide-and-conquer algorithms\\nWhen an algorithm contains a recursive call, you can often describe its\\nrunning time by a recurrence equation or recurrence, which describes the\\noverall running time on a problem of size n in terms of the running time\\nof the same algorithm on smaller inputs. You can then use mathematical\\ntools to solve the recurrence and provide bounds on the performance of\\nthe algorithm.\\nA recurrence for the running time of a divide-and-conquer algorithm\\nfalls out from the three steps of the basic method. As we did for\\ninsertion sort, let T (n) be the worst-case running time on a problem of\\nsize n. If the problem size is small enough, say n < n0 for some constant\\nn0 > 0, the straightforward solution takes constant time, which we write', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 69}),\n",
              " Document(page_content='as Θ (1).16 Suppose that the division of the problem yields a\\nsubproblems, each with size n/b, that is, 1/b the size of the original. For\\nmerge sort, both a and b are 2, but we’ll see other divide-and-conquer\\nalgorithms in which a ≠ b. It takes T (n/b) time to solve one subproblem\\nof size n/b, and so it takes aT (n/b) time to solve all a of them. If it takes\\nD(n) time to divide the problem into subproblems and C(n) time to\\ncombine the solutions to the subproblems into the solution to the\\noriginal problem, we get the recurrence\\nChapter 4 shows how to solve common recurrences of this form.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 70}),\n",
              " Document(page_content='Figure 2.4 The operation of merge sort on the array A with length 8 that initially contains the\\nsequence 〈12, 3, 7, 9, 14, 6, 11, 2〉. The indices p, q, and r into each subarray appear above their\\nvalues. Numbers in italics indicate the order in which the MERGE-SORT and MERGE\\nprocedures are called following the initial call of MERGE-SORT(A, 1, 8).\\nSometimes, the n/b size of the divide step isn’t an integer. For\\nexample, the MERGE-SORT procedure divides a problem of size n into\\nsubproblems of sizes ⌈n/2 ⌉ and ⌊n/2 ⌋. Since the difference between ⌈n/2 ⌉\\nand ⌊n/2 ⌋ is at most 1, which for large n is much smaller than the effect\\nof dividing n by 2, we’ll squint a little and just call them both size n/2.\\nAs Chapter 4 will discuss, this simpliﬁcation of ignoring ﬂoors and', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 71}),\n",
              " Document(page_content='ceilings does not generally affect the order of growth of a solution to a\\ndivide-and-conquer recurrence.\\nAnother convention we’ll adopt is to omit a statement of the base\\ncases of the recurrence, which we’ll also discuss in more detail in\\nChapter 4. The reason is that the base cases are pretty much always T\\n(n) = Θ (1) if n < n0 for some constant n0 > 0. That’s because the\\nrunning time of an algorithm on an input of constant size is constant.\\nWe save ourselves a lot of extra writing by adopting this convention.\\nAnalysis of merge sort\\nHere’s how to set up the recurrence for T (n), the worst-case running\\ntime of merge sort on n numbers.\\nDivide: The divide step just computes the middle of the subarray, which\\ntakes constant time. Thus, D(n) = Θ (1).\\nConquer: Recursively solving two subproblems, each of size n/2,\\ncontributes 2T (n/2) to the running time (ignoring the ﬂoors and\\nceilings, as we discussed).\\nCombine: Since the MERGE procedure on an n-element subarray takes\\nΘ(n) time, we have C(n) = Θ (n).\\nWhen we add the functions D(n) and C(n) for the merge sort\\nanalysis, we are adding a function that is Θ (n) and a function that is\\nΘ(1). This sum is a linear function of n. That is, it is roughly\\nproportional to n when n is large, and so merge sort’s dividing and\\ncombining times together are Θ (n). Adding Θ (n) to the 2T (n/2) term\\nfrom the conquer step gives the recurrence for the worst-case running\\ntime T (n) of merge sort:\\nChapter 4 presents the “master theorem,” which shows that T (n) = Θ (n\\nlg n).17 Compared with insertion sort, whose worst-case running time is\\nΘ(n2), merge sort trades away a factor of n for a factor of lg n. Because\\nthe logarithm function grows more slowly than any linear function,\\nthat’s a good trade. For large enough inputs, merge sort, with its Θ (n lg', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 72}),\n",
              " Document(page_content='n) worst-case running time, outperforms insertion sort, whose worst-\\ncase running time is Θ (n2).\\nWe do not need the master theorem, however, to understand\\nintuitively why the solution to recurrence (2.3) is T (n) = Θ (n lg n). For\\nsimplicity, assume that n is an exact power of 2 and that the implicit\\nbase case is n = 1. Then recurrence (2.3) is essentially\\nwhere the constant c1 > 0 represents the time required to solve a\\nproblem of size 1, and c2 > 0 is the time per array element of the divide\\nand combine steps.18\\nFigure 2.5 illustrates one way of ﬁguring out the solution to\\nrecurrence (2.4). Part (a) of the ﬁgure shows T (n), which part (b)\\nexpands into an equivalent tree representing the recurrence. The c2n\\nterm denotes the cost of dividing and combining at the top level of\\nrecursion, and the two subtrees of the root are the two smaller\\nrecurrences T (n/2). Part (c) shows this process carried one step further\\nby expanding T (n/2). The cost for dividing and combining at each of\\nthe two nodes at the second level of recursion is c2n/2. Continue to\\nexpand each node in the tree by breaking it into its constituent parts as\\ndetermined by the recurrence, until the problem sizes get down to 1,\\neach with a cost of c1. Part (d) shows the resulting recursion tree.\\nNext, add the costs across each level of the tree. The top level has\\ntotal cost c2n, the next level down has total cost c2(n/2) + c2(n/2) = c2n,\\nthe level after that has total cost c2(n/4) + c2(n/4) + c2(n/4) + c2(n/4) =\\nc2n, and so on. Each level has twice as many nodes as the level above,\\nbut each node contributes only half the cost of a node from the level\\nabove. From one level to the next, doubling and halving cancel each\\nother out, so that the cost across each level is the same: c2n. In general,\\nthe level that is i levels below the top has 2i nodes, each contributing a\\ncost of c2(n/2i), so that the ith level below the top has total cost 2i ·', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 73}),\n",
              " Document(page_content='c2(n/2i) = c2n. The bottom level has n nodes, each contributing a cost of\\nc1, for a total cost of c1n.\\nThe total number of levels of the recursion tree in Figure 2.5 is lg n +\\n1, where n is the number of leaves, corresponding to the input size. An\\ninformal inductive argument justiﬁes this claim. The base case occurs\\nwhen n = 1, in which case the tree has only 1 level. Since lg 1 = 0, we\\nhave that lg n + 1 gives the correct number of levels. Now assume as an\\ninductive hypothesis that the number of levels of a recursion tree with 2i\\nleaves is lg 2i + 1 = i + 1 (since for any value of i, we have that lg 2i = i).\\nBecause we assume that the input size is an exact power of 2, the next\\ninput size to consider is 2i + 1. A tree with n = 2i + 1 leaves has 1 more\\nlevel than a tree with 2i leaves, and so the total number of levels is (i + 1)\\n+ 1 = lg 2i + 1 + 1.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 74}),\n",
              " Document(page_content='Figure 2.5 How to construct a recursion tree for the recurrence (2.4). Part (a) shows T (n), which\\nprogressively expands in (b)–(d) to form the recursion tree. The fully expanded tree in part (d)\\nhas lg n + 1 levels. Each level above the leaves contributes a total cost of c2n, and the leaf level\\ncontributes c1n. The total cost, therefore, is c2n lg n + c1n = Θ (n lg n).', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 75}),\n",
              " Document(page_content='To compute the total cost represented by the recurrence (2.4), simply\\nadd up the costs of all the levels. The recursion tree has lg n + 1 levels.\\nThe levels above the leaves each cost c2n, and the leaf level costs c1n, for\\na total cost of c2n lg n + c1n = Θ (n lg n).\\nExercises\\n2.3-1\\nUsing Figure 2.4 as a model, illustrate the operation of merge sort on an\\narray initially containing the sequence 〈3, 41, 52, 26, 38, 57, 9, 49〉.\\n2.3-2\\nThe test in line 1 of the MERGE-SORT procedure reads “if\\xa0p ≥ r”\\nrather than “if\\xa0p ≠ r.” If MERGE-SORT is called with p > r, then the\\nsubarray A[p : r] is empty. Argue that as long as the initial call of\\nMERGE-SORT(A, 1, n) has n ≥ 1, the test “if\\xa0p ≠ r” sufﬁces to ensure\\nthat no recursive call has p > r.\\n2.3-3\\nState a loop invariant for the while loop of lines 12–18 of the MERGE\\nprocedure. Show how to use it, along with the while loops of lines 20–23\\nand 24–27, to prove that the MERGE procedure is correct.\\n2.3-4\\nUse mathematical induction to show that when n ≥ 2 is an exact power\\nof 2, the solution of the recurrence\\nis T(n) = n lg n.\\n2.3-5\\nYou can also think of insertion sort as a recursive algorithm. In order to\\nsort A[1 : n], recursively sort the subarray A[1 : n – 1] and then insert\\nA[n] into the sorted subarray A[1 : n – 1]. Write pseudocode for this', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 76}),\n",
              " Document(page_content='recursive version of insertion sort. Give a recurrence for its worst-case\\nrunning time.\\n2.3-6\\nReferring back to the searching problem (see Exercise 2.1-4), observe\\nthat if the subarray being searched is already sorted, the searching\\nalgorithm can check the midpoint of the subarray against v and\\neliminate half of the subarray from further consideration. The binary\\nsearch algorithm repeats this procedure, halving the size of the\\nremaining portion of the subarray each time. Write pseudocode, either\\niterative or recursive, for binary search. Argue that the worst-case\\nrunning time of binary search is Θ (lg n).\\n2.3-7\\nThe while loop of lines 5–7 of the INSERTION-SORT procedure in\\nSection 2.1 uses a linear search to scan (backward) through the sorted\\nsubarray A[1 : j – 1]. What if insertion sort used a binary search (see\\nExercise 2.3-6) instead of a linear search? Would that improve the\\noverall worst-case running time of insertion sort to Θ (n lg n)?\\n2.3-8\\nDescribe an algorithm that, given a set S of n integers and another\\ninteger x, determines whether S contains two elements that sum to\\nexactly x. Your algorithm should take Θ (n lg n) time in the worst case.\\nProblems\\n2-1\\xa0\\xa0\\xa0\\xa0\\xa0Insertion sort on small arrays in merge sort\\nAlthough merge sort runs in Θ (n lg n) worst-case time and insertion sort\\nruns in Θ (n2) worst-case time, the constant factors in insertion sort can\\nmake it faster in practice for small problem sizes on many machines.\\nThus it makes sense to coarsen the leaves of the recursion by using\\ninsertion sort within merge sort when subproblems become sufﬁciently\\nsmall. Consider a modiﬁcation to merge sort in which n/k sublists of', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 77}),\n",
              " Document(page_content='length k are sorted using insertion sort and then merged using the\\nstandard merging mechanism, where k is a value to be determined.\\na. Show that insertion sort can sort the n/k sublists, each of length k, in\\nΘ(nk) worst-case time.\\nb. Show how to merge the sublists in Θ (n lg(n/k)) worst-case time.\\nc. Given that the modiﬁed algorithm runs in Θ (nk + n lg(n/k)) worst-\\ncase time, what is the largest value of k as a function of n for which\\nthe modiﬁed algorithm has the same running time as standard merge\\nsort, in terms of Θ -notation?\\nd. How should you choose k in practice?\\n2-2\\xa0\\xa0\\xa0\\xa0\\xa0C orrectness of bubblesort\\nBubblesort is a popular, but inefﬁcient, sorting algorithm. It works by\\nrepeatedly swapping adjacent elements that are out of order. The\\nprocedure BUBBLESORT sorts array A[1 : n].\\nBUBBLESORT(A, n)\\n1for\\xa0i = 1 to\\xa0n – 1\\n2 for\\xa0j = n\\xa0downto\\xa0i + 1\\n3 if\\xa0A[j] < A[j – 1]\\n4 exchange A[j] with A[j – 1]\\na. Let A′ denote the array A after BUBBLESORT(A, n) is executed. To\\nprove that\\nIn order to show that BUBBLESORT actually sorts, what else do you\\nneed to prove?\\nThe next two parts prove inequality (2.5).\\nb. State precisely a loop invariant for the for loop in lines 2–4, and prove\\nthat this loop invariant holds. Your proof should use the structure of\\nthe loop-invariant proof presented in this chapter.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 78}),\n",
              " Document(page_content='c. Using the termination condition of the loop invariant proved in part\\n(b), state a loop invariant for the for loop in lines 1–4 that allows you\\nto prove inequality (2.5). Your proof should use the structure of the\\nloop-invariant proof presented in this chapter.\\nd. What is the worst-case running time of BUBBLESORT? How does it\\ncompare with the running time of INSERTION-SORT?\\n2-3\\xa0\\xa0\\xa0\\xa0\\xa0C orrectness of Horner’s rule\\nYou are given the coefﬁcents a0, a1, a2, … , an of a polynomial\\nand you want to evaluate this polynomial for a given value of x.\\nHorner’s rule says to evaluate the polynomial according to this\\nparenthesization:\\nThe procedure HORNER implements Horner’s rule to evaluate P(x),\\ngiven the coefﬁcients a0, a1, a2, … , an in an array A[0 : n] and the value\\nof x.\\nHORNER(A, n, x)\\n1p = 0\\n2for\\xa0i = n\\xa0downto 0\\n3 p = A[i] + x · p\\n4return\\xa0p\\na. In terms of Θ -notation, what is the running time of this procedure?\\nb. Write pseudocode to implement the naive polynomial-evaluation\\nalgorithm that computes each term of the polynomial from scratch.\\nWhat is the running time of this algorithm? How does it compare with\\nHORNER?', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 79}),\n",
              " Document(page_content='c. Consider the following loop invariant for the procedure HORNER:\\nAt the start of each iteration of the for loop of lines 2–3,\\nInterpret a summation with no terms as equaling 0. Following the\\nstructure of the loop-invariant proof presented in this chapter, use this\\nloop invariant to show that, at termination, \\n .\\n2-4\\xa0\\xa0\\xa0\\xa0\\xa0Inversions\\nLet A[1 : n] be an array of n distinct numbers. If i < j and A[i] > A[j],\\nthen the pair (i, j) is called an inversion of A.\\na. List the ﬁve inversions of the array 〈2, 3, 8, 6, 1〉.\\nb. What array with elements from the set {1, 2, … , n} has the most\\ninversions? How many does it have?\\nc. What is the relationship between the running time of insertion sort\\nand the number of inversions in the input array? Justify your answer.\\nd. Give an algorithm that determines the number of inversions in any\\npermutation on n elements in Θ (n lg n) worst-case time. (Hint: Modify\\nmerge sort.)\\nChapter notes\\nIn 1968, Knuth published the ﬁrst of three volumes with the general title\\nThe Art of Computer Programming [259, 260, 261]. The ﬁrst volume\\nushered in the modern study of computer algorithms with a focus on\\nthe analysis of running time. The full series remains an engaging and\\nworthwhile reference for many of the topics presented here. According\\nto Knuth, the word “algorithm” is derived from the name “al-\\nKhowârizmî,” a ninth-century Persian mathematician.\\nAho, Hopcroft, and Ullman [5] advocated the asymptotic analysis of\\nalgorithms—using notations that Chapter 3 introduces, including Θ -\\nnotation—as a means of comparing relative performance. They also', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 80}),\n",
              " Document(page_content='popularized the use of recurrence relations to describe the running times\\nof recursive algorithms.\\nKnuth [261] provides an encyclopedic treatment of many sorting\\nalgorithms. His comparison of sorting algorithms (page 381) includes\\nexact step-counting analyses, like the one we performed here for\\ninsertion sort. Knuth’s discussion of insertion sort encompasses several\\nvariations of the algorithm. The most important of these is Shell’s sort,\\nintroduced by D. L. Shell, which uses insertion sort on periodic\\nsubarrays of the input to produce a faster sorting algorithm.\\nMerge sort is also described by Knuth. He mentions that a\\nmechanical collator capable of merging two decks of punched cards in a\\nsingle pass was invented in 1938. J. von Neumann, one of the pioneers\\nof computer science, apparently wrote a program for merge sort on the\\nEDVAC computer in 1945.\\nThe early history of proving programs correct is described by Gries\\n[200], who credits P. Naur with the ﬁrst article in this ﬁeld. Gries\\nattributes loop invariants to R. W. Floyd. The textbook by Mitchell\\n[329] is a good reference on how to prove programs correct.\\n1 If you’re familiar with only Python, you can think of arrays as similar to Python lists.\\n2 When the loop is a for loop, the loop-invariant check just prior to the ﬁrst iteration occurs\\nimmediately after the initial assignment to the loop-counter variable and just before the ﬁrst test\\nin the loop header. In the case of INSERTION-SORT, this time is after assigning 2 to the\\nvariable i but before the ﬁrst test of whether i ≤ n.\\n3 In an if-else statement, we indent else at the same level as its matching if. The ﬁrst executable\\nline of an else clause appears on the same line as the keyword else. For multiway tests, we use\\nelseif for tests after the ﬁrst one. When it is the ﬁrst line in an else clause, an if statement appears\\non the line following else so that you do not misconstrue it as elseif.\\n4 Each pseudocode procedure in this book appears on one page so that you do not need to\\ndiscern levels of indentation in pseudocode that is split across pages.\\n5 Most block-structured languages have equivalent constructs, though the exact syntax may\\ndiffer. Python lacks repeat-until loops, and its for loops operate differently from the for loops in\\nthis book. Think of the pseudocode line “for\\xa0i = 1 to\\xa0n” as equivalent to “for i in range(1, n+1)”\\nin Python.\\n6 In Python, the loop counter retains its value after the loop is exited, but the value it retains is\\nthe value it had during the ﬁnal iteration of the for loop, rather than the value that exceeded the', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 81}),\n",
              " Document(page_content='loop bound. That is because a Python for loop iterates through a list, which may contain\\nnonnumeric values.\\n7 If you’re used to programming in Python, bear in mind that in this book, the subarray A[i : j]\\nincludes the element A[j]. In Python, the last element of A[i : j] is A[j – 1]. Python allows negative\\nindices, which count from the back end of the list. This book does not use negative array indices.\\n8 Python’s tuple notation allows return statements to return multiple values without creating\\nobjects from a programmer-deﬁned class.\\n9 We assume that each element of a given array occupies the same number of bytes and that the\\nelements of a given array are stored in contiguous memory locations. For example, if array A[1 :\\nn] starts at memory address 1000 and each element occupies four bytes, then element A[i] is at\\naddress 1000 + 4(i – 1). In general, computing the address in memory of a particular array\\nelement requires at most one subtraction (no subtraction for a 0-origin array), one\\nmultiplication (often implemented as a shift operation if the element size is an exact power of 2),\\nand one addition. Furthermore, for code that iterates through the elements of an array in order,\\nan optimizing compiler can generate the address of each element using just one addition, by\\nadding the element size to the address of the preceding element.\\n10 There are some subtleties here. Computational steps that we specify in English are often\\nvariants of a procedure that requires more than just a constant amount of time. For example, in\\nthe RADIX-SORT procedure on page 213, one line reads “use a stable sort to sort array A on\\ndigit i,” which, as we shall see, takes more than a constant amount of time. Also, although a\\nstatement that calls a subroutine takes only constant time, the subroutine itself, once invoked,\\nmay take more. That is, we separate the process of calling the subroutine—passing parameters to\\nit, etc.—from the process of executing the subroutine.\\n11 This characteristic does not necessarily hold for a resource such as memory. A statement that\\nreferences m words of memory and is executed n times does not necessarily reference mn distinct\\nwords of memory.\\n12 This procedure is the rare case that uses both 1-origin indexing (for array A) and 0-origin\\nindexing (for arrays L and R). Using 0-origin indexing for L and R makes for a simpler loop\\ninvariant in Exercise 2.3-3.\\n13 If you’re wondering where the “+1” comes from, imagine that r = p + 1. Then the subarray\\nA[p : r] consists of two elements, and r – p + 1 = 2.\\n14 Chapter 3 shows how to formally interpret equations containing Θ -notation.\\n15 The expression ⌈x ⌉ denotes the least integer greater than or equal to x, and ⌊x ⌋ denotes the\\ngreatest integer less than or equal to x. These notations are deﬁned in Section 3.3. The easiest\\nway to verify that setting q to ⌊(p + r)/2 ⌋ yields subarrays A[p : q] and A[q + 1 : r] of sizes ⌈n/2 ⌉\\nand ⌊n/2 ⌋, respectively, is to examine the four cases that arise depending on whether each of p\\nand r is odd or even.\\n16 If you’re wondering where Θ (1) comes from, think of it this way. When we say that n2/100 is\\nΘ(n2), we are ignoring the coefﬁcient 1/100 of the factor n2. Likewise, when we say that a', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 82}),\n",
              " Document(page_content='constant c is Θ (1), we are ignoring the coefﬁcient c of the factor 1 (which you can also think of\\nas n0).\\n17 The notation lg n stands for log2\\xa0n, although the base of the logarithm doesn’t matter here,\\nbut as computer scientists, we like logarithms base 2. Section 3.3 discusses other standard\\nnotation.\\n18 It is unlikely that c1 is exactly the time to solve problems of size 1 and that c2n is exactly the\\ntime of the divide and combine steps. We’ll look more closely at bounding recurrences in\\nChapter 4, where we’ll be more careful about this kind of detail.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 83}),\n",
              " Document(page_content='3\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0Characterizing Running Times\\nThe order of growth of the running time of an algorithm, deﬁned in\\nChapter 2, gives a simple way to characterize the algorithm’s efﬁciency\\nand also allows us to compare it with alternative algorithms. Once the\\ninput size n becomes large enough, merge sort, with its Θ (n lg n) worst-\\ncase running time, beats insertion sort, whose worst-case running time is\\nΘ(n2). Although we can sometimes determine the exact running time of\\nan algorithm, as we did for insertion sort in Chapter 2, the extra\\nprecision is rarely worth the effort of computing it. For large enough\\ninputs, the multiplicative constants and lower-order terms of an exact\\nrunning time are dominated by the effects of the input size itself.\\nWhen we look at input sizes large enough to make relevant only the\\norder of growth of the running time, we are studying the asymptotic\\nefﬁciency of algorithms. That is, we are concerned with how the running\\ntime of an algorithm increases with the size of the input in the limit, as\\nthe size of the input increases without bound. Usually, an algorithm\\nthat is asymptotically more efﬁcient is the best choice for all but very\\nsmall inputs.\\nThis chapter gives several standard methods for simplifying the\\nasymptotic analysis of algorithms. The next section presents informally\\nthe three most commonly used types of “asymptotic notation,” of which\\nwe have already seen an example in Θ -notation. It also shows one way\\nto use these asymptotic notations to reason about the worst-case\\nrunning time of insertion sort. Then we look at asymptotic notations\\nmore formally and present several notational conventions used', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 84}),\n",
              " Document(page_content='throughout this book. The last section reviews the behavior of functions\\nthat commonly arise when analyzing algorithms.\\n3.1\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0O-notation, Ω-notation, and Θ -notation\\nWhen we analyzed the worst-case running time of insertion sort in\\nChapter 2, we started with the complicated expression\\nWe then discarded the lower-order terms (c1 + c2 + c4 + c5/2 – c6/2 –\\nc7/2 + c8)n and c2 + c4 + c5 + c8, and we also ignored the coefﬁcient\\nc5/2 + c6/2 + c7/2 of n2. That left just the factor n2, which we put into\\nΘ-notation as Θ (n2). We use this style to characterize running times of\\nalgorithms: discard the lower-order terms and the coefﬁcient of the\\nleading term, and use a notation that focuses on the rate of growth of\\nthe running time.\\nΘ-notation is not the only such “asymptotic notation.” In this\\nsection, we’ll see other forms of asymptotic notation as well. We start\\nwith intuitive looks at these notations, revisiting insertion sort to see\\nhow we can apply them. In the next section, we’ll see the formal\\ndeﬁnitions of our asymptotic notations, along with conventions for\\nusing them.\\nBefore we get into speciﬁcs, bear in mind that the asymptotic\\nnotations we’ll see are designed so that they characterize functions in\\ngeneral. It so happens that the functions we are most interested in\\ndenote the running times of algorithms. But asymptotic notation can\\napply to functions that characterize some other aspect of algorithms\\n(the amount of space they use, for example), or even to functions that\\nhave nothing whatsoever to do with algorithms.\\nO-notation', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 85}),\n",
              " Document(page_content='O-notation characterizes an upper bound on the asymptotic behavior of\\na function. In other words, it says that a function grows no faster than a\\ncertain rate, based on the highest-order term. Consider, for example, the\\nfunction 7n3 + 100n2 – 20n + 6. Its highest-order term is 7n3, and so we\\nsay that this function’s rate of growth is n3. Because this function grows\\nno faster than n3, we can write that it is O(n3). You might be surprised\\nthat we can also write that the function 7n3 + 100n2 – 20n + 6 is O(n4).\\nWhy? Because the function grows more slowly than n4, we are correct in\\nsaying that it grows no faster. As you might have guessed, this function\\nis also O(n5), O(n6), and so on. More generally, it is O(nc) for any\\nconstant c ≥ 3.\\nΩ-notation\\nΩ-notation characterizes a lower bound on the asymptotic behavior of a\\nfunction. In other words, it says that a function grows at least as fast as\\na certain rate, based — as in O-notation—on the highest-order term.\\nBecause the highest-order term in the function 7n3 + 100n2 – 20n + 6\\ngrows at least as fast as n3, this function is Ω(n3). This function is also\\nΩ(n2) and Ω(n). More generally, it is Ω(nc) for any constant c ≤ 3.\\nΘ-notation\\nΘ-notation characterizes a tight bound on the asymptotic behavior of a\\nfunction. It says that a function grows precisely at a certain rate, based\\n—once again—on the highest-order term. Put another way, Θ -notation\\ncharacterizes the rate of growth of the function to within a constant\\nfactor from above and to within a constant factor from below. These\\ntwo constant factors need not be equal.\\nIf you can show that a function is both O(f (n)) and Ω(f (n)) for some\\nfunction f (n), then you have shown that the function is Θ (f (n)). (The\\nnext section states this fact as a theorem.) For example, since the\\nfunction 7n3 + 100n2 – 20n + 6 is both O(n3) and Ω(n3), it is also Θ (n3).', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 86}),\n",
              " Document(page_content='Example: Insertion sort\\nLet’s revisit insertion sort and see how to work with asymptotic\\nnotation to characterize its Θ (n2) worst-case running time without\\nevaluating summations as we did in Chapter 2. Here is the\\nINSERTION-SORT procedure once again:\\nINSERTION-SORT(A, n)\\n1for\\xa0i = 2 to\\xa0n\\n2 key = A[i]\\n3 // Insert A[i] into the sorted subarray A[1 : i – 1].\\n4 j = i – 1\\n5 while\\xa0j > 0 and A[j] > key\\n6 A[j + 1] = A[j]\\n7 j = j – 1\\n8 A[j + 1] = key\\nWhat can we observe about how the pseudocode operates? The\\nprocedure has nested loops. The outer loop is a for loop that runs n – 1\\ntimes, regardless of the values being sorted. The inner loop is a while\\nloop, but the number of iterations it makes depends on the values being\\nsorted. The loop variable j starts at i – 1 and decreases by 1 in each\\niteration until either it reaches 0 or A[j] ≤ key. For a given value of i, the\\nwhile loop might iterate 0 times, i – 1 times, or anywhere in between.\\nThe body of the while loop (lines 6–7) takes constant time per iteration\\nof the while loop.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 87}),\n",
              " Document(page_content='Figure 3.1 The Ω(n2) lower bound for insertion sort. If the ﬁrst n/3 positions contain the n/3\\nlargest values, each of these values must move through each of the middle n/3 positions, one\\nposition at a time, to end up somewhere in the last n/3 positions. Since each of n/3 values moves\\nthrough at least each of n/3 positions, the time taken in this case is at least proportional to (n/3)\\n(n/3) = n2/9, or Ω(n2).\\nThese observations sufﬁce to deduce an O(n2) running time for any\\ncase of INSERTION-SORT, giving us a blanket statement that covers\\nall inputs. The running time is dominated by the inner loop. Because\\neach of the n – 1 iterations of the outer loop causes the inner loop to\\niterate at most i – 1 times, and because i is at most n, the total number of\\niterations of the inner loop is at most (n – 1)(n – 1), which is less than\\nn2. Since each iteration of the inner loop takes constant time, the total\\ntime spent in the inner loop is at most a constant times n2, or O(n2).\\nWith a little creativity, we can also see that the worst-case running\\ntime of INSERTION-SORT is Ω(n2). By saying that the worst-case\\nrunning time of an algorithm is Ω(n2), we mean that for every input size\\nn above a certain threshold, there is at least one input of size n for which\\nthe algorithm takes at least cn2 time, for some positive constant c. It\\ndoes not necessarily mean that the algorithm takes at least cn2 time for\\nall inputs.\\nLet’s now see why the worst-case running time of INSERTION-\\nSORT is Ω(n2). For a value to end up to the right of where it started, it\\nmust have been moved in line 6. In fact, for a value to end up k\\npositions to the right of where it started, line 6 must have executed k\\ntimes. As Figure 3.1 shows, let’s assume that n is a multiple of 3 so that\\nwe can divide the array A into groups of n/3 positions. Suppose that in\\nthe input to INSERTION-SORT, the n/3 largest values occupy the ﬁrst', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 88}),\n",
              " Document(page_content='n/3 array positions A[1 : n/3]. (It does not matter what relative order\\nthey have within the ﬁrst n/3 positions.) Once the array has been sorted,\\neach of these n/3 values ends up somewhere in the last n/3 positions\\nA[2n/3 + 1 : n]. For that to happen, each of these n/3 values must pass\\nthrough each of the middle n/3 positions A[n/3 + 1 : 2n/3]. Each of these\\nn/3 values passes through these middle n/3 positions one position at a\\ntime, by at least n/3 executions of line 6. Because at least n/3 values have\\nto pass through at least n/3 positions, the time taken by INSERTION-\\nSORT in the worst case is at least proportional to (n/3)(n/3) = n2/9,\\nwhich is Ω(n2).\\nBecause we have shown that INSERTION-SORT runs in O(n2) time\\nin all cases and that there is an input that makes it take Ω(n2) time, we\\ncan conclude that the worst-case running time of INSERTION-SORT is\\nΘ(n2). It does not matter that the constant factors for upper and lower\\nbounds might differ. What matters is that we have characterized the\\nworst-case running time to within constant factors (discounting lower-\\norder terms). This argument does not show that INSERTION-SORT\\nruns in Θ (n2) time in all cases. Indeed, we saw in Chapter 2 that the\\nbest-case running time is Θ (n).\\nExercises\\n3.1-1\\nModify the lower-bound argument for insertion sort to handle input\\nsizes that are not necessarily a multiple of 3.\\n3.1-2\\nUsing reasoning similar to what we used for insertion sort, analyze the\\nrunning time of the selection sort algorithm from Exercise 2.2-2.\\n3.1-3\\nSuppose that α is a fraction in the range 0 < α < 1. Show how to\\ngeneralize the lower-bound argument for insertion sort to consider an\\ninput in which the αn largest values start in the ﬁrst αn positions. What\\nadditional restriction do you need to put on α? What value of α', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 89}),\n",
              " Document(page_content='maximizes the number of times that the αn largest values must pass\\nthrough each of the middle (1 – 2 α)n array positions?\\n3.2\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0Asymptotic notation: formal deﬁnitions\\nHaving seen asymptotic notation informally, let’s get more formal. The\\nnotations we use to describe the asymptotic running time of an\\nalgorithm are deﬁned in terms of functions whose domains are typically\\nthe set N of natural numbers or the set R of real numbers. Such\\nnotations are convenient for describing a running-time function T (n).\\nThis section deﬁnes the basic asymptotic notations and also introduces\\nsome common “proper” notational abuses.\\nFigure 3.2 Graphic examples of the O, Ω, and Θ  notations. In each part, the value of n0 shown is\\nthe minimum possible value, but any greater value also works. (a)\\xa0O-notation gives an upper\\nbound for a function to within a constant factor. We write f (n) = O(g(n)) if there are positive\\nconstants n0 and c such that at and to the right of n0, the value of f (n) always lies on or below\\ncg(n). (b) Ω-notation gives a lower bound for a function to within a constant factor. We write f\\n(n) = Ω(g(n)) if there are positive constants n0 and c such that at and to the right of n0, the value\\nof f (n) always lies on or above cg(n). (c) Θ-notation bounds a function to within constant\\nfactors. We write f (n) = Θ (g(n)) if there exist positive constants n0, c1, and c2 such that at and\\nto the right of n0, the value of f (n) always lies between c1g(n) and c2g(n) inclusive.\\nO-notation\\nAs we saw in Section 3.1, O-notation describes an asymptotic upper\\nbound. We use O-notation to give an upper bound on a function, to', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 90}),\n",
              " Document(page_content='within a constant factor.\\nHere is the formal deﬁnition of O-notation. For a given function\\ng(n), we denote by O(g(n)) (pronounced “big-oh of g of n” or sometimes\\njust “oh of g of n”) the set of functions\\nO(g(n))\\n= {f (n)\\xa0:\\xa0there exist positive constants c and n0 such\\nthat 0 ≤ f (n) ≤ cg(n) for all n ≥ n0}.1\\nA function f (n) belongs to the set O(g(n)) if there exists a positive\\nconstant c such that f (n) ≤ cg(n) for sufﬁciently large n. Figure 3.2(a)\\nshows the intuition behind O-notation. For all values n at and to the\\nright of n0, the value of the function f (n) is on or below cg(n).\\nThe deﬁnition of O(g(n)) requires that every function f (n) in the set\\nO(g(n)) be asymptotically nonnegative: f (n) must be nonnegative\\nwhenever n is sufﬁciently large. (An asymptotically positive function is\\none that is positive for all sufﬁciently large n.) Consequently, the\\nfunction g(n) itself must be asymptotically nonnegative, or else the set\\nO(g(n)) is empty. We therefore assume that every function used within\\nO-notation is asymptotically nonnegative. This assumption holds for\\nthe other asymptotic notations deﬁned in this chapter as well.\\nYou might be surprised that we deﬁne O-notation in terms of sets.\\nIndeed, you might expect that we would write “f (n) ∈ O(g(n))” to\\nindicate that f (n) belongs to the set O(g(n)). Instead, we usually write “f\\n(n) = O(g(n))” and say “f (n) is big-oh of g(n)” to express the same\\nnotion. Although it may seem confusing at ﬁrst to abuse equality in this\\nway, we’ll see later in this section that doing so has its advantages.\\nLet’s explore an example of how to use the formal deﬁnition of O-\\nnotation to justify our practice of discarding lower-order terms and\\nignoring the constant coefﬁcient of the highest-order term. We’ll show\\nthat 4n2 + 100n + 500 = O(n2), even though the lower-order terms have\\nmuch larger coefﬁcients than the leading term. We need to ﬁnd positive\\nconstants c and n0 such that 4n2 + 100n + 500 ≤ cn2 for all n ≥ n0.\\nDividing both sides by n2 gives 4 + 100/n + 500/n2 ≤ c. This inequality is\\nsatisﬁed for many choices of c and n0. For example, if we choose n0 = 1,', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 91}),\n",
              " Document(page_content='then this inequality holds for c = 604. If we choose n0 = 10, then c = 19\\nworks, and choosing n0 = 100 allows us to use c = 5.05.\\nWe can also use the formal deﬁnition of O-notation to show that the\\nfunction n3 – 100n2 does not belong to the set O(n2), even though the\\ncoefﬁcient of n2 is a large negative number. If we had n3 – 100n2 =\\nO(n2), then there would be positive constants c and n0 such that n3 –\\n100n2 ≤ cn2 for all n ≥ n0. Again, we divide both sides by n2, giving n –\\n100 ≤ c. Regardless of what value we choose for the constant c, this\\ninequality does not hold for any value of n > c + 100.\\nΩ-notation\\nJust as O-notation provides an asymptotic upper bound on a function,\\nΩ-notation provides an asymptotic lower bound. For a given function\\ng(n), we denote by Ω(g(n)) (pronounced “big-omega of g of n” or\\nsometimes just “omega of g of n”) the set of functions\\nΩ(g(n))\\n= {f (n)\\xa0:\\xa0there exist positive constants c and n0 such\\nthat 0 ≤ cg(n) ≤ f (n) for all n ≥ n0}.\\nFigure 3.2(b) shows the intuition behind Ω-notation. For all values n at\\nor to the right of n0, the value of f (n) is on or above cg(n).\\nWe’ve already shown that 4n2 + 100n + 500 = O(n2). Now let’s show\\nthat 4n2 + 100n + 500 = Ω(n2). We need to ﬁnd positive constants c and\\nn0 such that 4n2 + 100n + 500 ≥ cn2 for all n ≥ n0. As before, we divide\\nboth sides by n2, giving 4 + 100/n + 500/n2 ≥ c. This inequality holds\\nwhen n0 is any positive integer and c = 4.\\nWhat if we had subtracted the lower-order terms from the 4n2 term\\ninstead of adding them? What if we had a small coefﬁcient for the n2\\nterm? The function would still be Ω(n2). For example, let’s show that\\nn2/100 – 100n – 500 = Ω(n2). Dividing by n2 gives 1/100 – 100/n – 500/n2', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 92}),\n",
              " Document(page_content='≥ c. We can choose any value for n0 that is at least 10,005 and ﬁnd a\\npositive value for c. For example, when n0 = 10,005, we can choose c =\\n2.49 × 10–9. Yes, that’s a tiny value for c, but it is positive. If we select a\\nlarger value for n0, we can also increase c. For example, if n0 = 100,000,\\nthen we can choose c = 0.0089. The higher the value of n0, the closer to\\nthe coefﬁcient 1/100 we can choose c.\\nΘ-notation\\nWe use Θ -notation for asymptotically tight bounds. For a given function\\ng(n), we denote by Θ (g(n)) (“theta of g of n”) the set of functions\\nΘ(g(n))\\n= {f (n)\\xa0:\\xa0there exist positive constants c1, c2, and n0\\nsuch that 0 ≤ c1g(n) ≤ f (n) ≤ c2g(n) for all n ≥\\nn0}.\\nFigure 3.2(c) shows the intuition behind Θ -notation. For all values of n\\nat and to the right of n0, the value of f (n) lies at or above c1g(n) and at\\nor below c2g(n). In other words, for all n ≥ n0, the function f (n) is equal\\nto g(n) to within constant factors.\\nThe deﬁnitions of O-, Ω-, and Θ -notations lead to the following\\ntheorem, whose proof we leave as Exercise 3.2-4.\\nTheorem 3.1\\nFor any two functions f (n) and g(n), we have f (n) = Θ (g(n)) if and only\\nif f (n) = O(g(n)) and f (n) = Ω(g(n)).\\n▪\\nWe typically apply Theorem 3.1 to prove asymptotically tight bounds\\nfrom asymptotic upper and lower bounds.\\nAsymptotic notation and running times\\nWhen you use asymptotic notation to characterize an algorithm’s\\nrunning time, make sure that the asymptotic notation you use is as\\nprecise as possible without overstating which running time it applies to.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 93}),\n",
              " Document(page_content='Here are some examples of using asymptotic notation properly and\\nimproperly to characterize running times.\\nLet’s start with insertion sort. We can correctly say that insertion\\nsort’s worst-case running time is O(n2), Ω(n2), and—due to Theorem 3.1\\n—Θ(n2). Although all three ways to characterize the worst-case running\\ntimes are correct, the Θ (n2) bound is the most precise and hence the\\nmost preferred. We can also correctly say that insertion sort’s best-case\\nrunning time is O(n), Ω(n), and Θ (n), again with Θ (n) the most precise\\nand therefore the most preferred.\\nHere is what we cannot correctly say: insertion sort’s running time is\\nΘ(n2). That is an overstatement because by omitting “worst-case” from\\nthe statement, we’re left with a blanket statement covering all cases. The\\nerror here is that insertion sort does not run in Θ (n2) time in all cases\\nsince, as we’ve seen, it runs in Θ (n) time in the best case. We can\\ncorrectly say that insertion sort’s running time is O(n2), however,\\nbecause in all cases, its running time grows no faster than n2. When we\\nsay O(n2) instead of Θ (n2), there is no problem in having cases whose\\nrunning time grows more slowly than n2. Likewise, we cannot correctly\\nsay that insertion sort’s running time is Θ (n), but we can say that its\\nrunning time is Ω(n).\\nHow about merge sort? Since merge sort runs in Θ (n lg n) time in all\\ncases, we can just say that its running time is Θ (n lg n) without\\nspecifying worst-case, best-case, or any other case.\\nPeople occasionally conﬂate O-notation with Θ -notation by\\nmistakenly using O-notation to indicate an asymptotically tight bound.\\nThey say things like “an O(n lg n)-time algorithm runs faster than an\\nO(n2)-time algorithm.” Maybe it does, maybe it doesn’t. Since O-\\nnotation denotes only an asymptotic upper bound, that so-called O(n2)-\\ntime algorithm might actually run in Θ (n) time. You should be careful to\\nchoose the appropriate asymptotic notation. If you want to indicate an\\nasymptotically tight bound, use Θ -notation.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 94}),\n",
              " Document(page_content='We typically use asymptotic notation to provide the simplest and\\nmost precise bounds possible. For example, if an algorithm has a\\nrunning time of 3n2 + 20n in all cases, we use asymptotic notation to\\nwrite that its running time is Θ (n2). Strictly speaking, we are also\\ncorrect in writing that the running time is O(n3) or Θ (3n2 + 20n).\\nNeither of these expressions is as useful as writing Θ (n2) in this case,\\nhowever: O(n3) is less precise than Θ (n2) if the running time is 3n2 +\\n20n, and Θ (3n2 + 20n) introduces complexity that obscures the order of\\ngrowth. By writing the simplest and most precise bound, such as Θ (n2),\\nwe can categorize and compare different algorithms. Throughout the\\nbook, you will see asymptotic running times that are almost always\\nbased on polynomials and logarithms: functions such as n, n lg2\\xa0n, n2 lg\\nn, or n1/2. You will also see some other functions, such as exponentials,\\nlg lg n, and lg*n (see Section 3.3). It is usually fairly easy to compare the\\nrates of growth of these functions. Problem 3-3 gives you good practice.\\nAsymptotic notation in equations and i nequalities\\nAlthough we formally deﬁne asymptotic notation in terms of sets, we\\nuse the equal sign (=) instead of the set membership sign ( ∈ ) within\\nformulas. For example, we wrote that 4n2 + 100n + 500 = O(n2). We\\nmight also write 2n2 + 3n + 1 = 2n2 + Θ (n). How do we interpret such\\nformulas?\\nWhen the asymptotic notation stands alone (that is, not within a\\nlarger formula) on the right-hand side of an equation (or inequality), as\\nin 4n2 + 100n + 500 = O(n2), the equal sign means set membership: 4n2\\n+ 100n + 500 ∈  O(n2). In general, however, when asymptotic notation\\nappears in a formula, we interpret it as standing for some anonymous\\nfunction that we do not care to name. For example, the formula 2n2 +\\n3n + 1 = 2n2 + Θ (n) means that 2n2 + 3n + 1 = 2n2 + f (n), where f (n)\\n∈ Θ(n). In this case, we let f (n) = 3n + 1, which indeed belongs to Θ (n).', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 95}),\n",
              " Document(page_content='Using asymptotic notation in this manner can help eliminate\\ninessential detail and clutter in an equation. For example, in Chapter 2\\nwe expressed the worst-case running time of merge sort as the\\nrecurrence\\nT (n) = 2T (n/2) + Θ (n).\\nIf we are interested only in the asymptotic behavior of T (n), there is no\\npoint in specifying all the lower-order terms exactly, because they are all\\nunderstood to be included in the anonymous function denoted by the\\nterm Θ (n).\\nThe number of anonymous functions in an expression is understood\\nto be equal to the number of times the asymptotic notation appears. For\\nexample, in the expression\\nthere is only a single anonymous function (a function of i). This\\nexpression is thus not the same as O(1) + O(2) + ⋯ + O(n), which\\ndoesn’t really have a clean interpretation.\\nIn some cases, asymptotic notation appears on the left-hand side of\\nan equation, as in\\n2n2 + Θ (n) = Θ (n2).\\nInterpret such equations using the following rule: No matter how the\\nanonymous functions are chosen on the left of the equal sign, there is a\\nway to choose the anonymous functions on the right of the equal sign to\\nmake the equation valid. Thus, our example means that for any function\\nf (n) ∈ Θ (n), there is some function g(n) ∈ Θ (n2) such that 2n2 + f (n) =\\ng(n) for all n. In other words, the right-hand side of an equation\\nprovides a coarser level of detail than the left-hand side.\\nWe can chain together a number of such relationships, as in\\n2n2 + 3n + 1\\xa0=\\xa02n2 + Θ (n)\\n\\xa0=\\xa0Θ(n2).', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 96}),\n",
              " Document(page_content='By the rules above, interpret each equation separately. The ﬁrst equation\\nsays that there is some function f (n) ∈ Θ (n) such that 2n2 + 3n + 1 =\\n2n2 + f (n) for all n. The second equation says that for any function g(n)\\n∈ Θ(n) (such as the f (n) just mentioned), there is some function h(n) ∈\\nΘ(n2) such that 2n2 + g(n) = h(n) for all n. This interpretation implies\\nthat 2n2 + 3n + 1 = Θ (n2), which is what the chaining of equations\\nintuitively says.\\nProper abuses of asymptotic notation\\nBesides the abuse of equality to mean set membership, which we now\\nsee has a precise mathematical interpretation, another abuse of\\nasymptotic notation occurs when the variable tending toward ∞ must be\\ninferred from context. For example, when we say O(g(n)), we can\\nassume that we’re interested in the growth of g(n) as n grows, and if we\\nsay O(g(m)) we’re talking about the growth of g(m) as m grows. The free\\nvariable in the expression indicates what variable is going to ∞.\\nThe most common situation requiring contextual knowledge of\\nwhich variable tends to ∞ occurs when the function inside the\\nasymptotic notation is a constant, as in the expression O(1). We cannot\\ninfer from the expression which variable is going to ∞, because no\\nvariable appears there. The context must disambiguate. For example, if\\nthe equation using asymptotic notation is f (n) = O(1), it’s apparent that\\nthe variable we’re interested in is n. Knowing from context that the\\nvariable of interest is n, however, allows us to make perfect sense of the\\nexpression by using the formal deﬁnition of O-notation: the expression f\\n(n) = O(1) means that the function f (n) is bounded from above by a\\nconstant as n goes to ∞. Technically, it might be less ambiguous if we\\nexplicitly indicated the variable tending to ∞ in the asymptotic notation\\nitself, but that would clutter the notation. Instead, we simply ensure that\\nthe context makes it clear which variable (or variables) tend to ∞.\\nWhen the function inside the asymptotic notation is bounded by a\\npositive constant, as in T (n) = O(1), we often abuse asymptotic\\nnotation in yet another way, especially when stating recurrences. We\\nmay write something like T (n) = O(1) for n < 3. According to the', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 97}),\n",
              " Document(page_content='formal deﬁnition of O-notation, this statement is meaningless, because\\nthe deﬁnition only says that T (n) is bounded above by a positive\\nconstant c for n ≥ n0 for some n0 > 0. The value of T (n) for n < n0 need\\nnot be so bounded. Thus, in the example T (n) = O(1) for n < 3, we\\ncannot infer any constraint on T (n) when n < 3, because it might be\\nthat n0 > 3.\\nWhat is conventionally meant when we say T (n) = O(1) for n < 3 is\\nthat there exists a positive constant c such that T (n) ≤ c for n < 3. This\\nconvention saves us the trouble of naming the bounding constant,\\nallowing it to remain anonymous while we focus on more important\\nvariables in an analysis. Similar abuses occur with the other asymptotic\\nnotations. For example, T (n) = Θ (1) for n < 3 means that T (n) is\\nbounded above and below by positive constants when n < 3.\\nOccasionally, the function describing an algorithm’s running time\\nmay not be deﬁned for certain input sizes, for example, when an\\nalgorithm assumes that the input size is an exact power of 2. We still use\\nasymptotic notation to describe the growth of the running time,\\nunderstanding that any constraints apply only when the function is\\ndeﬁned. For example, suppose that f (n) is deﬁned only on a subset of\\nthe natural or nonnegative real numbers. Then f (n) = O(g(n)) means\\nthat the bound 0 ≤ T (n) ≤ cg(n) in the deﬁnition of O-notation holds for\\nall n ≥ n0 over the domain of f (n), that is, where f (n) is deﬁned. This\\nabuse is rarely pointed out, since what is meant is generally clear from\\ncontext.\\nIn mathematics, it’s okay — and often desirable — to abuse a\\nnotation, as long as we don’t misuse it. If we understand precisely what\\nis meant by the abuse and don’t draw incorrect conclusions, it can\\nsimplify our mathematical language, contribute to our higher-level\\nunderstanding, and help us focus on what really matters.\\no-notation\\nThe asymptotic upper bound provided by O-notation may or may not\\nbe asymptotically tight. The bound 2n2 = O(n2) is asymptotically tight,\\nbut the bound 2n = O(n2) is not. We use o-notation to denote an upper', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 98}),\n",
              " Document(page_content='bound that is not asymptotically tight. We formally deﬁne o(g(n))\\n(“little-oh of g of n”) as the set\\no(g(n)) =\\n{f (n)\\xa0:\\xa0for any positive constant c > 0, there exists a constant n0 > 0\\nsuch that 0 ≤ f (n) < cg(n) for all n ≥ n0}.\\nFor example, 2n = o(n2), but 2n2 ≠ o(n2).\\nThe deﬁnitions of O-notation and o-notation are similar. The main\\ndifference is that in f (n) = O(g(n)), the bound 0 ≤ f (n) ≤ cg(n) holds for\\nsome constant c > 0, but in f (n) = o(g(n)), the bound 0 ≤ f (n) < cg(n)\\nholds for all constants c > 0. Intuitively, in o-notation, the function f (n)\\nbecomes insigniﬁcant relative to g(n) as n gets large:\\nSome authors use this limit as a deﬁnition of the o-notation, but the\\ndeﬁnition in this book also restricts the anonymous functions to be\\nasymptotically nonnegative.\\nω-notation\\nBy analogy, ω-notation is to Ω-notation as o-notation is to O-notation.\\nWe use ω-notation to denote a lower bound that is not asymptotically\\ntight. One way to deﬁne it is by\\nf (n) ∈ ω(g(n)) if and only if g(n) ∈ o(f (n)).\\nFormally, however, we deﬁne ω(g(n)) (“little-omega of g of n”) as the set\\nω(g(n))\\n= {f (n)\\xa0:\\xa0for any positive constant c > 0, there exists a constant n0 > 0\\nsuch that 0 ≤ cg(n) < f (n) for all n ≥ n0}.\\nWhere the deﬁnition of o-notation says that f (n) < cg(n), the deﬁnition\\nof ω-notation says the opposite: that cg(n) < f (n). For examples of ω-\\nnotation, we have n2/2 = ω(n), but n2/2 ≠ ω(n2). The relation f (n) =\\nω(g(n)) implies that', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 99}),\n",
              " Document(page_content='if the limit exists. That is, f (n) becomes arbitrarily large relative to g(n)\\nas n gets large.\\nComparing functions\\nMany of the relational properties of real numbers apply to asymptotic\\ncomparisons as well. For the following, assume that f (n) and g(n) are\\nasymptotically positive.\\nTransitivity:\\nf (n) =\\nΘ(g(n))andg(n) =\\nΘ(h(n))implyf (n) =\\nΘ(h(n)),\\nf (n) =\\nO(g(n))andg(n) =\\nO(h(n))implyf (n) =\\nO(h(n)),\\nf (n) =\\nΩ(g(n))andg(n) =\\nΩ(h(n))implyf (n) =\\nΩ(h(n)),\\nf (n) =\\no(g(n))andg(n) =\\no(h(n))implyf (n) =\\no(h(n)),\\nf (n) =\\nω(g(n))andg(n) =\\nω(h(n))implyf (n) =\\nω(h(n)).\\nReﬂexivity:\\nf (n) = Θ (f (n)),\\nf (n) = O(f (n)),\\nf (n) = Ω(f (n)).\\nSymmetry:\\nf (n) = Θ (g(n)) if and only if g(n) = Θ (f (n)).\\nTranspose symmetry:\\nf (n) =if and onlyg(n) = Ω(f (n)),', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 100}),\n",
              " Document(page_content='O(g(n)) if\\nf (n) = o(g(n))if and only\\nifg(n) = ω(f\\n(n)).\\nBecause these properties hold for asymptotic notations, we can draw\\nan analogy between the asymptotic comparison of two functions f and g\\nand the comparison of two real numbers a and b:\\nf (n) = O(g(n))is likea ≤ b,\\nf (n) = Ω(g(n))is likea ≥ b,\\nf (n) = Θ (g(n))is likea = b,\\nf (n) = o(g(n))is likea < b,\\nf (n) = ω(g(n))is likea > b.\\nWe say that f (n) is asymptotically smaller than g(n) if f (n) = o(g(n)), and\\nf (n) is asymptotically larger than g(n) if f (n) = ω(g(n)).\\nOne property of real numbers, however, does not carry over to\\nasymptotic notation:\\nTrichotomy: For any two real numbers a and b, exactly one of the\\nfollowing must hold: a < b, a = b, or a > b.\\nAlthough any two real numbers can be compared, not all functions are\\nasymptotically comparable. That is, for two functions f (n) and g(n), it\\nmay be the case that neither f (n) = O(g(n)) nor f (n) = Ω(g(n)) holds. For\\nexample, we cannot compare the functions n and n1 + sin n using\\nasymptotic notation, since the value of the exponent in n1 + sin n\\noscillates between 0 and 2, taking on all values in between.\\nExercises\\n3.2-1\\nLet f (n) and g(n) be asymptotically nonnegative functions. Using the\\nbasic deﬁnition of Θ -notation, prove that max {f (n), g(n)} = Θ (f (n) +\\ng(n)).\\n3.2-2', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 101}),\n",
              " Document(page_content='Explain why the statement, “The running time of algorithm A is at least\\nO(n2),” is meaningless.\\n3.2-3\\nIs 2n + 1 = O(2n)? Is 22n = O(2n)?\\n3.2-4\\nProve Theorem 3.1.\\n3.2-5\\nProve that the running time of an algorithm is Θ (g(n)) if and only if its\\nworst-case running time is O(g(n)) and its best-case running time is\\nΩ(g(n)).\\n3.2-6\\nProve that o(g(n)) ∩ ω(g(n)) is the empty set.\\n3.2-7\\nWe can extend our notation to the case of two parameters n and m that\\ncan go to ∞ independently at different rates. For a given function g(n,\\nm), we denote by O(g(n, m)) the set of functions\\nO(g(n,\\nm)) = {f\\n(n, m)\\xa0:\\xa0there exist positive constants c, n0, and m0\\nsuch that 0 ≤ f (n, m) ≤ cg(n, m) for all n ≥ n0\\nor m ≥ m0}.\\nGive corresponding deﬁnitions for Ω(g(n, m)) and Θ (g(n, m)).\\n3.3\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0Standard notations and common functions\\nThis section reviews some standard mathematical functions and\\nnotations and explores the relationships among them. It also illustrates\\nthe use of the asymptotic notations.\\nMonotonicity', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 102}),\n",
              " Document(page_content='A function f (n) is monotonically increasing if m ≤ n implies f (m) ≤ f (n).\\nSimilarly, it is monotonically decreasing if m ≤ n implies f (m) ≥ f (n). A\\nfunction f (n) is strictly increasing if m < n implies f (m) < f (n) and\\nstrictly decreasing if m < n implies f (m) > f (n).\\nFloors and ceilings\\nFor any real number x, we denote the greatest integer less than or equal\\nto x by ⌊x ⌋ (read “the ﬂoor of x”) and the least integer greater than or\\nequal to x by ⌈x ⌉ (read “the ceiling of x”). The ﬂoor function is\\nmonotonically increasing, as is the ceiling function.\\nFloors and ceilings obey the following properties. For any integer n,\\nwe have\\nFor all real x, we have\\nWe also have\\nor equivalently,\\nFor any real number x ≥ 0 and integers a, b > 0, we have\\nFor any integer n and real number x, we have', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 103}),\n",
              " Document(page_content='Modular arithmetic\\nFor any integer a and any positive integer n, the value a mod n is the\\nremainder (or residue) of the quotient a/n:\\nIt follows that\\neven when a is negative.\\nGiven a well-deﬁned notion of the remainder of one integer when\\ndivided by another, it is convenient to provide special notation to\\nindicate equality of remainders. If (a mod n) = (b mod n), we write a = b\\n(mod n) and say that a is equivalent to b, modulo n. In other words, a =\\nb (mod n) if a and b have the same remainder when divided by n.\\nEquivalently, a = b (mod n) if and only if n is a divisor of b – a. We write\\na ≠ b (mod n) if a is not equivalent to b, modulo n.\\nPolynomials\\nGiven a nonnegative integer d, a polynomial in n of degree d is a function\\np(n) of the form\\nwhere the constants a0, a1, … , ad are the coefﬁcients of the polynomial\\nand ad ≠ 0. A polynomial is asymptotically positive if and only if ad > 0.\\nFor an asymptotically positive polynomial p(n) of degree d, we have p(n)\\n= Θ(nd). For any real constant a ≥ 0, the function na is monotonically\\nincreasing, and for any real constant a ≤ 0, the function na is\\nmonotonically decreasing. We say that a function f (n) is polynomially\\nbounded if f (n) = O(nk) for some constant k.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 104}),\n",
              " Document(page_content='Exponentials\\nFor all real a > 0, m, and n, we have the following identities:\\na0=1,\\na1=a,\\na–1=1/a,\\n(am)n=amn,\\n(am)n=(an)m,\\naman=am+n.\\nFor all n and a ≥ 1, the function an is monotonically increasing in n.\\nWhen convenient, we assume that 00 = 1.\\nWe can relate the rates of growth of polynomials and exponentials by\\nthe following fact. For all real constants a > 1 and b, we have\\nfrom which we can conclude that\\nThus, any exponential function with a base strictly greater than 1 grows\\nfaster than any polynomial function.\\nUsing e to denote 2.71828 …, the base of the natural-logarithm\\nfunction, we have for all real x,\\nwhere “!” denotes the factorial function deﬁned later in this section. For\\nall real x, we have the inequality\\n', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 105}),\n",
              " Document(page_content='where equality holds only when x = 0. When |x| ≤ 1, we have the\\napproximation\\nWhen x → 0, the approximation of ex by 1 + x is quite good:\\nex = 1 + x + Θ (x2).\\n(In this equation, the asymptotic notation is used to describe the\\nlimiting behavior as x → 0 rather than as x → ∞.) We have for all x,\\nLogarithms\\nWe use the following notations:\\nlg n=log2\\xa0n(binary logarithm),\\nln n=loge\\xa0n(natural logarithm),\\nlgk\\xa0n=(lg n)k(exponentiation),\\nlg lg n=lg(lg n)(composition).\\nWe adopt the following notational convention: in the absence of\\nparentheses, a logarithm function applies only to the next term in the\\nformula, so that lg n + 1 means (lg n) + 1 and not lg(n + 1).\\nFor any constant b > 1, the function logb n is undeﬁned if n ≤ 0,\\nstrictly increasing if n > 0, negative if 0 < n < 1, positive if n > 1, and 0 if\\nn = 1. For all real a > 0, b > 0, c > 0, and n, we have', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 106}),\n",
              " Document(page_content='where, in each equation above, logarithm bases are not 1.\\nBy equation (3.19), changing the base of a logarithm from one\\nconstant to another changes the value of the logarithm by only a\\nconstant factor. Consequently, we often use the notation “lg n” when we\\ndon’t care about constant factors, such as in O-notation. Computer\\nscientists ﬁnd 2 to be the most natural base for logarithms because so\\nmany algorithms and data structures involve splitting a problem into\\ntwo parts.\\nThere is a simple series expansion for ln(1 + x) when |x| < 1:\\nWe also have the following inequalities for x > – 1:\\nwhere equality holds only for x = 0.\\nWe say that a function f (n) is polylogarithmically bounded if f (n) =\\nO(lgk n) for some constant k. We can relate the growth of polynomials\\nand polylogarithms by substituting lg n for n and 2a for a in equation\\n(3.13). For all real constants a > 0 and b, we have\\nThus, any positive polynomial function grows faster than any\\npolylogarithmic function.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 107}),\n",
              " Document(page_content='Factorials\\nThe notation n! (read “n factorial”) is deﬁned for integers n ≥ 0 as\\nThus, n! = 1 · 2 · 3 ⋯ n.\\nA weak upper bound on the factorial function is n! ≤ nn, since each\\nof the n terms in the factorial product is at most n. Stirling’s\\napproximation,\\nwhere e is the base of the natural logarithm, gives us a tighter upper\\nbound, and a lower bound as well. Exercise 3.3-4 asks you to prove the\\nthree facts\\nwhere Stirling’s approximation is helpful in proving equation (3.28). The\\nfollowing equation also holds for all n ≥ 1:\\nwhere\\nFunctional iteration\\nWe use the notation f(i) (n) to denote the function f (n) iteratively\\napplied i times to an initial value of n. Formally, let f (n) be a function\\nover the reals. For nonnegative integers i, we recursively deﬁne', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 108}),\n",
              " Document(page_content='For example, if f (n) = 2n, then f(i) (n) = 2in.\\nThe iterated logarithm function\\nWe use the notation lg*n (read “log star of n”) to denote the iterated\\nlogarithm, deﬁned as follows. Let lg(i)\\xa0n be as deﬁned above, with f (n) =\\nlg n. Because the logarithm of a nonpositive number is undeﬁned, lg(i)\\nn is deﬁned only if lg(i–1)\\xa0n > 0. Be sure to distinguish lg(i)\\xa0n (the\\nlogarithm function applied i times in succession, starting with argument\\nn) from lgi n (the logarithm of n raised to the ith power). Then we deﬁne\\nthe iterated logarithm function as\\nlg*n = min {i ≥ 0 : lg(i)\\xa0n ≤ 1}.\\nThe iterated logarithm is a very slowly growing function:\\nlg* 2=1,\\nlg* 4=2,\\nlg* 16=3,\\nlg* 65536=4,\\nlg* (265536)=5.\\nSince the number of atoms in the observable universe is estimated to be\\nabout 1080, which is much less than 265536 = 1065536/lg 10 ≈ 1019,728,\\nwe rarely encounter an input size n for which lg*\\xa0n > 5.\\nFibonacci numbers\\nWe deﬁne the Fibonacci numbers\\xa0Fi, for i ≥ 0, as follows:', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 109}),\n",
              " Document(page_content='Thus, after the ﬁrst two, each Fibonacci number is the sum of the two\\nprevious ones, yielding the sequence\\n0, 1, 1, 2, 3, 5, 8, 13, 21, 34, 55, ….\\nFibonacci numbers are related to the golden ratio\\xa0 ϕ and its conjugate \\n,\\nwhich are the two roots of the equation\\nx2 = x + 1.\\nAs Exercise 3.3-7 asks you to prove, the golden ratio is given by\\nand its conjugate, by\\nSpeciﬁcally, we have\\nwhich can be proved by induction (Exercise 3.3-8). Since \\n , we have\\nwhich implies that', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 110}),\n",
              " Document(page_content='which is to say that the ith Fibonacci number Fi is equal to \\nrounded to the nearest integer. Thus, Fibonacci numbers grow\\nexponentially.\\nExercises\\n3.3-1\\nShow that if f (n) and g(n) are monotonically increasing functions, then\\nso are the functions f (n) + g(n) and f (g(n)), and if f (n) and g(n) are in\\naddition nonnegative, then f (n) · g(n) is monotonically increasing.\\n3.3-2\\nProve that ⌊ αn ⌋ + ⌈(1 – α)n ⌉ = n for any integer n and real number α in\\nthe range 0 ≤ α ≤ 1.\\n3.3-3\\nUse equation (3.14) or other means to show that (n + o(n))k = Θ (nk) for\\nany real constant k. Conclude that ⌈n ⌉k = Θ (nk) and ⌊n ⌋k = Θ (nk).\\n3.3-4\\nProve the following:\\na. Equation (3.21).\\nb. Equations (3.26)–(3.28).\\nc. lg(Θ (n)) = Θ (lg n).\\n★ 3.3-5\\nIs the function ⌈lg n ⌉! polynomially bounded? Is the function ⌈lg lg n ⌉!\\npolynomially bounded?\\n★ 3.3-6\\nWhich is asymptotically larger: lg(lg*\\xa0n) or lg*(lg n)?', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 111}),\n",
              " Document(page_content='3.3-7\\nShow that the golden ratio ϕ and its conjugate \\n both satisfy the\\nequation x2 = x + 1.\\n3.3-8\\nProve by induction that the ith Fibonacci number satisﬁes the equation\\nwhere ϕ is the golden ratio and \\n is its conjugate.\\n3.3-9\\nShow that k lg k = Θ (n) implies k = Θ (n/lg n).\\nProblems\\n3-1\\xa0\\xa0\\xa0\\xa0\\xa0A symptotic behavior of polynom ials\\nLet\\nwhere ad > 0, be a degree-d polynomial in n, and let k be a constant.\\nUse the deﬁnitions of the asymptotic notations to prove the following\\nproperties.\\na. If k ≥ d, then p(n) = O(nk).\\nb. If k ≤ d, then p(n) = Ω(nk).\\nc. If k = d, then p(n) = Θ (nk).\\nd. If k > d, then p(n) = o(nk).\\ne. If k < d, then p(n) = ω(nk).\\n3-2\\xa0\\xa0\\xa0\\xa0\\xa0R elative asymptotic growths', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 112}),\n",
              " Document(page_content='Indicate, for each pair of expressions (A, B) in the table below whether\\nA is O, o, Ω, ω, or Θ  of B. Assume that k ≥ 1, ϵ > 0, and c > 1 are\\nconstants. Write your answer in the form of the table with “yes” or “no”\\nwritten in each box.\\n3-3\\xa0\\xa0\\xa0\\xa0\\xa0O rdering by asymptotic growth rates\\na. Rank the following functions by order of growth. That is, ﬁnd an\\narrangement g1, g2, … , g30 of the functions satisfying g1 = Ω(g2), g2\\n= Ω(g3), … , g29 = Ω(g30). Partition your list into equivalence classes\\nsuch that functions f (n) and g(n) belong to the same class if and only\\nif f (n) = Θ (g(n)).\\nlg(lg*\\xa0n)2lg* n\\nn2 n! (lg\\nn)!\\n(3/2)nn3lg2\\xa0nlg(n!)\\nn1/lg\\nn\\nln ln nlg*\\xa0nn · 2nnlg lg\\nnln n 1\\n2lg n(lg n)lg\\nnen4lg n(n +\\n1)!\\nlg*(lg n)\\nn2nn lg n\\nb. Give an example of a single nonnegative function f (n) such that for\\nall functions gi(n) in part (a), f (n) is neither O(gi(n)) nor Ω(gi(n)).', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 113}),\n",
              " Document(page_content='3-4\\xa0\\xa0\\xa0\\xa0\\xa0A symptotic notation properties\\nLet f (n) and g(n) be asymptotically positive functions. Prove or disprove\\neach of the following conjectures.\\na.\\xa0f (n) = O(g(n)) implies g(n) = O(f (n)).\\nb.\\xa0f (n) + g(n) = Θ (min {f (n), g(n)}).\\nc.\\xa0f (n) = O(g(n)) implies lg f (n) = O(lg g(n)), where lg g(n) ≥ 1 and f (n)\\n≥ 1 for all sufﬁciently large n.\\nd.\\xa0f (n) = O(g(n)) implies 2f(n) = O (2g(n)).\\ne.\\xa0f (n) = O ((f (n))2).\\nf.\\xa0f (n) = O(g(n)) implies g(n) = Ω(f (n)).\\ng.\\xa0f (n) = Θ (f (n/2)).\\nh.\\xa0f (n) + o(f (n)) = Θ (f (n)).\\n3-5\\xa0\\xa0\\xa0\\xa0\\xa0M anipulating asymptotic notation\\nLet f (n) and g(n) be asymptotically positive functions. Prove the\\nfollowing identities:\\na. Θ(Θ (f (n))) = Θ (f (n)).\\nb. Θ(f (n)) + O(f (n)) = Θ (f (n)).\\nc. Θ(f (n)) + Θ (g(n)) = Θ (f (n) + g(n)).\\nd. Θ(f (n)) · Θ (g(n)) = Θ (f (n) · g(n)).\\ne. Argue that for any real constants a1, b1 > 0 and integer constants k1,\\nk2, the following asymptotic bound holds:\\n★ f. Prove that for S ⊆ Z, we have\\n', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 114}),\n",
              " Document(page_content='assuming that both sums converge.\\n★ g. Show that for S ⊆ Z, the following asymptotic bound does not\\nnecessarily hold, even assuming that both products converge, by\\ngiving a counterexample:\\n3-6\\xa0\\xa0\\xa0\\xa0\\xa0Variations on O and Ω\\nSome authors deﬁne Ω-notation in a slightly different way than this\\ntextbook does. We’ll use the nomenclature \\n (read “omega inﬁnity”) for\\nthis alternative deﬁnition. We say that \\n  if there exists a\\npositive constant c such that f (n) ≥ cg(n) ≥ 0 for inﬁnitely many integers\\nn.\\na. Show that for any two asymptotically nonnegative functions f (n) and\\ng(n), we have f (n) = O(g(n)) or \\n  (or both).\\nb. Show that there exist two asymptotically nonnegative functions f (n)\\nand g(n) for which neither f (n) = O(g(n)) nor f (n) = Ω(g(n)) holds.\\nc. Describe the potential advantages and disadvantages of using \\n-\\nnotation instead of Ω-notation to characterize the running times of\\nprograms.\\nSome authors also deﬁne O in a slightly different manner. We’ll use O′\\nfor the alternative deﬁnition: f (n) = O′(g(n)) if and only if |f (n)| =\\nO(g(n)).\\nd. What happens to each direction of the “if and only if” in Theorem\\n3.1 on page 56 if we substitute O′ for O but still use Ω?\\nSome authors deﬁne \\n (read “soft-oh”) to mean O with logarithmic\\nfactors ignored:\\n\\xa0:\\xa0there exist positive constants c, k, and n0', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 115}),\n",
              " Document(page_content='such that 0 ≤ f (n) ≤ cg(n) lgk(n) for all n\\n≥ n0}.\\ne. Deﬁne \\n and \\n in a similar manner. Prove the corresponding analog\\nto Theorem 3.1.\\n3-7\\xa0\\xa0\\xa0\\xa0\\xa0Iterated functions\\nWe can apply the iteration operator * used in the lg* function to any\\nmonotonically increasing function f (n) over the reals. For a given\\nconstant c ∈ R, we deﬁne the iterated function \\n by\\nwhich need not be well deﬁned in all cases. In other words, the quantity \\n is the minimum number of iterated applications of the function f\\nrequired to reduce its argument down to c or less.\\nFor each of the functions f (n) and constants c in the table below,\\ngive as tight a bound as possible on \\n . If there is no i such that f(i)(n)\\n≤ c, write “undeﬁned” as your answer.\\nf (n) c\\na. n – 1 0\\nb. lg n 1\\nc. n/2 1\\nd. n/2 2\\ne.\\n 2\\nf.\\n 1\\ng. n1/3 2\\nChapter notes\\nKnuth [259] traces the origin of the O-notation to a number-theory text\\nby P. Bachmann in 1892. The o-notation was invented by E. Landau in', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 116}),\n",
              " Document(page_content='1909 for his discussion of the distribution of prime numbers. The Ω and\\nΘ notations were advocated by Knuth [265] to correct the popular, but\\ntechnically sloppy, practice in the literature of using O-notation for both\\nupper and lower bounds. As noted earlier in this chapter, many people\\ncontinue to use the O-notation where the Θ -notation is more technically\\nprecise. The soft-oh notation \\n in Problem 3-6 was introduced by Babai,\\nLuks, and Seress [31], although it was originally written as O~. Some\\nauthors now deﬁne \\n  as ignoring factors that are logarithmic in\\ng(n), rather than in n. With this deﬁnition, we can say that \\n ,\\nbut with the deﬁnition in Problem 3-6, this statement is not true.\\nFurther discussion of the history and development of asymptotic\\nnotations appears in works by Knuth [259, 265] and Brassard and\\nBratley [70].\\nNot all authors deﬁne the asymptotic notations in the same way,\\nalthough the various deﬁnitions agree in most common situations. Some\\nof the alternative deﬁnitions encompass functions that are not\\nasymptotically nonnegative, as long as their absolute values are\\nappropriately bounded.\\nEquation (3.29) is due to Robbins [381]. Other properties of\\nelementary mathematical functions can be found in any good\\nmathematical reference, such as Abramowitz and Stegun [1] or\\nZwillinger [468], or in a calculus book, such as Apostol [19] or Thomas\\net al. [433]. Knuth [259] and Graham, Knuth, and Patashnik [199]\\ncontain a wealth of material on discrete mathematics as used in\\ncomputer science.\\n1 Within set notation, a colon means “such that.”', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 117}),\n",
              " Document(page_content='4\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0Divide-and-Conquer\\nThe divide-and-conquer method is a powerful strategy for designing\\nasymptotically efﬁcient algorithms. We saw an example of divide-and-\\nconquer in Section 2.3.1 when learning about merge sort. In this\\nchapter, we’ll explore applications of the divide-and-conquer method\\nand acquire valuable mathematical tools that you can use to solve the\\nrecurrences that arise when analyzing divide-and-conquer algorithms.\\nRecall that for divide-and-conquer, you solve a given problem\\n(instance) recursively. If the problem is small enough—the base case—\\nyou just solve it directly without recursing. Otherwise—the recursive\\ncase—you perform three characteristic steps:\\nDivide the problem into one or more subproblems that are smaller\\ninstances of the same problem.\\nConquer the subproblems by solving them recursively.\\nCombine the subproblem solutions to form a solution to the original\\nproblem.\\nA divide-and-conquer algorithm breaks down a large problem into\\nsmaller subproblems, which themselves may be broken down into even\\nsmaller subproblems, and so forth. The recursion bottoms out when it\\nreaches a base case and the subproblem is small enough to solve directly\\nwithout further recursing.\\nRecurrences', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 118}),\n",
              " Document(page_content='To analyze recursive divide-and-conquer algorithms, we’ll need some\\nmathematical tools. A recurrence is an equation that describes a\\nfunction in terms of its value on other, typically smaller, arguments.\\nRecurrences go hand in hand with the divide-and-conquer method\\nbecause they give us a natural way to characterize the running times of\\nrecursive algorithms mathematically. You saw an example of a\\nrecurrence in Section 2.3.2 when we analyzed the worst-case running\\ntime of merge sort.\\nFor the divide-and-conquer matrix-multiplication algorithms\\npresented in Sections 4.1 and 4.2, we’ll derive recurrences that describe\\ntheir worst-case running times. To understand why these two divide-\\nand-conquer algorithms perform the way they do, you’ll need to learn\\nhow to solve the recurrences that describe their running times. Sections\\n4.3–4.7 teach several methods for solving recurrences. These sections\\nalso explore the mathematics behind recurrences, which can give you\\nstronger intuition for designing your own divide-and-conquer\\nalgorithms.\\nWe want to get to the algorithms as soon as possible. So, let’s just\\ncover a few recurrence basics now, and then we’ll look more deeply at\\nrecurrences, especially how to solve them, after we see the matrix-\\nmultiplication examples.\\nThe general form of a recurrence is an equation or inequality that\\ndescribes a function over the integers or reals using the function itself. It\\ncontains two or more cases, depending on the argument. If a case\\ninvolves the recursive invocation of the function on different (usually\\nsmaller) inputs, it is a recursive case. If a case does not involve a\\nrecursive invocation, it is a base case. There may be zero, one, or many\\nfunctions that satisfy the statement of the recurrence. The recurrence is\\nwell deﬁned if there is at least one function that satisﬁes it, and ill deﬁned\\notherwise.\\nAlgorithmic recurrences\\nWe’ll be particularly interested in recurrences that describe the running\\ntimes of divide-and-conquer algorithms. A recurrence T (n) is', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 119}),\n",
              " Document(page_content='algorithmic if, for every sufﬁciently large threshold constant n0 > 0, the\\nfollowing two properties hold:\\n1. For all n < n0, we have T (n) = Θ (1).\\n2. For all n ≥ n0, every path of recursion terminates in a deﬁned base\\ncase within a ﬁnite number of recursive invocations.\\nSimilar to how we sometimes abuse asymptotic notation (see page 60),\\nwhen a function is not deﬁned for all arguments, we understand that\\nthis deﬁnition is constrained to values of n for which T (n) is deﬁned.\\nWhy would a recurrence T (n) that represents a (correct) divide-and-\\nconquer algorithm’s worst-case running time satisfy these properties for\\nall sufﬁciently large threshold constants? The ﬁrst property says that\\nthere exist constants c1, c2 such that 0 < c1 ≤ T (n) ≤ c2 for n < n0. For\\nevery legal input, the algorithm must output the solution to the problem\\nit’s solving in ﬁnite time (see Section 1.1). Thus we can let c1 be the\\nminimum amount of time to call and return from a procedure, which\\nmust be positive, because machine instructions need to be executed to\\ninvoke a procedure. The running time of the algorithm may not be\\ndeﬁned for some values of n if there are no legal inputs of that size, but\\nit must be deﬁned for at least one, or else the “algorithm” doesn’t solve\\nany problem. Thus we can let c2 be the algorithm’s maximum running\\ntime on any input of size n < n0, where n0 is sufﬁciently large that the\\nalgorithm solves at least one problem of size less than n0. The\\nmaximum is well deﬁned, since there are at most a ﬁnite number of\\ninputs of size less than n0, and there is at least one if n0 is sufﬁciently\\nlarge. Consequently, T (n) satisﬁes the ﬁrst property. If the second\\nproperty fails to hold for T (n), then the algorithm isn’t correct, because\\nit would end up in an inﬁnite recursive loop or otherwise fail to\\ncompute a solution. Thus, it stands to reason that a recurrence for the\\nworst-case running time of a correct divide-and-conquer algorithm\\nwould be algorithmic.\\nConventions for recurrences', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 120}),\n",
              " Document(page_content='We adopt the following convention:\\nWhenever a recurrence is stated without an explicit base case, we\\nassume that the recurrence is algorithmic.\\nThat means you’re free to pick any sufﬁciently large threshold constant\\nn0 for the range of base cases where T (n) = Θ (1). Interestingly, the\\nasymptotic solutions of most algorithmic recurrences you’re likely to see\\nwhen analyzing algorithms don’t depend on the choice of threshold\\nconstant, as long as it’s large enough to make the recurrence well\\ndeﬁned.\\nAsymptotic solutions of algorithmic divide-and-conquer recurrences\\nalso don’t tend to change when we drop any ﬂoors or ceilings in a\\nrecurrence deﬁned on the integers to convert it to a recurrence deﬁned\\non the reals. Section 4.7 gives a sufﬁcient condition for ignoring ﬂoors\\nand ceilings that applies to most of the divide-and-conquer recurrences\\nyou’re likely to see. Consequently, we’ll frequently state algorithmic\\nrecurrences without ﬂoors and ceilings. Doing so generally simpliﬁes the\\nstatement of the recurrences, as well as any math that we do with them.\\nYou may sometimes see recurrences that are not equations, but\\nrather inequalities, such as T (n) ≤ 2T (n/2) + Θ (n). Because such a\\nrecurrence states only an upper bound on T (n), we express its solution\\nusing O-notation rather than Θ -notation. Similarly, if the inequality is\\nreversed to T (n) ≥ 2T (n/2) + Θ (n), then, because the recurrence gives\\nonly a lower bound on T (n), we use Ω-notation in its solution.\\nDivide-and-conquer and recurrences\\nThis chapter illustrates the divide-and-conquer method by presenting\\nand using recurrences to analyze two divide-and-conquer algorithms for\\nmultiplying n × n matrices. Section 4.1 presents a simple divide-and-\\nconquer algorithm that solves a matrix-multiplication problem of size n\\nby breaking it into four subproblems of size n/2, which it then solves\\nrecursively. The running time of the algorithm can be characterized by\\nthe recurrence\\nT (n) = 8T (n/2) + Θ (1),', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 121}),\n",
              " Document(page_content='which turns out to have the solution T (n) = Θ (n3). Although this\\ndivide-and-conquer algorithm is no faster than the straightforward\\nmethod that uses a triply nested loop, it leads to an asymptotically\\nfaster divide-and-conquer algorithm due to V. Strassen, which we’ll\\nexplore in Section 4.2. Strassen’s remarkable algorithm divides a\\nproblem of size n into seven subproblems of size n/2 which it solves\\nrecursively. The running time of Strassen’s algorithm can be described\\nby the recurrence\\nT (n) = 7T (n/2) + Θ (n2),\\nwhich has the solution T (n) = Θ (nlg 7) = O(n2.81). Strassen’s algorithm\\nbeats the straightforward looping method asymptotically.\\nThese two divide-and-conquer algorithms both break a problem of\\nsize n into several subproblems of size n/2. Although it is common when\\nusing divide-and-conquer for all the subproblems to have the same size,\\nthat isn’t always the case. Sometimes it’s productive to divide a problem\\nof size n into subproblems of different sizes, and then the recurrence\\ndescribing the running time reﬂects the irregularity. For example,\\nconsider a divide-and-conquer algorithm that divides a problem of size\\nn into one subproblem of size n/3 and another of size 2n/3, taking Θ (n)\\ntime to divide the problem and combine the solutions to the\\nsubproblems. Then the algorithm’s running time can be described by the\\nrecurrence\\nT (n) = T (n/3) + T (2n/3) + Θ (n),\\nwhich turns out to have solution T (n) = Θ (n lg n). We’ll even see an\\nalgorithm in Chapter 9 that solves a problem of size n by recursively\\nsolving a subproblem of size n/5 and another of size 7n/10, taking Θ (n)\\ntime for the divide and combine steps. Its performance satisﬁes the\\nrecurrence\\nT (n) = T (n/5) + T (7n/10) + Θ (n),\\nwhich has solution T (n) = Θ (n).\\nAlthough divide-and-conquer algorithms usually create subproblems\\nwith sizes a constant fraction of the original problem size, that’s not', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 122}),\n",
              " Document(page_content='always the case. For example, a recursive version of linear search (see\\nExercise 2.1-4) creates just one subproblem, with one element less than\\nthe original problem. Each recursive call takes constant time plus the\\ntime to recursively solve a subproblem with one less element, leading to\\nthe recurrence\\nT (n) = T (n – 1) + Θ (1),\\nwhich has solution T (n) = Θ (n). Nevertheless, the vast majority of\\nefﬁcient divide-and-conquer algorithms solve subproblems that are a\\nconstant fraction of the size of the original problem, which is where\\nwe’ll focus our efforts.\\nSolving recurrences\\nAfter learning about divide-and-conquer algorithms for matrix\\nmultiplication in Sections 4.1 and 4.2, we’ll explore several\\nmathematical tools for solving recurrences—that is, for obtaining\\nasymptotic Θ -, O-, or Ω-bounds on their solutions. We want simple-to-\\nuse tools that can handle the most commonly occurring situations. But\\nwe also want general tools that work, perhaps with a little more effort,\\nfor less common cases. This chapter offers four methods for solving\\nrecurrences:\\nIn the substitution method (Section 4.3), you guess the form of a\\nbound and then use mathematical induction to prove your guess\\ncorrect and solve for constants. This method is perhaps the most\\nrobust method for solving recurrences, but it also requires you to\\nmake a good guess and to produce an inductive proof.\\nThe recursion-tree method (Section 4.4) models the recurrence as a\\ntree whose nodes represent the costs incurred at various levels of\\nthe recursion. To solve the recurrence, you determine the costs at\\neach level and add them up, perhaps using techniques for\\nbounding summations from Section A.2. Even if you don’t use\\nthis method to formally prove a bound, it can be helpful in\\nguessing the form of the bound for use in the substitution\\nmethod.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 123}),\n",
              " Document(page_content='The master method (Sections 4.5 and 4.6) is the easiest method,\\nwhen it applies. It provides bounds for recurrences of the form\\nT (n) = aT (n/b) + f (n),\\nwhere a > 0 and b > 1 are constants and f (n) is a given “driving”\\nfunction. This type of recurrence tends to arise more frequently in\\nthe study of algorithms than any other. It characterizes a divide-\\nand-conquer algorithm that creates a subproblems, each of which\\nis 1/b times the size of the original problem, using f (n) time for\\nthe divide and combine steps. To apply the master method, you\\nneed to memorize three cases, but once you do, you can easily\\ndetermine asymptotic bounds on running times for many divide-\\nand-conquer algorithms.\\nThe Akra-Bazzi method (Section 4.7) is a general method for\\nsolving divide-and-conquer recurrences. Although it involves\\ncalculus, it can be used to attack more complicated recurrences\\nthan those addressed by the master method.\\n4.1\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0Multiplying square matrices\\nWe can use the divide-and-conquer method to multiply square matrices.\\nIf you’ve seen matrices before, then you probably know how to multiply\\nthem. (Otherwise, you should read Section D.1.) Let A = (aik) and B =\\n(bjk) be square n × n matrices. The matrix product C = A · B is also an n\\n× n matrix, where for i, j = 1, 2, … , n, the (i, j) entry of C is given by\\nGenerally, we’ll assume that the matrices are dense, meaning that most\\nof the n2 entries are not 0, as opposed to sparse, where most of the n2\\nentries are 0 and the nonzero entries can be stored more compactly than\\nin an n × n array.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 124}),\n",
              " Document(page_content='Computing the matrix C requires computing n2 matrix entries, each\\nof which is the sum of n pairwise products of input elements from A and\\nB. The MATRIX-MULTIPLY procedure implements this strategy in a\\nstraightforward manner, and it generalizes the problem slightly. It takes\\nas input three n × n matrices A, B, and C, and it adds the matrix\\nproduct A · B to C, storing the result in C. Thus, it computes C = C + A\\n· B, instead of just C = A · B. If only the product A · B is needed, just\\ninitialize all n2 entries of C to 0 before calling the procedure, which\\ntakes an additional Θ (n2) time. We’ll see that the cost of matrix\\nmultiplication asymptotically dominates this initialization cost.\\nMATRIX-MULTIPLY(A, B, C, n)\\n1for\\xa0i = 1 to\\xa0n // compute entries in each of n rows\\n2 for\\xa0j = 1 to\\xa0n // compute n entries in row i\\n3 for\\xa0k = 1 to\\xa0n\\n4 cij = cij + aik · bkj// add in another term of equation\\n(4.1)\\nThe pseudocode for MATRIX-MULTIPLY works as follows. The\\nfor loop of lines 1–4 computes the entries of each row i, and within a\\ngiven row i, the for loop of lines 2–4 computes each of the entries cij for\\neach column j. Each iteration of the for loop of lines 3–4 adds in one\\nmore term of equation (4.1).\\nBecause each of the triply nested for loops runs for exactly n\\niterations, and each execution of line 4 takes constant time, the\\nMATRIX-MULTIPLY procedure operates in Θ (n3) time. Even if we\\nadd in the Θ (n2) time for initializing C to 0, the running time is still\\nΘ(n3).\\nA simple divide-and-conquer algorithm\\nLet’s see how to compute the matrix product A · B using divide-and-\\nconquer. For n > 1, the divide step partitions the n × n matrices into\\nfour n/2 × n/2 submatrices. We’ll assume that n is an exact power of 2, so', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 125}),\n",
              " Document(page_content='that as the algorithm recurses, we are guaranteed that the submatrix\\ndimensions are integer. (Exercise 4.1-1 asks you to relax this\\nassumption.) As with MATRIX-MULTIPLY, we’ll actually compute C\\n= C + A · B. But to simplify the math behind the algorithm, let’s assume\\nthat C has been initialized to the zero matrix, so that we are indeed\\ncomputing C = A · B.\\nThe divide step views each of the n × n matrices A, B, and C as four\\nn/2 × n/2 submatrices:\\nThen we can write the matrix product as\\nwhich corresponds to the equations\\nEquations (4.5)–(4.8) involve eight n/2 × n/2 multiplications and four\\nadditions of n/2 × n/2 submatrices.\\nAs we look to transform these equations to an algorithm that can be\\ndescribed with pseudocode, or even implemented for real, there are two\\ncommon approaches for implementing the matrix partitioning.\\nOne strategy is to allocate temporary storage to hold A’s four\\nsubmatrices A11, A12, A21, and A22 and B’s four submatrices B11,\\nB12, B21, and B22. Then copy each element in A and B to its\\ncorresponding location in the appropriate submatrix. After the recursive\\nconquer step, copy the elements in each of C’s four submatrices C11,', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 126}),\n",
              " Document(page_content='C12, C21, and C22 to their corresponding locations in C. This approach\\ntakes Θ (n2) time, since 3n2 elements are copied.\\nThe second approach uses index calculations and is faster and more\\npractical. A submatrix can be speciﬁed within a matrix by indicating\\nwhere within the matrix the submatrix lies without touching any matrix\\nelements. Partitioning a matrix (or recursively, a submatrix) only\\ninvolves arithmetic on this location information, which has constant\\nsize independent of the size of the matrix. Changes to the submatrix\\nelements update the original matrix, since they occupy the same storage.\\nGoing forward, we’ll assume that index calculations are used and\\nthat partitioning can be performed in Θ (1) time. Exercise 4.1-3 asks you\\nto show that it makes no difference to the overall asymptotic running\\ntime of matrix multiplication, however, whether the partitioning of\\nmatrices uses the ﬁrst method of copying or the second method of index\\ncalculation. But for other divide-and-conquer matrix calculations, such\\nas matrix addition, it can make a difference, as Exercise 4.1-4 asks you\\nto show.\\nThe procedure MATRIX-MULTIPLY-RECURSIVE uses equations\\n(4.5)–(4.8) to implement a divide-and-conquer strategy for square-\\nmatrix multiplication. Like MATRIX-MULTIPLY, the procedure\\nMATRIX-MULTIPLY-RECURSIVE computes C = C + A · B since, if\\nnecessary, C can be initialized to 0 before the procedure is called in\\norder to compute only C = A · B.\\nMATRIX-MULTIPLY-RECURSIVE(A, B, C, n)\\n\\xa0\\xa01if\\xa0n == 1\\n\\xa0\\xa02// Base case.\\n\\xa0\\xa03c11 = c11 + a11 · b11\\n\\xa0\\xa04return\\n\\xa0\\xa05// Divide.\\n\\xa0\\xa06partition A, B, and C into n/2 × n/2 submatrices\\nA11, A12, A21, A22; B11, B12, B21, B22;\\nand C11, C12, C21, C22; respectively\\n\\xa0\\xa07// Conquer.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 127}),\n",
              " Document(page_content='\\xa0\\xa08MATRIX-MULTIPLY-RECURSIVE(A11, B11, C11, n/2)\\n\\xa0\\xa09MATRIX-MULTIPLY-RECURSIVE(A11, B12, C12, n/2)\\n10MATRIX-MULTIPLY-RECURSIVE(A21, B11, C21, n/2)\\n11MATRIX-MULTIPLY-RECURSIVE(A21, B12, C22, n/2)\\n12MATRIX-MULTIPLY-RECURSIVE(A12, B21, C11, n/2)\\n13MATRIX-MULTIPLY-RECURSIVE(A12, B22, C12, n/2)\\n14MATRIX-MULTIPLY-RECURSIVE(A22, B21, C21, n/2)\\n15MATRIX-MULTIPLY-RECURSIVE(A22, B22, C22, n/2)\\nAs we walk through the pseudocode, we’ll derive a recurrence to\\ncharacterize its running time. Let T (n) be the worst-case time to\\nmultiply two n × n matrices using this procedure.\\nIn the base case, when n = 1, line 3 performs just the one scalar\\nmultiplication and one addition, which means that T (1) = Θ (1). As is\\nour convention for constant base cases, we can omit this base case in the\\nstatement of the recurrence.\\nThe recursive case occurs when n > 1. As discussed, we’ll use index\\ncalculations to partition the matrices in line 6, taking Θ (1) time. Lines\\n8–15 recursively call MATRIX-MULTIPLY-RECURSIVE a total of\\neight times. The ﬁrst four recursive calls compute the ﬁrst terms of\\nequations (4.5)–(4.8), and the subsequent four recursive calls compute\\nand add in the second terms. Each recursive call adds the product of a\\nsubmatrix of A and a submatrix of B to the appropriate submatrix of C\\nin place, thanks to index calculations. Because each recursive call\\nmultiplies two n/2 × n/2 matrices, thereby contributing T (n/2) to the\\noverall running time, the time taken by all eight recursive calls is 8T\\n(n/2). There is no combine step, because the matrix C is updated in\\nplace. The total time for the recursive case, therefore, is the sum of the\\npartitioning time and the time for all the recursive calls, or Θ (1) + 8T\\n(n/2).\\nThus, omitting the statement of the base case, our recurrence for the\\nrunning time of MATRIX-MULTIPLY-RECURSIVE is\\n', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 128}),\n",
              " Document(page_content='As we’ll see from the master method in Section 4.5, recurrence (4.9) has\\nthe solution T (n) = Θ (n3), which means that it has the same asymptotic\\nrunning time as the straightforward MATRIX-MULTIPLY procedure.\\nWhy is the Θ (n3) solution to this recurrence so much larger than the\\nΘ(n lg n) solution to the merge-sort recurrence (2.3) on page 41? After\\nall, the recurrence for merge sort contains a Θ (n) term, whereas the\\nrecurrence for recursive matrix multiplication contains only a Θ (1) term.\\nLet’s think about what the recursion tree for recurrence (4.9) would\\nlook like as compared with the recursion tree for merge sort, illustrated\\nin Figure 2.5 on page 43. The factor of 2 in the merge-sort recurrence\\ndetermines how many children each tree node has, which in turn\\ndetermines how many terms contribute to the sum at each level of the\\ntree. In comparison, for the recurrence (4.9) for MATRIX-MULTIPLY-\\nRECURSIVE, each internal node in the recursion tree has eight\\nchildren, not two, leading to a “bushier” recursion tree with many more\\nleaves, despite the fact that the internal nodes are each much smaller.\\nConsequently, the solution to recurrence (4.9) grows much more quickly\\nthan the solution to recurrence (2.3), which is borne out in the actual\\nsolutions: Θ (n3) versus Θ (n lg n).\\nExercises\\nNote: You may wish to read Section 4.5 before attempting some of these\\nexercises.\\n4.1-1\\nGeneralize MATRIX-MULTIPLY-RECURSIVE to multiply n × n\\nmatrices for which n is not necessarily an exact power of 2. Give a\\nrecurrence describing its running time. Argue that it runs in Θ (n3) time\\nin the worst case.\\n4.1-2\\nHow quickly can you multiply a k n × n matrix (k n rows and n\\ncolumns) by an n × k n matrix, where k ≥ 1, using MATRIX-\\nMULTIPLY-RECURSIVE as a subroutine? Answer the same question', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 129}),\n",
              " Document(page_content='for multiplying an n × k n matrix by a k n × n matrix. Which is\\nasymptotically faster, and by how much?\\n4.1-3\\nSuppose that instead of partitioning matrices by index calculation in\\nMATRIX-MULTIPLY-RECURSIVE, you copy the appropriate\\nelements of A, B, and C into separate n/2 × n/2 submatrices A11, A12,\\nA21, A22; B11, B12, B21, B22; and C11, C12, C21, C22, respectively.\\nAfter the recursive calls, you copy the results from C11, C12, C21, and\\nC22 back into the appropriate places in C. How does recurrence (4.9)\\nchange, and what is its solution?\\n4.1-4\\nWrite pseudocode for a divide-and-conquer algorithm MATRIX-ADD-\\nRECURSIVE that sums two n × n matrices A and B by partitioning\\neach of them into four n/2 × n/2 submatrices and then recursively\\nsumming corresponding pairs of submatrices. Assume that matrix\\npartitioning uses Θ (1)-time index calculations. Write a recurrence for\\nthe worst-case running time of MATRIX-ADD-RECURSIVE, and\\nsolve your recurrence. What happens if you use Θ (n2)-time copying to\\nimplement the partitioning instead of index calculations?\\n4.2\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0Strassen’s algorithm for matrix multiplication\\nYou might ﬁnd it hard to imagine that any matrix multiplication\\nalgorithm could take less than Θ (n3) time, since the natural deﬁnition of\\nmatrix multiplication requires n3 scalar multiplications. Indeed, many\\nmathematicians presumed that it was not possible to multiply matrices\\nin o(n3) time until 1969, when V. Strassen [424] published a remarkable\\nrecursive algorithm for multiplying n × n matrices. Strassen’s algorithm\\nruns in Θ (nlg 7) time. Since lg 7 = 2.8073549 …, Strassen’s algorithm\\nruns in O(n2.81) time, which is asymptotically better than the Θ (n3)', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 130}),\n",
              " Document(page_content='MATRIX-MULTIPLY and MATRIX-MULTIPLY-RECURSIVE\\nprocedures.\\nThe key to Strassen’s method is to use the divide-and-conquer idea\\nfrom the MATRIX-MULTIPLY-RECURSIVE procedure, but make\\nthe recursion tree less bushy. We’ll actually increase the work for each\\ndivide and combine step by a constant factor, but the reduction in\\nbushiness will pay off. We won’t reduce the bushiness from the eight-way\\nbranching of recurrence (4.9) all the way down to the two-way\\nbranching of recurrence (2.3), but we’ll improve it just a little, and that\\nwill make a big difference. Instead of performing eight recursive\\nmultiplications of n/2 × n/2 matrices, Strassen’s algorithm performs only\\nseven. The cost of eliminating one matrix multiplication is several new\\nadditions and subtractions of n/2 × n/2 matrices, but still only a\\nconstant number. Rather than saying “additions and subtractions”\\neverywhere, we’ll adopt the common terminology of calling them both\\n“additions” because subtraction is structurally the same computation as\\naddition, except for a change of sign.\\nTo get an inkling how the number of multiplications might be\\nreduced, as well as why reducing the number of multiplications might be\\ndesirable for matrix calculations, suppose that you have two numbers x\\nand y, and you want to calculate the quantity x2 – y2. The\\nstraightforward calculation requires two multiplications to square x and\\ny, followed by one subtraction (which you can think of as a “negative\\naddition”). But let’s recall the old algebra trick x2 – y2 = x2 – xy + xy –\\ny2 = x(x – y) + y(x – y) = (x + y)(x – y). Using this formulation of the\\ndesired quantity, you could instead compute the sum x + y and the\\ndifference x – y and then multiply them, requiring only a single\\nmultiplication and two additions. At the cost of an extra addition, only\\none multiplication is needed to compute an expression that looks as if it\\nrequires two. If x and y are scalars, there’s not much difference: both\\napproaches require three scalar operations. If x and y are large matrices,\\nhowever, the cost of multiplying outweighs the cost of adding, in which\\ncase the second method outperforms the ﬁrst, although not\\nasymptotically.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 131}),\n",
              " Document(page_content='Strassen’s strategy for reducing the number of matrix multiplications\\nat the expense of more matrix additions is not at all obvious—perhaps\\nthe biggest understatement in this book! As with MATRIX-\\nMULTIPLY-RECURSIVE, Strassen’s algorithm uses the divide-and-\\nconquer method to compute C = C + A · B, where A, B, and C are all n\\n× n matrices and n is an exact power of 2. Strassen’s algorithm\\ncomputes the four submatrices C11, C12, C21, and C22 of C from\\nequations (4.5)–(4.8) on page 82 in four steps. We’ll analyze costs as we\\ngo along to develop a recurrence T (n) for the overall running time. Let’s\\nsee how it works:\\n1. If n = 1, the matrices each contain a single element. Perform a\\nsingle scalar multiplication and a single scalar addition, as in line\\n3 of MATRIX-MULTIPLY-RECURSIVE, taking Θ (1) time,\\nand return. Otherwise, partition the input matrices A and B and\\noutput matrix C into n/2 × n/2 submatrices, as in equation (4.2).\\nThis step takes Θ (1) time by index calculation, just as in\\nMATRIX-MULTIPLY-RECURSIVE.\\n2. Create n/2 × n/2 matrices S1, S2, … , S10, each of which is the\\nsum or difference of two submatrices from step 1. Create and\\nzero the entries of seven n/2 × n/2 matrices P1, P2, … , P7 to\\nhold seven n/2 × n/2 matrix products. All 17 matrices can be\\ncreated, and the Pi initialized, in Θ (n2) time.\\n3. Using the submatrices from step 1 and the matrices S1, S2, … ,\\nS10 created in step 2, recursively compute each of the seven\\nmatrix products P1, P2, … , P7, taking 7T (n/2) time.\\n4. Update the four submatrices C11, C12, C21, C22 of the result\\nmatrix C by adding or subtracting various Pi matrices, which\\ntakes Θ (n2) time.\\nWe’ll see the details of steps 2–4 in a moment, but we already have\\nenough information to set up a recurrence for the running time of\\nStrassen’s method. As is common, the base case in step 1 takes Θ (1)', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 132}),\n",
              " Document(page_content='time, which we’ll omit when stating the recurrence. When n > 1, steps 1,\\n2, and 4 take a total of Θ (n2) time, and step 3 requires seven\\nmultiplications of n/2 × n/2 matrices. Hence, we obtain the following\\nrecurrence for the running time of Strassen’s algorithm:\\nCompared with MATRIX-MULTIPLY-RECURSIVE, we have traded\\noff one recursive submatrix multiplication for a constant number of\\nsubmatrix additions. Once you understand recurrences and their\\nsolutions, you’ll be able to see why this trade-off actually leads to a\\nlower asymptotic running time. By the master method in Section 4.5,\\nrecurrence (4.10) has the solution T (n) = Θ (nlg 7) = O(n2.81), beating\\nthe Θ (n3)-time algorithms.\\nNow, let’s delve into the details. Step 2 creates the following 10\\nmatrices:\\nS1=B12 – B22,\\nS2=A11 + A12,\\nS3=A21 + A22,\\nS4=B21 – B11,\\nS5=A11 + A22,\\nS6=B11 + B22,\\nS7=A12 – A22,\\nS8=B21 + B22,\\nS9=A11 – A21,\\nS10=B11 + B12.\\nThis step adds or subtracts n/2 × n/2 matrices 10 times, taking Θ (n2)\\ntime.\\nStep 3 recursively multiplies n/2 × n/2 matrices 7 times to compute\\nthe following n/2 × n/2 matrices, each of which is the sum or difference', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 133}),\n",
              " Document(page_content='of products of A and B submatrices:\\nP1=A11 · S1(= A11 · B12 – A11 · B22),\\nP2=S2 · B22(= A11 · B22 + A12 · B22),\\nP3=S3 · B11(= A21 · B11 + A22 · B11),\\nP4=A22 · S4(= A22 · B21 – A22 · B11),\\nP5=S5 · S6(= A11 · B11 + A11 · B22 + A22 · B11 + A22 · B22),\\nP6=S7 · S8(= A12 · B21 + A12 · B22 – A22 · B21 – A22 · B22),\\nP7=S9 · S10(= A11 · B11 + A11 · B12 – A21 · B11 – A21 · B12).\\nThe only multiplications that the algorithm performs are those in the\\nmiddle column of these equations. The right-hand column just shows\\nwhat these products equal in terms of the original submatrices created\\nin step 1, but the terms are never explicitly calculated by the algorithm.\\nStep 4 adds to and subtracts from the four n/2 × n/2 submatrices of\\nthe product C the various Pi matrices created in step 3. We start with\\nC11 = C11 + P5 + P4 – P2 + P6.\\nExpanding the calculation on the right-hand side, with the expansion of\\neach Pi on its own line and vertically aligning terms that cancel out, we\\nsee that the update to C11 equals\\nwhich corresponds to equation (4.5). Similarly, setting\\nC12 = C12 + P1 + P2\\nmeans that the update to C12 equals', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 134}),\n",
              " Document(page_content='corresponding to equation (4.6). Setting\\nC21 = C21 + P3 + P4\\nmeans that the update to C21 equals\\ncorresponding to equation (4.7). Finally, setting\\nC22 = C22 + P5 + P1 – P3 – P7\\nmeans that the update to C22 equals\\nwhich corresponds to equation (4.8). Altogether, since we add or\\nsubtract n/2×n/2 matrices 12 times in step 4, this step indeed takes Θ (n2)\\ntime.\\nWe can see that Strassen’s remarkable algorithm, comprising steps 1–\\n4, produces the correct matrix product using 7 submatrix\\nmultiplications and 18 submatrix additions. We can also see that\\nrecurrence (4.10) characterizes its running time. Since Section 4.5 shows\\nthat this recurrence has the solution T (n) = Θ (nlg 7) = o(n3), Strassen’s\\nmethod asymptotically beats the Θ (n3) MATRIX-MULTIPLY and\\nMATRIX-MULTIPLY-RECURSIVE procedures.\\nExercises', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 135}),\n",
              " Document(page_content='Note: You may wish to read Section 4.5 before attempting some of these\\nexercises.\\n4.2-1\\nUse Strassen’s algorithm to compute the matrix product\\nShow your work.\\n4.2-2\\nWrite pseudocode for Strassen’s algorithm.\\n4.2-3\\nWhat is the largest k such that if you can multiply 3 × 3 matrices using k\\nmultiplications (not assuming commutativity of multiplication), then\\nyou can multiply n × n matrices in o(nlg 7) time? What is the running\\ntime of this algorithm?\\n4.2-4\\nV. Pan discovered a way of multiplying 68 × 68 matrices using 132,464\\nmultiplications, a way of multiplying 70 × 70 matrices using 143,640\\nmultiplications, and a way of multiplying 72 × 72 matrices using\\n155,424 multiplications. Which method yields the best asymptotic\\nrunning time when used in a divide-and-conquer matrix-multiplication\\nalgorithm? How does it compare with Strassen’s algorithm?\\n4.2-5\\nShow how to multiply the complex numbers a + bi and c + d i using\\nonly three multiplications of real numbers. The algorithm should take a,\\nb, c, and d as input and produce the real component ac – bd and the\\nimaginary component ad + bc separately.\\n4.2-6\\nSuppose that you have a Θ (nα)-time algorithm for squaring n × n\\nmatrices, where α ≥ 2. Show how to use that algorithm to multiply two\\ndifferent n × n matrices in Θ (nα) time.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 136}),\n",
              " Document(page_content='4.3\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0The substitution method for solving recurrences\\nNow that you have seen how recurrences characterize the running times\\nof divide-and-conquer algorithms, let’s learn how to solve them. We\\nstart in this section with the substitution method, which is the most\\ngeneral of the four methods in this chapter. The substitution method\\ncomprises two steps:\\n1. Guess the form of the solution using symbolic constants.\\n2. Use mathematical induction to show that the solution works,\\nand ﬁnd the constants.\\nTo apply the inductive hypothesis, you substitute the guessed solution\\nfor the function on smaller values—hence the name “substitution\\nmethod.” This method is powerful, but you must guess the form of the\\nanswer. Although generating a good guess might seem difﬁcult, a little\\npractice can quickly improve your intuition.\\nYou can use the substitution method to establish either an upper or a\\nlower bound on a recurrence. It’s usually best not to try to do both at\\nthe same time. That is, rather than trying to prove a Θ -bound directly,\\nﬁrst prove an O-bound, and then prove an Ω-bound. Together, they give\\nyou a Θ -bound (Theorem 3.1 on page 56).\\nAs an example of the substitution method, let’s determine an\\nasymptotic upper bound on the recurrence:\\nThis recurrence is similar to recurrence (2.3) on page 41 for merge sort,\\nexcept for the ﬂoor function, which ensures that T (n) is deﬁned over the\\nintegers. Let’s guess that the asymptotic upper bound is the same—T (n)\\n= O(n lg n)—and use the substitution method to prove it.\\nWe’ll adopt the inductive hypothesis that T (n) ≤ c n lg n for all n ≥\\nn0, where we’ll choose the speciﬁc constants c > 0 and n0 > 0 later, after\\nwe see what constraints they need to obey. If we can establish this\\ninductive hypothesis, we can conclude that T (n) = O(n lg n). It would be\\ndangerous to use T (n) = O(n lg n) as the inductive hypothesis because', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 137}),\n",
              " Document(page_content='the constants matter, as we’ll see in a moment in our discussion of\\npitfalls.\\nAssume by induction that this bound holds for all numbers at least\\nas big as n0 and less than n. In particular, therefore, if n ≥ 2n0, it holds\\nfor ⌊n/2 ⌋, yielding T ( ⌊n/2 ⌋) ≤ c ⌊n/2 ⌋ lg( ⌊n/2 ⌋). Substituting into\\nrecurrence (4.11)—hence the name “substitution” method—yields\\nT (n)≤2(c ⌊n/2 ⌋ lg( ⌊n/2 ⌋)) + Θ (n)\\n≤2(c(n/2) lg(n/2)) + Θ (n)\\n=cn lg(n/2) + Θ (n)\\n=cn lg n – cn lg 2 + Θ (n)\\n=cn lg n – cn + Θ (n)\\n≤cn lg n,\\nwhere the last step holds if we constrain the constants n0 and c to be\\nsufﬁciently large that for n ≥ 2n0, the quantity cn dominates the\\nanonymous function hidden by the Θ (n) term.\\nWe’ve shown that the inductive hypothesis holds for the inductive\\ncase, but we also need to prove that the inductive hypothesis holds for\\nthe base cases of the induction, that is, that T (n) ≤ cn lg n when n0 ≤ n <\\n2n0. As long as n0 > 1 (a new constraint on n0), we have lg n > 0, which\\nimplies that n lg n > 0. So let’s pick n0 = 2. Since the base case of\\nrecurrence (4.11) is not stated explicitly, by our convention, T (n) is\\nalgorithmic, which means that T (2) and T (3) are constant (as they\\nshould be if they describe the worst-case running time of any real\\nprogram on inputs of size 2 or 3). Picking c = max {T (2), T (3)} yields\\nT (2) ≤ c < (2 lg 2)c and T (3) ≤ c < (3 lg 3)c, establishing the inductive\\nhypothesis for the base cases.\\nThus, we have T (n) ≤ cn lg n for all n ≥ 2, which implies that the\\nsolution to recurrence (4.11) is T (n) = O(n lg n).\\nIn the algorithms literature, people rarely carry out their substitution\\nproofs to this level of detail, especially in their treatment of base cases.\\nThe reason is that for most algorithmic divide-and-conquer recurrences,\\nthe base cases are all handled in pretty much the same way. You ground', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 138}),\n",
              " Document(page_content='the induction on a range of values from a convenient positive constant\\nn0 up to some constant \\n  such that for \\n , the recurrence\\nalways bottoms out in a constant-sized base case between n0 and \\n.\\n(This example used \\n .) Then, it’s usually apparent, without\\nspelling out the details, that with a suitably large choice of the leading\\nconstant (such as c for this example), the inductive hypothesis can be\\nmade to hold for all the values in the range from n0 to \\n.\\nMaking a good guess\\nUnfortunately, there is no general way to correctly guess the tightest\\nasymptotic solution to an arbitrary recurrence. Making a good guess\\ntakes experience and, occasionally, creativity. Fortunately, learning some\\nrecurrence-solving heuristics, as well as playing around with recurrences\\nto gain experience, can help you become a good guesser. You can also\\nuse recursion trees, which we’ll see in Section 4.4, to help generate good\\nguesses.\\nIf a recurrence is similar to one you’ve seen before, then guessing a\\nsimilar solution is reasonable. As an example, consider the recurrence\\nT (n) = 2T (n/2 + 17) + Θ (n),\\ndeﬁned on the reals. This recurrence looks somewhat like the merge-sort\\nrecurrence (2.3), but it’s more complicated because of the added “17” in\\nthe argument to T on the right-hand side. Intuitively, however, this\\nadditional term shouldn’t substantially affect the solution to the\\nrecurrence. When n is large, the relative difference between n/2 and n/2 +\\n17 is not that large: both cut n nearly in half. Consequently, it makes\\nsense to guess that T (n) = O(n lg n), which you can verify is correct\\nusing the substitution method (see Exercise 4.3-1).\\nAnother way to make a good guess is to determine loose upper and\\nlower bounds on the recurrence and then reduce your range of\\nuncertainty. For example, you might start with a lower bound of T (n) =\\nΩ(n) for recurrence (4.11), since the recurrence includes the term Θ (n),\\nand you can prove an initial upper bound of T (n) = O(n2). Then split\\nyour time between trying to lower the upper bound and trying to raise', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 139}),\n",
              " Document(page_content='the lower bound until you converge on the correct, asymptotically tight\\nsolution, which in this case is T (n) = Θ (n lg n).\\nA trick of the trade: subtracting a low-order term\\nSometimes, you might correctly guess a tight asymptotic bound on the\\nsolution of a recurrence, but somehow the math fails to work out in the\\ninduction proof. The problem frequently turns out to be that the\\ninductive assumption is not strong enough. The trick to resolving this\\nproblem is to revise your guess by subtracting a lower-order term when\\nyou hit such a snag. The math then often goes through.\\nConsider the recurrence\\ndeﬁned on the reals. Let’s guess that the solution is T (n) = O(n) and try\\nto show that T (n) ≤ cn for n ≥ n0, where we choose the constants c, n0 >\\n0 suitably. Substituting our guess into the recurrence, we obtain\\nT (n)≤2(c(n/2)) + Θ (1)\\n=cn + Θ (1),\\nwhich, unfortunately, does not imply that T (n) ≤ cn for any choice of c.\\nWe might be tempted to try a larger guess, say T (n) = O(n2). Although\\nthis larger guess works, it provides only a loose upper bound. It turns\\nout that our original guess of T (n) = O(n) is correct and tight. In order\\nto show that it is correct, however, we must strengthen our inductive\\nhypothesis.\\nIntuitively, our guess is nearly right: we are off only by Θ (1), a lower-\\norder term. Nevertheless, mathematical induction requires us to prove\\nthe exact form of the inductive hypothesis. Let’s try our trick of\\nsubtracting a lower-order term from our previous guess: T (n) ≤ cn – d,\\nwhere d ≥ 0 is a constant. We now have\\nT (n)≤2(c(n/2) – d) + Θ (1)\\n=cn – 2d + Θ (1)\\n≤cn – d – (d – Θ(1))', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 140}),\n",
              " Document(page_content='≤cn – d\\nas long as we choose d to be larger than the anonymous upper-bound\\nconstant hidden by the Θ -notation. Subtracting a lower-order term\\nworks! Of course, we must not forget to handle the base case, which is to\\nchoose the constant c large enough that cn – d dominates the implicit\\nbase cases.\\nYou might ﬁnd the idea of subtracting a lower-order term to be\\ncounterintuitive. After all, if the math doesn’t work out, shouldn’t you\\nincrease your guess? Not necessarily! When the recurrence contains\\nmore than one recursive invocation (recurrence (4.12) contains two), if\\nyou add a lower-order term to the guess, then you end up adding it once\\nfor each of the recursive invocations. Doing so takes you even further\\naway from the inductive hypothesis. On the other hand, if you subtract a\\nlower-order term from the guess, then you get to subtract it once for\\neach of the recursive invocations. In the above example, we subtracted\\nthe constant d twice because the coefﬁcient of T (n/2) is 2. We ended up\\nwith the inequality T (n) ≤ cn – d – (d – Θ (1)), and we readily found a\\nsuitable value for d.\\nAvoiding pitfalls\\nAvoid using asymptotic notation in the inductive hypothesis for the\\nsubstitution method because it’s error prone. For example, for\\nrecurrence (4.11), we can falsely “prove” that T (n) = O(n) if we\\nunwisely adopt T (n) = O(n) as our inductive hypothesis:\\nT (n)≤2 · O( ⌊n/2 ⌋) + Θ (n)\\n=2 · O(n) + Θ (n)\\n=O(n).\\n \\xa0wrong!\\nThe problem with this reasoning is that the constant hidden by the O-\\nnotation changes. We can expose the fallacy by repeating the “proof”\\nusing an explicit constant. For the inductive hypothesis, assume that T\\n(n) ≤ cn for all n ≥ n0, where c, n0 > 0 are constants. Repeating the ﬁrst\\ntwo steps in the inequality chain yields', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 141}),\n",
              " Document(page_content='T (n)≤2(c ⌊n/2 ⌋) + Θ (n)\\n≤cn + Θ (n).\\nNow, indeed cn + Θ (n) = O(n), but the constant hidden by the O-\\nnotation must be larger than c because the anonymous function hidden\\nby the Θ (n) is asymptotically positive. We cannot take the third step to\\nconclude that cn + Θ (n) ≤ cn, thus exposing the fallacy.\\nWhen using the substitution method, or more generally\\nmathematical induction, you must be careful that the constants hidden\\nby any asymptotic notation are the same constants throughout the\\nproof. Consequently, it’s best to avoid asymptotic notation in your\\ninductive hypothesis and to name constants explicitly.\\nHere’s another fallacious use of the substitution method to show that\\nthe solution to recurrence (4.11) is T (n) = O(n). We guess T (n) ≤ cn and\\nthen argue\\nT (n)≤2(c ⌊n/2 ⌋) + Θ (n)\\n≤cn + Θ (n)\\n=O(n),\\n \\xa0wrong!\\nsince c is a positive constant. The mistake stems from the difference\\nbetween our goal—to prove that T (n) = O(n)—and our inductive\\nhypothesis—to prove that T (n) ≤ cn. When using the substitution\\nmethod, or in any inductive proof, you must prove the exact statement\\nof the inductive hypothesis. In this case, we must explicitly prove that T\\n(n) ≤ cn to show that T (n) = O(n).\\nExercises\\n4.3-1\\nUse the substitution method to show that each of the following\\nrecurrences deﬁned on the reals has the asymptotic solution speciﬁed:\\na.\\xa0T (n) = T (n – 1) + n has solution T (n) = O(n2).\\nb.\\xa0T (n) = T (n/2) + Θ (1) has solution T (n) = O(lg n).\\nc.\\xa0T (n) = 2T (n/2) + n has solution T (n) = Θ (n lg n).', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 142}),\n",
              " Document(page_content='d.\\xa0T (n) = 2T (n/2 + 17) + n has solution T (n) = O(n lg n).\\ne.\\xa0T (n) = 2T (n/3) + Θ (n) has solution T (n) = Θ (n).\\nf.\\xa0T (n) = 4T (n/2) + Θ (n) has solution T (n) = Θ (n2).\\n4.3-2\\nThe solution to the recurrence T (n) = 4T (n/2)+n turns out to be T (n)\\n= Θ(n2). Show that a substitution proof with the assumption T (n) ≤ cn2\\nfails. Then show how to subtract a lower-order term to make a\\nsubstitution proof work.\\n4.3-3\\nThe recurrence T (n) = 2T (n – 1) + 1 has the solution T (n) = O(2n).\\nShow that a substitution proof fails with the assumption T (n) ≤ c 2n,\\nwhere c > 0 is constant. Then show how to subtract a lower-order term\\nto make a substitution proof work.\\n4.4\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0The recursion-tree method f or solving recurrences\\nAlthough you can use the substitution method to prove that a solution\\nto a recurrence is correct, you might have trouble coming up with a\\ngood guess. Drawing out a recursion tree, as we did in our analysis of\\nthe merge-sort recurrence in Section 2.3.2, can help. In a recursion tree,\\neach node represents the cost of a single subproblem somewhere in the\\nset of recursive function invocations. You typically sum the costs within\\neach level of the tree to obtain the per-level costs, and then you sum all\\nthe per-level costs to determine the total cost of all levels of the\\nrecursion. Sometimes, however, adding up the total cost takes more\\ncreativity.\\nA recursion tree is best used to generate intuition for a good guess,\\nwhich you can then verify by the substitution method. If you are\\nmeticulous when drawing out a recursion tree and summing the costs,\\nhowever, you can use a recursion tree as a direct proof of a solution to a\\nrecurrence. But if you use it only to generate a go od guess, you can often\\ntolerate a small amount of “sloppiness,” which can simplify the math.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 143}),\n",
              " Document(page_content='When you verify your guess with the substitution method later on, your\\nmath should be precise. This section demonstrates how you can use\\nrecursion trees to solve recurrences, generate good guesses, and gain\\nintuition for recurrences.\\nAn illustrative example\\nLet’s see how a recursion tree can provide a good guess for an upper-\\nbound solution to the recurrence\\nFigure 4.1 shows how to derive the recursion tree for T (n) = 3T (n/4) +\\ncn2, where the constant c > 0 is the upper-bound constant in the Θ (n2)\\nterm. Part (a) of the ﬁgure shows T (n), which part (b) expands into an\\nequivalent tree representing the recurrence. The cn2 term at the root\\nrepresents the cost at the top level of recursion, and the three subtrees of\\nthe root represent the costs incurred by the subproblems of size n/4. Part\\n(c) shows this process carried one step further by expanding each node\\nwith cost T (n/4) from part (b). The cost for each of the three children of\\nthe root is c(n/4)2. We continue expanding each node in the tree by\\nbreaking it into its constituent parts as determined by the recurrence.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 144}),\n",
              " Document(page_content='Figure 4.1 Constructing a recursion tree for the recurrence T (n) = 3T (n/4) + cn2. Part (a) shows\\nT (n), which progressively expands in (b)–(d) to form the recursion tree. The fully expanded tree\\nin (d) has height log4\\xa0n.\\nBecause subproblem sizes decrease by a factor of 4 every time we go\\ndown one level, the recursion must eventually bottom out in a base case\\nwhere n < n0. By convention, the base case is T (n) = Θ (1) for n < n0,\\nwhere n0 > 0 is any threshold constant sufﬁciently large that the', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 145}),\n",
              " Document(page_content='recurrence is well deﬁned. For the purpose of intuition, however, let’s\\nsimplify the math a little. Let’s assume that n is an exact power of 4 and\\nthat the base case is T (1) = Θ (1). As it turns out, these assumptions\\ndon’t affect the asymptotic solution.\\nWhat’s the height of the recursion tree? The subproblem size for a\\nnode at depth i is n/4i. As we descend the tree from the root, the\\nsubproblem size hits n = 1 when n/4i = 1 or, equivalently, when i =\\nlog4\\xa0n. Thus, the tree has internal nodes at depths 0, 1, 2, … , log4\\xa0n – 1\\nand leaves at depth log4\\xa0n.\\nPart (d) of Figure 4.1 shows the cost at each level of the tree. Each\\nlevel has three times as many nodes as the level above, and so the\\nnumber of nodes at depth i is 3i. Because subproblem sizes reduce by a\\nfactor of 4 for each level further from the root, each internal node at\\ndepth i = 0, 1, 2, … , log4\\xa0n – 1 has a cost of c(n/4i)2. Multiplying, we\\nsee that the total cost of all nodes at a given depth i is 3ic(n/4i)2 =\\n(3/16)icn2. The bottom level, at depth log4\\xa0n, contains \\nleaves (using equation (3.21) on page 66). Each leaf contributes Θ (1),\\nleading to a total leaf cost of \\n .\\nNow we add up the costs over all levels to determine the cost for the\\nentire tree:\\n', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 146}),\n",
              " Document(page_content='We’ve derived the guess of T (n) = O(n2) for the original recurrence. In\\nthis example, the coefﬁcients of cn2 form a decreasing geometric series.\\nBy equation (A.7), the sum of these coefﬁcients is bounded from above\\nby the constant 16/13. Since the root’s contribution to the total cost is\\ncn2, the cost of the root dominates the total cost of the tree.\\nIn fact, if O(n2) is indeed an upper bound for the recurrence (as we’ll\\nverify in a moment), then it must be a tight bound. Why? The ﬁrst\\nrecursive call contributes a cost of Θ (n2), and so Ω(n2) must be a lower\\nbound for the recurrence.\\nLet’s now use the substitution method to verify that our guess is\\ncorrect, namely, that T (n) = O(n2) is an upper bound for the recurrence\\nT (n) = 3T (n/4)+Θ (n2). We want to show that T (n) ≤ dn2 for some\\nconstant d > 0. Using the same constant c > 0 as before, we have\\nT (n)≤3T (n/4) + cn2\\n≤3d(n/4)2 + cn2\\n=\\n≤dn2,\\nwhere the last step holds if we choose d ≥ (16/13)c.\\nFor the base case of the induction, let n0 > 0 be a sufﬁciently large\\nthreshold constant that the recurrence is well deﬁned when T (n) = Θ (1)\\nfor n < n0. We can pick d large enough that d dominates the constant\\nhidden by the Θ , in which case dn2 ≥ d ≥ T (n) for 1 ≤ n < n0, completing\\nthe proof of the base case.\\nThe substitution proof we just saw involves two named constants, c\\nand d. We named c and used it to stand for the upper-bound constant\\nhidden and guaranteed to exist by the Θ -notation. We cannot pick c\\narbitrarily—it’s given to us—although, for any such c, any constant c′ ≥\\nc also sufﬁces. We also named d, but we were free to choose any value\\nfor it that ﬁt our needs. In this example, the value of d happened to', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 147}),\n",
              " Document(page_content='depend on the value of c, which is ﬁne, since d is constant if c is\\nconstant.\\nAn irregular example\\nLet’s ﬁnd an asymptotic upper bound for another, more irregular,\\nexample. Figure 4.2 shows the recursion tree for the recurrence\\nThis recursion tree is unbalanced, with different root-to-leaf paths\\nhaving different lengths. Going left at any node produces a subproblem\\nof one-third the size, and going right produces a subproblem of two-\\nthirds the size. Let n0 > 0 be the implicit threshold constant such that T\\n(n) = Θ (1) for 0 < n < n0, and let c represent the upper-bound constant\\nhidden by the Θ (n) term for n ≥ n0. There are actually two n0 constants\\nhere—one for the threshold in the recurrence, and the other for the\\nthreshold in the Θ -notation, so we’ll let n0 be the larger of the two\\nconstants.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 148}),\n",
              " Document(page_content='Figure 4.2 A recursion tree for the recurrence T (n) = T (n/3) + T (2n/3) + cn.\\nThe height of the tree runs down the right edge of the tree,\\ncorresponding to subproblems of sizes n, (2/3)n, (4/9)n, … , Θ (1) with\\ncosts bounded by cn, c(2n/3), c(4n/9), … , Θ (1), respectively. We hit the\\nrightmost leaf when (2/3)hn < n0 ≤ (2/3)h–1n, which happens when h =\\n⌊log3/2(n/n0) ⌋ + 1 since, applying the ﬂoor bounds in equation (3.2) on\\npage 64 with x = log3/2 (n/n0), we have (2/3)hn = (2/3)⌊x ⌋+1n < (2/3)xn\\n= (n0/n)n = n0 and (2/3)h–1n = (2/3)⌊x ⌋n > (2/3)xn = (n0/n)n = n0. Thus,\\nthe height of the tree is h = Θ (lg n).\\nWe’re now in a position to understand the upper bound. Let’s\\npostpone dealing with the leaves for a moment. Summing the costs of\\ninternal nodes across each level, we have at most cn per level times the\\nΘ(lg n) tree height for a total cost of O(n lg n) for all internal nodes.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 149}),\n",
              " Document(page_content='It remains to deal with the leaves of the recursion tree, which\\nrepresent base cases, each costing Θ (1). How many leaves are there? It’s\\ntempting to upper-bound their number by the number of leaves in a\\ncomplete binary tree of height h = ⌊log3/2(n/n0) ⌋ + 1, since the recursion\\ntree is contained within such a complete binary tree. But this approach\\nturns out to give us a poor bound. The complete binary tree has 1 node\\nat the root, 2 nodes at depth 1, and generally 2k nodes at depth k. Since\\nthe height is h = ⌊log3/2\\xa0n ⌋ + 1, there are \\nleaves in the complete binary tree, which is an upper bound on the\\nnumber of leaves in the recursion tree. Because the cost of each leaf is\\nΘ(1), this analysis says that the total cost of all leaves in the recursion\\ntree is \\n , which is an asymptotically greater bound\\nthan the O(n lg n) cost of all internal nodes. In fact, as we’re about to\\nsee, this bound is not tight. The cost of all leaves in the recursion tree is\\nO(n)—asymptotically less than O(n lg n). In other words, the cost of the\\ninternal nodes dominates the cost of the leaves, not vice versa.\\nRather than analyzing the leaves, we could quit right now and prove\\nby substitution that T (n) = Θ (n lg n). This approach works (see Exercise\\n4.4-3), but it’s instructive to understand how many leaves this recursion\\ntree has. You may see recurrences for which the cost of leaves dominates\\nthe cost of internal nodes, and then you’ll be in better shape if you’ve\\nhad some experience analyzing the number of leaves.\\nTo ﬁgure out how many leaves there really are, let’s write a\\nrecurrence L(n) for the number of leaves in the recursion tree for T (n).\\nSince all the leaves in T (n) belong either to the left subtree or the right\\nsubtree of the root, we have\\nThis recurrence is similar to recurrence (4.14), but it’s missing the Θ (n)\\nterm, and it contains an explicit base case. Because this recurrence omits\\nthe Θ (n) term, it is much easier to solve. Let’s apply the substitution\\nmethod to show that it has solution L(n) = O(n). Using the inductive', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 150}),\n",
              " Document(page_content='hypothesis L(n) ≤ dn for some constant d > 0, and assuming that the\\ninductive hypothesis holds for all values less than n, we have\\nL(n)=L(n/3) + L(2n/3)\\n≤dn/3 + 2(dn)/3\\n≤dn,\\nwhich holds for any d > 0. We can now choose d large enough to handle\\nthe base case L(n) = 1 for 0 < n < n0, for which d = 1 sufﬁces, thereby\\ncompleting the substitution method for the upper bound on leaves.\\n(Exercise 4.4-2 asks you to prove that L(n) = Θ (n).)\\nReturning to recurrence (4.14) for T (n), it now becomes apparent\\nthat the total cost of leaves over all levels must be L(n) · Θ (1) = Θ (n).\\nSince we have derived the bound of O(n lg n) on the cost of the internal\\nnodes, it follows that the solution to recurrence (4.14) is T (n) = O(n lg\\nn) + Θ (n) = O(n lg n). (Exercise 4.4-3 asks you to prove that T (n) = Θ (n\\nlg n).)\\nIt’s wise to verify any bound obtained with a recursion tree by using\\nthe substitution method, especially if you’ve made simplifying\\nassumptions. But another strategy altogether is to use more-powerful\\nmathematics, typically in the form of the master method in the next\\nsection (which unfortunately doesn’t apply to recurrence (4.14)) or the\\nAkra-Bazzi method (which does, but requires calculus). Even if you use\\na powerful method, a recursion tree can improve your intuition for\\nwhat’s going on beneath the heavy m ath.\\nExercises\\n4.4-1\\nFor each of the following recurrences, sketch its recursion tree, and\\nguess a good asymptotic upper bound on its solution. Then use the\\nsubstitution method to verify your answer.\\na.\\xa0T (n) = T (n/2) + n3.\\nb.\\xa0T (n) = 4T (n/3) + n.\\nc.\\xa0T (n) = 4T (n/2) + n.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 151}),\n",
              " Document(page_content='d.\\xa0T (n) = 3T (n – 1) + 1.\\n4.4-2\\nUse the substitution method to prove that recurrence (4.15) has the\\nasymptotic lower bound L(n) = Ω(n). Conclude that L(n) = Θ (n).\\n4.4-3\\nUse the substitution method to prove that recurrence (4.14) has the\\nsolution T (n) = Ω(n lg n). Conclude that T (n) = Θ (n lg n).\\n4.4-4\\nUse a recursion tree to justify a good guess for the solution to the\\nrecurrence T (n) = T ( αn)+T ((1– α)n)+Θ(n), where α is a constant in the\\nrange 0 < α < 1.\\n4.5\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0The master method for solving recurrences\\nThe master method provides a “cookbook” method for solving\\nalgorithmic recurrences of the form\\nwhere a > 0 and b > 1 are constants. We call f (n) a driving function, and\\nwe call a recurrence of this general form a master recurrence. To use the\\nmaster method, you need to memorize three cases, but then you’ll be\\nable to solve many master recurrences quite easily.\\nA master recurrence describes the running time of a divide-and-\\nconquer algorithm that divides a problem of size n into a subproblems,\\neach of size n/b < n. The algorithm solves the a subproblems recursively,\\neach in T (n/b) time. The driving function f (n) encompasses the cost of\\ndividing the problem before the recursion, as well as the cost of\\ncombining the results of the recursive solutions to subproblems. For\\nexample, the recurrence arising from Strassen’s algorithm is a master\\nrecurrence with a = 7, b = 2, and driving function f (n) = Θ (n2).\\nAs we have mentioned, in solving a recurrence that describes the\\nrunning time of an algorithm, one technicality that we’d often prefer to\\nignore is the requirement that the input size n be an integer. For', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 152}),\n",
              " Document(page_content='example, we saw that the running time of merge sort can be described by\\nrecurrence (2.3), T (n) = 2T (n/2) + Θ (n), on page 41. But if n is an odd\\nnumber, we really don’t have two problems of exactly half the size.\\nRather, to ensure that the problem sizes are integers, we round one\\nsubproblem down to size ⌊n/2 ⌋ and the other up to size ⌈n/2 ⌉, so the true\\nrecurrence is T (n) = T ( ⌈n/2 ⌉ + T ( ⌊n/2 ⌋) + Θ (n). But this ﬂoors-and-\\nceilings recurrence is longer to write and messier to deal with than\\nrecurrence (2.3), which is deﬁned on the reals. We’d rather not worry\\nabout ﬂoors and ceilings, if we don’t have to, especially since the two\\nrecurrences have the same Θ (n lg n) solution.\\nThe master method allows you to state a master recurrence without\\nﬂoors and ceilings and implicitly infer them. No matter how the\\narguments are rounded up or down to the nearest integer, the\\nasymptotic bounds that it provides remain the same. Moreover, as we’ll\\nsee in Section 4.6, if you deﬁne your master recurrence on the reals,\\nwithout implicit ﬂoors and ceilings, the asymptotic bounds still don’t\\nchange. Thus you can ignore ﬂoors and ceilings for master recurrences.\\nSection 4.7 gives sufﬁcient conditions for ignoring ﬂoors and ceilings in\\nmore general divide-and-conquer recurrences.\\nThe master theorem\\nThe master method depends upon the following theorem.\\nTheorem 4.1 (Master theorem)\\nLet a > 0 and b > 1 be constants, and let f (n) be a driving function that\\nis deﬁned and nonnegative on all sufﬁciently large reals. Deﬁne the\\nrecurrence T (n) on n ∈ N by\\nwhere aT (n/b) actually means a′T ( ⌊n/b ⌋) + a″T ( ⌈n/b ⌉) for some\\nconstants a′ ≥ 0 and a″ ≥ 0 satisfying a = a′ + a″. Then the asymptotic\\nbehavior of T (n) can be characterized as follows:\\n1. If there exists a constant ϵ > 0 such that \\n , then \\n.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 153}),\n",
              " Document(page_content='2. If there exists a constant k ≥ 0 such that \\n ,\\nthen \\n .\\n3. If there exists a constant ϵ > 0 such that \\n , and if\\nf (n) additionally satisﬁes the regularity condition\\xa0af (n/b) ≤ cf (n)\\nfor some constant c < 1 and all sufﬁciently large n, then T (n) =\\nΘ(f (n)).\\n▪\\nBefore applying the master theorem to some examples, let’s spend a\\nfew moments to understand broadly what it says. The function \\n  is\\ncalled the watershed function. In each of the three cases, we compare the\\ndriving function f (n) to the watershed function \\n . Intuitively, if the\\nwatershed function grows asymptotically faster than the driving\\nfunction, then case 1 applies. Case 2 applies if the two functions grow at\\nnearly the same asymptotic rate. Case 3 is the “opposite” of case 1,\\nwhere the driving function grows asymptotically faster than the\\nwatershed function. But the technical details matter.\\nIn case 1, not only must the watershed function grow asymptotically\\nfaster than the driving function, it must grow polynomially faster. That\\nis, the watershed function \\n  must be asymptotically larger than the\\ndriving function f (n) by at least a factor of Θ (nϵ) for some constant ϵ >\\n0. The master theorem then says that the solution is \\n . In\\nthis case, if we look at the recursion tree for the recurrence, the cost per\\nlevel grows at least geometrically from root to leaves, and the total cost\\nof leaves dominates the total cost of the internal nodes.\\nIn case 2, the watershed and driving functions grow at nearly the\\nsame asymptotic rate. But more speciﬁcally, the driving function grows\\nfaster than the watershed function by a factor of Θ (lgk n), where k ≥ 0.\\nThe master theorem says that we tack on an extra lg n factor to f (n),\\nyielding the solution \\n . In this case, each level of\\nthe recursion tree costs approximately the same—\\n —and\\nthere are Θ (lg n) levels. In practice, the most common situation for case\\n2 occurs when k = 0, in which case the watershed and driving functions', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 154}),\n",
              " Document(page_content='have the same asymptotic growth, and the solution is \\n.\\nCase 3 mirrors case 1. Not only must the driving function grow\\nasymptotically faster than the watershed function, it must grow\\npolynomially faster. That is, the driving function f (n) must be\\nasymptotically larger than the watershed function \\n  by at least a\\nfactor of Θ (nϵ) for some constant ϵ > 0. Moreover, the driving function\\nmust satisfy the regularity condition that af (n/b) ≤ cf (n). This condition\\nis satisﬁed by most of the polynomially bounded functions that you’re\\nlikely to encounter when applying case 3. The regularity condition\\nmight not be satisﬁed if the driving function grows slowly in local areas,\\nyet relatively quickly overall. (Exercise 4.5-5 gives an example of such a\\nfunction.) For case 3, the master theorem says that the solution is T (n)\\n= Θ(f (n)). If we look at the recursion tree, the cost per level drops at\\nleast geometrically from the root to the leaves, and the root cost\\ndominates the cost of all other nodes.\\nIt’s worth looking again at the requirement that there be polynomial\\nseparation between the watershed function and the driving function for\\neither case 1 or case 3 to apply. The separation doesn’t need to be much,\\nbut it must be there, and it must grow polynomially. For example, for\\nthe recurrence T (n) = 4T (n/2) + n1.99 (admittedly not a recurrence\\nyou’re likely to see when analyzing an algorithm), the watershed\\nfunction is \\n . Hence the driving function f (n) = n1.99 is\\npolynomially smaller by a factor of n0.01. Thus case 1 applies with ϵ =\\n0.01.\\nUsing the master method\\nTo use the master method, you determine which case (if any) of the\\nmaster theorem applies and write down the answer.\\nAs a ﬁrst example, consider the recurrence T (n) = 9T (n/3) + n. For\\nthis recurrence, we have a = 9 and b = 3, which implies that \\n. Since f (n) = n = O(n2– ϵ) for any constant ϵ ≤ 1,', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 155}),\n",
              " Document(page_content='we can apply case 1 of the master theorem to conclude that the solution\\nis T (n) = Θ (n2).\\nNow consider the recurrence T (n) = T (2n/3) + 1, which has a = 1\\nand b = 3/2, which means that the watershed function is \\n. Case 2 applies since \\n. The solution to the recurrence is T (n)\\n= Θ(lg n).\\nFor the recurrence T (n) = 3T (n/4) + n lg n, we have a = 3 and b = 4,\\nwhich means that \\n . Since \\n, where ϵ can be as large as approximately 0.2,\\ncase 3 applies as long as the regularity condition holds for f (n). It does,\\nbecause for sufﬁciently large n, we have that af (n/b) = 3(n/4) lg(n/4) ≤\\n(3/4)n lg n = cf (n) for c = 3/4. By case 3, the solution to the recurrence is\\nT (n) = Θ (n lg n).\\nNext, let’s look at the recurrence T (n) = 2T (n/2) + n lg n, where we\\nhave a = 2, b = 2, and \\n . Case 2 applies since \\n. We conclude that the solution is T (n) = Θ (n\\nlg2\\xa0n).\\nWe can use the master method to solve the recurrences we saw in\\nSections 2.3.2, 4.1, and 4.2.\\nRecurrence (2.3), T (n) = 2T (n/2) + Θ (n), on page 41, characterizes\\nthe running time of merge sort. Since a = 2 and b = 2, the watershed\\nfunction is \\n . Case 2 applies because f (n) = Θ (n), and\\nthe solution is T (n) = Θ (n lg n).\\nRecurrence (4.9), T (n) = 8T (n/2) + Θ (1), on page 84, describes the\\nrunning time of the simple recursive algorithm for matrix\\nmultiplication. We have a = 8 and b = 2, which means that the\\nwatershed function is \\n . Since n3 is polynomially\\nlarger than the driving function f (n) = Θ (1)—indeed, we have f (n) =\\nO(n3– ϵ) for any positive ϵ < 3—case 1 applies. We conclude that T (n)\\n= Θ(n3).\\nFinally, recurrence (4.10), T (n) = 7T (n/2) + Θ (n2), on page 87, arose\\nfrom the analysis of Strassen’s algorithm for matrix multiplication. For', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 156}),\n",
              " Document(page_content='this recurrence, we have a = 7 and b = 2, and the watershed function is \\n. Observing that lg 7 = 2.807355 …, we can let ϵ = 0.8 and\\nbound the driving function f (n) = Θ (n2) = O(nlg 7– ϵ). Case 1 applies\\nwith solution T (n) = Θ (nlg 7).\\nWhen the master method doesn’t appl y\\nThere are situations where you can’t use the master theorem. For\\nexample, it can be that the watershed function and the driving function\\ncannot be asymptotically compared. We might have that \\nfor an inﬁnite number of values of n but also that \\n  for an\\ninﬁnite number of different values of n. As a practical matter, however,\\nmost of the driving functions that arise in the study of algorithms can\\nbe meaningfully compared with the watershed function. If you\\nencounter a master recurrence for which that’s not the case, you’ll have\\nto resort to substitution or other methods.\\nEven when the relative growths of the driving and watershed\\nfunctions can be compared, the master theorem does not cover all the\\npossibilities. There is a gap between cases 1 and 2 when \\n ,\\nyet the watershed function does not grow polynomially faster than the\\ndriving function. Similarly, there is a gap between cases 2 and 3 when \\n and the driving function grows more than\\npolylogarithmically faster than the watershed function, but it does not\\ngrow polynomially faster. If the driving function falls into one of these\\ngaps, or if the regularity condition in case 3 fails to hold, you’ll need to\\nuse something other than the master method to solve the recurrence.\\nAs an example of a driving function falling into a gap, consider the\\nrecurrence T (n) = 2T (n/2) + n/lg n. Since a = 2 and b = 2, the\\nwatershed function is \\n . The driving function is\\nn/lg n = o(n), which means that it grows asymptotically more slowly\\nthan the watershed function n. But n/lg n grows only logarithmically\\nslower than n, not polynomially slower. More precisely, equation (3.24)\\non page 67 says that lg n = o(nϵ) for any constant ϵ > 0, which means\\nthat 1/lg n = ω(n– ϵ) and \\n . Thus no constant', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 157}),\n",
              " Document(page_content='ϵ > 0 exists such that \\n , which is required for case 1 to\\napply. Case 2 fails to apply as well, since \\n , where k\\n= –1, but k must be nonnegative for case 2 to apply.\\nTo solve this kind of recurrence, you must use another method, such\\nas the substitution method (Section 4.3) or the Akra-Bazzi method\\n(Section 4.7). (Exercise 4.6-3 asks you to show that the answer is Θ (n lg\\nlg n).) Although the master theorem doesn’t handle this particular\\nrecurrence, it does handle the overwhelming majority of recurrences\\nthat tend to arise in practice.\\nExercises\\n4.5-1\\nUse the master method to give tight asymptotic bounds for the\\nfollowing recurrences.\\na.\\xa0T (n) = 2T (n/4) + 1.\\nb.\\xa0T (n) = 2T (n/4) + \\n.\\nc.\\xa0T (n) = 2T (n/4) + \\n .\\nd.\\xa0T (n) = 2T (n/4) + n.\\ne.\\xa0T (n) = 2T (n/4) + n2.\\n4.5-2\\nProfessor Caesar wants to develop a matrix-multiplication algorithm\\nthat is asymptotically faster than Strassen’s algorithm. His algorithm\\nwill use the divide-and-conquer method, dividing each matrix into n/4 ×\\nn/4 submatrices, and the divide and combine steps together will take\\nΘ(n2) time. Suppose that the professor’s algorithm creates a recursive\\nsubproblems of size n/4. What is the largest integer value of a for which\\nhis algorithm could possibly run asymptotically faster than Strassen’s?\\n4.5-3\\nUse the master method to show that the solution to the binary-search\\nrecurrence T (n) = T (n/2) + Θ (1) is T (n) = Θ (lg n). (See Exercise 2.3-6', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 158}),\n",
              " Document(page_content='for a description of binary search.)\\n4.5-4\\nConsider the function f (n) = lg n. Argue that although f (n/2) < f (n),\\nthe regularity condition af (n/b) ≤ cf (n) with a = 1 and b = 2 does not\\nhold for any constant c < 1. Argue further that for any ϵ > 0, the\\ncondition in case 3 that \\n  does not hold.\\n4.5-5\\nShow that for suitable constants a, b, and ϵ, the function f (n) = 2⌈lg n ⌉\\nsatisﬁes all the conditions in case 3 of the master theorem except the\\nregularity condition.\\n★ 4.6\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0Proof of the continuous master theorem\\nProving the master theorem (Theorem 4.1) in its full generality,\\nespecially dealing with the knotty technical issue of ﬂoors and ceilings,\\nis beyond the scope of this book. This section, however, states and\\nproves a variant of the master theorem, called the continuous master\\ntheorem1 in which the master recurrence (4.17) is deﬁned over\\nsufﬁciently large positive real numbers. The proof of this version,\\nuncomplicated by ﬂoors and ceilings, contains the main ideas needed to\\nunderstand how master recurrences behave. Section 4.7 discusses ﬂoors\\nand ceilings in divide-and-conquer recurrences at greater length,\\npresenting sufﬁcient conditions for them not to affect the asymptotic\\nsolutions.\\nOf course, since you need not understand the proof of the master\\ntheorem in order to apply the master method, you may choose to skip\\nthis section. But if you wish to study more-advanced algorithms beyond\\nthe scope of this textbook, you may appreciate a better understanding\\nof the underlying mathematics, which the proof of the continuous\\nmaster theorem provides.\\nAlthough we usually assume that recurrences are algorithmic and\\ndon’t require an explicit statement of a base case, we must be much\\nmore careful for proofs that justify the practice. The lemmas and', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 159}),\n",
              " Document(page_content='theorem in this section explicitly state the base cases, because the\\ninductive proofs require mathematical grounding. It is common in the\\nworld of mathematics to be extraordinarily careful proving theorems\\nthat justify acting more casually in practice.\\nThe proof of the continuous master theorem involves two lemmas.\\nLemma 4.2 uses a slightly simpliﬁed master recurrence with a threshold\\nconstant of n0 = 1, rather than the more general n0 > 0 threshold\\nconstant implied by the unstated base case. The lemma employs a\\nrecursion tree to reduce the solution of the simpliﬁed master recurrence\\nto that of evaluating a summation. Lemma 4.3 then provides asymptotic\\nbounds for the summation, mirroring the three cases of the master\\ntheorem. Finally, the continuous master theorem itself (Theorem 4.4)\\ngives asymptotic bounds for master recurrences, while generalizing to\\nan arbitrary threshold constant n0 > 0 as implied by the unstated base\\ncase.\\nSome of the proofs use the properties described in Problem 3-5 on\\npages 72–73 to combine and simplify complicated asymptotic\\nexpressions. Although Problem 3-5 addresses only Θ -notation, the\\nproperties enumerated there can be extended to O-notation and Ω-\\nnotation as well.\\nHere’s the ﬁrst lemma.\\nLemma 4.2\\nLet a > 0 and b > 1 be constants, and let f (n) be a function deﬁned over\\nreal numbers n ≥ 1. Then the recurrence\\nhas solution\\nProof\\xa0\\xa0\\xa0Consider the recursion tree in Figure 4.3. Let’s look ﬁrst at its\\ninternal nodes. The root of the tree has cost f (n), and it has a children,', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 160}),\n",
              " Document(page_content='each with cost f (n/b). (It is convenient to think of a as being an integer,\\nespecially when visualizing the recursion tree, but the mathematics does\\nnot require it.) Each of these children has a children, making a2 nodes\\nat depth 2, and each of the a children has cost f (n/b2). In general, there\\nare aj nodes at depth j, and each node has cost f (n/bj).\\nNow, let’s move on to understanding the leaves. The tree grows\\ndownward until n/bj becomes less than 1. Thus, the tree has height ⌊logb\\nn ⌋ + 1, because \\n  and \\n .\\nSince, as we have observed, the number of nodes at depth j is aj and all\\nthe leaves are at depth ⌊logb\\xa0n ⌋ + 1, the tree contains \\n  leaves.\\nUsing the identity (3.21) on page 66, we have \\n, since a is constant, and \\n. Consequently, the total number of\\nleaves is \\n —asymptotically, the watershed function.\\nWe are now in a position to derive equation (4.18) by summing the\\ncosts of the nodes at each depth in the tree, as shown in the ﬁgure. The\\nﬁrst term in the equation is the total costs of the leaves. Since each leaf\\nis at depth ⌊logbn ⌋ + 1 and \\n , the base case of the\\nrecurrence gives the cost of a leaf: \\n . Hence the cost of all \\nleaves is \\n  by Problem 3-5(d). The second term\\nin equation (4.18) is the cost of the internal nodes, which, in the\\nunderlying divide-and-conquer algorithm, represents the costs of\\ndividing problems into subproblems and then recombining the\\nsubproblems. Since the cost for all the internal nodes at depth j is aj f\\n(n/bj), the total cost of all internal nodes is\\n▪', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 161}),\n",
              " Document(page_content='Figure 4.3 The recursion tree generated by T (n) = aT (n/b) + f (n). The tree is a complete a-ary\\ntree with \\n  leaves and height ⌊logb n ⌋ + 1. The cost of the nodes at each depth is shown\\nat the right, and their sum is given in equation (4.18).\\nAs we’ll see, the three cases of the master theorem depend on the\\ndistribution of the total cost across levels of the recursion tree:\\nCase 1: The costs increase geometrically from the root to the leaves,\\ngrowing by a constant factor with each level.\\nCase 2: The costs depend on the value of k in the theorem. With k = 0,\\nthe costs are equal for each level; with k = 1, the costs grow linearly\\nfrom the root to the leaves; with k = 2, the growth is quadratic; and in\\ngeneral, the costs grow polynomially in k.\\nCase 3: The costs decrease geometrically from the root to the leaves,\\nshrinking by a constant factor with each level.\\nThe summation in equation (4.18) describes the cost of the dividing\\nand combining steps in the underlying divide-and-conquer algorithm.\\nThe next lemma provides asymptotic bounds on the summation’s\\ngrowth.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 162}),\n",
              " Document(page_content='Lemma 4.3\\nLet a > 0 and b > 1 be constants, and let f (n) be a function deﬁned over\\nreal numbers n ≥ 1. Then the asymptotic behavior of the function\\ndeﬁned for n ≥ 1, can be characterized as follows:\\n1. If there exists a constant ϵ > 0 such that \\n , then \\n.\\n2. If there exists a constant k ≥ 0 such that \\n , then\\n.\\n3. If there exists a constant c in the range 0 < c < 1 such that 0 < af\\n(n/b) ≤ cf (n) for all n ≥ 1, then g(n) = Θ (f (n)).\\nProof\\xa0\\xa0\\xa0For case 1, we have \\n , which implies that \\n. Substituting into equation (4.19) yields\\nthe last series being geometric. Since b and ϵ are constants, the bϵ – 1\\ndenominator doesn’t affect the asymptotic growth of g(n), and neither', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 163}),\n",
              " Document(page_content='does the –1 in the numerator. Since \\n, we obtain \\n, thereby proving case 1.\\nCase 2 assumes that \\n , from which we can\\nconclude that \\n . Substituting into\\nequation (4.19) and repeatedly applying Problem 3-5(c) yields\\nThe summation within the Θ -notation can be bounded from above as\\nfollows:', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 164}),\n",
              " Document(page_content='Exercise 4.6-1 asks you to show that the summation can similarly be\\nbounded from below by \\n . Since we have tight upper and lower\\nbounds, the summation is \\n , from which we can conclude that \\n, thereby completing the proof of case 2.\\nFor case 3, observe that f (n) appears in the deﬁnition (4.19) of g(n)\\n(when j = 0) and that all terms of g(n) are positive. Therefore, we must\\nhave g(n) = Ω(f (n)), and it only remains to prove that g(n) = O(f (n)).\\nPerforming j iterations of the inequality af (n/b) ≤ cf (n) yields aj f (n/bj)\\n≤ cj f (n). Substituting into equation (4.19), we obtain\\nThus, we can conclude that g(n) = Θ (f (n)). With case 3 proved, the\\nentire proof of the lemma is complete.\\n▪\\nWe can now state and prove the continuous master theorem.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 165}),\n",
              " Document(page_content='Theorem 4.4 (Continuous master theorem)\\nLet a > 0 and b > 1 be constants, and let f (n) be a driving function that\\nis deﬁned and nonnegative on all sufﬁciently large reals. Deﬁne the\\nalgorithmic recurrence T (n) on the positive real numbers by\\nT (n) = aT (n/b) + f (n).\\nThen the asymptotic behavior of T (n) can be characterized as follows:\\n1. If there exists a constant ϵ > 0 such that \\n , then \\n.\\n2. If there exists a constant k ≥ 0 such that \\n , then \\n.\\n3. If there exists a constant ϵ > 0 such that \\n , and if\\nf (n) additionally satisﬁes the regularity condition af (n/b) ≤ cf (n)\\nfor some constant c < 1 and all sufﬁciently large n, then T (n) =\\nΘ(f (n)).\\nProof\\xa0\\xa0\\xa0The idea is to bound the summation (4.18) from Lemma 4.2 by\\napplying Lemma 4.3. But we must account for Lemma 4.2 using a base\\ncase for 0 < n < 1, whereas this theorem uses an implicit base case for 0\\n< n < n0, where n0 > 0 is an arbitrary threshold constant. Since the\\nrecurrence is algorithmic, we can assume that f (n) is deﬁned for n ≥ n0.\\nFor n > 0, let us deﬁne two auxiliary functions T′(n) = T (n0\\xa0n) and\\nf\\xa0′(n) = f (n0\\xa0n). We have\\nWe have obtained a recurrence for T\\xa0′(n) that satisﬁes the conditions of\\nLemma 4.2, and by that lemma, the solution is', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 166}),\n",
              " Document(page_content='To solve T\\xa0′(n), we ﬁrst need to bound f\\xa0′(n). Let’s examine the\\nindividual cases in the theorem.\\nThe condition for case 1 is \\n  for some constant ϵ > 0.\\nWe have\\nsince a, b, n0, and ϵ are all constant. The function f\\xa0′(n) satisﬁes the\\nconditions of case 1 of Lemma 4.3, and the summation in equation\\n(4.18) of Lemma 4.2 evaluates to \\n . Because a, b and n0 are all\\nconstants, we have\\nthereby completing case 1 of the theorem.\\nThe condition for case 2 is \\n  for some constant k ≥\\n0. We have\\nSimilar to the proof of case 1, the function f′(n) satisﬁes the conditions\\nof case 2 of Lemma 4.3. The summation in equation (4.18) of Lemma\\n4.2 is therefore \\n , which implies that', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 167}),\n",
              " Document(page_content='which proves case 2 of the theorem.\\nFinally, the condition for case 3 is \\n  for some\\nconstant ϵ > 0 and f (n) additionally satisﬁes the regularity condition af\\n(n/b) ≤ cf (n) for all n ≥ n0 and some constants c < 1 and n0 > 1. The ﬁrst\\npart of case 3 is like case 1:\\nUsing the deﬁnition of f\\xa0′(n) and the fact that n0\\xa0n ≥ n0 for all n ≥ 1, we\\nhave for n ≥ 1 that\\naf\\xa0′(n/b)=af (n0\\xa0n/b)\\n≤cf (n0\\xa0n)\\n=cf\\xa0′(n).\\nThus f\\xa0′(n) satisﬁes the requirements for case 3 of Lemma 4.3, and the\\nsummation in equation (4.18) of Lemma 4.2 evaluates to Θ (f\\xa0′(n)),\\nyielding\\nT (n)=T\\xa0′(n/n0)\\n=\\n=Θ(f\\xa0′(n/n0))\\n=Θ(f (n)),\\nwhich completes the proof of case 3 of the theorem and thus the whole\\ntheorem.\\n▪', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 168}),\n",
              " Document(page_content='Exercises\\n4.6-1\\nShow that \\n .\\n★ 4.6-2\\nShow that case 3 of the master theorem is overstated (which is also why\\ncase 3 of Lemma 4.3 does not require that \\n  in the\\nsense that the regularity condition af (n/b) ≤ cf (n) for some constant c <\\n1 implies that there exists a constant ϵ > 0 such that \\n .\\n★ 4.6-3\\nFor \\n , prove that the summation in equation (4.19)\\nhas solution \\n . Conclude that a master recurrence T\\n(n) using f (n) as its driving function has solution \\n .\\n★ 4.7\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0Akra-Bazzi recurrences\\nThis section provides an overview of two advanced topics related to\\ndivide-and-conquer recurrences. The ﬁrst deals with technicalities\\narising from the use of ﬂoors and ceilings, and the second discusses the\\nAkra-Bazzi method, which involves a little calculus, for solving\\ncomplicated divide-and-conquer recurrences.\\nIn particular, we’ll look at the class of algorithmic divide-and-\\nconquer recurrences originally studied by M. Akra and L. Bazzi [13].\\nThese Akra-Bazzi recurrences take the form\\nwhere k is a positive integer; all the constants a1, a2, … , ak ∈ R are\\nstrictly positive; all the constants b1, b2, … , bk ∈ R are strictly greater\\nthan 1; and the driving function f (n) is deﬁned on sufﬁciently large\\nnonnegative reals and is itself nonnegative.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 169}),\n",
              " Document(page_content='Akra-Bazzi recurrences generalize the class of recurrences addressed\\nby the master theorem. Whereas master recurrences characterize the\\nrunning times of divide-and-conquer algorithms that break a problem\\ninto equal-sized subproblems (modulo ﬂoors and ceilings), Akra-Bazzi\\nrecurrences can describe the running time of divide-and-conquer\\nalgorithms that break a problem into different-sized subproblems. The\\nmaster theorem, however, allows you to ignore ﬂoors and ceilings, but\\nthe Akra-Bazzi method for solving Akra-Bazzi recurrences needs an\\nadditional requirement to deal with ﬂoors and ceilings.\\nBut before diving into the Akra-Bazzi method itself, let’s understand\\nthe limitations involved in ignoring ﬂoors and ceilings in Akra-Bazzi\\nrecurrences. As you’re aware, algorithms generally deal with integer-\\nsized inputs. The mathematics for recurrences is often easier with real\\nnumbers, however, than with integers, where we must cope with ﬂoors\\nand ceilings to ensure that terms are well deﬁned. The difference may\\nnot seem to be much—especially because that’s often the truth with\\nrecurrences—but to be mathematically correct, we must be careful with\\nour assumptions. Since our end goal is to understand algorithms and\\nnot the vagaries of mathematical corner cases, we’d like to be casual yet\\nrigorous. How can we treat ﬂoors and ceilings casually while still\\nensuring rigor?\\nFrom a mathematical point of view, the difﬁculty in dealing with\\nﬂoors and ceilings is that some driving functions can be really, really\\nweird. So it’s not okay in general to ignore ﬂoors and ceilings in Akra-\\nBazzi recurrences. Fortunately, most of the driving functions we\\nencounter in the study of algorithms behave nicely, and ﬂoors and\\nceilings don’t make a difference.\\nThe polynomial-growth condition\\nIf the driving function f (n) in equation (4.22) is well behaved in the\\nfollowing sense, it’s okay to drop ﬂoors and ceilings.\\nA function f (n) deﬁned on all sufﬁciently large positive reals\\nsatisﬁes the polynomial-growth condition if there exists a\\nconstant \\n  such that the following holds: for every constant', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 170}),\n",
              " Document(page_content='ϕ ≥ 1, there exists a constant d > 1 (depending on ϕ) such that f\\n(n)/d ≤ f ( ψ n) ≤ df (n) for all 1 ≤ ψ ≤ ϕ and \\n .\\nThis deﬁnition may be one of the hardest in this textbook to get your\\nhead around. To a ﬁrst order, it says that f (n) satisﬁes the property that\\nf (Θ(n)) = Θ (f (n)), although the polynomial-growth condition is\\nactually somewhat stronger (see Exercise 4.7-4). The deﬁnition also\\nimplies that f (n) is asymptotically positive (see Exercise 4.7-3).\\nExamples of functions that satisfy the polynomial-growth condition\\ninclude any function of the form f (n) = Θ (nα lgβ n lg lgγn), where α, β,\\nand γ are constants. Most of the polynomially bounded functions used\\nin this book satisfy the condition. Exponentials and superexponentials\\ndo not (see Exercise 4.7-2, for example), and there also exist\\npolynomially bounded functions that do not.\\nFloors and ceilings in “nice” recurrences\\nWhen the driving function in an Akra-Bazzi recurrence satisﬁes the\\npolynomial-growth condition, ﬂoors and ceilings don’t change the\\nasymptotic behavior of the solution. The following theorem, which is\\npresented without proof, formalizes this notion.\\nTheorem 4.5\\nLet T (n) be a function deﬁned on the nonnegative reals that satisﬁes\\nrecurrence (4.22), where f (n) satisﬁes the polynomial-growth condition.\\nLet T\\xa0′(n) be another function deﬁned on the natural numbers also\\nsatisfying recurrence (4.22), except that each T (n/bi) is replaced either\\nwith T ( ⌈n/bi ⌉) or with T ( ⌊n/bi ⌋). Then we have T\\xa0′(n) = Θ (T (n)).\\n▪\\nFloors and ceilings represent a minor perturbation to the arguments\\nin the recursion. By inequality (3.2) on page 64, they perturb an\\nargument by at most 1. But much larger perturbations are tolerable. As\\nlong as the driving function f (n) in recurrence (4.22) satisﬁes the\\npolynomial-growth condition, it turns out that replacing any term T', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 171}),\n",
              " Document(page_content='(n/bi) with T (n/bi + hi(n)), where |hi(n)| = O(n/lg1+ ϵ\\xa0n) for some\\nconstant ϵ > 0 and sufﬁciently large n, leaves the asymptotic solution\\nunaffected. Thus, the divide step in a divide-and-conquer algorithm can\\nbe moderately coarse without affecting the solution to its running-time\\nrecurrence.\\nThe Akra-Bazzi method\\nThe Akra-Bazzi method, not surprisingly, was developed to solve Akra-\\nBazzi recurrences (4.22), which by dint of Theorem 4.5, applies in the\\npresence of ﬂoors and ceilings or even larger perturbations, as just\\ndiscussed. The method involves ﬁrst determining the unique real\\nnumber p such that \\n . Such a p always exists, because when\\np → –∞, the sum goes to ∞; it decreases as p increases; and when p → ∞,\\nit goes to 0. The Akra-Bazzi method then gives the solution to the\\nrecurrence as\\nAs an example, consider the recurrence\\nWe’ll see the similar recurrence (9.1) on page 240 when we study an\\nalgorithm for selecting the ith smallest element from a set of n numbers.\\nThis recurrence has the form of equation (4.22), where a1 = a2 = 1, b1 =\\n5, b2 = 10/7, and f (n) = n. To solve it, the Akra-Bazzi method says that\\nwe should determine the unique p satisfying\\nSolving for p is kind of messy—it turns out that p = 0.83978 …—but we\\ncan solve the recurrence without actually knowing the exact value for p.\\nObserve that (1/5)0 + (7/10)0 = 2 and (1/5)1 + (7/10)1 = 9/10, and thus p\\nlies in the range 0 < p < 1. That turns out to be sufﬁcient for the Akra-', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 172}),\n",
              " Document(page_content='Bazzi method to give us the solution. We’ll use the fact from calculus\\nthat if k ≠ –1, then ∫ xkdx = xk + 1/(k + 1), which we’ll apply with k = –\\np ≠ –1. The Akra-Bazzi solution (4.23) gives us\\nAlthough the Akra-Bazzi method is more general than the master\\ntheorem, it requires calculus and sometimes a bit more reasoning. You\\nalso must ensure that your driving function satisﬁes the polynomial-\\ngrowth condition if you want to ignore ﬂoors and ceilings, although\\nthat’s rarely a problem. When it applies, the master method is much\\nsimpler to use, but only when subproblem sizes are more or less equal.\\nThey are both good tools for your algorithmic toolkit.\\nExercises\\n★ 4.7-1\\nConsider an Akra-Bazzi recurrence T (n) on the reals as given in\\nrecurrence (4.22), and deﬁne T\\xa0′(n) as\\nwhere c > 0 is constant. Prove that whatever the implicit initial\\nconditions for T (n) might be, there exist initial conditions for T\\xa0′(n)\\nsuch that T\\xa0′(n) = cT (n) for all n > 0. Conclude that we can drop the', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 173}),\n",
              " Document(page_content='asymptotics on a driving function in any Akra-Bazzi recurrence without\\naffecting its asymptotic solution.\\n4.7-2\\nShow that f (n) = n2 satisﬁes the polynomial-growth condition but that f\\n(n) = 2n does not.\\n4.7-3\\nLet f (n) be a function that satisﬁes the polynomial-growth condition.\\nProve that f (n) is asymptotically positive, that is, there exists a constant\\nn0 ≥ 0 such that f (n) ≥ 0 for all n ≥ n0.\\n★ 4.7-4\\nGive an example of a function f (n) that does not satisfy the polynomial-\\ngrowth condition but for which f (Θ(n)) = Θ (f (n)).\\n4.7-5\\nUse the Akra-Bazzi method to solve the following recurrences.\\na.\\xa0T (n) = T (n/2) + T (n/3) + T (n/6) + n lg n.\\nb.\\xa0T (n) = 3T (n/3) + 8T (n/4) + n2/lg n.\\nc.\\xa0T (n) = (2/3)T (n/3) + (1/3)T (2n/3) + lg n.\\nd.\\xa0T (n) = (1/3)T (n/3) + 1/n.\\ne.\\xa0T (n) = 3T (n/3) + 3T (2n/3) + n2.\\n★ 4.7-6\\nUse the Akra-Bazzi method to prove the continuous master theorem.\\nProblems\\n4-1\\xa0\\xa0\\xa0\\xa0\\xa0R ecurrence examples\\nGive asymptotically tight upper and lower bounds for T (n) in each of\\nthe following algorithmic recurrences. Justify your answers.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 174}),\n",
              " Document(page_content='a.\\xa0T (n) = 2T (n/2) + n3.\\nb.\\xa0T (n) = T (8n/11) + n.\\nc.\\xa0T (n) = 16T (n/4) + n2.\\nd.\\xa0T (n) = 4T (n/2) + n2 lg n.\\ne.\\xa0T (n) = 8T (n/3) + n2.\\nf.\\xa0T (n) = 7T (n/2) + n2 lg n.\\ng.\\xa0\\n .\\nh.\\xa0T (n) = T (n –2) + n2.\\n4-2\\xa0\\xa0\\xa0\\xa0\\xa0Parameter-passing costs\\nThroughout this book, we assume that parameter passing during\\nprocedure calls takes constant time, even if an N-element array is being\\npassed. This assumption is valid in most systems because a pointer to\\nthe array is passed, not the array itself. This problem examines the\\nimplications of three parameter-passing strategies:\\n1. Arrays are passed by pointer. Time = Θ (1).\\n2. Arrays are passed by copying. Time = Θ (N), where N is the size\\nof the array.\\n3. Arrays are passed by copying only the subrange that might be\\naccessed by the called procedure. Time = Θ (n) if the subarray\\ncontains n elements.\\nConsider the following three algorithms:\\na. The recursive binary-search algorithm for ﬁnding a n umber in a\\nsorted array (see Exercise 2.3-6).\\nb. The MERGE-SORT procedure from Section 2.3.1.\\nc. The MATRIX-MULTIPLY-RECURSIVE procedure from Section\\n4.1.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 175}),\n",
              " Document(page_content='Give nine recurrences Ta1(N, n), Ta2(N, n), … , Tc3(N, n) for the worst-\\ncase running times of each of the three algorithms above when arrays\\nand matrices are passed using each of the three parameter-passing\\nstrategies above. Solve your recurrences, giving tight asymptotic bounds.\\n4-3\\xa0\\xa0\\xa0\\xa0\\xa0Solving recurrences with a chang e of variables\\nSometimes, a little algebraic manipulation can make an unknown\\nrecurrence similar to one you have seen before. Let’s solve the recurrence\\nby using the change-of-variables method.\\na. Deﬁne m = lg n and S(m) = T (2m). Rewrite recurrence (4.25) in\\nterms of m and S(m).\\nb. Solve your recurrence for S(m).\\nc. Use your solution for S(m) to conclude that T (n) = Θ (lg n lg lg n).\\nd. Sketch the recursion tree for recurrence (4.25), and use it to explain\\nintuitively why the solution is T (n) = Θ (lg n lg lg n).\\nSolve the following recurrences by changing variables:\\ne.\\xa0\\n .\\nf.\\xa0\\n .\\n4-4\\xa0\\xa0\\xa0\\xa0\\xa0M ore recurrence examples\\nGive asymptotically tight upper and lower bounds for T (n) in each of\\nthe following recurrences. Justify your answers.\\na.\\xa0T (n) = 5T (n/3) + n lg n.\\nb.\\xa0T (n) = 3T (n/3) + n/lg n.\\nc.\\xa0\\n .\\nd.\\xa0T (n) = 2T (n/2 –2) + n/2.\\ne.\\xa0T (n) = 2T (n/2) + n/lg n.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 176}),\n",
              " Document(page_content='f.\\xa0T (n) = T (n/2) + T (n/4) + T (n/8) + n.\\ng.\\xa0T (n) = T (n – 1) + 1/n.\\nh.\\xa0T (n) = T (n – 1) + lg n.\\ni.\\xa0T (n) = T (n – 2) + 1/lg n.\\nj.\\xa0\\n .\\n4-5\\xa0\\xa0\\xa0\\xa0\\xa0Fibonacci numbers\\nThis problem develops properties of the Fibonacci numbers, which are\\ndeﬁned by recurrence (3.31) on page 69. We’ll explore the technique of\\ngenerating functions to solve the Fibonacci recurrence. Deﬁne the\\ngenerating function (or formal power series) F as\\nwhere Fi is the ith Fibonacci number.\\na. Show that F (z) = z + z F (z) + z2F (z).\\nb. Show that\\nwhere ϕ is the golden ratio, and \\n is its conjugate (see page 69).\\nc. Show that\\nYou may use without proof the generating-function version of\\nequation (A.7) on page 1142, \\n . Because this\\nequation involves a generating function, x is a formal variable, not a', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 177}),\n",
              " Document(page_content='real-valued variable, so that you don’t have to worry about\\nconvergence of the summation or about the requirement in equation\\n(A.7) that |x| < 1, which doesn’t make sense here.\\nd. Use part (c) to prove that \\n  for i > 0, rounded to the nearest\\ninteger. (Hint: Observe that \\n .)\\ne. Prove that Fi+2 ≥ ϕi for i ≥ 0.\\n4-6\\xa0\\xa0\\xa0\\xa0\\xa0C hip testing\\nProfessor Diogenes has n supposedly identical integrated-circuit chips\\nthat in principle are capable of testing each other. The professor’s test jig\\naccommodates two chips at a time. When the jig is loaded, each chip\\ntests the other and reports whether it is good or bad. A good chip\\nalways reports accurately whether the other chip is good or bad, but the\\nprofessor cannot trust the answer of a bad chip. Thus, the four possible\\noutcomes of a test are as follows:\\nChip A saysChip B saysConclusion\\nB is good A is goodboth are good, or both are bad\\nB is good A is bad at least one is bad\\nB is bad A is goodat least one is bad\\nB is bad A is bad at least one is bad\\na. Show that if at least n/2 chips are bad, the professor cannot\\nnecessarily determine which chips are good using any strategy based\\non this kind of pairwise test. Assume that the bad chips can conspire\\nto fool the professor.\\nNow you will design an algorithm to identify which chips are good and\\nwhich are bad, assuming that more than n/2 of the chips are good. First,\\nyou will determine how to identify one good chip.\\nb. Show that ⌊n/2 ⌋ pairwise tests are sufﬁcient to reduce the problem to\\none of nearly half the size. That is, show how to use ⌊n/2 ⌋ pairwise', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 178}),\n",
              " Document(page_content='tests to obtain a set with at most ⌈n/2 ⌉ chips that still has the property\\nthat more than half of the chips are good.\\nc. Show how to apply the solution to part (b) recursively to identify one\\ngood chip. Give and solve the recurrence that describes the number of\\ntests needed to identify one good chip.\\nYou have now determined how to identify one good chip.\\nd. Show how to identify all the good chips with an additional Θ (n)\\npairwise tests.\\n4-7\\xa0\\xa0\\xa0\\xa0\\xa0M onge arrays\\nAn m × n array A of real numbers is a Monge array if for all i, j, k, and l\\nsuch that 1 ≤ i < k ≤ m and 1 ≤ j < l ≤ n, we have\\nA[i, j] + A[k, l] ≤ A[i, l] + A[k, j].\\nIn other words, whenever we pick two rows and two columns of a\\nMonge array and consider the four elements at the intersections of the\\nrows and the columns, the sum of the upper-left and lower-right\\nelements is less than or equal to the sum of the lower-left and upper-\\nright elements. For example, the following array is Monge:\\n1017132823\\n1722162923\\n2428223424\\n11136177\\n4544323723\\n363319216\\n7566515334\\na. Prove that an array is Monge if and only if for all i = 1, 2, …, m – 1\\nand j = 1, 2, …, n – 1, we have\\nA[i, j] + A[i + 1, j + 1] ≤ A[i, j + 1] + A[i + 1, j].\\n(Hint: For the “if” part, use induction separately on rows and\\ncolumns.)', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 179}),\n",
              " Document(page_content='b. The following array is not Monge. Change one element in order to\\nmake it Monge. (Hint: Use part (a).)\\n37 232232\\n21 6 7 10\\n53 343031\\n32 13 9 6\\n43 2115 8\\nc. Let f (i) be the index of the column containing the leftmost minimum\\nelement of row i. Prove that f (1) ≤ f (2) ≤ ⋯ ≤ f (m) for any m × n\\nMonge array.\\nd. Here is a description of a divide-and-conquer algorithm that\\ncomputes the leftmost minimum element in each row of an m × n\\nMonge array A:\\nConstruct a submatrix A′ of A consisting of the even-numbered\\nrows of A. Recursively determine the leftmost minimum for\\neach row of A′. Then compute the leftmost minimum in the\\nodd-numbered rows of A.\\nExplain how to compute the leftmost minimum in the odd-numbered\\nrows of A (given that the leftmost minimum of the even-numbered\\nrows is known) in O(m + n) time.\\ne. Write the recurrence for the running time of the algorithm in part (d).\\nShow that its solution is O(m + n log m).\\nChapter notes\\nDivide-and-conquer as a technique for designing algorithms dates back\\nat least to 1962 in an article by Karatsuba and Ofman [242], but it might\\nhave been used well before then. According to Heideman, Johnson, and\\nBurrus [211], C. F. Gauss devised the ﬁrst fast Fourier transform\\nalgorithm in 1805, and Gauss’s formulation breaks the problem into\\nsmaller subproblems whose solutions are combined.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 180}),\n",
              " Document(page_content='Strassen’s algorithm [424] caused much excitement when it appeared\\nin 1969. Before then, few imagined the possibility of an algorithm\\nasymptotically faster than the basic MATRIX-MULTIPLY procedure.\\nShortly thereafter, S. Winograd reduced the number of submatrix\\nadditions from 18 to 15 while still using seven submatrix multiplications.\\nThis improvement, which Winograd apparently never published (and\\nwhich is frequently miscited in the literature), may enhance the\\npracticality of the method, but it does not affect its asymptotic\\nperformance. Probert [368] described Winograd’s algorithm and showed\\nthat with seven multiplications, 15 ad ditions is the minimum possible.\\nStrassen’s Θ (nlg 7) = O(n2.81) bound for matrix multiplication held\\nuntil 1987, when Coppersmith and Winograd [103] made a signiﬁcant\\nadvance, improving the bound to O(n2.376) time with a mathematically\\nsophisticated but wildly impractical algorithm based on tensor\\nproducts. It took approximately 25 years before the asymptotic upper\\nbound was again improved. In 2012 Vassilevska Williams [445]\\nimproved it to O(n2.37287), and two years later Le Gall [278] achieved\\nO(n2.37286), both of them using mathematically fascinating but\\nimpractical algorithms. The best lower bound to date is just the obvious\\nΩ(n2) bound (obvious because any algorithm for matrix multiplication\\nmust ﬁll in the n2 elements of the product matrix).\\nThe performance of MATRIX-MULTIPLY-RECURSIVE can be\\nimproved in practice by coarsening the leaves of the recursion. It also\\nexhibits better cache behavior than MATRIX-MULTIPLY, although\\nMATRIX-MULTIPLY can be improved by “tiling.” Leiserson et al.\\n[293] conducted a performance-engineering study of matrix\\nmultiplication in which a parallel and vectorized divide-and-conquer\\nalgorithm achieved the highest performance. Strassen’s algorithm can be\\npractical for large dense matrices, although large matrices tend to be\\nsparse, and sparse methods can be much faster. When using limited-\\nprecision ﬂoating-point values, Strassen’s algorithm produces larger\\nnumerical errors than the Θ (n3) algorithms do, although Higham [215]', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 181}),\n",
              " Document(page_content='demonstrated that Strassen’s algorithm is amply accurate for some\\napplications.\\nRecurrences were studied as early as 1202 by Leonardo Bonacci [66],\\nalso known as Fibonacci, for whom the Fibonacci numbers are named,\\nalthough Indian mathematicians had discovered Fibonacci numbers\\ncenturies before. The French mathematician De Moivre [108]\\nintroduced the method of generating functions with which he studied\\nFibonacci numbers (see Problem 4-5). Knuth [259] and Liu [302] are\\ngood resources for learning the method of generating functions.\\nAho, Hopcroft, and Ullman [5, 6] offered one of the ﬁrst general\\nmethods for solving recurrences arising from the analysis of divide-and-\\nconquer algorithms. The master method was adapted from Bentley,\\nHaken, and Saxe [52]. The Akra-Bazzi method is due (unsurprisingly)\\nto Akra and Bazzi [13]. Divide-and-conquer recurrences have been\\nstudied by many researchers, including Campbell [79], Graham, Knuth,\\nand Patashnik [199], Kuszmaul and Leiserson [274], Leighton [287],\\nPurdom and Brown [371], Roura [389], Verma [447], and Yap [462].\\nThe issue of ﬂoors and ceilings in divide-and-conquer recurrences,\\nincluding a theorem similar to Theorem 4.5, was studied by Leighton\\n[287]. Leighton proposed a version of the polynomial-growth condition.\\nCampbell [79] removed several limitations in Leighton’s statement of it\\nand showed that there were polynomially bounded functions that do\\nnot satisfy Leighton’s condition. Campbell also carefully studied many\\nother technical issues, including the well-deﬁnedness of divide-and-\\nconquer recurrences. Kuszmaul and Leiserson [274] provided a proof of\\nTheorem 4.5 that does not involve calculus or other higher math. Both\\nCampbell and Leighton explored the perturbations of arguments\\nbeyond simple ﬂoors and ceilings.\\n1 This terminology does not mean that either T (n) or f (n) need be continuous, only that the\\ndomain of T (n) is the real numbers, as opposed to integers.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 182}),\n",
              " Document(page_content='5\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0Probabilistic Analysis and Randomized\\nAlgorithms\\nThis chapter introduces probabilistic analysis and randomized\\nalgorithms. If you are unfamiliar with the basics of probability theory,\\nyou should read Sections C.1–C.4 of Appendix C, which review this\\nmaterial. We’ll revisit probabilistic analysis and randomized algorithms\\nseveral times throughout this book.\\n5.1\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0The hiring problem\\nSuppose that you need to hire a new ofﬁce assistant. Your previous\\nattempts at hiring have been unsuccessful, and you decide to use an\\nemployment agency. The employment agency sends you one candidate\\neach day. You interview that person and then decide either to hire that\\nperson or not. You must pay the employment agency a small fee to\\ninterview an applicant. To actually hire an applicant is more costly,\\nhowever, since you must ﬁre your current ofﬁce assistant and also pay a\\nsubstantial hiring fee to the employment agency. You are committed to\\nhaving, at all times, the best possible person for the job. Therefore, you\\ndecide that, after interviewing each applicant, if that applicant is better\\nqualiﬁed than the current ofﬁce assistant, you will ﬁre the current ofﬁce\\nassistant and hire the new applicant. You are willing to pay the resulting\\nprice of this strategy, but you wish to estimate what that price will be.\\nThe procedure HIRE-ASSISTANT on the facing page expresses this\\nstrategy for hiring in pseudocode. The candidates for the ofﬁce assistant', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 183}),\n",
              " Document(page_content='job are numbered 1 through n and interviewed in that order. The\\nprocedure assumes that after interviewing candidate i, you can\\ndetermine whether candidate i is the best candidate you have seen so far.\\nIt starts by creating a dummy candidate, numbered 0, who is less\\nqualiﬁed than each of the other candidates.\\nThe cost model for this problem differs from the model described in\\nChapter 2. We focus not on the running time of HIRE-ASSISTANT,\\nbut instead on the fees paid for interviewing and hiring. On the surface,\\nanalyzing the cost of this algorithm may seem very different from\\nanalyzing the running time of, say, merge sort. The analytical\\ntechniques used, however, are identical whether we are analyzing cost or\\nrunning time. In either case, we are counting the number of times\\ncertain basic operations are executed.\\nHIRE-ASSISTANT(n)\\n1best = 0// candidate 0 is a least-qualiﬁed dummy candidate\\n2for\\xa0i = 1 to\\xa0n\\n3 interview candidate i\\n4 if candidate i is better than candidate best\\n5 best = i\\n6 hire candidate i\\nInterviewing has a low cost, say ci, whereas hiring is expensive,\\ncosting ch. Letting m be the number of people hired, the total cost\\nassociated with this algorithm is O(cin + chm). No matter how many\\npeople you hire, you always interview n candidates and thus always\\nincur the cost cin associated with interviewing. We therefore concentrate\\non analyzing chm, the hiring cost. This quantity depends on the order in\\nwhich you interview candidates.\\nThis scenario serves as a model for a common computational\\nparadigm. Algorithms often need to ﬁnd the maximum or minimum\\nvalue in a sequence by examining each element of the sequence and\\nmaintaining a current “winner.” The hiring problem models how often\\na procedure updates its notion of which element is currently winning.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 184}),\n",
              " Document(page_content='Worst-case analysis\\nIn the worst case, you actually hire every candidate that you interview.\\nThis situation occurs if the candidates come in strictly increasing order\\nof quality, in which case you hire n times, for a total hiring cost of\\nO(chn).\\nOf course, the candidates do not always come in increasing order of\\nquality. In fact, you have no idea about the order in which they arrive,\\nnor do you have any control over this order. Therefore, it is natural to\\nask what we expect to happen in a typical or average case.\\nProbabilistic analysis\\nProbabilistic analysis is the use of probability in the analysis of\\nproblems. Most commonly, we use probabilistic analysis to analyze the\\nrunning time of an algorithm. Sometimes we use it to analyze other\\nquantities, such as the hiring cost in procedure HIRE-ASSISTANT. In\\norder to perform a probabilistic analysis, we must use knowledge of, or\\nmake assumptions about, the distribution of the inputs. Then we\\nanalyze our algorithm, computing an average-case running time, where\\nwe take the average, or expected value, over the distribution of the\\npossible inputs. When reporting such a running time, we refer to it as\\nthe average-case running time.\\nYou must be careful in deciding on the distribution of inputs. For\\nsome problems, you may reasonably assume something about the set of\\nall possible inputs, and then you can use probabilistic analysis as a\\ntechnique for designing an efﬁcient algorithm and as a means for\\ngaining insight into a problem. For other problems, you cannot\\ncharacterize a reasonable input distribution, and in these cases you\\ncannot use probabilistic analysis.\\nFor the hiring problem, we can assume that the applicants come in a\\nrandom order. What does that mean for this problem? We assume that\\nyou can compare any two candidates and decide which one is better\\nqualiﬁed, which is to say that there is a total order on the candidates.\\n(See Section B.2 for the deﬁnition of a total order.) Thus, you can rank\\neach candidate with a unique number from 1 through n, using rank(i) to\\ndenote the rank of applicant i, and adopt the convention that a higher', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 185}),\n",
              " Document(page_content='rank corresponds to a better qualiﬁed applicant. The ordered list\\n〈rank(1), rank(2), … , rank(n)〉 is a permutation of the list 〈1, 2, … , n〉.\\nSaying that the applicants come in a random order is equivalent to\\nsaying that this list of ranks is equally likely to be any one of the n!\\npermutations of the numbers 1 through n. Alternatively, we say that the\\nranks form a uniform random permutation, that is, each of the possible n!\\npermutations appears with equal probability.\\nSection 5.2 contains a probabilistic analysis of the hiring problem.\\nRandomized algorithms\\nIn order to use probabilistic analysis, you need to know something\\nabout the distribution of the inputs. In many cases, you know little\\nabout the input distribution. Even if you do know something about the\\ndistribution, you might not be able to model this knowledge\\ncomputationally. Yet, probability and randomness often serve as tools\\nfor algorithm design and analysis, by making part of the algorithm\\nbehave randomly.\\nIn the hiring problem, it may seem as if the candidates are being\\npresented to you in a random order, but you have no way of knowing\\nwhether they really are. Thus, in order to develop a randomized\\nalgorithm for the hiring problem, you need greater control over the\\norder in which you’ll interview the candidates. We will, therefore,\\nchange the model slightly. The employment agency sends you a list of\\nthe n candidates in advance. On each day, you choose, randomly, which\\ncandidate to interview. Although you know nothing about the\\ncandidates (besides their names), we have made a signiﬁcant change.\\nInstead of accepting the order given to you by the employment agency\\nand hoping that it’s random, you have instead gained control of the\\nprocess and enforced a random order.\\nMore generally, we call an algorithm randomized if its behavior is\\ndetermined not only by its input but also by values produced by a\\nrandom-number generator. We assume that we have at our disposal a\\nrandom-number generator RANDOM. A call to RANDOM(a, b)\\nreturns an integer between a and b, inclusive, with each such integer\\nbeing equally likely. For example, RANDOM(0, 1) produces 0 with', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 186}),\n",
              " Document(page_content='probability 1/2, and it produces 1 with probability 1/2. A call to\\nRANDOM(3, 7) returns any one of 3, 4, 5, 6, or 7, each with\\nprobability 1/5. Each integer returned by RANDOM is independent of\\nthe integers returned on previous calls. You may imagine RANDOM as\\nrolling a (b – a + 1)-sided die to obtain its output. (In practice, most\\nprogramming environments offer a pseudorandom-number generator: a\\ndeterministic algorithm returning numbers that “look” statistically\\nrandom.)\\nWhen analyzing the running time of a randomized algorithm, we\\ntake the expectation of the running time over the distribution of values\\nreturned by the random number generator. We distinguish these\\nalgorithms from those in which the input is random by referring to the\\nrunning time of a randomized algorithm as an expected running time. In\\ngeneral, we discuss the average-case running time when the probability\\ndistribution is over the inputs to the algorithm, and we discuss the\\nexpected running time when the algorithm itself makes random choices.\\nExercises\\n5.1-1\\nShow that the assumption that you are always able to determine which\\ncandidate is best, in line 4 of procedure HIRE-ASSISTANT, implies\\nthat you know a total order on the ranks of the candidates.\\n★ 5.1-2\\nDescribe an implementation of the procedure RANDOM(a, b) that\\nmakes calls only to RANDOM(0, 1). What is the expected running time\\nof your procedure, as a function of a and b?\\n★ 5.1-3\\nYou wish to implement a program that outputs 0 with probability 1/2\\nand 1 with probability 1/2. At your disposal is a procedure BIASED-\\nRANDOM that outputs either 0 or 1, but it outputs 1 with some\\nprobability p and 0 with probability 1 – p, where 0 < p < 1. You do not\\nknow what p is. Give an algorithm that uses BIASED-RANDOM as a\\nsubroutine, and returns an unbiased answer, returning 0 with', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 187}),\n",
              " Document(page_content='probability 1/2 and 1 with probability 1/2. What is the expected running\\ntime of your algorithm as a function of p?\\n5.2\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0Indicator random variables\\nIn order to analyze many algorithms, including the hiring problem, we\\nuse indicator random variables. Indicator random variables provide a\\nconvenient method for converting between probabilities and\\nexpectations. Given a sample space S and an event A, the indicator\\nrandom variable I {A} associated with event A is deﬁned as\\nAs a simple example, let us determine the expected number of heads\\nobtained when ﬂipping a fair coin. The sample space for a single coin\\nﬂip is S = {H, T}, with Pr {H} = Pr {T} = 1/2. We can then deﬁne an\\nindicator random variable XH, associated with the coin coming up\\nheads, which is the event H. This variable counts the number of heads\\nobtained in this ﬂip, and it is 1 if the coin comes up heads and 0\\notherwise. We write\\nThe expected number of heads obtained in one ﬂip of the coin is simply\\nthe expected value of our indicator variable XH:\\nE [XH]=E [I {H}]\\n=1 · Pr {H} + 0 · Pr {T}\\n=1 · (1/2) + 0 · (1/2)\\n=1/2.\\nThus the expected number of heads obtained by one ﬂip of a fair coin is\\n1/2. As the following lemma shows, the expected value of an indicator', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 188}),\n",
              " Document(page_content='random variable associated with an event A is equal to the probability\\nthat A occurs.\\nLemma 5.1\\nGiven a sample space S and an event A in the sample space S, let XA =\\nI {A}. Then E [XA] = Pr {A}.\\nProof\\xa0\\xa0\\xa0By the deﬁnition of an indicator random variable from equation\\n(5.1) and the deﬁnition of expected value, we have\\nE [XA]=E [I {A}]\\n=1 · Pr {A} + 0 · Pr {A}\\n=Pr {A},\\nwhere A denotes S – A, the complement of A.\\n▪\\nAlthough indicator random variables may seem cumbersome for an\\napplication such as counting the expected number of heads on a ﬂip of a\\nsingle coin, they are useful for analyzing situations that perform\\nrepeated random trials. In Appendix C, for example, indicator random\\nvariables provide a simple way to determine the expected number of\\nheads in n coin ﬂips. One option is to consider separately the probability\\nof obtaining 0 heads, 1 head, 2 heads, etc. to arrive at the result of\\nequation (C.41) on page 1199. Alternatively, we can employ the simpler\\nmethod proposed in equation (C.42), which uses indicator random\\nvariables implicitly. Making this argument more explicit, let Xi be the\\nindicator random variable associated with the event in which the ith ﬂip\\ncomes up heads: Xi = I {the ith ﬂip results in the event H}. Let X be the\\nrandom variable denoting the total number of heads in the n coin ﬂips,\\nso that\\n', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 189}),\n",
              " Document(page_content='In order to compute the expected number of heads, take the expectation\\nof both sides of the above equation to obtain\\nBy Lemma 5.1, the expectation of each of the random variables is E [Xi]\\n= 1/2 for i = 1, 2, … , n. Then we can compute the sum of the\\nexpectations: \\n . But equation (5.2) calls for the\\nexpectation of the sum, not the sum of the expectations. How can we\\nresolve this conundrum? Linearity of expectation, equation (C.24) on\\npage 1192, to the rescue: the expectation of the sum always equals the\\nsum of the expectations. Linearity of expectation applies even when\\nthere is dependence among the random variables. Combining indicator\\nrandom variables with linearity of expectation gives us a powerful\\ntechnique to compute expected values when multiple events occur. We\\nnow can compute the expected number of heads:\\nThus, compared with the method used in equation (C.41), indicator\\nrandom variables greatly simplify the calculation. We use indicator\\nrandom variables throughout this book.\\nAnalysis of the hiring problem using indicator random  variables\\nReturning to the hiring problem, we now wish to compute the expected\\nnumber of times that you hire a new ofﬁce assistant. In order to use a\\nprobabilistic analysis, let’s assume that the candidates arrive in a\\nrandom order, as discussed in Section 5.1. (We’ll see in Section 5.3 how', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 190}),\n",
              " Document(page_content='to remove this assumption.) Let X be the random variable whose value\\nequals the number of times you hire a new ofﬁce assistant. We could\\nthen apply the deﬁnition of expected value from equation (C.23) on\\npage 1192 to obtain\\nbut this calculation would be cumbersome. Instead, let’s simplify the\\ncalculation by using indicator random variables.\\nTo use indicator random variables, instead of computing E [X] by\\ndeﬁning just one variable denoting the number of times you hire a new\\nofﬁce assistant, think of the process of hiring as repeated random trials\\nand deﬁne n variables indicating whether each particular candidate is\\nhired. In particular, let Xi be the indicator random variable associated\\nwith the event in which the ith candidate is hired. Thus,\\nand\\nLemma 5.1 gives\\nE [Xi] = Pr {candidate i is hired},\\nand we must therefore compute the probability that lines 5–6 of HIRE-\\nASSISTANT are executed.\\nCandidate i is hired, in line 6, exactly when candidate i is better than\\neach of candidates 1 through i – 1. Because we have assumed that the\\ncandidates arrive in a random order, the ﬁrst i candidates have appeared\\nin a random order. Any one of these ﬁrst i candidates is equally likely to\\nbe the best qualiﬁed so far. Candidate i has a probability of 1/i of being\\nbetter qualiﬁed than candidates 1 through i – 1 and thus a probability of\\n1/i of being hired. By Lemma 5.1, we conclude that', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 191}),\n",
              " Document(page_content='Now we can compute E [X]:\\nEven though you interview n people, you actually hire only\\napproximately ln n of them, on average. We summarize this result in the\\nfollowing lemma.\\nLemma 5.2\\nAssuming that the candidates are presented in a random order,\\nalgorithm HIRE-ASSISTANT has an average-case total hiring cost of\\nO(ch ln n).\\nProof\\xa0\\xa0\\xa0The bound follows immediately from our deﬁnition of the hiring\\ncost and equation (5.6), which shows that the expected number of hires\\nis approximately ln n.\\n▪\\nThe average-case hiring cost is a signiﬁcant improvement over the\\nworst-case hiring cost of O(chn).\\nExercises\\n5.2-1\\nIn HIRE-ASSISTANT, assuming that the candidates are presented in a\\nrandom order, what is the probability that you hire exactly one time?\\nWhat is the probability that you hire exactly n times?', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 192}),\n",
              " Document(page_content='5.2-2\\nIn HIRE-ASSISTANT, assuming that the candidates are presented in a\\nrandom order, what is the probability that you hire exactly twice?\\n5.2-3\\nUse indicator random variables to compute the expected value of the\\nsum of n dice.\\n5.2-4\\nThis exercise asks you to (partly) verify that linearity of expectation\\nholds even if the random variables are not independent. Consider two 6-\\nsided dice that are rolled independently. What is the expected value of\\nthe sum? Now consider the case where the ﬁrst die is rolled normally\\nand then the second die is set equal to the value shown on the ﬁrst die.\\nWhat is the expected value of the sum? Now consider the case where the\\nﬁrst die is rolled normally and the second die is set equal to 7 minus the\\nvalue of the ﬁrst die. What is the expected value of the sum?\\n5.2-5\\nUse indicator random variables to solve the following problem, which is\\nknown as the hat-check problem. Each of n customers gives a hat to a\\nhat-check person at a restaurant. The hat-check person gives the hats\\nback to the customers in a random order. What is the expected number\\nof customers who get back their own hat?\\n5.2-6\\nLet A[1 : n] be an array of n distinct numbers. If i < j and A[i] > A[j],\\nthen the pair (i, j) is called an inversion of A. (See Problem 2-4 on page\\n47 for more on inversions.) Suppose that the elements of A form a\\nuniform random permutation of 〈1, 2, … , n〉. Use indicator random\\nvariables to compute the expected number of inversions.\\n5.3\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0Randomized algorithms\\nIn the previous section, we showed how knowing a distribution on the\\ninputs can help us to analyze the average-case behavior of an algorithm.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 193}),\n",
              " Document(page_content='What if you do not know the distribution? Then you cannot perform an\\naverage-case analysis. As mentioned in Section 5.1, however, you might\\nbe able to use a randomized algorithm.\\nFor a problem such as the hiring problem, in which it is helpful to\\nassume that all permutations of the input are equally likely, a\\nprobabilistic analysis can guide us when developing a randomized\\nalgorithm. Instead of assuming a distribution of inputs, we impose a\\ndistribution. In particular, before running the algorithm, let’s randomly\\npermute the candidates in order to enforce the property that every\\npermutation is equally likely. Although we have modiﬁed the algorithm,\\nwe still expect to hire a new ofﬁce assistant approximately ln n times.\\nBut now we expect this to be the case for any input, rather than for\\ninputs drawn from a particular distribution.\\nLet us further explore the distinction between probabilistic analysis\\nand randomized algorithms. In Section 5.2, we claimed that, assuming\\nthat the candidates arrive in a random order, the expected number of\\ntimes you hire a new ofﬁce assistant is about ln n. This algorithm is\\ndeterministic: for any particular input, the number of times a new ofﬁce\\nassistant is hired is always the same. Furthermore, the number of times\\nyou hire a new ofﬁce assistant differs for different inputs, and it depends\\non the ranks of the various candidates. Since this number depends only\\non the ranks of the candidates, to represent a particular input, we can\\njust list, in order, the ranks 〈rank(1), rank(2), … , rank(n)〉 of the\\ncandidates. Given the rank list A1 = 〈1, 2, 3, 4, 5, 6, 7, 8, 9, 10〉, a new\\nofﬁce assistant is always hired 10 times, since each successive candidate\\nis better than the previous one, and lines 5–6 of HIRE-ASSISTANT are\\nexecuted in each iteration. Given the list of ranks A2 = 〈10, 9, 8, 7, 6, 5,\\n4, 3, 2, 1〉, a new ofﬁce assistant is hired only once, in the ﬁrst iteration.\\nGiven a list of ranks A3 = 〈5, 2, 1, 8, 4, 7, 10, 9, 3, 6〉, a new ofﬁce\\nassistant is hired three times, upon interviewing the candidates with\\nranks 5, 8, and 10. Recalling that the cost of our algorithm depends on\\nhow many times you hire a new ofﬁce assistant, we see that there are\\nexpensive inputs such as A1, inexpensive inputs such as A2, and\\nmoderately expensive inputs such as A3.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 194}),\n",
              " Document(page_content='Consider, on the other hand, the randomized algorithm that ﬁrst\\npermutes the list of candidates and then determines the best candidate.\\nIn this case, we randomize in the algorithm, not in the input\\ndistribution. Given a particular input, say A3 above, we cannot say how\\nmany times the maximum is updated, because this quantity differs with\\neach run of the algorithm. The ﬁrst time you run the algorithm on A3, it\\nmight produce the permutation A1 and perform 10 updates. But the\\nsecond time you run the algorithm, it might produce the permutation\\nA2 and perform only one update. The third time you run the algorithm,\\nit might perform some other number of updates. Each time you run the\\nalgorithm, its execution depends on the random choices made and is\\nlikely to differ from the previous execution of the algorithm. For this\\nalgorithm and many other randomized algorithms, no particular input\\nelicits its worst-case behavior. Even your worst enemy cannot produce a\\nbad input array, since the random permutation makes the input order\\nirrelevant. The randomized algorithm performs badly only if the\\nrandom-number generator produces an “unlucky” permutation.\\nFor the hiring problem, the only change needed in the code is to\\nrandomly permute the array, as done in the RANDOMIZED-HIRE-\\nASSISTANT procedure. This simple change creates a randomized\\nalgorithm whose performance matches that obtained by assuming that\\nthe candidates were presented in a random order.\\nRANDOMIZED-HIRE-ASSISTANT(n)\\n1randomly permute the list\\nof candidates\\n2HIRE-ASSISTANT(n)\\nLemma 5.3\\nThe expected hiring cost of the procedure RANDOMIZED-HIRE-\\nASSISTANT is O(ch ln n).\\nProof\\xa0\\xa0\\xa0Permuting the input array achieves a situation identical to that\\nof the probabilistic analysis of HIRE-ASSISTANT in Secetion 5.2.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 195}),\n",
              " Document(page_content='▪\\nBy carefully comparing Lemmas 5.2 and 5.3, you can see the\\ndifference between probabilistic analysis and randomized algorithms.\\nLemma 5.2 makes an assumption about the input. Lemma 5.3 makes no\\nsuch assumption, although randomizing the input takes some\\nadditional time. To remain consistent with our terminology, we couched\\nLemma 5.2 in terms of the average-case hiring cost and Lemma 5.3 in\\nterms of the expected hiring cost. In the remainder of this section, we\\ndiscuss some issues involved in randomly permuting inputs.\\nRandomly permuting arrays\\nMany randomized algorithms randomize the input by permuting a\\ngiven input array. We’ll see elsewhere in this book other ways to\\nrandomize an algorithm, but now, let’s see how we can randomly\\npermute an array of n elements. The goal is to produce a uniform\\nrandom permutation, that is, a permutation that is as likely as any other\\npermutation. Since there are n! possible permutations, we want the\\nprobability that any particular permutation is produced to be 1/n!.\\nYou might think that to prove that a permutation is a uniform\\nrandom permutation, it sufﬁces to show that, for each element A[i], the\\nprobability that the element winds up in position j is 1/n. Exercise 5.3-4\\nshows that this weaker condition is, in fact, insufﬁcient.\\nOur method to generate a random permutation permutes the array\\nin place: at most a constant number of elements of the input array are\\never stored outside the array. The procedure RANDOMLY-\\nPERMUTE permutes an array A[1 : n] in place in Θ (n) time. In its ith\\niteration, it chooses the element A[i] randomly from among elements\\nA[i] through A[n]. After the ith iteration, A[i] is never altered.\\nRANDOMLY-PERMUTE(A, n)\\n1for\\xa0i = 1 to\\xa0n\\n2 swap A[i] with A[RANDOM(i, n)]', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 196}),\n",
              " Document(page_content='We use a loop invariant to show that procedure RANDOMLY-\\nPERMUTE produces a uniform random permutation. A k-permutation\\non a set of n elements is a sequence containing k of the n elements, with\\nno repetitions. (See page 1180 in Appendix C.) There are n!/(n – k)! such\\npossible k-permutations.\\nLemma 5.4\\nProcedure RANDOMLY-PERMUTE computes a uniform random\\npermutation.\\nProof\\xa0\\xa0\\xa0We use the following loop invariant:\\nJust prior to the ith iteration of the for loop of lines 1–2, for\\neach possible (i – 1)-permutation of the n elements, the subarray\\nA[1 : i – 1] contains this (i – 1)-permutation with probability (n\\n– i + 1)!/n!.\\nWe need to show that this invariant is true prior to the ﬁrst loop\\niteration, that each iteration of the loop maintains the invariant, that\\nthe loop terminates, and that the invariant provides a useful property to\\nshow correctness when the loop terminates.\\nInitialization: Consider the situation just before the ﬁrst loop iteration,\\nso that i = 1. The loop invariant says that for each possible 0-\\npermutation, the subarray A[1 : 0] contains this 0-permutation with\\nprobability (n – i + 1)!/n! = n!/n! = 1. The subarray A[1 : 0] is an empty\\nsubarray, and a 0-permutation has no elements. Thus, A[1 : 0]\\ncontains any 0-permutation with probability 1, and the loop invariant\\nholds prior to the ﬁrst iteration.\\nMaintenance: By the loop invariant, we assume that just before the ith\\niteration, each possible (i – 1)-permutation appears in the subarray\\nA[1 : i – 1] with probability (n – i + 1)!/n!. We shall show that after the\\nith iteration, each possible i-permutation appears in the subarray A[1 :\\ni] with probability (n – i)!/n!. Incrementing i for the next iteration then\\nmaintains the loop invariant.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 197}),\n",
              " Document(page_content='Let us examine the ith iteration. Consider a particular i-permutation,\\nand denote the elements in it by 〈x1, x2, … , xi〉. This permutation\\nconsists of an (i – 1)-permutation 〈x1, … , xi–1〉 followed by the value\\nxi that the algorithm places in A[i]. Let E1 denote the event in which\\nthe ﬁrst i – 1 iterations have created the particular (i – 1)-permutation\\n〈x1, … , xi–1〉 in A[1 : i – 1]. By the loop invariant, Pr {E1} = (n – i +\\n1)!/n!. Let E2 be the event that the ith iteration puts xi in position A[i].\\nThe i-permutation 〈x1, … , xi〉 appears in A[1 : i] precisely when both\\nE1 and E2 occur, and so we wish to compute Pr {E2 ∩ E1}. Using\\nequation (C.16) on page 1187, we have\\nPr {E2 ∩ E1} = Pr {E2 | E1} Pr {E1}.\\nThe probability Pr {E2 | E1} equals 1/(n – i + 1) because in line 2 the\\nalgorithm chooses xi randomly from the n – i + 1 values in positions\\nA[i : n]. Thus, we have\\nTermination: The loop terminates, since it is a for loop iterating n times.\\nAt termination, i = n + 1, and we have that the subarray A[1 : n] is a\\ngiven n-permutation with probability (n – (n + 1) + 1)!/n! = 0!/n! =\\n1/n!.\\nThus, RANDOMLY-PERMUTE produces a uniform random\\npermutation.\\n▪\\nA randomized algorithm is often the simplest and most efﬁcient way\\nto solve a problem.\\nExercises', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 198}),\n",
              " Document(page_content='5.3-1\\nProfessor Marceau objects to the loop invariant used in the proof of\\nLemma 5.4. He questions whether it holds prior to the ﬁrst iteration. He\\nreasons that we could just as easily declare that an empty subarray\\ncontains no 0-permutations. Therefore, the probability that an empty\\nsubarray contains a 0-permutation should be 0, thus invalidating the\\nloop invariant prior to the ﬁrst iteration. Rewrite the procedure\\nRANDOMLY-PERMUTE so that its associated loop invariant applies\\nto a nonempty subarray prior to the ﬁrst iteration, and modify the\\nproof of Lemma 5.4 for your procedure.\\n5.3-2\\nProfessor Kelp decides to write a procedure that produces at random\\nany permutation except the identity permutation, in which every element\\nends up where it started. He proposes the procedure PERMUTE-\\nWITHOUT-IDENTITY. Does this procedure do what Professor Kelp\\nintends?\\nPERMUTE-WITHOUT-IDENTITY(A, n)\\n1for\\xa0i = 1 to\\xa0n – 1\\n2 swap A[i] with A[RANDOM(i + 1, n)]\\n5.3-3\\nConsider the PERMUTE-WITH-ALL procedure on the facing page,\\nwhich instead of swapping element A[i] with a random element from the\\nsubarray A[i : n], swaps it with a random element from anywhere in the\\narray. Does PERMUTE-WITH-ALL produce a uniform random\\npermutation? Why or why not?\\nPERMUTE-WITH-ALL(A, n)\\n1for\\xa0i = 1 to\\xa0n\\n2 swap A[i] with A[RANDOM(1, n)]\\n5.3-4', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 199}),\n",
              " Document(page_content='Professor Knievel suggests the procedure PERMUTE-BY-CYCLE to\\ngenerate a uniform random permutation. Show that each element A[i]\\nhas a 1/n probability of winding up in any particular position in B. Then\\nshow that Professor Knievel is mistaken by showing that the resulting\\npermutation is not uniformly random.\\nPERMUTE-BY-CYCLE(A, n)\\n1let B[1 : n] be a new array\\n2offset = RANDOM(1, n)\\n3for\\xa0i = 1 to\\xa0n\\n4 dest = i + offset\\n5 if\\xa0dest > n\\n6 dest = dest – n\\n7 B[dest] = A[i]\\n8return\\xa0B\\n5.3-5\\nProfessor Gallup wants to create a random sample of the set {1, 2, 3, … ,\\nn}, that is, an m-element subset S, where 0 ≤ m ≤ n, such that each m-\\nsubset is equally likely to be created. One way is to set A[i] = i, for i = 1,\\n2, 3, … , n, call RANDOMLY-PERMUTE(A), and then take just the\\nﬁrst m array elements. This method makes n calls to the RANDOM\\nprocedure. In Professor Gallup’s application, n is much larger than m,\\nand so the professor wants to create a random sample with fewer calls\\nto RANDOM.\\nRANDOM-SAMPLE(m, n)\\n1S = ∅\\n2for\\xa0k = n – m + 1 to\\xa0n // iterates m times\\n3 i = RANDOM(1, k)\\n4 if\\xa0i ∈ S\\n5 S = S ⋃ {k}\\n6 else\\xa0S = S ⋃ {i}\\n7return\\xa0S', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 200}),\n",
              " Document(page_content='Show that the procedure RANDOM-SAMPLE on the previous page\\nreturns a random m-subset S of {1, 2, 3, … , n}, in which each m-subset\\nis equally likely, while making only m calls to RANDOM.\\n★ 5.4\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0Probabilistic analysis and f urther uses of indicator\\nrandom variables\\nThis advanced section further illustrates probabilistic analysis by way of\\nfour examples. The ﬁrst determines the probability that in a room of k\\npeople, two of them share the same birthday. The second example\\nexamines what happens when randomly tossing balls into bins. The\\nthird investigates “streaks” of consecutive heads when ﬂipping coins.\\nThe ﬁnal example analyzes a variant of the hiring problem in which you\\nhave to make decisions without actually interviewing all the candidates.\\n5.4.1\\xa0\\xa0\\xa0\\xa0The birthday paradox\\nOur ﬁrst example is the birthday paradox. How many people must there\\nbe in a room before there is a 50% chance that two of them were born\\non the same day of the year? The answer is surprisingly few. The\\nparadox is that it is in fact far fewer than the number of days in a year,\\nor even half the number of days in a year, as we shall see.\\nTo answer this question, we index the people in the room with the\\nintegers 1, 2, … , k, where k is the number of people in the room. We\\nignore the issue of leap years and assume that all years have n = 365\\ndays. For i = 1, 2, … , k, let bi be the day of the year on which person i’s\\nbirthday falls, where 1 ≤ bi ≤ n. We also assume that birthdays are\\nuniformly distributed across the n days of the year, so that Pr {bi = r} =\\n1/n for i = 1, 2, … , k and r = 1, 2, … , n.\\nThe probability that two given people, say i and j, have matching\\nbirthdays depends on whether the random selection of birthdays is\\nindependent. We assume from now on that birthdays are independent,\\nso that the probability that i’s birthday and j’s birthday both fall on day\\nr is', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 201}),\n",
              " Document(page_content='Thus, the probability that they both fall on the same day is\\nMore intuitively, once bi is chosen, the probability that bj is chosen to be\\nthe same day is 1/n. As long as the birthdays are independent, the\\nprobability that i and j have the same birthday is the same as the\\nprobability that the birthday of one of them falls on a given day.\\nWe can analyze the probability of at least 2 out of k people having\\nmatching birthdays by looking at the complementary event. The\\nprobability that at least two of the birthdays match is 1 minus the\\nprobability that all the birthdays are different. The event Bk that k\\npeople have distinct birthdays is\\nwhere Ai is the event that person i’s birthday is different from person j’s\\nfor all j < i. Since we can write Bk = Ak ∩ Bk–1, we obtain from\\nequation (C.18) on page 1189 the recurrence\\nwhere we take Pr {B1} = Pr {A1} = 1 as an initial condition. In other\\nwords, the probability that b1, b2, … , bk are distinct birthdays equals\\nthe probability that b1, b2, … , bk–1 are distinct birthdays multiplied by', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 202}),\n",
              " Document(page_content='the probability that bk ≠ bi for i = 1, 2, … , k – 1, given that b1, b2, … ,\\nbk–1 are distinct.\\nIf b1, b2, … , bk–1 are distinct, the conditional probability that bk ≠\\nbi for i = 1, 2, … , k – 1 is Pr {Ak | Bk–1} = (n – k + 1)/n, since out of\\nthe n days, n – (k – 1) days are not taken. We iteratively apply the\\nrecurrence (5.8) to obtain\\nInequality (3.14) on page 66, 1 + x ≤ ex, gives us\\nwhen –k(k – 1)/2n ≤ ln(1/2). The probability that all k birthdays are\\ndistinct is at most 1/2 when k(k – 1) ≥ 2n ln 2 or, solving the quadratic\\nequation, when \\n . For n = 365, we must have k ≥\\n23. Thus, if at least 23 people are in a room, the probability is at least\\n1/2 that at least two people have the same birthday. Since a year on\\nMars is 669 Martian days long, it takes 31 Martians to get the same\\neffect.\\nAn analysis using indicator random variables\\nIndicator random variables afford a simpler but approximate analysis of\\nthe birthday paradox. For each pair (i, j) of the k people in the room,', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 203}),\n",
              " Document(page_content='deﬁne the indicator random variable Xij, for 1 ≤ i < j ≤ k, by\\nBy equation (5.7), the probability that two people have matching\\nbirthdays is 1/n, and thus by Lemma 5.1 on page 130,  we have\\nE [Xij]=Pr {person i and person j have the same birthday}\\n=1/n.\\nLetting X be the random variable that counts the number of pairs of\\nindividuals having the same birthday, we have\\nTaking expectations of both sides and applying linearity of expectation,\\nwe obtain\\nWhen k(k – 1) ≥ 2n, therefore, the expected number of pairs of people\\nwith the same birthday is at least 1. Thus, if we have at least \\nindividuals in a room, we can expect at least two to have the same\\nbirthday. For n = 365, if k = 28, the expected number of pairs with the\\nsame birthday is (28 · 27)/(2 · 365) ≈ 1.0356.  Thus, with at least 28', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 204}),\n",
              " Document(page_content='people, we expect to ﬁnd at least one matching pair of birthdays. On\\nMars, with 669 days per year, we need at least 38 M artians.\\nThe ﬁrst analysis, which used only probabilities, determined the\\nnumber of people required for the probability to exceed 1/2 that a\\nmatching pair of birthdays exists, and the second analysis, which used\\nindicator random variables, determined the number such that the\\nexpected number of matching birthdays is 1. Although the exact\\nnumbers of people differ for the two situations, they are the same\\nasymptotically: \\n .\\n5.4.2\\xa0\\xa0\\xa0\\xa0Balls and bins\\nConsider a process in which you randomly toss identical balls into b\\nbins, numbered 1, 2, … , b. The tosses are independent, and on each\\ntoss the ball is equally likely to end up in any bin. The probability that a\\ntossed ball lands in any given bin is 1/b. If we view the ball-tossing\\nprocess as a sequence of Bernoulli trials (see Appendix C.4), where\\nsuccess means that the ball falls in the given bin, then each trial has a\\nprobability 1/b of success. This model is particularly useful for analyzing\\nhashing (see Chapter 11), and we can answer a variety of interesting\\nquestions about the ball-tossing process. (Problem C-2 asks additional\\nquestions about balls and bins.)\\nHow many balls fall in a given bin? The number of balls that fall in\\na given bin follows the binomial distribution b(k;n, 1/b). If you\\ntoss n balls, equation (C.41) on page 1199 tells us that the\\nexpected number of balls that fall in the given bin is n/b.\\nHow many balls must you toss, on the average, until a given bin\\ncontains a ball? The number of tosses until the given bin receives a\\nball follows the geometric distribution with probability 1/b and,\\nby equation (C.36) on page 1197, the expected number of tosses\\nuntil success is 1/(1/b) = b.\\nHow many balls must you toss until every bin contains at least one\\nball? Let us call a toss in which a ball falls into an empty bin a\\n“hit.” We want to know the expected number n of tosses required\\nto get b hits.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 205}),\n",
              " Document(page_content='Using the hits, we can partition the n tosses into stages. The ith\\nstage consists of the tosses after the (i – 1)st hit up to and\\nincluding the ith hit. The ﬁrst stage consists of the ﬁrst toss, since\\nyou are guaranteed to have a hit when all bins are empty. For each\\ntoss during the ith stage, i – 1 bins contain balls and b – i + 1 bins\\nare empty. Thus, for each toss in the ith stage, the probability of\\nobtaining a hit is (b – i + 1)/b.\\nLet ni denote the number of tosses in the ith stage. The number of\\ntosses required to get b hits is \\n . Each random variable\\nni has a geometric distribution with probability of success (b – i +\\n1)/b and thus, by equation (C.36), we have\\nBy linearity of expectation, we have\\nIt therefore takes approximately b ln b tosses before we can expect\\nthat every bin has a ball. This problem is also known as the\\ncoupon collector’s problem, which says that if you are trying to\\ncollect each of b different coupons, then you should expect to\\nacquire approximately b ln b randomly obtained coupons in order\\nto succeed.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 206}),\n",
              " Document(page_content='5.4.3\\xa0\\xa0\\xa0\\xa0Streaks\\nSuppose that you ﬂip a fair coin n times. What is the longest streak of\\nconsecutive heads that you expect to see? We’ll prove upper and lower\\nbounds separately to show that the answer is Θ (lg n).\\nWe ﬁrst prove that the expected length of the longest streak of heads\\nis O(lg n). The probability that each coin ﬂip is a head is 1/2. Let Aik be\\nthe event that a streak of heads of length at least k begins with the ith\\ncoin ﬂip or, more precisely, the event that the k consecutive coin ﬂips i, i\\n+ 1, … , i + k – 1 yield only heads, where 1 ≤ k ≤ n and 1 ≤ i ≤ n – k + 1.\\nSince coin ﬂips are mutually independent, for any given event Aik, the\\nprobability that all k ﬂips are heads is\\nand thus the probability that a streak of heads of length at least 2 ⌈lg n ⌉\\nbegins in position i is quite small. There are at most n – 2 ⌈lg n ⌉ + 1\\npositions where such a streak can begin. The probability that a streak of\\nheads of length at least 2 ⌈lg n ⌉ begins anywhere is therefore', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 207}),\n",
              " Document(page_content='We can use inequality (5.10) to bound the length of the longest\\nstreak. For j = 0, 1, 2, … , n, let Lj be the event that the longest streak of\\nheads has length exactly j, and let L be the length of the longest streak.\\nBy the deﬁnition of expected value, we have\\nWe could try to evaluate this sum using upper bounds on each Pr {Lj}\\nsimilar to those computed in inequality (5.10). Unfortunately, this\\nmethod yields weak bounds. We can use some intuition gained by the\\nabove analysis to obtain a good bound, however. For no individual term\\nin the summation in equation (5.11) are both the factors j and Pr {Lj}\\nlarge. Why? When j ≥ 2 ⌈lg n ⌉, then Pr {Lj} is very small, and when j < 2\\n⌈lg n ⌉, then j is fairly small. More precisely, since the events Lj for j = 0,\\n1, … , n are disjoint, the probability that a streak of heads of length at\\nleast 2 ⌈lg n ⌉ begins anywhere is \\n . Inequality (5.10) tells us\\nthat the probability that a streak of heads of length at least 2 ⌈lg n ⌉\\nbegins anywhere is less than 1/n, which means that \\n .\\nAlso, noting that \\n , we have that \\n . Thus,\\nwe obtain', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 208}),\n",
              " Document(page_content='The probability that a streak of heads exceeds r ⌈lg n ⌉ ﬂips\\ndiminishes quickly with r. Let’s get a rough bound on the probability\\nthat a streak of at least r ⌈lg n ⌉ heads occurs, for r ≥ 1. The probability\\nthat a streak of at least r ⌈lg n ⌉ heads starts in position i is\\nA streak of at least r ⌈lg n ⌉ heads cannot start in the last n – r ⌈lg n ⌉ + 1\\nﬂips, but let’s overestimate the probability of such a streak by allowing it\\nto start anywhere within the n coin ﬂips. Then the probability that a\\nstreak of at least r ⌈lg n ⌉ heads occurs is at most\\n', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 209}),\n",
              " Document(page_content='Equivalently, the probability is at least 1 – 1/nr–1 that the longest streak\\nhas length less than r ⌈lg n ⌉.\\nAs an example, during n = 1000 coin ﬂips, the probability of\\nencountering a streak of at least 2 ⌈lg n ⌉ = 20 heads is at most 1/n =\\n1/1000. The chance of a streak of at least 3 ⌈lg n ⌉ = 30 heads is at most\\n1/n2 = 1/1,000,000.\\nLet’s now prove a complementary lower bound: the expected length\\nof the longest streak of heads in n coin ﬂips is Ω(lg n). To prove this\\nbound, we look for streaks of length s by partitioning the n ﬂips into\\napproximately n/s groups of s ﬂips each. If we choose s = ⌊(lg n)/2 ⌋, we’ll\\nsee that it is likely that at least one of these groups comes up all heads,\\nwhich means that it’s likely that the longest streak has length at least s =\\nΩ(lg n). We’ll then show that the longest streak has expected length Ω(lg\\nn).\\nLet’s partition the n coin ﬂips into at least ⌊n/ ⌊(lg n)/2 ⌋ ⌋ groups of\\n⌊(lg n)/2 ⌋ consecutive ﬂips and bound the probability that no group\\ncomes up all heads. By equation (5.9), the probability that the group\\nstarting in position i comes up all heads is\\nThe probability that a streak of heads of length at least ⌊(lg n)/2 ⌋ does\\nnot begin in position i is therefore at most \\n . Since the ⌊n/ ⌊(lg\\nn)/2 ⌋ ⌋ groups are formed from mutually exclusive, independent coin\\nﬂips, the probability that every one of these groups fails to be a streak of\\nlength ⌊(lg n)/2 ⌋ is at most', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 210}),\n",
              " Document(page_content='For this argument, we used inequality (3.14), 1 + x ≤ ex, on page 66 and\\nthe fact, which you may verify, that \\n  for sufﬁciently\\nlarge n.\\nWe want to bound the probability that the longest streak equals or\\nexceeds ⌊(lg n)/2 ⌋. To do so, let L be the event that the longest streak of\\nheads equals or exceeds s = ⌊(lg n)/2 ⌋. Let L be the complementary\\nevent, that the longest streak of heads is strictly less than s, so that Pr\\n{L} + Pr {L} = 1. Let F be the event that every group of s ﬂips fails to\\nbe a streak of s heads. By inequality (5.12), we have Pr {F} = O(1/n). If\\nthe longest streak of heads is less than s, then certainly every group of s\\nﬂips fails to be a streak of s heads, which means that event L implies\\nevent F. Of course, event F could occur even if event L does not (for\\nexample, if a streak of s or more heads crosses over the boundary\\nbetween two groups), and so we have Pr {L} ≤ Pr {F} = O(1/n). Since Pr\\n{L} + Pr {L} = 1, we have that\\nPr {L}=1 – Pr {L}\\n≥1 – Pr {F}\\n=1 – O(1/n).\\nThat is, the probability that the longest streak equals or exceeds ⌊(lg\\nn)/2 ⌋ is\\nWe can now calculate a lower bound on the expected length of the\\nlongest streak, beginning with equation (5.11) and proceeding in a\\nmanner similar to our analysis of the upper bound:', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 211}),\n",
              " Document(page_content='As with the birthday paradox, we can obtain a simpler, but\\napproximate, analysis using indicator random variables. Instead of\\ndetermining the expected length of the longest streak, we’ll ﬁnd the\\nexpected number of streaks with at least a given length. Let Xik = I\\n{Aik} be the indicator random variable associated with a streak of\\nheads of length at least k beginning with the ith coin ﬂip. To count the\\ntotal number of such streaks, deﬁne\\nTaking expectations and using linearity of expectation, we have', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 212}),\n",
              " Document(page_content='By plugging in various values for k, we can calculate the expected\\nnumber of streaks of length at least k. If this expected number is large\\n(much greater than 1), then we expect many streaks of length k to occur,\\nand the probability that one occurs is high. If this expected number is\\nsmall (much less than 1), then we expect to see few streaks of length k,\\nand the probability that one occurs is low. If k = c lg n, for some positive\\nconstant c, we obtain\\nIf c is large, the expected number of streaks of length c lg n is small, and\\nwe conclude that they are unlikely to occur. On the other hand, if c =\\n1/2, then we obtain E [X(1/2) lg n] = Θ (1/n1/2–1) = Θ (n1/2), and we\\nexpect there to be numerous streaks of length (1/2) lg n. Therefore, one\\nstreak of such a length is likely to occur. We can conclude that the\\nexpected length of the longest streak is Θ (lg n).\\n5.4.4\\xa0\\xa0\\xa0\\xa0The online hiring problem', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 213}),\n",
              " Document(page_content='As a ﬁnal example, let’s consider a variant of the hiring problem.\\nSuppose now that you do not wish to interview all the candidates in\\norder to ﬁnd the best one. You also want to avoid hiring and ﬁring as\\nyou ﬁnd better and better applicants. Instead, you are willing to settle\\nfor a candidate who is close to the best, in exchange for hiring exactly\\nonce. You must obey one company requirement: after each interview\\nyou must either immediately offer the position to the applicant or\\nimmediately reject the applicant. What is the trade-off between\\nminimizing the amount of interviewing and maximizing the quality of\\nthe candidate hired?\\nWe can model this problem in the following way. After meeting an\\napplicant, you are able to give each one a score. Let score(i) denote the\\nscore you give to the ith applicant, and assume that no two applicants\\nreceive the same score. After you have seen j applicants, you know which\\nof the j has the highest score, but you do not know whether any of the\\nremaining n – j applicants will receive a higher score. You decide to\\nadopt the strategy of selecting a positive integer k < n, interviewing and\\nthen rejecting the ﬁrst k applicants, and hiring the ﬁrst applicant\\nthereafter who has a higher score than all preceding applicants. If it\\nturns out that the best-qualiﬁed applicant was among the ﬁrst k\\ninterviewed, then you hire the nth applicant—the last one interviewed.\\nWe formalize this strategy in the procedure ONLINE-MAXIMUM(k,\\nn), which returns the index of the candidate you wish to hire.\\nONLINE-MAXIMUM(k, n)\\n1best-score = –∞\\n2for\\xa0i = 1 to\\xa0k\\n3 if\\xa0score(i) > best-score\\n4 best-score = score(i)\\n5for\\xa0i = k + 1 to\\xa0n\\n6 if\\xa0score(i) > best-score\\n7 return\\xa0i\\n8return\\xa0n', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 214}),\n",
              " Document(page_content='If we determine, for each possible value of k, the probability that you\\nhire the most qualiﬁed applicant, then you can choose the best possible\\nk and implement the strategy with that value. For the moment, assume\\nthat k is ﬁxed. Let M(j) = max {score(i) : 1 ≤ i ≤ j} denote the maximum\\nscore among applicants 1 through j. Let S be the event that you succeed\\nin choosing the best-qualiﬁed applicant, and let Si be the event that you\\nsucceed when the best-qualiﬁed applicant is the ith one interviewed.\\nSince the various Si are disjoint, we have that \\n .\\nNoting that you never succeed when the best-qualiﬁed applicant is one\\nof the ﬁrst k, we have that Pr {Si} = 0 for i = 1, 2, … , k. Thus, we\\nobtain\\nWe now compute Pr {Si}. In order to succeed when the best-\\nqualiﬁed applicant is the ith one, two things must happen. First, the\\nbest-qualiﬁed applicant must be in position i, an event which we denote\\nby Bi. Second, the algorithm must not select any of the applicants in\\npositions k + 1 through i – 1, which happens only if, for each j such that\\nk + 1 ≤ j ≤ i – 1, line 6 ﬁnds that score(j) < best-score. (Because scores are\\nunique, we can ignore the possibility of score(j) = best-score.) In other\\nwords, all of the values score(k + 1) through score(i – 1) must be less\\nthan M(k). If any are greater than M(k), the algorithm instead returns\\nthe index of the ﬁrst one that is greater. We use Oi to denote the event\\nthat none of the applicants in position k + 1 through i – 1 are chosen.\\nFortunately, the two events Bi and Oi are independent. The event Oi\\ndepends only on the relative ordering of the values in positions 1\\nthrough i – 1, whereas Bi depends only on whether the value in position\\ni is greater than the values in all other positions. The ordering of the\\nvalues in positions 1 through i – 1 does not affect whether the value in\\nposition i is greater than all of them, and the value in position i does not\\naffect the ordering of the values in positions 1 through i – 1. Thus, we\\ncan apply equation (C.17) on page 1188 t o obtain', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 215}),\n",
              " Document(page_content='Pr {Si} = Pr {Bi ∩ Oi} = Pr {Bi} Pr {Oi}.\\nWe have Pr {Bi} = 1/n since the maximum is equally likely to be in any\\none of the n positions. For event Oi to occur, the maximum value in\\npositions 1 through i –1, which is equally likely to be in any of these i – 1\\npositions, must be in one of the ﬁrst k positions. Consequently, Pr {Oi}\\n= k/(i – 1) and Pr {Si} = k/(n(i – 1)). Using equation (5.14), we have\\nWe approximate by integrals to bound this summation from above and\\nbelow. By the inequalities (A.19) on page 1150,  we have\\nEvaluating these deﬁnite integrals gives us the bounds\\nwhich provide a rather tight bound for Pr {S}. Because you wish to\\nmaximize your probability of success, let us focus on choosing the value\\nof k that maximizes the lower bound on Pr {S}. (Besides, the lower-\\nbound expression is easier to maximize than the upper-bound\\nexpression.) Differentiating the expression (k/n)(ln n – ln k) with respect\\nto k, we obtain\\n', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 216}),\n",
              " Document(page_content='Setting this derivative equal to 0, we see that you maximize the lower\\nbound on the probability when ln k = ln n – 1 = ln(n/e) or, equivalently,\\nwhen k = n/e. Thus, if you implement our strategy with k = n/e, you\\nsucceed in hiring the best-qualiﬁed applicant with probability at least\\n1/e.\\nExercises\\n5.4-1\\nHow many people must there be in a room before the probability that\\nsomeone has the same birthday as you do is at least 1/2? How many\\npeople must there be before the probability that at least two people have\\na birthday on July 4 is greater than 1/2?\\n5.4-2\\nHow many people must there be in a room before the probability that\\ntwo people have the same birthday is at least 0.99? For that many\\npeople, what is the expected number of pairs of people who have the\\nsame birthday?\\n5.4-3\\nYou toss balls into b bins until some bin contains two balls. Each toss is\\nindependent, and each ball is equally likely to end up in any bin. What\\nis the expected number of ball tosses?\\n★ 5.4-4\\nFor the analysis of the birthday paradox, is it important that the\\nbirthdays be mutually independent, or is pairwise independence\\nsufﬁcient? Justify your answer.\\n★ 5.4-5\\nHow many people should be invited to a party in order to make it likely\\nthat there are three people with the same birthday?\\n★ 5.4-6\\nWhat is the probability that a k-string (deﬁned on page 1179) over a set\\nof size n forms a k-permutation? How does this question relate to the', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 217}),\n",
              " Document(page_content='birthday paradox?\\n★ 5.4-7\\nYou toss n balls into n bins, where each toss is independent and the ball\\nis equally likely to end up in any bin. What is the expected number of\\nempty bins? What is the expected number of bins with exactly one ball?\\n★ 5.4-8\\nSharpen the lower bound on streak length by showing that in n ﬂips of a\\nfair coin, the probability is at least 1 – 1/n that a streak of length lg n – 2\\nlg lg n consecutive heads occurs.\\nProblems\\n5-1\\xa0\\xa0\\xa0\\xa0\\xa0P robabilistic counting\\nWith a b-bit counter, we can ordinarily only count up to 2b – 1. With R.\\nMorris’s probabilistic counting, we can count up to a much larger value\\nat the expense of some loss of precision.\\nWe let a counter value of i represent a count of ni for i = 0, 1, … , 2b\\n– 1, where the ni form an increasing sequence of nonnegative values. We\\nassume that the initial value of the counter is 0, representing a count of\\nn0 = 0. The INCREMENT operation works on a counter containing\\nthe value i in a probabilistic manner. If i = 2b – 1, then the operation\\nreports an overﬂow error. Otherwise, the INCREMENT operation\\nincreases the counter by 1 with probability 1/(ni + 1 – ni), and it leaves\\nthe counter unchanged with probability 1 – 1/ (ni + 1 – ni).\\nIf we select ni = i for all i ≥ 0, then the counter is an ordinary one.\\nMore interesting situations arise if we select, say, ni = 2i – 1 for i > 0 or\\nni = Fi (the ith Fibonacci number—see equation (3.31) on page 69).\\nFor this problem, assume that \\n  is large enough that the\\nprobability of an overﬂow error is negligible.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 218}),\n",
              " Document(page_content='a. Show that the expected value represented by the counter after n\\nINCREMENT operations have been performed is exactly n.\\nb. The analysis of the variance of the count represented by the counter\\ndepends on the sequence of the ni. Let us consider a simple case: ni =\\n100i for all i ≥ 0. Estimate the variance in the value represented by the\\nregister after n INCREMENT operations have been performed.\\n5-2\\xa0\\xa0\\xa0\\xa0\\xa0Searching an unsorted array\\nThis problem examines three algorithms for searching for a value x in\\nan unsorted array A consisting of n elements.\\nConsider the following randomized strategy: pick a random index i\\ninto A. If A[i] = x, then terminate; otherwise, continue the search by\\npicking a new random index into A. Continue picking random indices\\ninto A until you ﬁnd an index j such that A[j] = x or until every element\\nof A has been checked. This strategy may examine a given element more\\nthan once, because it picks from the whole set of indices each time.\\na. Write pseudocode for a procedure RANDOM-SEARCH to\\nimplement the strategy above. Be sure that your algorithm terminates\\nwhen all indices into A have been picked.\\nb. Suppose that there is exactly one index i such that A[i] = x. What is\\nthe expected number of indices into A that must be picked before x is\\nfound and RANDOM-SEARCH terminates?\\nc. Generalizing your solution to part (b), suppose that there are k ≥ 1\\nindices i such that A[i] = x. What is the expected number of indices\\ninto A that must be picked before x is found and RANDOM-\\nSEARCH terminates? Your answer should be a function of n and k.\\nd. Suppose that there are no indices i such that A[i] = x. What is the\\nexpected number of indices into A that must be picked before all\\nelements of A have been checked and RANDOM-SEARCH\\nterminates?\\nNow consider a deterministic linear search algorithm. The algorithm,\\nwhich we call DETERMINISTIC-SEARCH, searches A for x in order,', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 219}),\n",
              " Document(page_content='considering A[1], A[2], A[3], … , A[n] until either it ﬁnds A[i] = x or it\\nreaches the end of the array. Assume that all possible permutations of\\nthe input array are equally likely.\\ne. Suppose that there is exactly one index i such that A[i] = x. What is\\nthe average-case running time of DETERMINISTIC-SEARCH?\\nWhat is the worst-case running time of DETERMINISTIC-\\nSEARCH?\\nf. Generalizing your solution to part (e), suppose that there are k ≥ 1\\nindices i such that A[i] = x. What is the average-case running time of\\nDETERMINISTIC-SEARCH? What is the worst-case running time\\nof DETERMINISTIC-SEARCH? Your answer should be a function\\nof n and k.\\ng. Suppose that there are no indices i such that A[i] = x. What is the\\naverage-case running time of DETERMINISTIC-SEARCH? What is\\nthe worst-case running time of DETERMINISTIC-SEARCH?\\nFinally, consider a randomized algorithm SCRAMBLE-SEARCH that\\nﬁrst randomly permutes the input array and then runs the deterministic\\nlinear search given above on the resulting permuted array.\\nh. Letting k be the number of indices i such that A[i] = x, give the worst-\\ncase and expected running times of SCRAMBLE-SEARCH for the\\ncases in which k = 0 and k = 1. Generalize your solution to handle the\\ncase in which k ≥ 1.\\ni. Which of the three searching algorithms would you use? Explain your\\nanswer.\\nChapter notes\\nBollobás [65], Hofri [223], and Spencer [420] contain a wealth of\\nadvanced probabilistic techniques. The advantages of randomized\\nalgorithms are discussed and surveyed by Karp [249] and Rabin [372].\\nThe textbook by Motwani and Raghavan [336] gives an extensive\\ntreatment of randomized algorithms.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 220}),\n",
              " Document(page_content='The RANDOMLY-PERMUTE procedure is by Durstenfeld [128],\\nbased on an earlier procedure by Fisher and Yates [143, p. 34].\\nSeveral variants of the hiring problem have been widely studied.\\nThese problems are more commonly referred to as “secretary\\nproblems.” Examples of work in this area are the paper by Ajtai,\\nMeggido, and Waarts [11] and another by Kleinberg [258], which ties\\nthe secretary problem to online ad auctions.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 221}),\n",
              " Document(page_content='Part II\\xa0\\xa0\\xa0\\xa0Sorting and Order Statistics', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 222}),\n",
              " Document(page_content='\\xa0\\n\\xa0\\nIntroduction\\nThis part presents several algorithms that solve the following sorting\\nproblem:\\nInput: A sequence of n numbers 〈a1, a2, … , an〉.\\nOutput: A permutation (reordering) \\n  of the input sequence\\nsuch that \\n .\\nThe input sequence is usually an n-element array, although it may be\\nrepresented in some other fashion, such as a linked list.\\nThe structure of the data\\nIn practice, the numbers to be sorted are rarely isolated values. Each is\\nusually part of a collection of data called a record. Each record contains\\na key, which is the value to be sorted. The remainder of the record\\nconsists of satellite data, which are usually carried around with the key.\\nIn practice, when a sorting algorithm permutes the keys, it must\\npermute the satellite data as well. If each record includes a large amount\\nof satellite data, it often pays to permute an array of pointers to the\\nrecords rather than the records themselves in order to minimize data\\nmovement.\\nIn a sense, it is these implementation details that distinguish an\\nalgorithm from a full-blown program. A sorting algorithm describes the\\nmethod to determine the sorted order, regardless of whether what’s\\nbeing sorted are individual numbers or large records containing many', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 223}),\n",
              " Document(page_content='bytes of satellite data. Thus, when focusing on the problem of sorting,\\nwe typically assume that the input consists only of numbers. Translating\\nan algorithm for sorting numbers into a program for sorting records is\\nconceptually straightforward, although in a given engineering situation\\nother subtleties may make the actual programming task a challenge.\\nWhy sorting?\\nMany computer scientists consider sorting to be the most fundamental\\nproblem in the study of algorithms. There are several reasons:\\nSometimes an application inherently needs to sort information.\\nFor example, in order to prepare customer statements, banks need\\nto sort checks by check number.\\nAlgorithms often use sorting as a key subroutine. For example, a\\nprogram that renders graphical objects which are layered on top\\nof each other might have to sort the objects according to an\\n“above” relation so that it can draw these objects from bottom to\\ntop. We will see numerous algorithms in this text that use sorting\\nas a subroutine.\\nWe can draw from among a wide variety of sorting algorithms,\\nand they employ a rich set of techniques. In fact, many important\\ntechniques used throughout algorithm design appear in sorting\\nalgorithms that have been developed over the years. In this way,\\nsorting is also a problem of historical interest.\\nWe can prove a nontrivial lower bound for sorting (as we’ll do in\\nChapter 8). Since the best upper bounds match the lower bound\\nasymptotically, we can conclude that certain of our sorting\\nalgorithms are asymptotically optimal. Moreover, we can use the\\nlower bound for sorting to prove lower bounds for various other\\nproblems.\\nMany engineering issues come to the fore when implementing\\nsorting algorithms. The fastest sorting program for a particular\\nsituation may depend on many factors, such as prior knowledge\\nabout the keys and satellite data, the memory hierarchy (caches\\nand virtual memory) of the host computer, and the software', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 224}),\n",
              " Document(page_content='environment. Many of these issues are best dealt with at the\\nalgorithmic level, rather than by “tweaking” the code.\\nSorting algorithms\\nWe introduced two algorithms that sort n real numbers in Chapter 2.\\nInsertion sort takes Θ (n2) time in the worst case. Because its inner loops\\nare tight, however, it is a fast sorting algorithm for small input sizes.\\nMoreover, unlike merge sort, it sorts in place, meaning that at most a\\nconstant number of elements of the input array are ever stored outside\\nthe array, which can be advantageous for space efﬁciency. Merge sort\\nhas a better asymptotic running time, Θ (n lg n), but the MERGE\\nprocedure it uses does not operate in place. (We’ll see a parallelized\\nversion of merge sort in Section 26.3.)\\nThis part introduces two more algorithms that sort arbitrary real\\nnumbers. Heapsort, presented in Chapter 6, sorts n numbers in place in\\nO(n lg n) time. It uses an important data structure, called a heap, which\\ncan also implement a priority queue.\\nQuicksort, in Chapter 7, also sorts n numbers in place, but its worst-\\ncase running time is Θ (n2). Its expected running time is Θ (n lg n),\\nhowever, and it generally outperforms heapsort in practice. Like\\ninsertion sort, quicksort has tight code, and so the hidden constant\\nfactor in its running time is small. It is a popular algorithm for sorting\\nlarge arrays.\\nInsertion sort, merge sort, heapsort, and quicksort are all\\ncomparison sorts: they determine the sorted order of an input array by\\ncomparing elements. Chapter 8 begins by introducing the decision-tree\\nmodel in order to study the performance limitations of comparison\\nsorts. Using this model, we prove a lower bound of Ω(n lg n) on the\\nworst-case running time of any comparison sort on n inputs, thus\\nshowing that heapsort and merge sort are asymptotically optimal\\ncomparison sorts.\\nChapter 8 then goes on to show that we might be able to beat this\\nlower bound of Ω(n lg n) if an algorithm can gather information about\\nthe sorted order of the input by means other than comparing elements.\\nThe counting sort algorithm, for example, assumes that the input', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 225}),\n",
              " Document(page_content='numbers belong to the set {0, 1, … , k}. By using array indexing as a\\ntool for determining relative order, counting sort can sort n numbers in\\nΘ(k + n) time. Thus, when k = O(n), counting sort runs in time that is\\nlinear in the size of the input array. A related algorithm, radix sort, can\\nbe used to extend the range of counting sort. If there are n integers to\\nsort, each integer has d digits, and each digit can take on up to k\\npossible values, then radix sort can sort the numbers in Θ (d(n + k)) time.\\nWhen d is a constant and k is O(n), radix sort runs in linear time. A\\nthird algorithm, bucket sort, requires knowledge of the probabilistic\\ndistribution of numbers in the input array. It can sort n real numbers\\nuniformly distributed in the half-open interval [0, 1) in average-case\\nO(n) time.\\nThe table on the following page summarizes the running times of the\\nsorting algorithms from Chapters 2 and 6–8. As usual, n denotes the\\nnumber of items to sort. For counting sort, the items to sort are integers\\nin the set {0, 1, … , k}. For radix sort, each item is a d-digit number,\\nwhere each digit takes on k possible values. For bucket sort, we assume\\nthat the keys are real numbers uniformly distributed in the half-open\\ninterval [0, 1). The rightmost column gives the average-case or expected\\nrunning time, indicating which one it gives when it differs from the\\nworst-case running time. We omit the average-case running time of\\nheapsort because we do not analyze it in this book.\\nAlgorithmWorst-case\\nrunning timeAverage-case/expected\\nrunning time\\nInsertion\\nsortΘ(n2) Θ(n2)\\nMerge\\nsortΘ(n lg n) Θ(n lg n)\\nHeapsortO(n lg n) —\\nQuicksortΘ(n2)Θ(n lg n) (expected)\\nCounting\\nsortΘ(k + n) Θ(k + n)\\nRadix Θ(d(n + k)) Θ(d(n + k))', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 226}),\n",
              " Document(page_content='sort\\nBucket\\nsortΘ(n2)Θ(n) (average-case)\\nOrder statistics\\nThe ith order statistic of a set of n numbers is the ith smallest number in\\nthe set. You can, of course, select the ith order statistic by sorting the\\ninput and indexing the ith element of the output. With no assumptions\\nabout the input distribution, this method runs in Ω(n lg n) time, as the\\nlower bound proved in Chapter 8 shows.\\nChapter 9 shows how to ﬁnd the ith smallest element in O(n) time,\\neven when the elements are arbitrary real numbers. We present a\\nrandomized algorithm with tight pseudocode that runs in Θ (n2) time in\\nthe worst case, but whose expected running time is O(n). We also give a\\nmore complicated algorithm that runs in O(n) worst-case time.\\nBackground\\nAlthough most of this part does not rely on difﬁcult mathematics, some\\nsections do require mathematical sophistication. In particular, analyses\\nof quicksort, bucket sort, and the order-statistic algorithm use\\nprobability, which is reviewed in Appendix C, and the material on\\nprobabilistic analysis and randomized algorithms in Chapter 5.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 227}),\n",
              " Document(page_content='6\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0Heapsort\\nThis chapter introduces another sorting algorithm: heapsort. Like\\nmerge sort, but unlike insertion sort, heapsort’s running time is O(n lg\\nn). Like insertion sort, but unlike merge sort, heapsort sorts in place:\\nonly a constant number of array elements are stored outside the input\\narray at any time. Thus, heapsort combines the better attributes of the\\ntwo sorting algorithms we have already discussed.\\nHeapsort also introduces another algorithm design technique: using\\na data structure, in this case one we call a “heap,” to manage\\ninformation. Not only is the heap data structure useful for heapsort, but\\nit also makes an efﬁcient priority queue. The heap data structure will\\nreappear in algorithms in later chapters.\\nThe term “heap” was originally coined in the context of heapsort,\\nbut it has since come to refer to “garbage-collected storage,” such as the\\nprogramming languages Java and Python provide. Please don’t be\\nconfused. The heap data structure is not garbage-collected storage. This\\nbook is consistent in using the term “heap” to refer to the data\\nstructure, not the storage class.\\n6.1\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0Heaps\\nThe (binary) heap data structure is an array object that we can view as a\\nnearly complete binary tree (see Section B.5.3), as shown in Figure 6.1.\\nEach node of the tree corresponds to an element of the array. The tree is\\ncompletely ﬁlled on all levels except possibly the lowest, which is ﬁlled', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 228}),\n",
              " Document(page_content='from the left up to a point. An array A[1 : n] that represents a heap is an\\nobject with an attribute A.heap-size, which represents how many\\nelements in the heap are stored within array A. That is, although A[1 : n]\\nmay contain numbers, only the elements in A[1 : A.heap-size], where 0 ≤\\nA.heap-size ≤ n, are valid elements of the heap. If A.heap-size = 0, then\\nthe heap is empty. The root of the tree is A[1], and given the index i of a\\nnode, there’s a simple way to compute the indices of its parent, left\\nchild, and right child with the one-line procedures PARENT, LEFT,\\nand RIGHT.\\nFigure 6.1 A max-heap viewed as (a) a binary tree and (b) an array. The number within the circle\\nat each node in the tree is the value stored at that node. The number above a node is the\\ncorresponding index in the array. Above and below the array are lines showing parent-child\\nrelationships, with parents always to the left of their children. The tree has height 3, and the\\nnode at index 4 (with value 8) has height 1.\\nPARENT(i)\\n1return ⌊i/2 ⌋\\nLEFT(i)\\n1return 2i\\nRIGHT(i)\\n1return 2i + 1\\nOn most computers, the LEFT procedure can compute 2i in one\\ninstruction by simply shifting the binary representation of i left by one', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 229}),\n",
              " Document(page_content='bit position. Similarly, the RIGHT procedure can quickly compute 2i +\\n1 by shifting the binary representation of i left by one bit position and\\nthen adding 1. The PARENT procedure can compute ⌊i/2 ⌋ by shifting i\\nright one bit position. Good implementations of heapsort often\\nimplement these procedures as macros or inline procedures.\\nThere are two kinds of binary heaps: max-heaps and min-heaps. In\\nboth kinds, the values in the nodes satisfy a heap property, the speciﬁcs\\nof which depend on the kind of heap. In a max-heap, the max-heap\\nproperty is that for every node i other than the root,\\nA[PARENT(i)] ≥ A[i],\\nthat is, the value of a node is at most the value of its parent. Thus, the\\nlargest element in a max-heap is stored at the root, and the subtree\\nrooted at a node contains values no larger than that contained at the\\nnode itself. A min-heap is organized in the opposite way: the min-heap\\nproperty is that for every node i other than the root,\\nA[PARENT(i)] ≤ A[i].\\nThe smallest element in a min-heap is at the root.\\nThe heapsort algorithm uses max-heaps. Min-heaps commonly\\nimplement priority queues, which we discuss in Section 6.5. We’ll be\\nprecise in specifying whether we need a max-heap or a min-heap for any\\nparticular application, and when properties apply to either max-heaps\\nor min-heaps, we just use the term “heap.”\\nViewing a heap as a tree, we deﬁne the height of a node in a heap to\\nbe the number of edges on the longest simple downward path from the\\nnode to a leaf, and we deﬁne the height of the heap to be the height of\\nits root. Since a heap of n elements is based on a complete binary tree,\\nits height is Θ (lg n) (see Exercise 6.1-2). As we’ll see, the basic\\noperations on heaps run in time at most proportional to the height of\\nthe tree and thus take O(lg n) time. The remainder of this chapter\\npresents some basic procedures and shows how they are used in a\\nsorting algorithm and a priority-queue data structure.\\nThe MAX-HEAPIFY procedure, which runs in O(lg n) time, is\\nthe key to maintaining the max-heap property.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 230}),\n",
              " Document(page_content='The BUILD-MAX-HEAP procedure, which runs in linear time,\\nproduces a max-heap from an unordered input array.\\nThe HEAPSORT procedure, which runs in O(n lg n) time, sorts\\nan array in place.\\nThe procedures MAX-HEAP-INSERT, MAX-HEAP-\\nEXTRACT-MAX, MAX-HEAP-INCREASE-KEY, and MAX-\\nHEAP-MAXIMUM allow the heap data structure to implement\\na priority queue. They run in O(lg n) time plus the time for\\nmapping between objects being inserted into the priority queue\\nand indices in the heap.\\nExercises\\n6.1-1\\nWhat are the minimum and maximum numbers of elements in a heap of\\nheight h?\\n6.1-2\\nShow that an n-element heap has height ⌊lg n ⌋.\\n6.1-3\\nShow that in any subtree of a max-heap, the root of the subtree contains\\nthe largest value occurring anywhere in that subtree.\\n6.1-4\\nWhere in a max-heap might the smallest element reside, assuming that\\nall elements are distinct?\\n6.1-5\\nAt which levels in a max-heap might the kth largest element reside, for 2\\n≤ k ≤ ⌊n/2 ⌋, assuming that all elements are distinct?\\n6.1-6\\nIs an array that is in sorted order a min-heap?\\n6.1-7\\nIs the array with values 〈33, 19, 20, 15, 13, 10, 2, 13, 16, 12 〉 a max-heap?', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 231}),\n",
              " Document(page_content='6.1-8\\nShow that, with the array representation for storing an n-element heap,\\nthe leaves are the nodes indexed by ⌊n/2 ⌋ + 1, ⌊n/2 ⌋ + 2, … , n.\\n6.2\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0Maintaining the heap property\\nThe procedure MAX-HEAPIFY on the facing page maintains the max-\\nheap property. Its inputs are an array A with the heap-size attribute and\\nan index i into the array. When it is called, MAX-HEAPIFY assumes\\nthat the binary trees rooted at LEFT(i) and RIGHT(i) are max-heaps,\\nbut that A[i] might be smaller than its children, thus violating the max-\\nheap property. MAX-HEAPIFY lets the value at A[i] “ﬂoat down” in\\nthe max-heap so that the subtree rooted at index i obeys the max-heap\\nproperty.\\nFigure 6.2 illustrates the action of MAX-HEAPIFY. Each step\\ndetermines the largest of the elements A[i], A[LEFT(i)], and\\nA[RIGHT(i)] and stores the index of the largest element in largest. If\\nA[i] is largest, then the subtree rooted at node i is already a max-heap\\nand nothing else needs to be done. Otherwise, one of the two children\\ncontains the largest element. Positions i and largest swap their contents,\\nwhich causes node i and its children to satisfy the max-heap property.\\nThe node indexed by largest, however, just had its value decreased, and\\nthus the subtree rooted at largest might violate the max-heap property.\\nConsequently, MAX-HEAPIFY calls itself recursively on that subtree.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 232}),\n",
              " Document(page_content='Figure 6.2 The action of MAX-HEAPIFY(A, 2), where A.heap-size = 10. The node that\\npotentially violates the max-heap property is shown in blue. (a) The initial conﬁguration, with\\nA[2] at node i = 2 violating the max-heap property since it is not larger than both children. The\\nmax-heap property is restored for node 2 in (b) by exchanging A[2] with A[4], which destroys the\\nmax-heap property for node 4. The recursive call MAX-HEAPIFY(A, 4) now has i = 4. After\\nA[4] and A[9] are swapped, as shown in (c), node 4 is ﬁxed up, and the recursive call MAX-\\nHEAPIFY(A, 9) yields no further change to the data structure.\\nMAX-HEAPIFY(A, i)\\n\\xa0\\xa01l = LEFT(i)\\n\\xa0\\xa02r = RIGHT(i)\\n\\xa0\\xa03if\\xa0l ≤ A.heap-size and A[l] > A[i]\\n\\xa0\\xa04largest = l\\n\\xa0\\xa05else\\xa0largest = i\\n\\xa0\\xa06if\\xa0r ≤ A.heap-size and A[r] > A[largest]\\n\\xa0\\xa07largest = r', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 233}),\n",
              " Document(page_content='\\xa0\\xa08if\\xa0largest ≠ i\\n\\xa0\\xa09exchange A[i] with A[largest]\\n10MAX-HEAPIFY(A, largest)\\nTo analyze MAX-HEAPIFY, let T (n) be the worst-case running\\ntime that the procedure takes on a subtree of size at most n. For a tree\\nrooted at a given node i, the running time is the Θ (1) time to ﬁx up the\\nrelationships among the elements A[i], A[LEFT(i)], and A[RIGHT(i)],\\nplus the time to run MAX-HEAPIFY on a subtree rooted at one of the\\nchildren of node i (assuming that the recursive call occurs). The\\nchildren’s subtrees each have size at most 2n/3 (see Exercise 6.2-2), and\\ntherefore we can describe the running time of MAX-HEAPIFY by the\\nrecurrence\\nThe solution to this recurrence, by case 2 of the master theorem\\n(Theorem 4.1 on page 102), is T (n) = O(lg n). Alternatively, we can\\ncharacterize the running time of MAX-HEAPIFY on a node of height\\nh as O(h).\\nExercises\\n6.2-1\\nUsing Figure 6.2 as a model, illustrate the operation of MAX-\\nHEAPIFY(A, 3) on the array A = 〈27, 17, 3, 16, 13, 10, 1, 5, 7, 12, 4, 8,\\n9, 0〉.\\n6.2-2\\nShow that each child of the root of an n-node heap is the root of a\\nsubtree containing at most 2n/3 nodes. What is the smallest constant α\\nsuch that each subtree has at most α n nodes? How does that affect the\\nrecurrence (6.1) and its solution?\\n6.2-3\\nStarting with the procedure MAX-HEAPIFY, write pseudocode for the\\nprocedure MIN-HEAPIFY(A, i), which performs the corresponding', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 234}),\n",
              " Document(page_content='manipulation on a min-heap. How does the running time of MIN-\\nHEAPIFY compare with that of MAX-HEAPIFY?\\n6.2-4\\nWhat is the effect of calling MAX-HEAPIFY(A, i) when the element\\nA[i] is larger than its children?\\n6.2-5\\nWhat is the effect of calling MAX-HEAPIFY(A, i) for i > A.heap-\\nsize/2?\\n6.2-6\\nThe code for MAX-HEAPIFY is quite efﬁcient in terms of constant\\nfactors, except possibly for the recursive call in line 10, for which some\\ncompilers might produce inefﬁcient code. Write an efﬁcient MAX-\\nHEAPIFY that uses an iterative control construct (a loop) instead of\\nrecursion.\\n6.2-7\\nShow that the worst-case running time of MAX-HEAPIFY on a heap\\nof size n is Ω(lg n). (Hint: For a heap with n nodes, give node values that\\ncause MAX-HEAPIFY to be called recursively at every node on a\\nsimple path from the root down to a leaf.)\\n6.3\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0Building a heap\\nThe procedure BUILD-MAX-HEAP converts an array A[1 : n] into a\\nmax-heap by calling MAX-HEAPIFY in a bottom-up manner. Exercise\\n6.1-8 says that the elements in the subarray A[ ⌊n/2 ⌋ + 1 : n] are all leaves\\nof the tree, and so each is a 1-element heap to begin with. BUILD-\\nMAX-HEAP goes through the remaining nodes of the tree and runs\\nMAX-HEAPIFY on each one. Figure 6.3 shows an example of the\\naction of BUILD-MAX-HEAP.\\nBUILD-MAX-HEAP(A, n)\\n1A.heap-size = n', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 235}),\n",
              " Document(page_content='2for\\xa0i = ⌊n/2 ⌋ downto 1\\n3 MAX-HEAPIFY(A, i)\\nTo show why BUILD-MAX-HEAP works correctly, we use the\\nfollowing loop invariant:\\nAt the start of each iteration of the for loop of lines 2–3, each\\nnode i + 1, i + 2, … , n is the root of a max-heap.\\nWe need to show that this invariant is true prior to the ﬁrst loop\\niteration, that each iteration of the loop maintains the invariant, that\\nthe loop terminates, and that the invariant provides a useful property to\\nshow correctness when the loop terminates.\\nInitialization: Prior to the ﬁrst iteration of the loop, i = ⌊n/2 ⌋. Each node\\n⌊n/2 ⌋ + 1, ⌊n/2 ⌋ + 2, … , n is a leaf and is thus the root of a trivial max-\\nheap.\\nMaintenance: To see that each iteration maintains the loop invariant,\\nobserve that the children of node i are numbered higher than i. By the\\nloop invariant, therefore, they are both roots of max-heaps. This is\\nprecisely the condition required for the call MAX-HEAPIFY(A, i) to\\nmake node i a max-heap root. Moreover, the MAX-HEAPIFY call\\npreserves the property that nodes i + 1, i + 2, … , n are all roots of\\nmax-heaps. Decrementing i in the for loop update reestablishes the\\nloop invariant for the next iteration.\\n', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 236}),\n",
              " Document(page_content='Figure 6.3 The operation of BUILD-MAX-HEAP, showing the data structure before the call to\\nMAX-HEAPIFY in line 3 of BUILD-MAX-HEAP. The node indexed by i in each iteration is\\nshown in blue. (a) A 10-element input array A and the binary tree it represents. The loop index i\\nrefers to node 5 before the call MAX-HEAPIFY(A, i). (b) The data structure that results. The\\nloop index i for the next iteration refers to node 4. (c)–(e) Subsequent iterations of the for loop\\nin BUILD-MAX-HEAP. Observe that whenever MAX-HEAPIFY is called on a node, the two\\nsubtrees of that node are both max-heaps. (f) The max-heap after BUILD-MAX-HEAP\\nﬁnishes.\\nTermination: The loop makes exactly ⌊n/2 ⌋ iterations, and so it\\nterminates. At termination, i = 0. By the loop invariant, each node 1,\\n2, … , n is the root of a max-heap. In particular, node 1 is.\\nWe can compute a simple upper bound on the running time of\\nBUILD-MAX-HEAP as follows. Each call to MAX-HEAPIFY costs\\nO(lg n) time, and BUILD-MAX-HEAP makes O(n) such calls. Thus,\\nthe running time is O(n lg n). This upper bound, though correct, is not\\nas tight as it can be.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 237}),\n",
              " Document(page_content='We can derive a tighter asymptotic bound by observing that the time\\nfor MAX-HEAPIFY to run at a node varies with the height of the node\\nin the tree, and that the heights of most nodes are small. Our tighter\\nanalysis relies on the properties that an n-element heap has height ⌊lg n ⌋\\n(see Exercise 6.1-2) and at most ⌈n/2h + 1⌉ nodes of any height h (see\\nExercise 6.3-4).\\nThe time required by MAX-HEAPIFY when called on a node of\\nheight h is O(h). Letting c be the constant implicit in the asymptotic\\nnotation, we can express the total cost of BUILD-MAX-HEAP as\\nbeing bounded from above by \\n . As Exercise 6.3-2 shows,\\nwe have ⌈n/2h + 1⌉ ≥ 1/2 for 0 ≤ h ≤ ⌊lg n ⌋. Since ⌈x ⌉ ≤ 2x for any x ≥ 1/2,\\nwe have ⌈n/2h + 1⌉ ≤ n/2h. We thus obtain\\nHence, we can build a max-heap from an unordered array in linear time.\\nTo build a min-heap, use the procedure BUILD-MIN-HEAP, which\\nis the same as BUILD-MAX-HEAP but with the call to MAX-\\nHEAPIFY in line 3 replaced by a call to MIN-HEAPIFY (see Exercise\\n6.2-3). BUILD-MIN-HEAP produces a min-heap from an unordered\\nlinear array in linear time.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 238}),\n",
              " Document(page_content='Exercises\\n6.3-1\\nUsing Figure 6.3 as a model, illustrate the operation of BUILD-MAX-\\nHEAP on the array A = 〈5, 3, 17, 10, 84, 19, 6, 22, 9〉.\\n6.3-2\\nShow that ⌈n/2h + 1⌉ ≥ 1/2 for 0 ≤ h ≤ ⌊lg n ⌋.\\n6.3-3\\nWhy does the loop index i in line 2 of BUILD-MAX-HEAP decrease\\nfrom ⌊n/2 ⌋ to 1 rather than increase from 1 to ⌊n/2 ⌋?\\n6.3-4\\nShow that there are at most ⌈n/2h + 1⌉ nodes of height h in any n-\\nelement heap.\\n6.4\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0The heapsort algorithm\\nThe heapsort algorithm, given by the procedure HEAPSORT, starts by\\ncalling the BUILD-MAX-HEAP procedure to build a max-heap on the\\ninput array A[1 : n]. Since the maximum element of the array is stored at\\nthe root A[1], HEAPSORT can place it into its correct ﬁnal position by\\nexchanging it with A[n]. If the procedure then discards node n from the\\nheap—and it can do so by simply decrementing A.heap-size—the\\nchildren of the root remain max-heaps, but the new root element might\\nviolate the max-heap property. To restore the max-heap property, the\\nprocedure just calls MAX-HEAPIFY(A, 1), which leaves a max-heap in\\nA[1 : n – 1]. The HEAPSORT procedure then repeats this process for\\nthe max-heap of size n – 1 down to a heap of size 2. (See Exercise 6.4-2\\nfor a precise loop invariant.)\\nHEAPSORT(A, n)\\n1BUILD-MAX-HEAP(A, n)\\n2for\\xa0i = n\\xa0downto 2', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 239}),\n",
              " Document(page_content='3 exchange A[1] with A[i]\\n4 A.heap-size = A.heap-size – 1\\n5 MAX-HEAPIFY(A, 1)\\nFigure 6.4 shows an example of the operation of HEAPSORT after\\nline 1 has built the initial max-heap. The ﬁgure shows the max-heap\\nbefore the ﬁrst iteration of the for loop of lines 2–5 and after each\\niteration.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 240}),\n",
              " Document(page_content='Figure 6.4 The operation of HEAPSORT. (a) The max-heap data structure just after BUILD-\\nMAX-HEAP has built it in line 1. (b)–(j) The max-heap just after each call of MAX-HEAPIFY\\nin line 5, showing the value of i at that time. Only blue nodes remain in the heap. Tan nodes\\ncontain the largest values in the array, in sorted order. (k) The resulting sorted array A.\\nThe HEAPSORT procedure takes O(n lg n) time, since the call to\\nBUILD-MAX-HEAP takes O(n) time and each of the n – 1 calls to\\nMAX-HEAPIFY takes O(lg n) time.\\nExercises\\n6.4-1', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 241}),\n",
              " Document(page_content='Using Figure 6.4 as a model, illustrate the operation of HEAPSORT on\\nthe array A = 〈5, 13, 2, 25, 7, 17, 20, 8, 4〉.\\n6.4-2\\nArgue the correctness of HEAPSORT using the following loop\\ninvariant:\\nAt the start of each iteration of the for loop of lines 2–5, the\\nsubarray A[1 : i] is a max-heap containing the i smallest\\nelements of A[1 : n], and the subarray A[i + 1 : n] contains the n\\n– i largest elements of A[1 : n], sorted.\\n6.4-3\\nWhat is the running time of HEAPSORT on an array A of length n that\\nis already sorted in increasing order? How about if the array is already\\nsorted in decreasing order?\\n6.4-4\\nShow that the worst-case running time of HEAPSORT is Ω(n lg n).\\n★ 6.4-5\\nShow that when all the elements of A are distinct, the best-case running\\ntime of HEAPSORT is Ω(n lg n).\\n6.5\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0Priority queues\\nIn Chapter 8, we will see that any comparison-based sorting algorithm\\nrequires Ω(n lg n) comparisons and hence Ω(n lg n) time. Therefore,\\nheapsort is asymptotically optimal among comparison-based sorting\\nalgorithms. Yet, a good implementation of quicksort, presented in\\nChapter 7, usually beats it in practice. Nevertheless, the heap data\\nstructure itself has many uses. In this section, we present one of the\\nmost popular applications of a heap: as an efﬁcient priority queue. As\\nwith heaps, priority queues come in two forms: max-priority queues and\\nmin-priority queues. We’ll focus here on how to implement max-priority', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 242}),\n",
              " Document(page_content='queues, which are in turn based on max-heaps. Exercise 6.5-3 asks you\\nto write the procedures for min-priority queues.\\nA priority queue is a data structure for maintaining a set S of\\nelements, each with an associated value called a key. A max-priority\\nqueue supports the following operations:\\nINSERT(S, x, k) inserts the element x with key k into the set S, which is\\nequivalent to the operation S = S ⋃ {x}.\\nMAXIMUM(S) returns the element of S with the largest key.\\nEXTRACT-MAX(S) removes and returns the element of S with the\\nlargest key.\\nINCREASE-KEY(S, x, k) increases the value of element x’s key to the\\nnew value k, which is assumed to be at least as large as x’s current key\\nvalue.\\nAmong their other applications, you can use max-priority queues to\\nschedule jobs on a computer shared among multiple users. The max-\\npriority queue keeps track of the jobs to be performed and their relative\\npriorities. When a job is ﬁnished or interrupted, the scheduler selects the\\nhighest-priority job from among those pending by calling EXTRACT-\\nMAX. The scheduler can add a new job to the queue at any time by\\ncalling INSERT.\\nAlternatively, a min-priority queue supports the operations INSERT,\\nMINIMUM, EXTRACT-MIN, and DECREASE-KEY. A min-\\npriority queue can be used in an event-driven simulator. The items in\\nthe queue are events to be simulated, each with an associated time of\\noccurrence that serves as its key. The events must be simulated in order\\nof their time of occurrence, because the simulation of an event can cause\\nother events to be simulated in the future. The simulation program calls\\nEXTRACT-MIN at each step to choose the next event to simulate. As\\nnew events are produced, the simulator inserts them into the min-\\npriority queue by calling INSERT. We’ll see other uses for min-priority\\nqueues, highlighting the DECREASE-KEY operation, in Chapters 21\\nand 22.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 243}),\n",
              " Document(page_content='When you use a heap to implement a priority queue within a given\\napplication, elements of the priority queue correspond to objects in the\\napplication. Each object contains a key. If the priority queue is\\nimplemented by a heap, you need to determine which application object\\ncorresponds to a given heap element, and vice versa. Because the heap\\nelements are stored in an array, you need a way to map application\\nobjects to and from array indices.\\nOne way to map between application objects and heap elements uses\\nhandles, which are additional information stored in the objects and heap\\nelements that give enough information to perform the mapping.\\nHandles are often implemented to be opaque to the surrounding code,\\nthereby maintaining an abstraction barrier between the application and\\nthe priority queue. For example, the handle within an application object\\nmight contain the corresponding index into the heap array. But since\\nonly the code for the priority queue accesses this index, the index is\\nentirely hidden from the application code. Because heap elements\\nchange locations within the array during heap operations, an actual\\nimplementation of the priority queue, upon relocating a heap element,\\nmust also update the array indices in the corresponding handles.\\nConversely, each element in the heap might contain a pointer to the\\ncorresponding application object, but the heap element knows this\\npointer as only an opaque handle and the application maps this handle\\nto an application object. Typically, the worst-case overhead for\\nmaintaining handles is O(1) per access.\\nAs an alternative to incorporating handles in application objects, you\\ncan store within the priority queue a mapping from application objects\\nto array indices in the heap. The advantage of doing so is that the\\nmapping is contained entirely within the priority queue, so that the\\napplication objects need no further embellishment. The disadvantage\\nlies in the additional cost of establishing and maintaining the mapping.\\nOne option for the mapping is a hash table (see Chapter 11).1 The\\nadded expected time for a hash table to map an object to an array index\\nis just O(1), though the worst-case time can be as bad as Θ (n).\\nLet’s see how to implement the operations of a max-priority queue\\nusing a max-heap. In the previous sections, we treated the array', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 244}),\n",
              " Document(page_content='elements as the keys to be sorted, implicitly assuming that any satellite\\ndata moved with the corresponding keys. When a heap implements a\\npriority queue, we instead treat each array element as a pointer to an\\nobject in the priority queue, so that the object is analogous to the\\nsatellite data when sorting. We further assume that each such object has\\nan attribute key, which determines where in the heap the object belongs.\\nFor a heap implemented by an array A, we refer to A[i].key.\\nThe procedure MAX-HEAP-MAXIMUM on the facing page\\nimplements the MAXIMUM operation in Θ (1) time, and MAX-HEAP-\\nEXTRACT-MAX implements the operation EXTRACT-MAX. MAX-\\nHEAP-EXTRACT-MAX is similar to the for loop body (lines 3–5) of\\nthe HEAPSORT procedure. We implicitly assume that MAX-\\nHEAPIFY compares priority-queue objects based on their key\\nattributes. We also assume that when MAX-HEAPIFY exchanges\\nelements in the array, it is exchanging pointers and also that it updates\\nthe mapping between objects and array indices. The running time of\\nMAX-HEAP-EXTRACT-MAX is O(lg n), since it performs only a\\nconstant amount of work on top of the O(lg n) time for MAX-\\nHEAPIFY, plus whatever overhead is incurred within MAX-\\nHEAPIFY for mapping priority-queue objects to array indices.\\nThe procedure MAX-HEAP-INCREASE-KEY on page 176\\nimplements the INCREASE-KEY operation. It ﬁrst veriﬁes that the\\nnew key k will not cause the key in the object x to decrease, and if there\\nis no problem, it gives x the new key value. The procedure then ﬁnds the\\nindex i in the array corresponding to object x, so that A[i] is x. Because\\nincreasing the key of A[i] might violate the max-heap property, the\\nprocedure then, in a manner reminiscent of the insertion loop (lines 5–\\n7) of INSERTION-SORT on page 19, traverses a simple path from this\\nnode toward the root to ﬁnd a proper place for the newly increased key.\\nAs MAX-HEAP-INCREASE-KEY traverses this path, it repeatedly\\ncompares an element’s key to that of its parent, exchanging pointers and\\ncontinuing if the element’s key is larger, and terminating if the element’s\\nkey is smaller, since the max-heap property now holds. (See Exercise\\n6.5-7 for a precise loop invariant.) Like MAX-HEAPIFY when used in\\na priority queue, MAX-HEAP-INCREASE-KEY updates the\\ninformation that maps objects to array indices when array elements are', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 245}),\n",
              " Document(page_content='exchanged. Figure 6.5 shows an example of a MAX-HEAP-\\nINCREASE-KEY operation. In addition to the overhead for mapping\\npriority queue objects to array indices, the running time of MAX-\\nHEAP-INCREASE-KEY on an n-element heap is O(lg n), since the\\npath traced from the node updated in line 3 to the root has length O(lg\\nn).\\nMAX-HEAP-MAXIMUM(A)\\n1if\\xa0A.heap-size < 1\\n2 error “heap underﬂow”\\n3return\\xa0A[1]\\nMAX-HEAP-EXTRACT-MAX(A)\\n1max = MAX-HEAP-MAXIMUM(A)\\n2A[1] = A[A.heap-size]\\n3A.heap-size = A.heap-size – 1\\n4MAX-HEAPIFY(A, 1)\\n5return\\xa0max\\nThe procedure MAX-HEAP-INSERT on the next page implements\\nthe INSERT operation. It takes as inputs the array A implementing the\\nmax-heap, the new object x to be inserted into the max-heap, and the\\nsize n of array A. The procedure ﬁrst veriﬁes that the array has room for\\nthe new element. It then expands the max-heap by adding to the tree a\\nnew leaf whose key is –∞. Then it calls MAX-HEAP-INCREASE-KEY\\nto set the key of this new element to its correct value and maintain the\\nmax-heap property. The running time of MAX-HEAP-INSERT on an\\nn-element heap is O(lg n) plus the overhead for mapping priority queue\\nobjects to indices.\\nIn summary, a heap can support any priority-queue operation on a\\nset of size n in O(lg n) time, plus the overhead for mapping priority\\nqueue objects to array indices.\\nMAX-HEAP-INCREASE-KEY(A, x, k)\\n1if\\xa0k < x.key', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 246}),\n",
              " Document(page_content='2 error “new key is smaller than current key”\\n3x.key = k\\n4ﬁnd the index i in array A where object x occurs\\n5while\\xa0i > 1 and A[PARENT(i)].key < A[i].key\\n6 exchange A[i] with A[PARENT(i)], updating the information\\nthat maps priority queue objects to array indices\\n7 i = PARENT(i)\\nMAX-HEAP-INSERT(A, x, n)\\n1if\\xa0A.heap-size == n\\n2 error “heap overﬂow”\\n3A.heap-size = A.heap-size + 1\\n4k = x.key\\n5x.key = –∞\\n6A[A.heap-size] = x\\n7map x to index heap-size in the array\\n8MAX-HEAP-INCREASE-KEY(A, x, k)\\nExercises\\n6.5-1\\nSuppose that the objects in a max-priority queue are just keys. Illustrate\\nthe operation of MAX-HEAP-EXTRACT-MAX on the heap A = 〈15,\\n13, 9, 5, 12, 8, 7, 4, 0, 6, 2, 1〉.\\n6.5-2\\nSuppose that the objects in a max-priority queue are just keys. Illustrate\\nthe operation of MAX-HEAP-INSERT(A, 10) on the heap A = 〈15, 13,\\n9, 5, 12, 8, 7, 4, 0, 6, 2, 1〉.\\n6.5-3\\nWrite pseudocode to implement a min-priority queue with a min-heap\\nby writing the procedures MIN-HEAP-MINIMUM, MIN-HEAP-\\nEXTRACT-MIN, MIN-HEAP-DECREASE-KEY, and MIN-HEAP-\\nINSERT.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 247}),\n",
              " Document(page_content='6.5-4\\nWrite pseudocode for the procedure MAX-HEAP-DECREASE-\\nKEY(A, x, k) in a max-heap. What is the running time of your\\nprocedure?\\nFigure 6.5 The operation of MAX-HEAP-INCREASE-KEY. Only the key of each element in\\nthe priority queue is shown. The node indexed by i in each iteration is shown in blue. (a) The\\nmax-heap of Figure 6.4(a) with i indexing the node whose key is about to be increased. (b) This\\nnode has its key increased to 15. (c) After one iteration of the while loop of lines 5–7, the node\\nand its parent have exchanged keys, and the index i moves up to the parent. (d) The max-heap\\nafter one more iteration of the while loop. At this point, A[PARENT(i)] ≥ A[i]. The max-heap\\nproperty now holds and the procedure terminates.\\n6.5-5\\nWhy does MAX-HEAP-INSERT bother setting the key of the inserted\\nobject to –∞ in line 5 given that line 8 will set the object’s key to the\\ndesired value?', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 248}),\n",
              " Document(page_content='6.5-6\\nProfessor Uriah suggests replacing the while loop of lines 5–7 in MAX-\\nHEAP-INCREASE-KEY by a call to MAX-HEAPIFY. Explain the\\nﬂaw in the professor’s idea.\\n6.5-7\\nArgue the correctness of MAX-HEAP-INCREASE-KEY using the\\nfollowing loop invariant:\\nAt the start of each iteration of the while loop of lines 5–7:\\na. If both nodes PARENT(i) and LEFT(i) exist, then\\nA[PARENT(i)].key ≥ A[LEFT(i)].key.\\nb. If both nodes PARENT(i) and RIGHT(i) exist, then\\nA[PARENT(i)].key ≥ A[RIGHT(i)].key.\\nc. The subarray A[1 : A.heap-size] satisﬁes the max-heap property,\\nexcept that there may be one violation, which is that A[i].key may\\nbe greater than A[PARENT(i)].key.\\nYou may assume that the subarray A[1 : A.heap-size] satisﬁes the max-\\nheap property at the time MAX-HEAP-INCREASE-KEY is called.\\n6.5-8\\nEach exchange operation on line 6 of MAX-HEAP-INCREASE-KEY\\ntypically requires three assignments, not counting the updating of the\\nmapping from objects to array indices. Show how to use the idea of the\\ninner loop of INSERTION-SORT to reduce the three assignments to\\njust one assignment.\\n6.5-9\\nShow how to implement a ﬁrst-in, ﬁrst-out queue with a priority queue.\\nShow how to implement a stack with a priority queue. (Queues and\\nstacks are deﬁned in Section 10.1.3.)\\n6.5-10\\nThe operation MAX-HEAP-DELETE(A, x) deletes the object x from\\nmax-heap A. Give an implementation of MAX-HEAP-DELETE for an\\nn-element max-heap that runs in O(lg n) time plus the overhead for\\nmapping priority queue objects to array indices.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 249}),\n",
              " Document(page_content='6.5-11\\nGive an O(n lg k)-time algorithm to merge k sorted lists into one sorted\\nlist, where n is the total number of elements in all the input lists. (Hint:\\nUse a min-heap for k-way merging.)\\nProblems\\n6-1\\xa0\\xa0\\xa0\\xa0\\xa0B uilding a heap using insertion\\nOne way to build a heap is by repeatedly calling MAX-HEAP-INSERT\\nto insert the elements into the heap. Consider the procedure BUILD-\\nMAX-HEAP′ on the facing page. It assumes that the objects being\\ninserted are just the heap elements.\\nBUILD-MAX-HEAP′ (A, n)\\n1A.heap-size = 1\\n2for\\xa0i = 2 to\\xa0n\\n3 MAX-HEAP-INSERT(A, A[i], n)\\na. Do the procedures BUILD-MAX-HEAP and BUILD-MAX-HEAP′\\nalways create the same heap when run on the same input array? Prove\\nthat they do, or provide a counterexample.\\nb. Show that in the worst case, BUILD-MAX-HEAP′ requires Θ (n lg n)\\ntime to build an n-element heap.\\n6-2\\xa0\\xa0\\xa0\\xa0\\xa0A nalysis of d-ary heaps\\nA d-ary heap is like a binary heap, but (with one possible exception)\\nnonleaf nodes have d children instead of two children. In all parts of\\nthis problem, assume that the time to maintain the mapping between\\nobjects and heap elements is O(1) per operation.\\na. Describe how to represent a d-ary heap in an array.\\nb. Using Θ -notation, express the height of a d-ary heap of n elements in\\nterms of n and d.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 250}),\n",
              " Document(page_content='c. Give an efﬁcient implementation of EXTRACT-MAX in a d-ary\\nmax-heap. Analyze its running time in terms of d and n.\\nd. Give an efﬁcient implementation of INCREASE-KEY in a d-ary\\nmax-heap. Analyze its running time in terms of d and n.\\ne. Give an efﬁcient implementation of INSERT in a d-ary max-heap.\\nAnalyze its running time in terms of d and n.\\n6-3 Young tableaus\\nAn m × n\\xa0Young tableau is an m × n matrix such that the entries of each\\nrow are in sorted order from left to right and the entries of each column\\nare in sorted order from top to bottom. Some of the entries of a Young\\ntableau may be ∞, which we treat as nonexistent elements. Thus, a\\nYoung tableau can be used to hold r ≤ mn ﬁnite numbers.\\na. Draw a 4 × 4 Young tableau containing the elements {9, 16, 3, 2, 4, 8,\\n5, 14, 12}.\\nb. Argue that an m × n Young tableau Y is empty if Y [1, 1] = ∞. Argue\\nthat Y is full (contains mn elements) if Y [m, n] < ∞.\\nc. Give an algorithm to implement EXTRACT-MIN on a nonempty m\\n× n Young tableau that runs in O(m + n) time. Your algorithm should\\nuse a recursive subroutine that solves an m × n problem by recursively\\nsolving either an (m – 1) × n or an m × (n – 1) subproblem. (Hint:\\nThink about MAX-HEAPIFY.) Explain why your implementation of\\nEXTRACT-MIN runs in O(m + n) time.\\nd. Show how to insert a new element into a nonfull m × n Young tableau\\nin O(m + n) time.\\ne. Using no other sorting method as a subroutine, show how to use an n\\n× n Young tableau to sort n2 numbers in O(n3) time.\\nf. Give an O(m + n)-time algorithm to determine whether a given\\nnumber is stored in a given m × n Young tableau.\\nChapter notes', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 251}),\n",
              " Document(page_content='The heapsort algorithm was invented by Williams [456], who also\\ndescribed how to implement a priority queue with a heap. The BUILD-\\nMAX-HEAP procedure was suggested by Floyd [145]. Schaffer and\\nSedgewick [395] showed that in the best case, the number of times\\nelements move in the heap during heapsort is approximately (n/2) lg n\\nand that the average number of moves is approximately n lg n.\\nWe use min-heaps to implement min-priority queues in Chapters 15,\\n21, and 22. Other, more complicated, data structures give better time\\nbounds for certain min-priority queue operations. Fredman and Tarjan\\n[156] developed Fibonacci heaps, which support INSERT and\\nDECREASE-KEY in O(1) amortized time (see Chapter 16). That is,\\nthe average worst-case running time for these operations is O(1). Brodal,\\nLagogiannis, and Tarjan [73] subsequently devised strict Fibonacci\\nheaps, which make these time bounds the actual running times. If the\\nkeys are unique and drawn from the set {0, 1, … , n – 1} of nonnegative\\nintegers, van Emde Boas trees [440, 441] support the operations\\nINSERT, DELETE, SEARCH, MINIMUM, MAXIMUM,\\nPREDECESSOR, and SUCCESSOR in O(lg lg n) time.\\nIf the data are b-bit integers, and the computer memory consists of\\naddressable b-bit words, Fredman and Willard [157] showed how to\\nimplement MINIMUM in O(1) time and INSERT and EXTRACT-\\nMIN in \\n  time. Thorup [436] has improved the \\n  bound to\\nO(lg lg n) time by using randomized hashing, requiring only linear\\nspace.\\nAn important special case of priority queues occurs when the\\nsequence of EXTRACT-MIN operations is monotone, that is, the values\\nreturned by successive EXTRACT-MIN operations are monotonically\\nincreasing over time. This case arises in several important applications,\\nsuch as Dijkstra’s single-source shortest-paths algorithm, which we\\ndiscuss in Chapter 22, and in discrete-event simulation. For Dijkstra’s\\nalgorithm it is particularly important that the DECREASE-KEY\\noperation be implemented efﬁciently. For the monotone case, if the data\\nare integers in the range 1, 2, … , C, Ahuja, Mehlhorn, Orlin, and\\nTarjan [8] describe how to implement EXTRACT-MIN and INSERT in\\nO(lg C) amortized time (Chapter 16 presents amortized analysis) and', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 252}),\n",
              " Document(page_content='DECREASE-KEY in O(1) time, using a data structure called a radix\\nheap. The O(lg C) bound can be improved to \\n  using Fibonacci\\nheaps in conjunction with radix heaps. Cherkassky, Goldberg, and\\nSilverstein [90] further improved the bound to O(lg1/3+ ϵ\\xa0C) expected\\ntime by combining the multilevel bucketing structure of Denardo and\\nFox [112] with the heap of Thorup mentioned earlier. Raman [375]\\nfurther improved these results to obtain a bound of O(min {lg1/4+ ϵ\\xa0C,\\nlg1/3+ ϵ\\xa0n}), for any ﬁxed ϵ > 0.\\nMany other variants of heaps have been proposed. Brodal [72]\\nsurveys some of these developments.\\n1 In Python, dictionaries are implemented with hash tables.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 253}),\n",
              " Document(page_content='7\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0Quicksort\\nThe quicksort algorithm has a worst-case running time of Θ (n2) on an\\ninput array of n numbers. Despite this slow worst-case running time,\\nquicksort is often the best practical choice for sorting because it is\\nremarkably efﬁcient on average: its expected running time is Θ (n lg n)\\nwhen all numbers are distinct, and the constant factors hidden in the\\nΘ(n lg n) notation are small. Unlike merge sort, it also has the\\nadvantage of sorting in place (see page 158), and it works well even in\\nvirtual-memory environments.\\nOur study of quicksort is broken into four sections. Section 7.1\\ndescribes the algorithm and an important subroutine used by quicksort\\nfor partitioning. Because the behavior of quicksort is complex, we’ll\\nstart with an intuitive discussion of its performance in Section 7.2 and\\nanalyze it precisely at the end of the chapter. Section 7.3 presents a\\nrandomized version of quicksort. When all elements are distinct,1 this\\nrandomized algorithm has a good expected running time and no\\nparticular input elicits its worst-case behavior. (See Problem 7-2 for the\\ncase in which elements may be equal.) Section 7.4 analyzes the\\nrandomized algorithm, showing that it runs in Θ (n2) time in the worst\\ncase and, assuming distinct elements, in expected O(n lg n) time.\\n7.1\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0Description of quicksort', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 254}),\n",
              " Document(page_content='Quicksort, like merge sort, applies the divide-and-conquer method\\nintroduced in Section 2.3.1. Here is the three-step divide-and-conquer\\nprocess for sorting a subarray A[p : r]:\\nDivide by partitioning (rearranging) the array A[p : r] into two (possibly\\nempty) subarrays A[p : q – 1] (the low side) and A[q + 1 : r] (the high\\nside) such that each element in the low side of the partition is less than\\nor equal to the pivot\\xa0A[q], which is, in turn, less than or equal to each\\nelement in the high side. Compute the index q of the pivot as part of\\nthis partitioning procedure.\\nConquer by calling quicksort recursively to sort each of the subarrays\\nA[p : q – 1] and A[q + 1 : r].\\nCombine by doing nothing: because the two subarrays are already\\nsorted, no work is needed to combine them. All elements in A[p : q –\\n1] are sorted and less than or equal to A[q], and all elements in A[q + 1\\n: r] are sorted and greater than or equal to the pivot A[q]. The entire\\nsubarray A[p : r] cannot help but be sorted!\\nThe QUICKSORT procedure implements quicksort. To sort an\\nentire n-element array A[1 : n], the initial call is QUICKSORT (A, 1, n).\\nQUICKSORT(A, p, r)\\n1if\\xa0p < r\\n2 // Partition the subarray around the pivot, which ends up in A[q].\\n3 q = PARTITION(A, p, r)\\n4 QUICKSORT(A, p, q – 1) // recursively sort the low side\\n5 QUICKSORT(A, q + 1, r) // recursively sort the high side\\nPartitioning the array\\nThe key to the algorithm is the PARTITION procedure on the next\\npage, which rearranges the subarray A[p : r] in place, returning the index\\nof the dividing point between the two sides of the partition.\\nFigure 7.1 shows how PARTITION works on an 8-element array.\\nPARTITION always selects the element x = A[r] as the pivot. As the', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 255}),\n",
              " Document(page_content='procedure runs, each element falls into exactly one of four regions, some\\nof which may be empty. At the start of each iteration of the for loop in\\nlines 3–6, the regions satisfy certain properties, shown in Figure 7.2. We\\nstate these properties as a loop invariant:\\nPARTITION(A, p, r)\\n1x = A[r] // the pivot\\n2i = p – 1 // highest index into the low side\\n3for\\xa0j = p\\xa0to\\xa0r – 1 // process each element other than the\\npivot\\n4 if\\xa0A[j] ≤ x // does this element belong on the low\\nside?\\n5 i = i + 1 // index of a new slot in the low side\\n6 exchange A[i] with\\nA[j]// put this element there\\n7exchange A[i + 1] with\\nA[r]// pivot goes just to the right of the low\\nside\\n8return\\xa0i + 1 // new index of the pivot\\nAt the beginning of each iteration of the loop of lines 3–6, for\\nany array index k, the following conditions hold:\\n1. if p ≤ k ≤ i, then A[k] ≤ x (the tan region of Figure 7.2);\\n2. if i + 1 ≤ k ≤ j – 1, then A[k] > x (the blue region);\\n3. if k = r, then A[k] = x (the yellow region).\\nWe need to show that this loop invariant is true prior to the ﬁrst\\niteration, that each iteration of the loop maintains the invariant, that\\nthe loop terminates, and that correctness follows from the invariant\\nwhen the loop terminates.\\nInitialization: Prior to the ﬁrst iteration of the loop, we have i = p – 1\\nand j = p. Because no values lie between p and i and no values lie\\nbetween i + 1 and j – 1, the ﬁrst two conditions of the loop invariant\\nare trivially satisﬁed. The assignment in line 1 satisﬁes the third\\ncondition.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 256}),\n",
              " Document(page_content='Maintenance: As Figure 7.3 shows, we consider two cases, depending on\\nthe outcome of the test in line 4. Figure 7.3(a) shows what happens\\nwhen A[j] > x: the only action in the loop is to increment j. After j has\\nbeen incremented, the second condition holds for A[j – 1] and all\\nother entries remain unchanged. Figure 7.3(b) shows what happens\\nwhen A[j] ≤ x: the loop increments i, swaps A[i] and A[j], and then\\nincrements j. Because of the swap, we now have that A[i] ≤ x, and\\ncondition 1 is satisﬁed. Similarly, we also have that A[j – 1] > x, since\\nthe item that was swapped into A[j – 1] is, by the loop invariant,\\ngreater than x.\\nTermination: Since the loop makes exactly r – p iterations, it terminates,\\nwhereupon j = r. At that point, the unexamined subarray A[j : r – 1] is\\nempty, and every entry in the array belongs to one of the other three\\nsets described by the invariant. Thus, the values in the array have been\\npartitioned into three sets: those less than or equal to x (the low side),\\nthose greater than x (the high side), and a singleton set containing x\\n(the pivot).', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 257}),\n",
              " Document(page_content='Figure 7.1 The operation of PARTITION on a sample array. Array entry A[r] becomes the pivot\\nelement x. Tan array elements all belong to the low side of the partition, with values at most x.\\nBlue elements belong to the high side, with values greater than x. White elements have not yet\\nbeen put into either side of the partition, and the yellow element is the pivot x. (a) The initial\\narray and variable settings. None of the elements have been placed into either side of the\\npartition. (b) The value 2 is “swapped with itself” and put into the low side. (c)–(d) The values 8\\nand 7 are placed into to high side. (e) The values 1 and 8 are swapped, and the low side grows. (f)\\nThe values 3 and 7 are swapped, and the low side grows. (g)–(h) The high side of the partition\\ngrows to include 5 and 6, and the loop terminates. (i) Line 7 swaps the pivot element so that it\\nlies between the two sides of the partition, and line 8 returns the pivot’s new index.\\nThe ﬁnal two lines of PARTITION ﬁnish up by swapping the pivot\\nwith the leftmost element greater than x, thereby moving the pivot into\\nits correct place in the partitioned array, and then returning the pivot’s\\nnew index. The output of PARTITION now satisﬁes the speciﬁcations\\ngiven for the divide step. In fact, it satisﬁes a slightly stronger condition:', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 258}),\n",
              " Document(page_content='after line 3 of QUICKSORT, A[q] is strictly less than every element of\\nA[q + 1 : r].\\nFigure 7.2 The four regions maintained by the procedure PARTITION on a subarray A[p : r].\\nThe tan values in A[p : i] are all less than or equal to x, the blue values in A[i + 1 : j – 1] are all\\ngreater than x, the white values in A[j : r – 1] have unknown relationships to x, and A[r] = x.\\nFigure 7.3 The two cases for one iteration of procedure PARTITION. (a) If A[j] > x, the only\\naction is to increment j, which maintains the loop invariant. (b) If A[j] ≤ x, index i is\\nincremented, A[i] and A[j] are swapped, and then j is incremented. Again, the loop invariant is\\nmaintained.\\nExercise 7.1-3 asks you to show that the running time of\\nPARTITION on a subarray A[p : r] of n = r – p + 1 elements is Θ (n).\\nExercises\\n7.1-1', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 259}),\n",
              " Document(page_content='Using Figure 7.1 as a model, illustrate the operation of PARTITION on\\nthe array A = 〈13, 19, 9, 5, 12, 8, 7, 4, 21, 2, 6, 11 〉.\\n7.1-2\\nWhat value of q does PARTITION return when all elements in the\\nsubarray A[p : r] have the same value? Modify PARTITION so that q =\\n⌊(p + r)/2 ⌋ when all elements in the subarray A[p : r] have the same\\nvalue.\\n7.1-3\\nGive a brief argument that the running time of PARTITION on a\\nsubarray of size n is Θ (n).\\n7.1-4\\nModify QUICKSORT to sort into monotonically decreasing order.\\n7.2\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0Performance of quicksort\\nThe running time of quicksort depends on how balanced each\\npartitioning is, which in turn depends on which elements are used as\\npivots. If the two sides of a partition are about the same size—the\\npartitioning is balanced—then the algorithm runs asymptotically as fast\\nas merge sort. If the partitioning is unbalanced, however, it can run\\nasymptotically as slowly as insertion sort. To allow you to gain some\\nintuition before diving into a formal analysis, this section informally\\ninvestigates how quicksort performs under the assumptions of balanced\\nversus unbalanced partitioning.\\nBut ﬁrst, let’s brieﬂy look at the maximum amount of memory that\\nquicksort requires. Although quicksort sorts in place according to the\\ndeﬁnition on page 158, the amount of memory it uses—as ide from the\\narray being sorted—is not constant. Since each recursive call requires a\\nconstant amount of space on the runtime stack, outside of the array\\nbeing sorted, quicksort requires space proportional to the maximum\\ndepth of the recursion. As we’ll see now, that could be as bad as Θ (n) in\\nthe worst case.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 260}),\n",
              " Document(page_content='Worst-case partitioning\\nThe worst-case behavior for quicksort occurs when the partitioning\\nproduces one subproblem with n – 1 elements and one with 0 elements.\\n(See Section 7.4.1.) Let us assume that this unbalanced partitioning\\narises in each recursive call. The partitioning costs Θ (n) time. Since the\\nrecursive call on an array of size 0 just returns without doing anything,\\nT (0) = Θ (1), and the recurrence for the running time is\\nT (n)=T (n – 1) + T (0) + Θ (n)\\n=T (n – 1) + Θ (n).\\nBy summing the costs incurred at each level of the recursion, we obtain\\nan arithmetic series (equation (A.3) on page 1141) , which evaluates to\\nΘ(n2). Indeed, the substitution method can be used to prove that the\\nrecurrence T (n) = T (n – 1) + Θ (n) has the solution T (n) = Θ (n2). (See\\nExercise 7.2-1.)\\nThus, if the partitioning is maximally unbalanced at every recursive\\nlevel of the algorithm, the running time is Θ (n2). The worst-case\\nrunning time of quicksort is therefore no better than that of insertion\\nsort. Moreover, the Θ (n2) running time occurs when the input array is\\nalready completely sorted—a situation in which insertion sort runs in\\nO(n) time.\\nBest-case partitioning\\nIn the most even possible split, PARTITION produces two\\nsubproblems, each of size no more than n/2, since one is of size ⌊(n –\\n1)/2 ⌋ ≤ n/2 and one of size ⌈(n – 1)/2 ⌉ – 1 ≤ n/2. In this case, quicksort\\nruns much faster. An upper bound on the running time can then be\\ndescribed by the recurrence\\nT (n) = 2T (n/2) + Θ (n).\\nBy case 2 of the master theorem (Theorem 4.1 on page 102), this\\nrecurrence has the solution T (n) = Θ (n lg n). Thus, if the partitioning is', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 261}),\n",
              " Document(page_content='equally balanced at every level of the recursion, an asymptotically faster\\nalgorithm results.\\nBalanced partitioning\\nAs the analyses in Section 7.4 will show, the average-case running time\\nof quicksort is much closer to the best case than to the worst case. By\\nappreciating how the balance of the partitioning affects the recurrence\\ndescribing the running time, we can gain an understanding of why.\\nSuppose, for example, that the partitioning algorithm always\\nproduces a 9-to-1 proportional split, which at ﬁrst blush seems quite\\nunbalanced. We then obtain the recurrence\\nT (n) = T (9n/10) + T (n/10) + Θ (n),\\non the running time of quicksort. Figure 7.4 shows the recursion tree\\nfor this recurrence, where for simplicity the Θ (n) driving function has\\nbeen replaced by n, which won’t affect the asymptotic solution of the\\nrecurrence (as Exercise 4.7-1 on page 118 justiﬁes). Every level of the\\ntree has cost n, until the recursion bottoms out in a base case at depth\\nlog10\\xa0n = Θ (lg n), and then the levels have cost at most n. The recursion\\nterminates at depth log10/9\\xa0n = Θ (lg n). Thus, with a 9-to-1\\nproportional split at every level of recursion, which intuitively seems\\nhighly unbalanced, quicksort runs in O(n lg n) time—asymptotically the\\nsame as if the split were right down the middle. Indeed, even a 99-to-1\\nsplit yields an O(n lg n) running time. In fact, any split of constant\\nproportionality yields a recursion tree of depth Θ (lg n), where the cost\\nat each level is O(n). The running time is therefore O(n lg n) whenever\\nthe split has constant proportionality. The ratio of the split affects only\\nthe constant hidden in the O-notation.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 262}),\n",
              " Document(page_content='Figure 7.4 A recursion tree for QUICKSORT in which PARTITION always produces a 9-to-1\\nsplit, yielding a running time of O(n lg n). Nodes show subproblem sizes, with per-level costs on\\nthe right.\\nIntuition for the average case\\nTo develop a clear notion of the expected behavior of quicksort, we\\nmust assume something about how its inputs are distributed. Because\\nquicksort determines the sorted order using only comparisons between\\ninput elements, its behavior depends on the relative ordering of the\\nvalues in the array elements given as the input, not on the particular\\nvalues in the array. As in the probabilistic analysis of the hiring problem\\nin Section 5.2, assume that all permutations of the input numbers are\\nequally likely and that the elements are distinct.\\nWhen quicksort runs on a random input array, the partitioning is\\nhighly unlikely to happen in the same way at every level, as our informal\\nanalysis has assumed. We expect that some of the splits will be\\nreasonably well balanced and that some will be fairly unbalanced. For\\nexample, Exercise 7.2-6 asks you to show that about 80% of the time\\nPARTITION produces a split that is at least as balanced as 9 to 1, and', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 263}),\n",
              " Document(page_content='about 20% of the time it produces a split that is less balanced than 9 to\\n1.\\nFigure 7.5 (a) Two levels of a recursion tree for quicksort. The partitioning at the root costs n\\nand produces a “bad” split: two subarrays of sizes 0 and n – 1. The partitioning of the subarray\\nof size n – 1 costs n – 1 and produces a “good” split: subarrays of size (n – 1)/2 – 1 and (n – 1)/2.\\n(b) A single level of a recursion tree that is well balanced. In both parts, the partitioning cost for\\nthe subproblems shown with blue shading is Θ (n). Yet the subproblems remaining to be solved\\nin (a), shown with tan shading, are no larger than the corresponding subproblems remaining to\\nbe solved in (b).\\nIn the average case, PARTITION produces a mix of “good” and\\n“bad” splits. In a recursion tree for an average-case execution of\\nPARTITION, the good and bad splits are distributed randomly\\nthroughout the tree. Suppose for the sake of intuition that the good and\\nbad splits alternate levels in the tree, and that the good splits are best-\\ncase splits and the bad splits are worst-case splits. Figure 7.5(a) shows\\nthe splits at two consecutive levels in the recursion tree. At the root of\\nthe tree, the cost is n for partitioning, and the subarrays produced have\\nsizes n – 1 and 0: the worst case. At the next level, the subarray of size n\\n– 1 undergoes best-case partitioning into subarrays of size (n – 1)/2 – 1\\nand (n – 1)/2. Let’s assume that the base-case cost is 1 for the subarray\\nof size 0.\\nThe combination of the bad split followed by the good split produces\\nthree subarrays of sizes 0, (n – 1)/2 – 1, and (n – 1)/2 at a combined\\npartitioning cost of Θ (n) + Θ (n – 1) = Θ (n). This situation is at most a\\nconstant factor worse than that in Figure 7.5(b), namely, where a single\\nlevel of partitioning produces two subarrays of size (n – 1)/2, at a cost of\\nΘ(n). Yet this latter situation is balanced! Intuitively, the Θ (n – 1) cost of\\nthe bad split in Figure 7.5(a) can be absorbed into the Θ (n) cost of the\\ngood split, and the resulting split is good. Thus, the running time of', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 264}),\n",
              " Document(page_content='quicksort, when levels alternate between good and bad splits, is like the\\nrunning time for good splits alone: still O(n lg n), but with a slightly\\nlarger constant hidden by the O-notation. We’ll analyze the expected\\nrunning time of a randomized version of quicksort rigorously in Section\\n7.4.2.\\nExercises\\n7.2-1\\nUse the substitution method to prove that the recurrence T (n) = T (n –\\n1) + Θ (n) has the solution T (n) = Θ (n2), as claimed at the beginning of\\nSection 7.2.\\n7.2-2\\nWhat is the running time of QUICKSORT when all elements of array A\\nhave the same value?\\n7.2-3\\nShow that the running time of QUICKSORT is Θ (n2) when the array A\\ncontains distinct elements and is sorted in decreasing order.\\n7.2-4\\nBanks often record transactions on an account in order of the times of\\nthe transactions, but many people like to receive their bank statements\\nwith checks listed in order by check number. People usually write checks\\nin order by check number, and merchants usually cash them with\\nreasonable dispatch. The problem of converting time-of-transaction\\nordering to check-number ordering is therefore the problem of sorting\\nalmost-sorted input. Explain persuasively why the procedure\\nINSERTION-SORT might tend to beat the procedure QUICKSORT\\non this problem.\\n7.2-5\\nSuppose that the splits at every level of quicksort are in the constant\\nproportion α to β, where α + β = 1 and 0 < α ≤ β < 1. Show that the\\nminimum depth of a leaf in the recursion tree is approximately log1/ α\\xa0n', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 265}),\n",
              " Document(page_content='and that the maximum depth is approximately log1/ β\\xa0n. (Don’t worry\\nabout integer round-off.)\\n7.2-6\\nConsider an array with distinct elements and for which all permutations\\nof the elements are equally likely. Argue that for any constant 0 < α ≤\\n1/2, the probability is approximately 1 – 2 α that PARTITION produces\\na split at least as balanced as 1 – α to α.\\n7.3\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0A randomized version of quicksort\\nIn exploring the average-case behavior of quicksort, we have assumed\\nthat all permutations of the input numbers are equally likely. This\\nassumption does not always hold, however, as, for example, in the\\nsituation laid out in the premise for Exercise 7.2-4. Section 5.3 showed\\nthat judicious randomization can sometimes be added to an algorithm\\nto obtain good expected performance over all inputs. For quicksort,\\nrandomization yields a fast and practical algorithm. Many software\\nlibraries provide a randomized version of quicksort as their algorithm\\nof choice for sorting large data sets.\\nIn Section 5.3, the RANDOMIZED-HIRE-ASSISTANT procedure\\nexplicitly permutes its input and then runs the deterministic HIRE-\\nASSISTANT procedure. We could do the same for quicksort as well,\\nbut a different randomization technique yields a simpler analysis.\\nInstead of always using A[r] as the pivot, a randomized version\\nrandomly chooses the pivot from the subarray A[p : r], where each\\nelement in A[p : r] has an equal probability of being chosen. It then\\nexchanges that element with A[r] before partitioning. Because the pivot\\nis chosen randomly, we expect the split of the input array to be\\nreasonably well balanced on average.\\nThe changes to PARTITION and QUICKSORT are small. The new\\npartitioning procedure, RANDOMIZED-PARTITION, simply swaps\\nbefore performing the partitioning. The new quicksort procedure,\\nRANDOMIZED-QUICKSORT, calls RANDOMIZED-PARTITION', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 266}),\n",
              " Document(page_content='instead of PARTITION. We’ll analyze this algorithm in the next\\nsection.\\nRANDOMIZED-PARTITION(A, p, r)\\n1i = RANDOM(p, r)\\n2exchange A[r] with A[i]\\n3return PARTITION(A, p, r)\\nRANDOMIZED-QUICKSORT(A, p, r)\\n1if\\xa0p < r\\n2 q = RANDOMIZED-PARTITION(A, p, r)\\n3 RANDOMIZED-QUICKSORT(A, p, q – 1)\\n4 RANDOMIZED-QUICKSORT(A, q + 1, r)\\nExercises\\n7.3-1\\nWhy do we analyze the expected running time of a randomized\\nalgorithm and not its worst-case running time?\\n7.3-2\\nWhen RANDOMIZED-QUICKSORT runs, how many calls are made\\nto the random-number generator RANDOM in the worst case? How\\nabout in the best case? Give your answer in terms of Θ -notation.\\n7.4\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0Analysis of quicksort\\nSection 7.2 gave some intuition for the worst-case behavior of quicksort\\nand for why we expect the algorithm to run quickly. This section\\nanalyzes the behavior of quicksort more rigorously. We begin with a\\nworst-case analysis, which applies to either QUICKSORT or\\nRANDOMIZED-QUICKSORT, and conclude with an analysis of the\\nexpected running time of RANDOMIZED-QUICKSORT.\\n7.4.1\\xa0\\xa0\\xa0\\xa0W orst-case analysis', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 267}),\n",
              " Document(page_content='We saw in Section 7.2 that a worst-case split at every level of recursion\\nin quicksort produces a Θ (n2) running time, which, intuitively, is the\\nworst-case running time of the algorithm. We now prove this assertion.\\nWe’ll use the substitution method (see Section 4.3) to show that the\\nrunning time of quicksort is O(n2). Let T (n) be the worst-case time for\\nthe procedure QUICKSORT on an input of size n. Because the\\nprocedure PARTITION produces two subproblems with total size n – 1,\\nwe obtain the recurrence\\nWe guess that T (n) ≤ cn2 for some constant c > 0. Substituting this\\nguess into recurrence (7.1) yields\\nT (n)≤max {cq2 + c(n – 1 – q)2 : 0 ≤ q ≤ n – 1} + Θ (n)\\n=c · max {q2 + (n – 1 – q)2 : 0 ≤ q ≤ n – 1} + Θ (n).\\nLet’s focus our attention on the maximization. For q = 0, 1, … , n –\\n1, we have\\nq2 + (n – 1 – q)2=q2 + (n – 1)2 – 2q(n – 1) + q2\\n=(n – 1)2 + 2q(q – (n – 1))\\n≤(n – 1)2\\nbecause q ≤ n – 1 implies that 2q(q – (n – 1)) ≤ 0. Thus every term in the\\nmaximization is bounded by (n – 1)2.\\nContinuing with our analysis of T (n), we obtain\\nT (n)≤c(n – 1)2 + Θ (n)\\n≤cn2 – c(2n – 1) + Θ (n)\\n≤cn2,\\nby picking the constant c large enough that the c(2n – 1) term dominates\\nthe Θ (n) term. Thus T (n) = O(n2). Section 7.2 showed a speciﬁc case', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 268}),\n",
              " Document(page_content='where quicksort takes Ω(n2) time: when partitioning is maximally\\nunbalanced. Thus, the worst-case running time of quicksort is Θ (n2).\\n7.4.2\\xa0\\xa0\\xa0\\xa0Expected running time\\nWe have already seen the intuition behind why the expected running\\ntime of RANDOMIZED-QUICKSORT is O(n lg n): if, in each level of\\nrecursion, the split induced by RANDOMIZED-PARTITION puts any\\nconstant fraction of the elements on one side of the partition, then the\\nrecursion tree has depth Θ (lg n) and O(n) work is performed at each\\nlevel. Even if we add a few new levels with the most unbalanced split\\npossible between these levels, the total time remains O(n lg n). We can\\nanalyze the expected running time of RANDOMIZED-QUICKSORT\\nprecisely by ﬁrst understanding how the partitioning procedure operates\\nand then using this understanding to derive an O(n lg n) bound on the\\nexpected running time. This upper bound on the expected running time,\\ncombined with the Θ (n lg n) best-case bound we saw in Section 7.2,\\nyields a Θ (n lg n) expected running time. We assume throughout that the\\nvalues of the elements being sorted are distinct.\\nRunning time and comparisons\\nThe QUICKSORT and RANDOMIZED-QUICKSORT procedures\\ndiffer only in how they select pivot elements. They are the same in all\\nother respects. We can therefore analyze RANDOMIZED-\\nQUICKSORT by considering the QUICKSORT and PARTITION\\nprocedures, but with the assumption that pivot elements are selected\\nrandomly from the subarray passed to RANDOMIZED-PARTITION.\\nLet’s start by relating the asymptotic running time of QUICKSORT to\\nthe number of times elements are compared (all in line 4 of\\nPARTITION), understanding that this analysis also applies to\\nRANDOMIZED-QUICKSORT. Note that we are counting the\\nnumber of times that array elements are compared, not comparisons of\\nindices.\\nLemma 7.1', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 269}),\n",
              " Document(page_content='The running time of QUICKSORT on an n-element array is O(n + X),\\nwhere X is the number of element comparisons performed.\\nProof\\xa0\\xa0\\xa0The running time of QUICKSORT is dominated by the time\\nspent in the PARTITION procedure. Each time PARTITION is called,\\nit selects a pivot element, which is never included in any future recursive\\ncalls to QUICKSORT and PARTITION. Thus, there can be at most n\\ncalls to PARTITION over the entire execution of the quicksort\\nalgorithm. Each time QUICKSORT calls PARTITION, it also\\nrecursively calls itself twice, so there are at most 2n calls to the\\nQUICKSORT procedure itself.\\nOne call to PARTITION takes O(1) time plus an amount of time\\nthat is proportional to the number of iterations of the for loop in lines\\n3–6. Each iteration of this for loop performs one comparison in line 4,\\ncomparing the pivot element to another element of the array A.\\nTherefore, the total time spent in the for loop across all executions is\\nproportional to X. Since there are at most n calls to PARTITION and\\nthe time spent outside the for loop is O(1) for each call, the total time\\nspent in PARTITION outside of the for loop is O(n). Thus the total\\ntime for quicksort is O(n + X).\\n▪\\nOur goal for analyzing RANDOMIZED-QUICKSORT, therefore,\\nis to compute the expected value E [X] of the random variable X\\ndenoting the total number of comparisons performed in all calls to\\nPARTITION. To do so, we must understand when the quicksort\\nalgorithm compares two elements of the array and when it does not. For\\nease of analysis, let’s index the elements of the array A by their position\\nin the sorted output, rather than their position in the input. That is,\\nalthough the elements in A may start out in any order, we’ll refer to\\nthem by z1, z2, … , zn, where z1 < z2 < ⋯ < zn, with strict inequality\\nbecause we assume that all elements are distinct. We denote the set {zi,\\nzi + 1, … , zj} by Zij.\\nThe next lemma characterizes when two elements are compared.\\nLemma 7.2', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 270}),\n",
              " Document(page_content='During the execution of RANDOMIZED-QUICKSORT on an array\\nof n distinct elements z1 < z2 < ⋯ < zn, an element zi is compared with\\nan element zj, where i < j, if and only if one of them is chosen as a pivot\\nbefore any other element in the set Zij. Moreover, no two elements are\\never compared twice.\\nProof\\xa0\\xa0\\xa0Let’s look at the ﬁrst time that an element x ∈ Zij is chosen as a\\npivot during the execution of the algorithm. There are three cases to\\nconsider. If x is neither zi nor zj—that is, zi < x < zj—then zi and zj are\\nnot compared at any subsequent time, because they fall into different\\nsides of the partition around x. If x = zi, then PARTITION compares zi\\nwith every other item in Zij. Similarly, if x = zj, then PARTITION\\ncompares zj with every other item in Zij. Thus, zi and zj are compared if\\nand only if the ﬁrst element to be chosen as a pivot from Zij is either zi\\nor zj. In the latter two cases, where one of zi and zj is chosen as a pivot,\\nsince the pivot is removed from future comparisons, it is never\\ncompared again with the other element.\\n▪\\nAs an example of this lemma, consider an input to quicksort of the\\nnumbers 1 through 10 in some arbitrary order. Suppose that the ﬁrst\\npivot element is 7. Then the ﬁrst call to PARTITION separates the\\nnumbers into two sets: {1, 2, 3, 4, 5, 6} and {8, 9, 10}. In the process,\\nthe pivot element 7 is compared with all other elements, but no number\\nfrom the ﬁrst set (e.g., 2) is or ever will be compared with any number\\nfrom the second set (e.g., 9). The values 7 and 9 are compared because 7\\nis the ﬁrst item from Z7,9 to be chosen as a pivot. In contrast, 2 and 9\\nare never compared because the ﬁrst pivot element chosen from Z2,9 is\\n7.\\nThe next lemma gives the probability that two elements are\\ncompared.\\nLemma 7.3', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 271}),\n",
              " Document(page_content='Consider an execution of the procedure RANDOMIZED-\\nQUICKSORT on an array of n distinct elements z1 < z2 < ⋯ < zn.\\nGiven two arbitrary elements zi and zj where i < j, the probability that\\nthey are compared is 2/(j – i + 1).\\nProof\\xa0\\xa0\\xa0Let’s look at the tree of recursive calls that RANDOMIZED-\\nQUICKSORT makes, and consider the sets of elements provided as\\ninput to each call. Initially, the root set contains all the elements of Zij,\\nsince the root set contains every element in A. The elements belonging\\nto Zij all stay together for each recursive call of RANDOMIZED-\\nQUICKSORT until PARTITION chooses some element x ∈ Zij as a\\npivot. From that point on, the pivot x appears in no subsequent input\\nset. The ﬁrst time that RANDOMIZED-SELECT chooses a pivot x ∈\\nZij from a set containing all the elements of Zij, each element in Zij is\\nequally likely to be x because the pivot is chosen uniformly at random.\\nSince |Zij| = j – i + 1, the probability is 1/(j – i + 1) that any given\\nelement in Zij is the ﬁrst pivot chosen from Zij. Thus, by Lemma 7.2, we\\nhave\\nPr {zi is compared with\\nzj}=Pr {zi or zj is the ﬁrst pivot chosen from\\nZij}\\n=Pr {zi is the ﬁrst pivot chosen from Zij}\\n+ Pr {zj is the ﬁrst pivot chosen from\\nZij}\\n=\\n,\\nwhere the second line follows from the ﬁrst because the two events are\\nmutually exclusive.\\n▪\\nWe can now complete the analysis of randomized quicksort.\\nTheorem 7.4', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 272}),\n",
              " Document(page_content='The expected running time of RANDOMIZED-QUICKSORT on an\\ninput of n distinct elements is O(n lg n).\\nProof\\xa0\\xa0\\xa0The analysis uses indicator random variables (see Section 5.2).\\nLet the n distinct elements be z1 < z2 < ⋯ < zn, and for 1 ≤ i < j ≤ n,\\ndeﬁne the indicator random variable Xij = I {zi is compared with zj}.\\nFrom Lemma 7.2, each pair is compared at most once, and so we can\\nexpress X as follows:\\nBy taking expectations of both sides and using linearity of expectation\\n(equation (C.24) on page 1192) and Lemma 5.1 on page 130,  we obtain\\nWe can evaluate this sum using a change of variables (k = j – i) and the\\nbound on the harmonic series in equation (A.9) on page 1142:', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 273}),\n",
              " Document(page_content='This bound and Lemma 7.1 allow us to conclude that the expected\\nrunning time of RANDOMIZED-QUICKSORT is O(n lg n) (assuming\\nthat the element values are distinct).\\n▪\\nExercises\\n7.4-1\\nShow that the recurrence\\nT (n) = max {T (q) + T (n – q – 1) : 0 ≤ q ≤ n – 1} + Θ (n)\\nhas a lower bound of T (n) = Ω (n2).\\n7.4-2\\nShow that quicksort’s best-case running time is Ω(n lg n).\\n7.4-3\\nShow that the expression q2 + (n – q – 1)2 achieves its maximum value\\nover q = 0, 1, … , n – 1 when q = 0 or q = n – 1.\\n7.4-4\\nShow that RANDOMIZED-QUICKSORT’s expected running time is\\nΩ(n lg n).\\n7.4-5', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 274}),\n",
              " Document(page_content='Coarsening the recursion, as we did in Problem 2-1 for merge sort, is a\\ncommon way to improve the running time of quicksort in practice. We\\nmodify the base case of the recursion so that if the array has fewer than\\nk elements, the subarray is sorted by insertion sort, rather than by\\ncontinued recursive calls to quicksort. Argue that the randomized\\nversion of this sorting algorithm runs in O(nk + n lg(n/k)) expected time.\\nHow should you pick k, both in theory and in practice?\\n★ 7.4-6\\nConsider modifying the PARTITION procedure by randomly picking\\nthree elements from subarray A[p : r] and partitioning about their\\nmedian (the middle value of the three elements). Approximate the\\nprobability of getting worse than an α-to-(1– α) split, as a function of α\\nin the range 0 < α < 1/2.\\nProblems\\n7-1\\xa0\\xa0\\xa0\\xa0\\xa0H oare partition correctness\\nThe version of PARTITION given in this chapter is not the original\\npartitioning algorithm. Here is the original partitioning algorithm,\\nwhich is due to C. A. R. Hoare.\\nHOARE-PARTITION(A, p, r)\\n\\xa0\\xa01x = A[p]\\n\\xa0\\xa02i = p – 1\\n\\xa0\\xa03j = r + 1\\n\\xa0\\xa04while\\xa0TRUE\\n\\xa0\\xa05 repeat\\n\\xa0\\xa06 j = j – 1\\n\\xa0\\xa07 until\\xa0A[j] ≤ x\\n\\xa0\\xa08 repeat\\n\\xa0\\xa09 i = i + 1\\n10 until\\xa0A[i] ≥ x\\n11 if\\xa0i < j\\n12 exchange A[i] with A[j]', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 275}),\n",
              " Document(page_content='13 else return\\xa0j\\na. Demonstrate the operation of HOARE-PARTITION on the array A\\n= 〈13, 19, 9, 5, 12, 8, 7, 4, 11, 2, 6, 21 〉, showing the values of the array\\nand the indices i and j after each iteration of the while loop in lines 4–\\n13.\\nb. Describe how the PARTITION procedure in Section 7.1 differs from\\nHOARE-PARTITION when all elements in A[p : r] are equal.\\nDescribe a practical advantage of HOARE-PARTITION over\\nPARTITION for use in quicksort.\\nThe next three questions ask you to give a careful argument that the\\nprocedure HOARE-PARTITION is correct. Assuming that the\\nsubarray A[p : r] contains at least two elements, prove the following:\\nc. The indices i and j are such that the procedure never accesses an\\nelement of A outside the subarray A[p : r].\\nd. When HOARE-PARTITION terminates, it returns a value j such\\nthat p ≤ j < r.\\ne. Every element of A[p : j] is less than or equal to every element of A[j +\\n1 : r] when HOARE-PARTITION terminates.\\nThe PARTITION procedure in Section 7.1 separates the pivot value\\n(originally in A[r]) from the two partitions it forms. The HOARE-\\nPARTITION procedure, on the other hand, always places the pivot\\nvalue (originally in A[p]) into one of the two partitions A[p : j] and A[j +\\n1 : r]. Since p ≤ j < r, neither partition is empty.\\nf. Rewrite the QUICKSORT procedure to use HOARE-PARTITION.\\n7-2\\xa0\\xa0\\xa0\\xa0\\xa0Q uicksort with equal element values\\nThe analysis of the expected running time of randomized quicksort in\\nSection 7.4.2 assumes that all element values are distinct. This problem\\nexamines what happens when they are not.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 276}),\n",
              " Document(page_content='a. Suppose that all element values are equal. What is randomized\\nquicksort’s running time in this case?\\nb. The PARTITION procedure returns an index q such that each\\nelement of A[p : q – 1] is less than or equal to A[q] and each element of\\nA[q + 1 : r] is greater than A[q]. Modify the PARTITION procedure\\nto produce a procedure PARTITION′ (A, p, r), which permutes the\\nelements of A[p : r] and returns two indices q and t, where p ≤ q ≤ t ≤ r,\\nsuch that\\nall elements of A[q : t] are equal,\\neach element of A[p : q – 1] is less than A[q], and\\neach element of A[t + 1 : r] is greater than A[q].\\nLike PARTITION, your PARTITION′ procedure should take Θ (r –\\np) time.\\nc. Modify the RANDOMIZED-PARTITION procedure to call\\nPARTITION′, and name the new procedure RANDOMIZED-\\nPARTITION′. Then modify the QUICKSORT procedure to produce\\na procedure QUICKSORT′ (A, p, r) that calls RANDOMIZED-\\nPARTITION′ and recurses only on partitions where elements are not\\nknown to be equal to each other.\\nd. Using QUICKSORT′, adjust the analysis in Section 7.4.2 to avoid the\\nassumption that all elements are distinct.\\n7-3\\xa0\\xa0\\xa0\\xa0\\xa0A lternative quicksort analysis\\nAn alternative analysis of the running time of randomized quicksort\\nfocuses on the expected running time of each individual recursive call to\\nRANDOMIZED-QUICKSORT, rather than on the number of\\ncomparisons performed. As in the analysis of Section 7.4.2, assume that\\nthe values of the elements are distinct.\\na. Argue that, given an array of size n, the probability that any\\nparticular element is chosen as the pivot is 1/n. Use this probability to\\ndeﬁne indicator random variables Xi = I {ith smallest element is\\nchosen as the pivot}. What is E [Xi]?', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 277}),\n",
              " Document(page_content='b. Let T (n) be a random variable denoting the running time of\\nquicksort on an array of size n. Argue that\\nc. Show how to rewrite equation (7.2) as\\nd. Show that\\nfor n ≥ 2. (Hint: Split the summation into two parts, one summation\\nfor q = 1, 2, … , ⌈n/2 ⌉ – 1 and one summation for q = ⌈n/2 ⌉ , … , n –\\n1.)\\ne. Using the bound from equation (7.4), show that the recurrence in\\nequation (7.3) has the solution E [T (n)] = O(n lg n). (Hint: Show, by\\nsubstitution, that E [T (n)] ≤ an lg n for sufﬁciently large n and for\\nsome positive constant a.)\\n7-4\\xa0\\xa0\\xa0\\xa0\\xa0Stooge sort\\nProfessors Howard, Fine, and Howard have proposed a deceptively\\nsimple sorting algorithm, named stooge sort in their honor, appearing\\non the following page.\\na. Argue that the call STOOGE-SORT(A, 1, n) correctly sorts the array\\nA[1 : n].\\nb. Give a recurrence for the worst-case running time of STOOGE-SORT\\nand a tight asymptotic ( Θ -notation) bound on the worst-case running\\ntime.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 278}),\n",
              " Document(page_content='c. Compare the worst-case running time of STOOGE-SORT with that\\nof insertion sort, merge sort, heapsort, and quicksort. Do the\\nprofessors deserve tenure?\\nSTOOGE-SORT(A, p, r)\\n1if\\xa0A[p] > A[r]\\n2 exchange A[p] with A[r]\\n3if\\xa0p + 1 < r\\n4 k = ⌊(r – p + 1)/3 ⌋ // round down\\n5 STOOGE-SORT(A, p,\\nr – k)// ﬁrst two-thirds\\n6 STOOGE-SORT(A, p\\n+ k, r)// last two-thirds\\n7 STOOGE-SORT(A, p,\\nr – k)// ﬁrst two-thirds\\nagain\\n7-5 Stack depth for quicksort\\nThe QUICKSORT procedure of Section 7.1 makes two recursive calls\\nto itself. After QUICKSORT calls PARTITION, it recursively sorts the\\nlow side of the partition and then it recursively sorts the high side of the\\npartition. The second recursive call in QUICKSORT is not really\\nnecessary, because the procedure can instead use an iterative control\\nstructure. This transformation technique, called tail-recursion\\nelimination, is provided automatically by good compilers. Applying tail-\\nrecursion elimination transforms QUICKSORT into the TRE-\\nQUICKSORT procedure.\\nTRE-QUICKSORT(A, p, r)\\n1while\\xa0p < r\\n2 // Partition and then sort the low side.\\n3 q = PARTITION(A, p, r)\\n4 TRE-QUICKSORT(A, p, q – 1)\\n5 p = q + 1', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 279}),\n",
              " Document(page_content='a. Argue that TRE-QUICKSORT(A, 1, n) correctly sorts the array A[1 :\\nn].\\nCompilers usually execute recursive procedures by using a stack that\\ncontains pertinent information, including the parameter values, for each\\nrecursive call. The information for the most recent call is at the top of\\nthe stack, and the information for the initial call is at the bottom. When\\na procedure is called, its information is pushed onto the stack, and when\\nit terminates, its information is popped. Since we assume that array\\nparameters are represented by pointers, the information for each\\nprocedure call on the stack requires O(1) stack space. The stack depth is\\nthe maximum amount of stack space used at any time during a\\ncomputation.\\nb. Describe a scenario in which TRE-QUICKSORT’s stack depth is\\nΘ(n) on an n-element input array.\\nc. Modify TRE-QUICKSORT so that the worst-case stack depth is Θ (lg\\nn). Maintain the O(n lg n) expected running time of the algorithm.\\n7-6\\xa0\\xa0\\xa0\\xa0\\xa0M edian-of-3 partition\\nOne way to improve the RANDOMIZED-QUICKSORT procedure is\\nto partition around a pivot that is chosen more carefully than by\\npicking a random element from the subarray. A common approach is\\nthe median-of-3 method: choose the pivot as the median (middle\\nelement) of a set of 3 elements randomly selected from the subarray.\\n(See Exercise 7.4-6.) For this problem, assume that the n elements in the\\ninput subarray A[p : r] are distinct and that n ≥ 3. Denote the sorted\\nversion of A[p : r] by z1, z2, … , zn. Using the median-of-3 method to\\nchoose the pivot element x, deﬁne pi = Pr {x = zi}.\\na. Give an exact formula for pi as a function of n and i for i = 2, 3, … , n\\n– 1. (Observe that p1 = pn = 0.)\\nb. By what amount does the median-of-3 method increase the likelihood\\nof choosing the pivot to be x = z ⌊(n + 1)/2 ⌋, the median of A[p : r],', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 280}),\n",
              " Document(page_content='compared with the ordinary implementation? Assume that n → ∞,\\nand give the limiting ratio of these probabilities.\\nc. Suppose that we deﬁne a “good” split to mean choosing the pivot as\\nx = zi, where n/3 ≤ i ≤ 2n/3. By what amount does the median-of-3\\nmethod increase the likelihood of getting a go od split compared with\\nthe ordinary implementation? (Hint: Approximate the sum by an\\nintegral.)\\nd. Argue that in the Ω(n lg n) running time of quicksort, the median-of-3\\nmethod affects only the constant factor.\\n7-7\\xa0\\xa0\\xa0\\xa0\\xa0Fuzzy sorting of intervals\\nConsider a sorting problem in which you do not know the numbers\\nexactly. Instead, for each number, you know an interval on the real line\\nto which it belongs. That is, you are given n closed intervals of the form\\n[ai, bi], where ai ≤ bi. The goal is to fuzzy-sort these intervals: to produce\\na permutation 〈i1, i2, … , in〉 of the intervals such that for j = 1, 2, … , n,\\nthere exist \\n  satisfying c1 ≤ c2 ≤ ⋯ ≤ cn.\\na. Design a randomized algorithm for fuzzy-sorting n intervals. Your\\nalgorithm should have the general structure of an algorithm that\\nquicksorts the left endpoints (the ai values), but it should take\\nadvantage of overlapping intervals to improve the running time. (As\\nthe intervals overlap more and more, the problem of fuzzy-sorting the\\nintervals becomes progressively easier. Your algorithm should take\\nadvantage of such overlapping, to the extent that it exists.)\\nb. Argue that your algorithm runs in Θ (n lg n) expected time in general,\\nbut runs in Θ (n) expected time when all of the intervals overlap (i.e.,\\nwhen there exists a value x such that x ∈ [ai, bi] for all i). Your\\nalgorithm should not be checking for this case explicitly, but rather, its\\nperformance should naturally improve as the amount of overlap\\nincreases.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 281}),\n",
              " Document(page_content='Chapter notes\\nQuicksort was invented by Hoare [219], and his version of PARTITION\\nappears in Problem 7-1. Bentley [51, p. 117] attributes the PARTITION\\nprocedure given in Section 7.1 to N. Lomuto. The analysis in Section 7.4\\nbased on an analysis due to Motwani and Raghavan [336]. Sedgewick\\n[401] and Bentley [51] provide good references on the details of\\nimplementation and how they matter.\\nMcIlroy [323] shows how to engineer a “killer adversary” that\\nproduces an array on which virtually any implementation of quicksort\\ntakes Θ (n2) time.\\n1 You can enforce the assumption that the values in an array A are distinct at the cost of Θ (n)\\nadditional space and only constant overhead in running time by converting each input value A[i]\\nto an ordered pair (A[i], i) with (A[i], i) < (A[j], j) if A[i] < A[j] or if A[i] = A[j] and i < j. There are\\nalso more practical variants of quicksort that work well when elements are not distinct.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 282}),\n",
              " Document(page_content='8\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0Sorting in Linear Time\\nWe have now seen a handful of algorithms that can sort n numbers in\\nO(n lg n) time. Whereas merge sort and heapsort achieve this upper\\nbound in the worst case, quicksort achieves it on average. Moreover, for\\neach of these algorithms, we can produce a sequence of n input numbers\\nthat causes the algorithm to run in Ω(n lg n) time.\\nThese algorithms share an interesting property: the sorted order they\\ndetermine is based only on comparisons between the input elements. We\\ncall such sorting algorithms comparison sorts. All the sorting algorithms\\nintroduced thus far are comparison sorts.\\nIn Section 8.1, we’ll prove that any comparison sort must make Ω(n\\nlg n) comparisons in the worst case to sort n elements. Thus, merge sort\\nand heapsort are asymptotically optimal, and no comparison sort exists\\nthat is faster by more than a constant factor.\\nSections 8.2, 8.3, and 8.4 examine three sorting algorithms—\\ncounting sort, radix sort, and bucket sort—that run in linear time on\\ncertain types of inputs. Of course, these algorithms use operations other\\nthan comparisons to determine the sorted order. Consequently, the Ω(n\\nlg n) lower bound does not apply to them.\\n8.1\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0Lower bounds for sorting\\nA comparison sort uses only comparisons between elements to gain\\norder information about an input sequence 〈a1, a2, … , an〉. That is,\\ngiven two elements ai and aj, it performs one of the tests ai < aj, ai ≤ aj,', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 283}),\n",
              " Document(page_content='ai = aj, ai ≥ aj, or ai > aj to determine their relative order. It may not\\ninspect the values of the elements or gain order information about them\\nin any other way.\\nSince we are proving a lower bound, we assume without loss of\\ngenerality in this section that all the input elements are distinct. After\\nall, a lower bound for distinct elements applies when elements may or\\nmay not be distinct. Consequently, comparisons of the form ai = aj are\\nuseless, which means that we can assume that no comparisons for exact\\nequality occur. Moreover, the comparisons ai ≤ aj, ai ≥ aj, ai > aj, and ai\\n< aj are all equivalent in that they yield identical information about the\\nrelative order of ai and aj. We therefore assume that all comparisons\\nhave the form ai ≤ aj.\\nFigure 8.1 The decision tree for insertion sort operating on three elements. An internal node\\n(shown in blue) annotated by i : j indicates a comparison between ai and aj. A leaf annotated by\\nthe permutation 〈π(1), π(2), … , π(n)〉 indicates the ordering aπ(1) ≤ aπ(2) ≤ ⋯ ≤ aπ(n). The\\nhighlighted path indicates the decisions made when sorting the input sequence 〈a1 = 6, a2 = 8,\\na3 = 5〉. Going left from the root node, labeled 1:2, indicates that a1 ≤ a2. Going right from the\\nnode labeled 2:3 indicates that a2 > a3. Going right from the node labeled 1:3 indicates that a1\\n> a3. Therefore, we have the ordering a3 ≤ a1 ≤ a2, as indicated in the leaf labeled 〈3, 1, 2〉.\\nBecause the three input elements have 3! = 6 possible permutations, the decision tree must have\\nat least 6 leaves.\\nThe decision-tree model\\nWe can view comparison sorts abstractly in terms of decision trees. A\\ndecision tree is a full binary tree (each node is either a leaf or has both\\nchildren) that represents the comparisons between elements that are', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 284}),\n",
              " Document(page_content='performed by a particular sorting algorithm operating on an input of a\\ngiven size. Control, data movement, and all other aspects of the\\nalgorithm are ignored. Figure 8.1 shows the decision tree corresponding\\nto the insertion sort algorithm from Section 2.1 operating on an input\\nsequence of three elements.\\nA decision tree has each internal node annotated by i : j for some i\\nand j in the range 1 ≤ i, j ≤ n, where n is the number of elements in the\\ninput sequence. We also annotate each leaf by a permutation 〈π(1), π(2),\\n… , π(n)〉. (See Section C.1 for background on permutations.) Indices in\\nthe internal nodes and the leaves always refer to the original positions of\\nthe array elements at the start of the sorting algorithm. The execution of\\nthe comparison sorting algorithm corresponds to tracing a simple path\\nfrom the root of the decision tree down to a leaf. Each internal node\\nindicates a comparison ai ≤ aj. The left subtree then dictates subsequent\\ncomparisons once we know that ai ≤ aj, and the right subtree dictates\\nsubsequent comparisons when ai > aj. Arriving at a leaf, the sorting\\nalgorithm has established the ordering aπ(1) ≤ aπ(2) ≤ ⋯ ≤ aπ(n).\\nBecause any correct sorting algorithm must be able to produce each\\npermutation of its input, each of the n! permutations on n elements\\nmust appear as at least one of the leaves of the decision tree for a\\ncomparison sort to be correct. Furthermore, each of these leaves must\\nbe reachable from the root by a downward path corresponding to an\\nactual execution of the comparison sort. (We call such leaves\\n“reachable.”) Thus, we consider only decision trees in which each\\npermutation appears as a reachable leaf.\\nA lower bound for the worst case\\nThe length of the longest simple path from the root of a decision tree to\\nany of its reachable leaves represents the worst-case number of\\ncomparisons that the corresponding sorting algorithm performs.\\nConsequently, the worst-case number of comparisons for a given\\ncomparison sort algorithm equals the height of its decision tree. A lower\\nbound on the heights of all decision trees in which each permutation\\nappears as a reachable leaf is therefore a lower bound on the running', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 285}),\n",
              " Document(page_content='time of any comparison sort algorithm. The following theorem\\nestablishes such a lower bound.\\nTheorem 8.1\\nAny comparison sort algorithm requires Ω(n lg n) comparisons in the\\nworst case.\\nProof\\xa0\\xa0\\xa0From the preceding discussion, it sufﬁces to determine the height\\nof a decision tree in which each permutation appears as a reachable leaf.\\nConsider a decision tree of height h with l reachable leaves\\ncorresponding to a comparison sort on n elements. Because each of the\\nn! permutations of the input appears as one or more leaves, we have n! ≤\\nl. Since a binary tree of height h has no more than 2h leaves, we have\\nn! ≤ l ≤ 2h,\\nwhich, by taking logarithms, implies\\nh≥lg(n!) (since the lg function is monotonically increasing)\\n=Ω (n lg n)(by equation (3.28) on page 67).\\n▪\\nCorollary 8.2\\nHeapsort and merge sort are asymptotically optimal comparison sorts.\\nProof\\xa0\\xa0\\xa0The O(n lg n) upper bounds on the running times for heapsort\\nand merge sort match the Ω(n lg n) worst-case lower bound from\\nTheorem 8.1.\\n▪\\nExercises\\n8.1-1\\nWhat is the smallest possible depth of a leaf in a decision tree for a\\ncomparison sort?\\n8.1-2', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 286}),\n",
              " Document(page_content='Obtain asymptotically tight bounds on lg(n!) without using Stirling’s\\napproximation. Instead, evaluate the summation \\n  using\\ntechniques from Section A.2.\\n8.1-3\\nShow that there is no comparison sort whose running time is linear for\\nat least half of the n! inputs of length n. What about a fraction of 1/n of\\nthe inputs of length n? What about a fraction 1/2n?\\n8.1-4\\nYou are given an n-element input sequence, and you know in advance\\nthat it is partly sorted in the following sense. Each element initially in\\nposition i such that i mod 4 = 0 is either already in its correct position,\\nor it is one place away from its correct position. For example, you know\\nthat after sorting, the element initially in position 12 belongs in position\\n11, 12, or 13. You have no advance information about the other\\nelements, in positions i where i mod 4 ≠ 0. Show that an Ω(n lg n) lower\\nbound on comparison-based sorting still holds in this case.\\n8.2\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0Counting sort\\nCounting sort assumes that each of the n input elements is an integer in\\nthe range 0 to k, for some integer k. It runs in Θ (n + k) time, so that\\nwhen k = O(n), counting sort runs in Θ (n) time.\\nCounting sort ﬁrst determines, for each input element x, the number\\nof elements less than or equal to x. It then uses this information to place\\nelement x directly into its position in the output array. For example, if\\n17 elements are less than or equal to x, then x belongs in output\\nposition 17. We must modify this scheme slightly to handle the situation\\nin which several elements have the same value, since we do not want\\nthem all to end up in the same position.\\nThe COUNTING-SORT procedure on the facing page takes as\\ninput an array A[1 : n], the size n of this array, and the limit k on the\\nnonnegative integer values in A. It returns its sorted output in the array\\nB[1 : n] and uses an array C [0 : k] for temporary working storage.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 287}),\n",
              " Document(page_content='COUNTING-SORT(A, n, k)\\n\\xa0\\xa01let B[1 : n] and C [0 : k] be new\\narrays\\n\\xa0\\xa02for\\xa0i = 0 to\\xa0k\\n\\xa0\\xa03C [i] = 0\\n\\xa0\\xa04for\\xa0j = 1 to\\xa0n\\n\\xa0\\xa05C [A[j]] = C [A[j]] + 1\\n\\xa0\\xa06//\\xa0C [i] now contains the number of elements equal to i.\\n\\xa0\\xa07for\\xa0i = 1 to\\xa0k\\n\\xa0\\xa08C [i] = C [i] + C [i – 1]\\n\\xa0\\xa09//\\xa0C [i] now contains the number of elements less than or\\nequal to i.\\n10// Copy A to B, starting from the end of A.\\n11for\\xa0j = n\\xa0downto 1\\n12B[C [A[j]]] = A[j]\\n13C [A[j]] = C [A[j]] – 1 // to handle duplicate\\nvalues\\n14return\\xa0B\\nFigure 8.2 illustrates counting sort. After the for loop of lines 2–3\\ninitializes the array C to all zeros, the for loop of lines 4–5 makes a pass\\nover the array A to inspect each input element. Each time it ﬁnds an\\ninput element whose value is i, it increments C [i]. Thus, after line 5, C\\n[i] holds the number of input elements equal to i for each integer i = 0,\\n1, … , k. Lines 7–8 determine for each i = 0, 1, … , k how many input\\nelements are less than or equal to i by keeping a running sum of the\\narray C.\\nFinally, the for loop of lines 11–13 makes another pass over A, but in\\nreverse, to place each element A[j] into its correct sorted position in the\\noutput array B. If all n elements are distinct, then when line 11 is ﬁrst\\nentered, for each A[j], the value C [A[j]] is the correct ﬁnal position of\\nA[j] in the output array, since there are C [A[j]] elements less than or\\nequal to A[j]. Because the elements might not be distinct, the loop\\ndecrements C [A[j]] each time it places a value A[j] into B. Decrementing\\nC [A[j]] causes the previous element in A with a value equal to A[j], if', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 288}),\n",
              " Document(page_content='one exists, to go to the position immediately before A[j] in the output\\narray B.\\nHow much time does counting sort require? The for loop of lines 2–3\\ntakes Θ (k) time, the for loop of lines 4–5 takes Θ (n) time, the for loop of\\nlines 7–8 takes Θ (k) time, and the for loop of lines 11–13 takes Θ (n)\\ntime. Thus, the overall time is Θ (k + n). In practice, we usually use\\ncounting sort when we have k = O(n), in which case the running time is\\nΘ(n).\\nCounting sort can beat the lower bound of Ω(n lg n) proved in\\nSection 8.1 because it is not a comparison sort. In fact, no comparisons\\nbetween input elements occur anywhere in the code. Instead, counting\\nsort uses the actual values of the elements to index into an array. The\\nΩ(n lg n) lower bound for sorting does not apply when we depart from\\nthe comparison sort model.\\nFigure 8.2 The operation of COUNTING-SORT on an input array A[1 : 8], where each element\\nof A is a nonnegative integer no larger than k = 5. (a) The array A and the auxiliary array C\\nafter line 5. (b) The array C after line 8. (c)–(e) The output array B and the auxiliary array C\\nafter one, two, and three iterations of the loop in lines 11–13, respectively. Only the tan elements\\nof array B have been ﬁlled in. (f) The ﬁnal sorted output array B.\\nAn important property of counting sort is that it is stable: elements\\nwith the same value appear in the output array in the same order as they\\ndo in the input array. That is, it breaks ties between two elements by the\\nrule that whichever element appears ﬁrst in the input array appears ﬁrst', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 289}),\n",
              " Document(page_content='in the output array. Normally, the property of stability is important only\\nwhen satellite data are carried around with the element being sorted.\\nCounting sort’s stability is important for another reason: counting sort\\nis often used as a subroutine in radix sort. As we shall see in the next\\nsection, in order for radix sort to work correctly, counting sort must be\\nstable.\\nExercises\\n8.2-1\\nUsing Figure 8.2 as a model, illustrate the operation of COUNTING-\\nSORT on the array A = 〈6, 0, 2, 0, 1, 3, 4, 6, 1, 3, 2〉.\\n8.2-2\\nProve that COUNTING-SORT is stable.\\n8.2-3\\nSuppose that we were to rewrite the for loop header in line 11 of the\\nCOUNTING-SORT as\\n11for\\xa0j = 1 to\\xa0n\\nShow that the algorithm still works properly, but that it is not stable.\\nThen rewrite the pseudocode for counting sort so that elements with the\\nsame value are written into the output array in order of increasing index\\nand the algorithm is stable.\\n8.2-4\\nProve the following loop invariant for COUNTING-SORT:\\nAt the start of each iteration of the for loop of lines 11–13, the\\nlast element in A with value i that has not yet been copied into\\nB belongs in B[C [i]].\\n8.2-5\\nSuppose that the array being sorted contains only integers in the range 0\\nto k and that there are no satellite data to move with those keys. Modify', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 290}),\n",
              " Document(page_content='counting sort to use just the arrays A and C, putting the sorted result\\nback into array A instead of into a new array B.\\n8.2-6\\nDescribe an algorithm that, given n integers in the range 0 to k,\\npreprocesses its input and then answers any query about how many of\\nthe n integers fall into a range [a : b] in O(1) time. Your algorithm\\nshould use Θ (n + k) preprocessing time.\\n8.2-7\\nCounting sort can also work efﬁciently if the input values have\\nfractional parts, but the number of digits in the fractional part is small.\\nSuppose that you are given n numbers in the range 0 to k, each with at\\nmost d decimal (base 10) digits to the right of the decimal point. Modify\\ncounting sort to run in Θ (n + 10d k) time.\\n8.3\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0Radix sort\\nRadix sort is the algorithm used by the card-sorting machines you now\\nﬁnd only in computer museums. The cards have 80 c olumns, and in each\\ncolumn a machine can punch a hole in one of 12 places. The sorter can\\nbe mechanically “programmed” to examine a given column of each card\\nin a deck and distribute the card into one of 12 bins depending on\\nwhich place has been punched. An operator can then gather the cards\\nbin by bin, so that cards with the ﬁrst place punched are on top of cards\\nwith the second place punched, and so on.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 291}),\n",
              " Document(page_content='Figure 8.3 The operation of radix sort on seven 3-digit numbers. The leftmost column is the\\ninput. The remaining columns show the numbers after successive sorts on increasingly\\nsigniﬁcant digit positions. Tan shading indicates the digit position sorted on to produce each list\\nfrom the previous one.\\nFor decimal digits, each column uses only 10 places. (The other two\\nplaces are reserved for encoding nonnumeric characters.) A d-digit\\nnumber occupies a ﬁeld of d columns. Since the card sorter can look at\\nonly one column at a time, the problem of sorting n cards on a d-digit\\nnumber requires a sorting algorithm.\\nIntuitively, you might sort numbers on their most signiﬁcant\\n(leftmost) digit, sort each of the resulting bins recursively, and then\\ncombine the decks in order. Unfortunately, since the cards in 9 of the 10\\nbins must be put aside to sort each of the bins, this procedure generates\\nmany intermediate piles of cards that you would have to keep track of.\\n(See Exercise 8.3-6.)\\nRadix sort solves the problem of card sorting—c ounterintuitively—\\nby sorting on the least signiﬁcant digit ﬁrst. The algorithm then\\ncombines the cards into a single deck, with the cards in the 0 bin\\npreceding the cards in the 1 bin preceding the cards in the 2 bin, and so\\non. Then it sorts the entire deck again on the second-least signiﬁcant\\ndigit and recombines the deck in a like manner. The process continues\\nuntil the cards have been sorted on all d digits. Remarkably, at that\\npoint the cards are fully sorted on the d-digit number. Thus, only d\\npasses through the deck are required to sort. Figure 8.3 shows how\\nradix sort operates on a “deck” of seven 3-digit numbers.\\nIn order for radix sort to work correctly, the digit sorts must be\\nstable. The sort performed by a card sorter is stable, but the operator\\nmust be careful not to change the order of the cards as they come out of', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 292}),\n",
              " Document(page_content='a bin, even though all the cards in a bin have the same digit in the\\nchosen column.\\nIn a typical computer, which is a sequential random-access machine,\\nwe sometimes use radix sort to sort records of information that are\\nkeyed by multiple ﬁelds. For example, we might wish to sort dates by\\nthree keys: year, month, and day. We could run a sorting algorithm with\\na comparison function that, given two dates, compares years, and if\\nthere is a tie, compares months, and if another tie occurs, compares\\ndays. Alternatively, we could sort the information three times with a\\nstable sort: ﬁrst on day (the “least signiﬁcant” part), next on month, and\\nﬁnally on year.\\nThe code for radix sort is straightforward. The RADIX-SORT\\nprocedure assumes that each element in array A[1 : n] has d digits, where\\ndigit 1 is the lowest-order digit and digit d is the highest-order digit.\\nRADIX-SORT(A, n, d)\\n1for\\xa0i = 1 to\\xa0d\\n2 use a stable sort to sort array A[1 : n] on\\ndigit i\\nAlthough the pseudocode for RADIX-SORT does not specify which\\nstable sort to use, COUNTING-SORT is commonly used. If you use\\nCOUNTING-SORT as the stable sort, you can make RADIX-SORT a\\nlittle more efﬁcient by revising COUNTING-SORT to take a pointer to\\nthe output array as a parameter, having RADIX-SORT preallocate this\\narray, and alternating input and output between the two arrays in\\nsuccessive iterations of the for loop in RADIX-SORT.\\nLemma 8.3\\nGiven n\\xa0d-digit numbers in which each digit can take on up to k possible\\nvalues, RADIX-SORT correctly sorts these numbers in Θ (d(n + k)) time\\nif the stable sort it uses takes Θ (n + k) time.\\nProof\\xa0\\xa0\\xa0The correctness of radix sort follows by induction on the column\\nbeing sorted (see Exercise 8.3-3). The analysis of the running time', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 293}),\n",
              " Document(page_content='depends on the stable sort used as the intermediate sorting algorithm.\\nWhen each digit lies in the range 0 to k – 1 (so that it can take on k\\npossible values), and k is not too large, counting sort is the obvious\\nchoice. Each pass over n d-digit numbers then takes Θ (n + k) time.\\nThere are d passes, and so the total time for radix sort is Θ (d(n + k)).\\n▪\\nWhen d is constant and k = O(n), we can make radix sort run in\\nlinear time. More generally, we have some ﬂexibility in how to break\\neach key into digits.\\nLemma 8.4\\nGiven n b-bit numbers and any positive integer r ≤ b, RADIX-SORT\\ncorrectly sorts these numbers in Θ ((b/r)(n + 2r)) time if the stable sort it\\nuses takes Θ (n + k) time for inputs in the range 0 to k.\\nProof\\xa0\\xa0\\xa0For a value r ≤ b, view each key as having d = ⌈b/r⌉ digits of r bits\\neach. Each digit is an integer in the range 0 to 2r – 1, so that we can use\\ncounting sort with k = 2r – 1. (For example, we can view a 32-bit word\\nas having four 8-bit digits, so that b = 32, r = 8, k = 2r – 1 = 255, and d\\n= b/r = 4.) Each pass of counting sort takes Θ (n + k) = Θ (n + 2r) time\\nand there are d passes, for a total running time of Θ (d(n + 2r)) = Θ ((b/r)\\n(n + 2r)).\\n▪\\nGiven n and b, what value of r ≤ b minimizes the expression (b/r)(n +\\n2r)? As r decreases, the factor b/r increases, but as r increases so does 2r.\\nThe answer depends on whether b < ⌊lg n⌋. If b < ⌊lg n⌋, then r ≤ b\\nimplies (n + 2r) = Θ (n). Thus, choosing r = b yields a running time of\\n(b/b)(n + 2b) = Θ (n), which is asymptotically optimal. If b ≥ ⌊lg n⌋, then\\nchoosing r = ⌊lg n⌋ gives the best running time to within a constant\\nfactor, which we can see as follows.1 Choosing r = ⌊lg n⌋ yields a\\nrunning time of Θ (bn/lg n). As r increases above ⌊lg n⌋, the 2r term in the', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 294}),\n",
              " Document(page_content='numerator increases faster than the r term in the denominator, and so\\nincreasing r above ⌊lg n⌋ yields a running time of Ω(bn / lg n). If instead r\\nwere to decrease below ⌊lg n⌋, then the b/r term increases and the n + 2r\\nterm remains at Θ (n).\\nIs radix sort preferable to a comparison-based sorting algorithm,\\nsuch as quicksort? If b = O(lg n), as is often the case, and r ≈ lg n, then\\nradix sort’s running time is Θ (n), which appears to be better than\\nquicksort’s expected running time of Θ (n lg n). The constant factors\\nhidden in the Θ -notation differ, however. Although radix sort may make\\nfewer passes than quicksort over the n keys, each pass of radix sort may\\ntake signiﬁcantly longer. Which sorting algorithm to prefer depends on\\nthe characteristics of the implementations, of the underlying machine\\n(e.g., quicksort often uses hardware caches more effectively than radix\\nsort), and of the input data. Moreover, the version of radix sort that\\nuses counting sort as the intermediate stable sort does not sort in place,\\nwhich many of the Θ (n lg n)-time comparison sorts do. Thus, when\\nprimary memory storage is at a premium, an in-place algorithm such as\\nquicksort could be the better choice.\\nExercises\\n8.3-1\\nUsing Figure 8.3 as a model, illustrate the operation of RADIX-SORT\\non the following list of English words: COW, DOG, SEA, RUG, ROW,\\nMOB, BOX, TAB, BAR, EAR, TAR, DIG, BIG, TEA, NOW, FOX.\\n8.3-2\\nWhich of the following sorting algorithms are stable: insertion sort,\\nmerge sort, heapsort, and quicksort? Give a simple scheme that makes\\nany comparison sort stable. How much additional time and space does\\nyour scheme entail?\\n8.3-3\\nUse induction to prove that radix sort works. Where does your proof\\nneed the assumption that the intermediate sort is stable?\\n8.3-4', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 295}),\n",
              " Document(page_content='Suppose that COUNTING-SORT is used as the stable sort within\\nRADIX-SORT. If RADIX-SORT calls COUNTING-SORT\\xa0d times,\\nthen since each call of COUNTING-SORT makes two passes over the\\ndata (lines 4–5 and 11–13), altogether 2d passes over the data occur.\\nDescribe how to reduce the total number of passes to d + 1.\\n8.3-5\\nShow how to sort n integers in the range 0 to n3 – 1 in O(n) time.\\n★ 8.3-6\\nIn the ﬁrst card-sorting algorithm in this section, which sorts on the\\nmost signiﬁcant digit ﬁrst, exactly how many sorting passes are needed\\nto sort d-digit decimal numbers in the worst case? How many piles of\\ncards does an operator need to keep track of in the worst case?\\n8.4\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0Bucket sort\\nBucket sort assumes that the input is drawn from a uniform distribution\\nand has an average-case running time of O(n). Like counting sort,\\nbucket sort is fast because it assumes something about the input.\\nWhereas counting sort assumes that the input consists of integers in a\\nsmall range, bucket sort assumes that the input is generated by a\\nrandom process that distributes elements uniformly and independently\\nover the interval [0, 1). (See Section C.2 for a deﬁnition of a uniform\\ndistribution.)\\nBucket sort divides the interval [0, 1) into n equal-sized subintervals,\\nor buckets, and then distributes the n input numbers into the buckets.\\nSince the inputs are uniformly and independently distributed over [0, 1),\\nwe do not expect many numbers to fall into each bucket. To produce the\\noutput, we simply sort the numbers in each bucket and then go through\\nthe buckets in order, listing the elements in each.\\nThe BUCKET-SORT procedure on the next page assumes that the\\ninput is an array A[1 : n] and that each element A[i] in the array satisﬁes\\n0 ≤ A[i] < 1. The code requires an auxiliary array B[0 : n – 1] of linked\\nlists (buckets) and assumes that there is a mechanism for maintaining', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 296}),\n",
              " Document(page_content='such lists. (Section 10.2 describes how to implement basic operations on\\nlinked lists.) Figure 8.4 shows the operation of bucket sort on an input\\narray of 10 numbers.\\nFigure 8.4 The operation of BUCKET-SORT for n = 10. (a) The input array A[1 : 10]. (b) The\\narray B[0 : 9] of sorted lists (buckets) after line 7 of the algorithm, with slashes indicating the\\nend of each bucket. Bucket i holds values in the half-open interval [i/10, (i + 1)/10). The sorted\\noutput consists of a concatenation of the lists B[0], B[1], … , B[9] in order.\\nBUCKET-SORT(A, n)\\n1let B[0 : n – 1] be a new array\\n2for\\xa0i = 0 to\\xa0n – 1\\n3 make B[i] an empty list\\n4for\\xa0i = 1 to\\xa0n\\n5 insert A[i] into list B[⌊n · A[i]⌋]\\n6for\\xa0i = 0 to\\xa0n – 1\\n7 sort list B[i] with insertion sort\\n8concatenate the lists B[0], B[1], … , B[n – 1] together in order\\n9return the concatenated lists\\nTo see that this algorithm works, consider two elements A[i] and A[j].\\nAssume without loss of generality that A[i] ≤ A[j]. Since ⌊n · A[i]⌋ ≤ ⌊n ·\\nA[j]⌋, either element A[i] goes into the same bucket as A[j] or it goes into', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 297}),\n",
              " Document(page_content='a bucket with a lower index. If A[i] and A[j] go into the same bucket,\\nthen the for loop of lines 6–7 puts them into the proper order. If A[i]\\nand A[j] go into different buckets, then line 8 puts them into the proper\\norder. Therefore, bucket sort works correctly.\\nTo analyze the running time, observe that, together, all lines except\\nline 7 take O(n) time in the worst case. We need to analyze the total time\\ntaken by the n calls to insertion sort in line 7.\\nTo analyze the cost of the calls to insertion sort, let ni be the random\\nvariable denoting the number of elements placed in bucket B[i]. Since\\ninsertion sort runs in quadratic time (see Section 2.2), the running time\\nof bucket sort is\\nWe now analyze the average-case running time of bucket sort, by\\ncomputing the expected value of the running time, where we take the\\nexpectation over the input distribution. Taking expectations of both\\nsides and using linearity of expectation (equation (C.24) on page 1192) ,\\nwe have\\nWe claim that\\nfor i = 0, 1, … , n – 1. It is no surprise that each bucket i has the same\\nvalue of \\n , since each value in the input array A is equally likely to\\nfall in any bucket.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 298}),\n",
              " Document(page_content='To prove equation (8.3), view each random variable ni as the number\\nof successes in n Bernoulli trials (see Section C.4). Success in a trial\\noccurs when an element goes into bucket B[i], with a probability p = 1/n\\nof success and q = 1 – 1/n of failure. A binomial distribution counts ni,\\nthe number of successes, in the n trials. By equations (C.41) and (C.44)\\non pages 1199–1200, we have E [ni] = np = n(1/n) = 1 and Var [ni] = npq\\n= 1 – 1/n. Equation (C.31) on page 1194 gives\\n=Var [ni] + E2 [ni]\\n=(1 – 1/n) + 12\\n=2 – 1/n,\\nwhich proves equation (8.3). Using this expected value in equation (8.2),\\nwe get that the average-case running time for bucket sort is Θ (n) + n ·\\nO(2 – 1/n) = Θ (n).\\nEven if the input is not drawn from a uniform distribution, bucket\\nsort may still run in linear time. As long as the input has the property\\nthat the sum of the squares of the bucket sizes is linear in the total\\nnumber of elements, equation (8.1) tells us that bucket sort runs in\\nlinear time.\\nExercises\\n8.4-1\\nUsing Figure 8.4 as a model, illustrate the operation of BUCKET-\\nSORT on the array A = 〈.79, .13, .16, .64, .39, .20, .89, .53, .71, .42〉.\\n8.4-2\\nExplain why the worst-case running time for bucket sort is Θ (n2). What\\nsimple change to the algorithm preserves its linear average-case running\\ntime and makes its worst-case running time O(n lg n)?\\n8.4-3\\nLet X be a random variable that is equal to the number of heads in two\\nﬂips of a fair coin. What is E [X2]? What is E2 [X]?', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 299}),\n",
              " Document(page_content='8.4-4\\nAn array A of size n > 10 is ﬁlled in the following way. For each element\\nA[i], choose two random variables xi and yi uniformly and\\nindependently from [0, 1). Then set\\nModify bucket sort so that it sorts the array A in O(n) expected time.\\n★ 8.4-5\\nYou are given n points in the unit disk, pi = (xi, yi), such that \\n for i = 1, 2, … , n. Suppose that the points are uniformly\\ndistributed, that is, the probability of ﬁnding a p oint in any region of the\\ndisk is proportional to the area of that region. Design an algorithm with\\nan average-case running time of Θ (n) to sort the n points by their\\ndistances \\n  from the origin. (Hint: Design the bucket sizes\\nin BUCKET-SORT to reﬂect the uniform distribution of the points in\\nthe unit disk.)\\n★ 8.4-6\\nA probability distribution function\\xa0P(x) for a random variable X is\\ndeﬁned by P(x) = Pr {X ≤ x}. Suppose that you draw a list of n random\\nvariables X1, X2, … , Xn from a continuous probability distribution\\nfunction P that is computable in O(1) time (given y you can ﬁnd x such\\nthat P(x) = y in O(1) time). Give an algorithm that sorts these numbers\\nin linear average-case time.\\nProblems\\n8-1\\xa0\\xa0\\xa0\\xa0\\xa0P robabilistic lower bounds on c omparison sorting\\nIn this problem, you will prove a probabilistic Ω(n lg n) lower bound on\\nthe running time of any deterministic or randomized comparison sort\\non n distinct input elements. You’ll begin by examining a deterministic', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 300}),\n",
              " Document(page_content='comparison sort A with decision tree TA. Assume that every\\npermutation of A’s inputs is equally likely.\\na. Suppose that each leaf of TA is labeled with the probability that it is\\nreached given a random input. Prove that exactly n! leaves are labeled\\n1/n! and that the rest are labeled 0.\\nb. Let D(T) denote the external path length of a decision tree T—the\\nsum of the depths of all the leaves of T. Let T be a decision tree with k\\n> 1 leaves, and let LT and RT be the left and right subtrees of T. Show\\nthat D(T) = D(LT) + D(RT) + k.\\nc. Let d(k) be the minimum value of D(T) over all decision trees T with\\nk > 1 leaves. Show that d(k) = min {d(i) + d(k – i) + k : 1 ≤ i ≤ k – 1}.\\n(Hint: Consider a decision tree T with k leaves that achieves the\\nminimum. Let i0 be the number of leaves in LT and k – i0 the number\\nof leaves in RT.)\\nd. Prove that for a given value of k > 1 and i in the range 1 ≤ i ≤ k – 1,\\nthe function i lg i + (k – i) lg(k – i) is minimized at i = k/2. Conclude\\nthat d(k) = Ω (k lg k).\\ne. Prove that D(TA) = Ω (n! lg(n!)), and conclude that the average-case\\ntime to sort n elements is Ω(n lg n).\\nNow consider a randomized comparison sort B. We can extend the\\ndecision-tree model to handle randomization by incorporating two\\nkinds of nodes: ordinary comparison nodes and “randomization”\\nnodes. A randomization node models a random choice of the form\\nRANDOM(1, r) made by algorithm B. The node has r children, each of\\nwhich is equally likely to be chosen during an execution of the\\nalgorithm.\\nf. Show that for any randomized comparison sort B, there exists a\\ndeterministic comparison sort A whose expected number of\\ncomparisons is no more than those made by B.\\n8-2\\xa0\\xa0\\xa0\\xa0\\xa0Sorting in place in linear time', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 301}),\n",
              " Document(page_content='You have an array of n data records to sort, each with a key of 0 or 1.\\nAn algorithm for sorting such a set of records might possess some\\nsubset of the following three desirable characteristics:\\n1. The algorithm runs in O(n) time.\\n2. The algorithm is stable.\\n3. The algorithm sorts in place, using no more than a constant\\namount of storage space in addition to the original array.\\na. Give an algorithm that satisﬁes criteria 1 an d 2 above.\\nb. Give an algorithm that satisﬁes criteria 1 an d 3 above.\\nc. Give an algorithm that satisﬁes criteria 2 an d 3 above.\\nd. Can you use any of your sorting algorithms from parts (a)–(c) as the\\nsorting method used in line 2 of RADIX-SORT, so that RADIX-\\nSORT sorts n records with b-bit keys in O(bn) time? Explain how or\\nwhy not.\\ne. Suppose that the n records have keys in the range from 1 to k. Show\\nhow to modify counting sort so that it sorts the records in place in\\nO(n + k) time. You may use O(k) storage outside the input array. Is\\nyour algorithm stable?\\n8-3\\xa0\\xa0\\xa0\\xa0\\xa0Sorting variable-length items\\na. You are given an array of integers, where different integers may have\\ndifferent numbers of digits, but the total number of digits over all the\\nintegers in the array is n. Show how to sort the array in O(n) time.\\nb. You are given an array of strings, where different strings may have\\ndifferent numbers of characters, but the total number of characters\\nover all the strings is n. Show how to sort the strings in O(n) time.\\n(The desired order is the standard alphabetical order: for example, a\\n< ab < b .)\\n8-4\\xa0\\xa0\\xa0\\xa0\\xa0W ater jugs', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 302}),\n",
              " Document(page_content='You are given n red and n blue water jugs, all of different shapes and\\nsizes. All the red jugs hold different amounts of water, as do all the blue\\njugs, and you cannot tell from the size of a jug how much water it holds.\\nMoreover, for every jug of one color, there is a jug of the other color\\nthat holds the same amount of water.\\nYour task is to group the jugs into pairs of red and blue jugs that\\nhold the same amount of water. To do so, you may perform the\\nfollowing operation: pick a pair of jugs in which one is red and one is\\nblue, ﬁll the red jug with water, and then pour the water into the blue\\njug. This operation tells you whether the red jug or the blue jug can hold\\nmore water, or that they have the same volume. Assume that such a\\ncomparison takes one time unit. Your goal is to ﬁnd an algorithm that\\nmakes a minimum number of comparisons to determine the grouping.\\nRemember that you may not directly compare two red jugs or two blue\\njugs.\\na. Describe a deterministic algorithm that uses Θ (n2) comparisons to\\ngroup the jugs into pairs.\\nb. Prove a lower bound of Ω(n lg n) for the number of comparisons that\\nan algorithm solving this problem must make.\\nc. Give a randomized algorithm whose expected number of\\ncomparisons is O(n lg n), and prove that this bound is correct. What is\\nthe worst-case number of comparisons for your algorithm?\\n8-5\\xa0\\xa0\\xa0\\xa0\\xa0A verage sorting\\nSuppose that, instead of sorting an array, we just require that the\\nelements increase on average. More precisely, we call an n-element array\\nA\\xa0k-sorted if, for all i = 1, 2, … , n – k, the following holds:\\na. What does it mean for an array to be 1-sorted?\\nb. Give a permutation of the numbers 1, 2, … , 10 that is 2-sorted, but\\nnot sorted.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 303}),\n",
              " Document(page_content='c. Prove that an n-element array is k-sorted if and only if A[i] ≤ A[i + k]\\nfor all i = 1, 2, … , n – k.\\nd. Give an algorithm that k-sorts an n-element array in O(n lg(n/k))\\ntime.\\nWe can also show a lower bound on the time to produce a k-sorted\\narray, when k is a constant.\\ne. Show how to sort a k-sorted array of length n in O(n lg k) time. (Hint:\\nUse the solution to Exercise 6.5-11.)\\nf. Show that when k is a constant, k-sorting an n-element array requires\\nΩ(n lg n) time. (Hint: Use the solution to part (e) along with the lower\\nbound on comparison sorts.)\\n8-6\\xa0\\xa0\\xa0\\xa0\\xa0L ower bound on merging sorted lists\\nThe problem of merging two sorted lists arises frequently. We have seen\\na procedure for it as the subroutine MERGE in Section 2.3.1. In this\\nproblem, you will prove a lower bound of 2n – 1 on the worst-case\\nnumber of comparisons required to merge two sorted lists, each\\ncontaining n items. First, you will show a lower bound of 2n – o(n)\\ncomparisons by using a decision tree.\\na. Given 2n numbers, compute the number of possible ways to divide\\nthem into two sorted lists, each with n numbers.\\nb. Using a decision tree and your answer to part (a), show that any\\nalgorithm that correctly merges two sorted lists must perform at least\\n2n – o(n) comparisons.\\nNow you will show a slightly tighter 2n – 1 bound.\\nc. Show that if two elements are consecutive in the sorted order and\\nfrom different lists, then they must be compared.\\nd. Use your answer to part (c) to show a lower bound of 2n – 1\\ncomparisons for merging two sorted lists.\\n8-7\\xa0\\xa0\\xa0\\xa0\\xa0T he 0-1 sorting lemma and columnsort', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 304}),\n",
              " Document(page_content='A compare-exchange operation on two array elements A[i] and A[j],\\nwhere i < j, has the form\\nCOMPARE-EXCHANGE(A, i, j)\\n1if\\xa0A[i] > A[j]\\n2 exchange A[i] with A[j]\\nAfter the compare-exchange operation, we know that A[i] ≤ A[j].\\nAn oblivious compare-exchange algorithm operates solely by a\\nsequence of prespeciﬁed compare-exchange operations. The indices of\\nthe positions compared in the sequence must be determined in advance,\\nand although they can depend on the number of elements being sorted,\\nthey cannot depend on the values being sorted, nor can they depend on\\nthe result of any prior compare-exchange operation. For example, the\\nCOMPARE-EXCHANGE-INSERTION-SORT procedure on the\\nfacing page shows a variation of insertion sort as an oblivious compare-\\nexchange algorithm. (Unlike the INSERTION-SORT procedure on\\npage 19, the oblivious version runs in Θ (n2) time in all cases.)\\nThe 0-1 sorting lemma provides a powerful way to prove that an\\noblivious compare-exchange algorithm produces a sorted result. It\\nstates that if an oblivious compare-exchange algorithm correctly sorts\\nall input sequences consisting of only 0s and 1s, then it correctly sorts\\nall inputs containing arbitrary values.\\nCOMPARE-EXCHANGE-INSERTION-SORT(A, n)\\n1for\\xa0i = 2 to\\xa0n\\n2 for\\xa0j = i – 1 downto 1\\n3 COMPARE-EXCHANGE(A, j, j + 1)\\nYou will prove the 0-1 sorting lemma by proving its contrapositive: if\\nan oblivious compare-exchange algorithm fails to sort an input\\ncontaining arbitrary values, then it fails to sort some 0-1 input. Assume\\nthat an oblivious compare-exchange algorithm X fails to correctly sort\\nthe array A[1 : n]. Let A[p] be the smallest value in A that algorithm X', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 305}),\n",
              " Document(page_content='puts into the wrong location, and let A[q] be the value that algorithm X\\nmoves to the location into which A[p] should have gone. Deﬁne an array\\nB[1 : n] of 0s and 1s as follows:\\na. Argue that A[q] > A[p], so that B[p] = 0 and B[q] = 1.\\nb. To complete the proof of the 0-1 sorting lemma, prove that algorithm\\nX fails to sort array B correctly.\\nNow you will use the 0-1 sorting lemma to prove that a particular\\nsorting algorithm works correctly. The algorithm, columnsort, works on\\na rectangular array of n elements. The array has r rows and s columns\\n(so that n = rs), subject to three restrictions:\\nr must be even,\\ns must be a divisor of r, and\\nr ≥ 2s2.\\nWhen columnsort completes, the array is sorted in column-major order:\\nreading down each column in turn, from left to right, the elements\\nmonotonically increase.\\nColumnsort operates in eight steps, regardless of the value of n. The\\nodd steps are all the same: sort each column individually. Each even\\nstep is a ﬁxed permutation. Here are the steps:\\n1. Sort each column.\\n2. Transpose the array, but reshape it back to r rows and s columns.\\nIn other words, turn the leftmost column into the top r/s rows, in\\norder; turn the next column into the next r/s rows, in order; and\\nso on.\\n3. Sort each column.\\n4. Perform the inverse of the permutation performed in step 2.\\n5. Sort each column.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 306}),\n",
              " Document(page_content='6. Shift the top half of each column into the bottom half of the\\nsame column, and shift the bottom half of each column into the\\ntop half of the next column to the right. Leave the top half of the\\nleftmost column empty. Shift the bottom half of the last column\\ninto the top half of a new rightmost column, and leave the\\nbottom half of this new column empty.\\n7. Sort each column.\\n8. Perform the inverse of the permutation performed in step 6.\\nFigure 8.5 The steps of columnsort. (a) The input array with 6 rows and 3 columns. (This\\nexample does not obey the r ≥ 2s2 requirement, but it works.) (b) After sorting each column in\\nstep 1. (c) After transposing and reshaping in step 2. (d) After sorting each column in step 3. (e)\\nAfter performing step 4, which inverts the permutation from step 2. (f) After sorting each\\ncolumn in step 5. (g) After shifting by half a column in step 6. (h) After sorting each column in\\nstep 7. (i) After performing step 8, which inverts the permutation from step 6. Steps 6–8 sort the\\nbottom half of each column with the top half of the next column. After step 8, the array is\\nsorted in column-major order.\\nYou can think of steps 6–8 as a single step that sorts the bottom half of\\neach column and the top half of the next column. Figure 8.5 shows an\\nexample of the steps of columnsort with r = 6 and s = 3. (Even though\\nthis example violates the requirement that r ≥ 2s2, it happens to work.)', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 307}),\n",
              " Document(page_content='c. Argue that we can treat columnsort as an oblivious compare-\\nexchange algorithm, even if we do not know what sorting method the\\nodd steps use.\\nAlthough it might seem hard to believe that columnsort actually\\nsorts, you will use the 0-1 sorting lemma to prove that it does. The 0-1\\nsorting lemma applies because we can treat columnsort as an oblivious\\ncompare-exchange algorithm. A couple of deﬁnitions will help you\\napply the 0-1 sorting lemma. We say that an area of an array is clean if\\nwe know that it contains either all 0s or all 1s or if it is empty.\\nOtherwise, the area might contain mixed 0s and 1s, and it is dirty. From\\nhere on, assume that the input array contains only 0s and 1s, and that\\nwe can treat it as an array with r rows and s columns.\\nd. Prove that after steps 1–3, the array consists of clean rows of 0s at the\\ntop, clean rows of 1s at the bottom, and at most s dirty rows between\\nthem. (One of the clean rows could be empty.)\\ne. Prove that after step 4, the array, read in column-major order, starts\\nwith a clean area of 0s, ends with a clean area of 1s, and has a dirty\\narea of at most s2 elements in the middle. (Again, one of the clean\\nareas could be empty.)\\nf. Prove that steps 5–8 produce a fully sorted 0-1 output. Conclude that\\ncolumnsort correctly sorts all inputs containing arbitrary values.\\ng. Now suppose that s does not divide r. Prove that after steps 1–3, the\\narray consists of clean rows of 0s at the top, clean rows of 1s at the\\nbottom, and at most 2s –1 dirty rows between them. (Once again, one\\nof the clean areas could be empty.) How large must r be, compared\\nwith s, for columnsort to correctly sort when s does not divide r?\\nh. Suggest a simple change to step 1 that allows us to maintain the\\nrequirement that r ≥ 2s2 even when s does not divide r, and prove that\\nwith your change, columnsort correctly sorts.\\nChapter notes', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 308}),\n",
              " Document(page_content='The decision-tree model for studying comparison sorts was introduced\\nby Ford and Johnson [150]. Knuth’s comprehensive treatise on sorting\\n[261] covers many variations on the sorting problem, including the\\ninformation-theoretic lower bound on the complexity of sorting given\\nhere. Ben-Or [46] studied lower bounds for sorting using generalizations\\nof the decision-tree model.\\nKnuth credits H. H. Seward with inventing counting sort in 1954,  as\\nwell as with the idea of combining counting sort with radix sort. Radix\\nsorting starting with the least signiﬁcant digit appears to be a folk\\nalgorithm widely used by operators of mechanical card-sorting\\nmachines. According to Knuth, the ﬁrst published reference to the\\nmethod is a 1929 document by L. J. Comrie describing punched-card\\nequipment. Bucket sorting has been in use since 1956, when the basic\\nidea was proposed by Isaac and Singleton [235].\\nMunro and Raman [338] give a stable sorting algorithm that\\nperforms O(n1+ ϵ) comparisons in the worst case, where 0 < ϵ ≤ 1 is any\\nﬁxed constant. Although any of the O(n lg n)-time algorithms make\\nfewer comparisons, the algorithm by Munro and Raman moves data\\nonly O(n) times and operates in place.\\nThe case of sorting n b-bit integers in o(n lg n) time has been\\nconsidered by many researchers. Several positive results have been\\nobtained, each under slightly different assumptions about the model of\\ncomputation and the restrictions placed on the algorithm. All the\\nresults assume that the computer memory is divided into addressable b-\\nbit words. Fredman and Willard [157] introduced the fusion tree data\\nstructure and used it to sort n integers in O(n lg n/lg lg n) time. This\\nbound was later improved to \\n  time by Andersson [17]. These\\nalgorithms require the use of multiplication and several precomputed\\nconstants. Andersson, Hagerup, Nilsson, and Raman [18] have shown\\nhow to sort n integers in O(n lg lg n) time without using multiplication,\\nbut their method requires storage that can be unbounded in terms of n.\\nUsing multiplicative hashing, we can reduce the storage needed to O(n),\\nbut then the O(n lg lg n) worst-case bound on the running time becomes\\nan expected-time bound. Generalizing the exponential search trees of\\nAndersson [17], Thorup [434] gave an O(n(lg lg n)2)-time sorting', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 309}),\n",
              " Document(page_content='algorithm that does not use multiplication or randomization, and it uses\\nlinear space. Combining these techniques with some new ideas, Han\\n[207] improved the bound for sorting to O(n lg lg n lg lg lg n) time.\\nAlthough these algorithms are important theoretical breakthroughs,\\nthey are all fairly complicated and at the present time seem unlikely to\\ncompete with existing sorting algorithms in practice.\\nThe columnsort algorithm in Problem 8-7 is by Leighton [286].\\n1 The choice of r = ⌊lg n⌋ assumes that n > 1. If n ≤ 1, there is nothing to sort.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 310}),\n",
              " Document(page_content='9\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0Medians and Order Statistics\\nThe ith order statistic of a set of n elements is the ith smallest element.\\nFor example, the minimum of a set of elements is the ﬁrst order statistic\\n(i = 1), and the maximum is the nth order statistic (i = n). A median,\\ninformally, is the “halfway point” of the set. When n is odd, the median\\nis unique, occurring at i = (n + 1)/2. When n is even, there are two\\nmedians, the lower median occurring at i = n/2 and the upper median\\noccurring at i = n/2 + 1. Thus, regardless of the parity of n, medians\\noccur at i = ⌊(n + 1)/2 ⌋ and i = ⌈(n + 1)/2 ⌉. For simplicity in this text,\\nhowever, we consistently use the phrase “the median” to refer to the\\nlower median.\\nThis chapter addresses the problem of selecting the ith order statistic\\nfrom a set of n distinct numbers. We assume for convenience that the set\\ncontains distinct numbers, although virtually everything that we do\\nextends to the situation in which a set contains repeated values. We\\nformally specify the selection problem as follows:\\nInput: A set A of n distinct numbers1 and an integer i, with 1 ≤ i ≤ n.\\nOutput: The element x ∈ A that is larger than exactly i – 1 other\\nelements of A.\\nWe can solve the selection problem in O(n lg n) time simply by sorting\\nthe numbers using heapsort or merge sort and then outputting the ith\\nelement in the sorted array. This chapter presents asymptotically faster\\nalgorithms.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 311}),\n",
              " Document(page_content='Section 9.1 examines the problem of selecting the minimum and\\nmaximum of a set of elements. More interesting is the general selection\\nproblem, which we investigate in the subsequent two sections. Section\\n9.2 analyzes a practical randomized algorithm that achieves an O(n)\\nexpected running time, assuming distinct elements. Section 9.3 contains\\nan algorithm of more theoretical interest that achieves the O(n) running\\ntime in the worst case.\\n9.1\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0Minimum and maximum\\nHow many comparisons are necessary to determine the minimum of a\\nset of n elements? To obtain an upper bound of n – 1 comparisons, just\\nexamine each element of the set in turn and keep track of the smallest\\nelement seen so far. The MINIMUM procedure assumes that the set\\nresides in array A[1 : n].\\nMINIMUM(A, n)\\n1min = A[1]\\n2for\\xa0i = 2 to\\xa0n\\n3 if\\xa0min > A[i]\\n4 min = A[i]\\n5return\\xa0min\\nIt’s no more difﬁcult to ﬁnd the maximum with n – 1 comparisons.\\nIs this algorithm for minimum the best we can do? Yes, because it\\nturns out that there’s a lower bound of n – 1 comparisons for the\\nproblem of determining the minimum. Think of any algorithm that\\ndetermines the minimum as a tournament among the elements. Each\\ncomparison is a match in the tournament in which the smaller of the\\ntwo elements wins. Since every element except the winner must lose at\\nleast one match, we can conclude that n – 1 comparisons are necessary\\nto determine the minimum. Hence the algorithm MINIMUM is\\noptimal with respect to the number of comparisons performed.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 312}),\n",
              " Document(page_content='Simultaneous minimum and maximum\\nSome applications need to ﬁnd both the minimum and the maximum of\\na set of n elements. For example, a graphics program may need to scale\\na set of (x, y) data to ﬁt onto a rectangular display screen or other\\ngraphical output device. To do so, the program must ﬁrst determine the\\nminimum and maximum value of each coordinate.\\nOf course, we can determine both the minimum and the maximum of\\nn elements using Θ (n) comparisons. We simply ﬁnd the minimum and\\nmaximum independently, using n – 1 comparisons for each, for a total\\nof 2n – 2 = Θ (n) comparisons.\\nAlthough 2n – 2 comparisons is asymptotically optimal, it is possible\\nto improve the leading constant. We can ﬁnd both the minimum and the\\nmaximum using at most 3 ⌊n/2 ⌋ comparisons. The trick is to maintain\\nboth the minimum and maximum elements seen thus far. Rather than\\nprocessing each element of the input by comparing it against the current\\nminimum and maximum, at a cost of 2 comparisons per element,\\nprocess elements in pairs. Compare pairs of elements from the input\\nﬁrst with each other, and then compare the smaller with the current\\nminimum and the larger to the current maximum, at a cost of 3\\ncomparisons for every 2 elements.\\nHow you set up initial values for the current minimum and\\nmaximum depends on whether n is odd or even. If n is odd, set both the\\nminimum and maximum to the value of the ﬁrst element, and then\\nprocess the rest of the elements in pairs. If n is even, perform 1\\ncomparison on the ﬁrst 2 elements to determine the initial values of the\\nminimum and maximum, and then process the rest of the elements in\\npairs as in the case for odd n.\\nLet’s count the total number of comparisons. If n is odd, then 3 ⌊n/2 ⌋\\ncomparisons occur. If n is even, 1 initial comparison occurs, followed by\\nanother 3(n – 2)/2 comparisons, for a total of 3n/2 – 2. Thus, in either\\ncase, the total number of comparisons is at most 3 ⌊n/2 ⌋.\\nExercises\\n9.1-1', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 313}),\n",
              " Document(page_content='Show that the second smallest of n elements can be found with n + ⌈lg\\nn ⌉ – 2 comparisons in the worst case. (Hint: Also ﬁnd the smallest\\nelement.)\\n9.1-2\\nGiven n > 2 distinct numbers, you want to ﬁnd a number that is neither\\nthe minimum nor the maximum. What is the smallest number of\\ncomparisons that you need to perform?\\n9.1-3\\nA racetrack can run races with ﬁve horses at a time to determine their\\nrelative speeds. For 25 horses, it takes six races to determine the fastest\\nhorse, assuming transitivity (see page 1159) . What’s the minimum\\nnumber of races it takes to determine the fastest three horses out of 25?\\n★ 9.1-4\\nProve the lower bound of ⌈3n/2 ⌉ – 2 comparisons in the worst case to\\nﬁnd both the maximum and minimum of n numbers. (Hint: Consider\\nhow many numbers are potentially either the maximum or minimum,\\nand investigate how a comparison affects these counts.)\\n9.2\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0Selection in expected linear time\\nThe general selection problem—ﬁnding the ith order statistic for any\\nvalue of i—appears more difﬁcult than the simple problem of ﬁnding a\\nminimum. Yet, surprisingly, the asymptotic running time for both\\nproblems is the same: Θ (n). This section presents a divide-and-conquer\\nalgorithm for the selection problem. The algorithm RANDOMIZED-\\nSELECT is modeled after the quicksort algorithm of Chapter 7. Like\\nquicksort it partitions the input array recursively. But unlike quicksort,\\nwhich recursively processes both sides of the partition,\\nRANDOMIZED-SELECT works on only one side of the partition.\\nThis difference shows up in the analysis: whereas quicksort has an\\nexpected running time of Θ (n lg n), the expected running time of\\nRANDOMIZED-SELECT is Θ (n), assuming that the elements are\\ndistinct.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 314}),\n",
              " Document(page_content='RANDOMIZED-SELECT uses the procedure RANDOMIZED-\\nPARTITION introduced in Section 7.3. Like RANDOMIZED-\\nQUICKSORT, it is a randomized algorithm, since its behavior is\\ndetermined in part by the output of a random-number generator. The\\nRANDOMIZED-SELECT procedure returns the ith smallest element\\nof the array A[p : r], where 1 ≤ i ≤ r – p + 1.\\nRANDOMIZED-SELECT(A, p, r, i)\\n1if\\xa0p == r\\n2 return\\xa0A[p]// 1 ≤ i ≤ r – p + 1 when p == r means that i = 1\\n3q = RANDOMIZED-PARTITION(A, p, r)\\n4k = q – p + 1\\n5if\\xa0i == k\\n6 return\\xa0A[q]// the pivot value is the answer\\n7elseif\\xa0i < k\\n8 return RANDOMIZED-SELECT(A, p, q – 1, i)\\n9else return RANDOMIZED-SELECT(A, q + 1, r, i – k)\\nFigure 9.1 illustrates how the RANDOMIZED-SELECT procedure\\nworks. Line 1 checks for the base case of the recursion, in which the\\nsubarray A[p : r] consists of just one element. In this case, i must equal\\n1, and line 2 simply returns A[p] as the ith smallest element. Otherwise,\\nthe call to RANDOMIZED-PARTITION in line 3 partitions the array\\nA[p : r] into two (possibly empty) subarrays A[p : q – 1] and A[q + 1 : r]\\nsuch that each element of A[p : q – 1] is less than or equal to A[q], which\\nin turn is less than each element of A[q + 1 : r]. (Although our analysis\\nassumes that the elements are distinct, the procedure still yields the\\ncorrect result even if equal elements are present.) As in quicksort, we’ll\\nrefer to A[q] as the pivot element. Line 4 computes the number k of\\nelements in the subarray A[p : q], that is, the number of elements in the\\nlow side of the partition, plus 1 for the pivot element. Line 5 then\\nchecks whether A[q] is the ith smallest element. If it is, then line 6\\nreturns A[q]. Otherwise, the algorithm determines in which of the two\\nsubarrays A[p: q – 1] and A[q + 1 : r] the ith smallest element lies. If i <\\nk, then the desired element lies on the low side of the partition, and line', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 315}),\n",
              " Document(page_content='8 recursively selects it from the subarray. If i > k, however, then the\\ndesired element lies on the high side of the partition. Since we already\\nknow k values that are smaller than the ith smallest element of A[p : r]—\\nnamely, the elements of A[p : q]—the desired element is the (i – k)th\\nsmallest element of A[q + 1 : r], which line 9 ﬁnds recursively. The code\\nappears to allow recursive calls to subarrays with 0 elements, but\\nExercise 9.2-1 asks you to show that this situation cannot happen.\\nFigure 9.1 The action of RANDOMIZED-SELECT as successive partitionings narrow the\\nsubarray A[p: r], showing the values of the parameters p, r, and i at each recursive call. The\\nsubarray A[p : r] in each recursive step is shown in tan, with the dark tan element selected as the\\npivot for the next partitioning. Blue elements are outside A[p : r]. The answer is the tan element\\nin the bottom array, where p = r = 5 and i = 1. The array designations A(0), A(1), … , A(5), the\\npartitioning numbers, and whether the partitioning is helpful are explained on the following\\npage.\\nThe worst-case running time for RANDOMIZED-SELECT is\\nΘ(n2), even to ﬁnd the minimum, because it could be extremely unlucky\\nand always partition around the largest remaining element before\\nidentifying the ith smallest when only one element remains. In this worst\\ncase, each recursive step removes only the pivot from consideration.\\nBecause partitioning n elements takes Θ (n) time, the recurrence for the', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 316}),\n",
              " Document(page_content='worst-case running time is the same as for QUICKSORT: T (n) = T (n –\\n1) + Θ (n), with the solution T (n) = Θ (n2). We’ll see that the algorithm\\nhas a linear expected running time, however, and because it is\\nrandomized, no particular input elicits the worst-case behavior.\\nTo see the intuition behind the linear expected running time, suppose\\nthat each time the algorithm randomly selects a pivot element, the pivot\\nlies somewhere within the second and third quartiles—the “middle\\nhalf”—of the remaining elements in sorted order. If the ith smallest\\nelement is less than the pivot, then all the elements greater than the\\npivot are ignored in all future recursive calls. These ignored elements\\ninclude at least the uppermost quartile, and possibly more. Likewise, if\\nthe ith smallest element is greater than the pivot, then all the elements\\nless than the pivot—at least the ﬁrst quartile—are ignored in all future\\nrecursive calls. Either way, therefore, at least 1/4 of the remaining\\nelements are ignored in all future recursive calls, leaving at most 3/4 of\\nthe remaining elements in play: residing in the subarray A[p : r]. Since\\nRANDOMIZED-PARTITION takes Θ (n) time on a subarray of n\\nelements, the recurrence for the worst-case running time is T (n) = T\\n(3n/4) + Θ (n). By case 3 of the master method (Theorem 4.1 on page\\n102), this recurrence has solution T (n) = Θ (n).\\nOf course, the pivot does not necessarily fall into the middle half\\nevery time. Since the pivot is selected at random, the probability that it\\nfalls into the middle half is about 1/2 each time. We can view the process\\nof selecting the pivot as a Bernoulli trial (see Section C.4) with success\\nequating to the pivot residing in the middle half. Thus the expected\\nnumber of trials needed for success is given by a geometric distribution:\\njust two trials on average (equation (C.36) on page 1197). In other\\nwords, we expect that half of the partitionings reduce the number of\\nelements still in play by at least 3/4 and that half of the partitionings do\\nnot help as much. Consequently, the expected number of partitionings\\nat most doubles from the case when the pivot always falls into the\\nmiddle half. The cost of each extra partitioning is less than the one that\\npreceded it, so that the expected running time is still Θ (n).\\nTo make the above argument rigorous, we start by deﬁning the\\nrandom variable A(j) as the set of elements of A that are still in play', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 317}),\n",
              " Document(page_content='after j partitionings (that is, within the subarray A[p : r] after j calls of\\nRANDOMIZED-SELECT), so that A(0) consists of all the elements in\\nA. Since each partitioning removes at least one element—the pivot—\\nfrom being in play, the sequence |A(0)|, |A(1)|, |A(2)|, … strictly\\ndecreases. Set A(j–1) is in play before the j th partitioning, and set A(j)\\nremains in play afterward. For convenience, assume that the initial set\\nA(0) is the result of a 0th “dummy” partitioning.\\nLet’s call the j th partitioning helpful if |A(j)| ≤ (3/4)|A(j–1)|. Figure\\n9.1 shows the sets A(j) and whether partitionings are helpful for an\\nexample array. A helpful partitioning corresponds to a successful\\nBernoulli trial. The following lemma shows that a partitioning is at least\\nas likely to be helpful as not.\\nLemma 9.1\\nA partitioning is helpful with probability at least 1/2.\\nProof\\xa0\\xa0\\xa0Whether a partitioning is helpful depends on the randomly\\nchosen pivot. We discussed the “middle half” in the informal argument\\nabove. Let’s more precisely deﬁne the middle half of an n-element\\nsubarray as all but the smallest ⌈n/4 ⌉ – 1 and greatest ⌈n/4 ⌉ – 1 elements\\n(that is, all but the ﬁrst ⌈n/4 ⌉ – 1 and last ⌈n/4 ⌉ – 1 elements if the\\nsubarray were sorted). We’ll prove that if the pivot falls into the middle\\nhalf, then the pivot leads to a helpful partitioning, and we’ll also prove\\nthat the probability of the pivot falling into the middle half is at least\\n1/2.\\nRegardless of where the pivot falls, either all the elements greater\\nthan it or all the elements less than it, along with the pivot itself, will no\\nlonger be in play after partitioning. If the pivot falls into the middle\\nhalf, therefore, at least ⌈n/4 ⌉ – 1 elements less than the pivot or ⌈n/4 ⌉ – 1\\nelements greater than the pivot, plus the pivot, will no longer be in play\\nafter partitioning. That is, at least ⌈n/4 ⌉ elements will no longer be in\\nplay. The number of elements remaining in play will be at most n –\\n⌈n/4 ⌉, which equals ⌊3n/4 ⌋ by Exercise 3.3-2 on page 70. Since ⌊3n/4 ⌋ ≤\\n3n/4, the partitioning is helpful.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 318}),\n",
              " Document(page_content='To determine a lower bound on the probability that a randomly\\nchosen pivot falls into the middle half, we determine an upper bound on\\nthe probability that it does not. That probability is\\nThus, the pivot has a probability of at least 1/2 of falling into the middle\\nhalf, and so the probability is at least 1/2 that a partitioning is helpful.\\n▪\\nWe can now bound the expected running time of RANDOMIZED-\\nSELECT.\\nTheorem 9.2\\nThe procedure RANDOMIZED-SELECT on an input array of n\\ndistinct elements has an expected running time of Θ (n).\\nProof\\xa0\\xa0\\xa0Since not every partitioning is necessarily helpful, let’s give each\\npartitioning an index starting at 0 and denote by 〈h0, h1, h2, … , hm〉\\nthe sequence of partitionings that are helpful, so that the hkth\\npartitioning is helpful for k = 0, 1, 2, … , m. Although the number m of\\nhelpful partitionings is a random variable, we can bound it, since after\\nat most ⌈log4/3\\xa0n ⌉ helpful partitionings, only one element remains in\\nplay. Consider the dummy 0th partitioning as helpful, so that h0 = 0.\\nDenote \\n  by nk, where n0 = |A(0)| is the original problem size. Since\\nthe hkth partitioning is helpful and the sizes of the sets A(j) strictly\\ndecrease, we have \\n  for k = 1, 2, …\\n, m. By iterating nk ≤ (3/4) nk–1, we have that nk ≤ (3/4)kn0 for k = 0, 1,\\n2, … , m.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 319}),\n",
              " Document(page_content='Figure 9.2 The sets within each generation in the proof of Theorem 9.2. Vertical lines represent\\nthe sets, with the height of each line indicating the size of the set, which equals the number of\\nelements in play. Each generation starts with a set \\n , which is the result of a helpful\\npartitioning. These sets are drawn in black and are at most 3/4 the size of the sets to their\\nimmediate left. Sets drawn in orange are not the ﬁrst within a generation. A generation may\\ncontain just one set. The sets in generation k are \\n , \\n . The sets \\n are deﬁned so that \\n . If the partitioning gets all the way to\\ngeneration hm, set \\n  has at most one element in play.\\nAs Figure 9.2 depicts, we break up the sequence of sets A(j) into\\nm\\xa0generations consisting of consecutively partitioned sets, starting with\\nthe result \\n  of a helpful partitioning and ending with the last set \\n before the next helpful partitioning, so that the sets in\\ngeneration k are \\n , \\n . Then for each set of elements\\nA(j) in the kth generation, we have that \\n .\\nNext, we deﬁne the random variable\\nXk = hk + 1 – hk\\nfor k = 0, 1, 2, … , m – 1. That is, Xk is the number of sets in the kth\\ngeneration, so that the sets in the kth generation are \\n , \\n.\\nBy Lemma 9.1, the probability that a partitioning is helpful is at\\nleast 1/2. The probability is actually even higher, since a partitioning is', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 320}),\n",
              " Document(page_content='helpful even if the pivot does not fall into the middle half but the ith\\nsmallest element happens to lie in the smaller side of the partitioning.\\nWe’ll just use the lower bound of 1/2, however, and then equation (C.36)\\ngives that E [Xk] ≤ 2 for k = 0, 1, 2, … , m – 1.\\nLet’s derive an upper bound on how many comparisons are made\\naltogether during partitioning, since the running time is dominated by\\nthe comparisons. Since we are calculating an upper bound, assume that\\nthe recursion goes all the way until only one element remains in play.\\nThe j th partitioning takes the set A(j–1) of elements in play, and it\\ncompares the randomly chosen pivot with all the other |A(j–1)| – 1\\nelements, so that the jth partitioning makes fewer than |A(j–1)|\\ncomparisons. The sets in the kth generation have sizes \\n. Thus, the total number of comparisons\\nduring partitioning is less than\\nSince E [Xk] ≤ 2, we have that the expected total number of comparisons\\nduring partitioning is less than', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 321}),\n",
              " Document(page_content='Since n0 is the size of the original array A, we conclude that the\\nexpected number of comparisons, and thus the expected running time,\\nfor RANDOMIZED-SELECT is O(n). All n elements are examined in\\nthe ﬁrst call of RANDOMIZED-PARTITION, giving a lower bound of\\nΩ(n). Hence the expected running time is Θ (n).\\n▪\\nExercises\\n9.2-1\\nShow that RANDOMIZED-SELECT never makes a recursive call to a\\n0-length array.\\n9.2-2\\nWrite an iterative version of RANDOMIZED-SELECT.\\n9.2-3\\nSuppose that RANDOMIZED-SELECT is used to select the minimum\\nelement of the array A = 〈2, 3, 0, 5, 7, 9, 1, 8, 6, 4〉. Describe a sequence\\nof partitions that results in a worst-case performance of\\nRANDOMIZED-SELECT.\\n9.2-4', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 322}),\n",
              " Document(page_content='Argue that the expected running time of RANDOMIZED-SELECT\\ndoes not depend on the order of the elements in its input array A[p : r].\\nThat is, the expected running time is the same for any permutation of\\nthe input array A[p : r]. (Hint: Argue by induction on the length n of the\\ninput array.)\\n9.3\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0Selection in worst-case linear time\\nWe’ll now examine a remarkable and theoretically interesting selection\\nalgorithm whose running time is Θ (n) in the worst case. Although the\\nRANDOMIZED-SELECT algorithm from Section 9.2 achieves linear\\nexpected time, we saw that its running time in the worst case was\\nquadratic. The selection algorithm presented in this section achieves\\nlinear time in the worst case, but it is not nearly as practical as\\nRANDOMIZED-SELECT. It is mostly of theoretical interest.\\nLike the expected linear-time RANDOMIZED-SELECT, the worst-\\ncase linear-time algorithm SELECT ﬁnds the desired element by\\nrecursively partitioning the input array. Unlike RANDOMIZED-\\nSELECT, however, SELECT\\xa0guarantees a good split by choosing a\\nprovably good pivot when partitioning the array. The cleverness in the\\nalgorithm is that it ﬁnds the pivot recursively. Thus, there are two\\ninvocations of SELECT: one to ﬁnd a good pivot, and a second to\\nrecursively ﬁnd the desired order statistic.\\nThe partitioning algorithm used by SELECT is like the deterministic\\npartitioning algorithm PARTITION from quicksort (see Section 7.1),\\nbut modiﬁed to take the element to partition around as an additional\\ninput parameter. Like PARTITION, the PARTITION-AROUND\\nalgorithm returns the index of the pivot. Since it’s so similar to\\nPARTITION, the pseudocode for PARTITION-AROUND is omitted.\\nThe SELECT procedure takes as input a subarray A[p : r] of n = r –\\np + 1 elements and an integer i in the range 1 ≤ i ≤ n. It returns the ith\\nsmallest element of A. The pseudocode is actually more understandable\\nthan it might appear at ﬁrst.\\nSELECT(A, p, r, i)', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 323}),\n",
              " Document(page_content='\\xa0\\xa01while (r – p + 1) mod 5 ≠ 0\\n\\xa0\\xa02for\\xa0j = p + 1 to\\xa0r // put the minimum into A[p]\\n\\xa0\\xa03 if\\xa0A[p] > A[j]\\n\\xa0\\xa04 exchange A[p] with A[j]\\n\\xa0\\xa05// If we want the minimum of A[p : r], we’re done.\\n\\xa0\\xa06if\\xa0i == 1\\n\\xa0\\xa07 return\\xa0A[p]\\n\\xa0\\xa08// Otherwise, we want the (i – 1)st element of A[p + 1 : r].\\n\\xa0\\xa09p = p + 1\\n10i = i – 1\\n11g = (r – p + 1)/5 // number of 5-element\\ngroups\\n12for\\xa0j = p\\xa0to\\xa0p + g – 1 // sort each group\\n13sort 〈A[j], A[j + g], A[j + 2g], A[j + 3g], A[j + 4g]〉 in place\\n14// All group medians now lie in the middle ﬁfth of A[p : r].\\n15// Find the pivot x recursively as the median of the group medians.\\n16x = SELECT(A, p + 2g, p + 3g – 1, ⌈g/2 ⌉)\\n17q = PARTITION-AROUND(A, p, r,\\nx)// partition around the pivot\\n18// The rest is just like lines 3–9 of RANDOMIZED-SELECT.\\n19k = q – p + 1\\n20if\\xa0i == k\\n21return\\xa0A[q] // the pivot value is the\\nanswer\\n22elseif\\xa0i < k\\n23return SELECT(A, p, q – 1, i)\\n24else return SELECT(A, q + 1, r, i – k)\\nThe pseudocode starts by executing the while loop in lines 1–10 to\\nreduce the number r – p + 1 of elements in the subarray until it is\\ndivisible by 5. The while loop executes 0 to 4 times, each time\\nrearranging the elements of A[p : r] so that A[p] contains the minimum\\nelement. If i = 1, which means that we actually want the minimum\\nelement, then the procedure simply returns it in line 7. Otherwise,\\nSELECT eliminates the minimum from the subarray A[p : r] and iterates', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 324}),\n",
              " Document(page_content='to ﬁnd the (i – 1)st element in A[p + 1 : r]. Lines 9–10 do so by\\nincrementing p and decrementing i. If the while loop completes all of its\\niterations without returning a result, the procedure executes the core of\\nthe algorithm in lines 11–24, assured that the number r – p + 1 of\\nelements in A[p : r] is evenly divisible by 5.\\nFigure 9.3 The relationships between elements (shown as circles) immediately after line 17 of the\\nselection algorithm SELECT. There are g = (r – p + 1)/5 groups of 5 elements, each of which\\noccupies a column. For example, the leftmost column contains elements A[p], A[p + g], A[p +\\n2g], A[p + 3g], A[p + 4g], and the next column contains A[p + 1], A[p + g + 1], A[p + 2g + 1], A[p\\n+ 3g + 1], A[p + 4g + 1]. The medians of the groups are red, and the pivot x is labeled. Arrows\\ngo from smaller elements to larger. The elements on the blue background are all known to be\\nless than or equal to x and cannot fall into the high side of the partition around x. The elements\\non the yellow background are known to be greater than or equal to x and cannot fall into the\\nlow side of the partition around x. The pivot x belongs to both the blue and yellow regions and\\nis shown on a green background. The elements on the white background could lie on either side\\nof the partition.\\nThe next part of the algorithm implements the following idea,\\nillustrated in Figure 9.3. Divide the elements in A[p : r] into g = (r – p +\\n1)/5 groups of 5 elements each. The ﬁrst 5-element group is\\n〈A[p], A[p + g], A[p + 2g], A[p + 3g], A[p + 4g]〉,\\nthe second is', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 325}),\n",
              " Document(page_content='〈A[p + 1], A[p + g + 1], A[p + 2g + 1], A[p + 3g + 1], A[p + 4g + 1]〉,\\nand so forth until the last, which is\\n〈A[p + g – 1], A[p + 2g – 1], A[p + 3g – 1], A[p + 4g – 1], A[r]〉.\\n(Note that r = p + 5g – 1.) Line 13 puts each group in order using, for\\nexample, insertion sort (Section 2.1), so that for j = p, p + 1, … , p + g –\\n1, we have\\nA[j] ≤ A[j + g] ≤ A[j + 2g] ≤ A[j + 3g] ≤ A[j + 4g].\\nEach vertical column in Figure 9.3 depicts a sorted group of 5 elements.\\nThe median of each 5-element group is A[j + 2g], and thus all the 5-\\nelement medians, shown in red, lie in the range A[p + 2g : p + 3g – 1].\\nNext, line 16 determines the pivot x by recursively calling SELECT\\nto ﬁnd the median (speciﬁcally, the ⌈g/2 ⌉th smallest) of the g group\\nmedians. Line 17 uses the modiﬁed PARTITION-AROUND algorithm\\nto partition the elements of A[p : r] around x, returning the index q of x,\\nso that A[q] = x, elements in A[p : q] are all at most x, and elements in\\nA[q : r] are greater than or equal to x.\\nThe remainder of the code mirrors that of RANDOMIZED-\\nSELECT. If the pivot x is the ith largest, the procedure returns it.\\nOtherwise, the procedure recursively calls itself on either A[p : q – 1] or\\nA[q + 1 : r], depending on the value of i.\\nLet’s analyze the running time of SELECT and see how the judicious\\nchoice of the pivot x plays into a guarantee on its worst-case running\\ntime.\\nTheorem 9.3\\nThe running time of SELECT on an input of n elements is Θ (n).\\nProof\\xa0\\xa0\\xa0Deﬁne T (n) as the worst-case time to run SELECT on any input\\nsubarray A[p : r] of size at most n, that is, for which r – p + 1 ≤ n. By this\\ndeﬁnition, T (n) is monotonically increasing.\\nWe ﬁrst determine an upper bound on the time spent outside the\\nrecursive calls in lines 16, 23, and 24. The while loop in lines 1–10\\nexecutes 0 to 4 times, which is O(1) times. Since the dominant time', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 326}),\n",
              " Document(page_content='within the loop is the computation of the minimum in lines 2–4, which\\ntakes Θ (n) time, lines 1–10 execute in O(1) · Θ (n) = O(n) time. The\\nsorting of the 5-element groups in lines 12–13 takes Θ (n) time because\\neach 5-element group takes Θ (1) time to sort (even using an\\nasymptotically inefﬁcient sorting algorithm such as insertion sort), and\\nthere are g elements to sort, where n/5 – 1 < g ≤ n/5. Finally, the time to\\npartition in line 17 is Θ (n), as Exercise 7.1-3 on page 187 asks you to\\nshow. Because the remaining bookkeeping only costs Θ (1) time, the\\ntotal amount of time spent outside of the recursive calls is O(n) + Θ (n) +\\nΘ(n) + Θ (1) = Θ (n).\\nNow let’s determine the running time for the recursive calls. The\\nrecursive call to ﬁnd the pivot in line 16 takes T (g) ≤ T (n/5) time, since\\ng ≤ n/5 and T (n) monotonically increases. Of the two recursive calls in\\nlines 23 and 24, at most one is executed. But we’ll see that no matter\\nwhich of these two recursive calls to SELECT actually executes, the\\nnumber of elements in the recursive call turns out to be at most 7n/10,\\nand hence the worst-case cost for lines 23 and 24 is at most T (7n/10).\\nLet’s now show that the machinations with group medians and the\\nchoice of the pivot x as the median of the group medians guarantees\\nthis property.\\nFigure 9.3 helps to visualize what’s going on. There are g ≤ n/5\\ngroups of 5 elements, with each group shown as a column sorted from\\nbottom to top. The arrows show the ordering of elements within the\\ncolumns. The columns are ordered from left to right with groups to the\\nleft of x’s group having a group median less than x and those to the\\nright of x’s group having a group median greater than x. Although the\\nrelative order within each group matters, the relative order among\\ngroups to the left of x’s column doesn’t really matter, and neither does\\nthe relative order among groups to the right of x’s column. The\\nimportant thing is that the groups to the left have group medians less\\nthan x (shown by the horizontal arrows entering x), and that the groups\\nto the right have group medians greater than x (shown by the horizontal\\narrows leaving x). Thus, the yellow region contains elements that we\\nknow are greater than or equal to x, and the blue region contains\\nelements that we know are less than or equal to x.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 327}),\n",
              " Document(page_content='These two regions each contain at least 3g/2 elements. The number of\\ngroup medians in the yellow region is ⌊g/2 ⌋ + 1, and for each group\\nmedian, two additional elements are greater than it, making a total of\\n3( ⌊g/2 ⌋ + 1) ≥ 3g/2 elements. Similarly, the number of group medians in\\nthe blue region is ⌈g/2 ⌉, and for each group median, two additional\\nelements are less than it, making a total of 3 ⌈g/2 ⌉ ≥ 3g/2.\\nThe elements in the yellow region cannot fall into the low side of the\\npartition around x, and those in the blue region cannot fall into the\\nhigh side. The elements in neither region—those lying on a white\\nbackground—could fall into either side of the partition. But since the\\nlow side of the partition excludes the elements in the yellow region, and\\nthere are a total of 5g elements, we know that the low side of the\\npartition can contain at most 5g – 3g/2 = 7g/2 ≤ 7n/10 elements.\\nLikewise, the high side of the partition excludes the elements in the blue\\nregion, and a similar calculation shows that it also contains at most\\n7n/10 elements.\\nAll of which leads to the following recurrence for the worst-case\\nrunning time of SELECT:\\nWe can show that T (n) = O(n) by substitution.2 More speciﬁcally, we’ll\\nprove that T (n) ≤ cn for some suitably large constant c > 0 and all n > 0.\\nSubstituting this inductive hypothesis into the right-hand side of\\nrecurrence (9.1) and assuming that n ≥ 5 yields\\nT (n)≤c(n/5) + c(7n/10) + Θ (n)\\n≤9cn/10 + Θ (n)\\n=cn – cn/10 + Θ (n)\\n≤cn\\nif c is chosen large enough that c/10 dominates the upper-bound\\nconstant hidden by the Θ (n). In addition to this constraint, we can pick\\nc large enough that T (n) ≤ cn for all n ≤ 4, which is the base case of the\\nrecursion within SELECT. The running time of SELECT is therefore', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 328}),\n",
              " Document(page_content='O(n) in the worst case, and because line 13 alone takes Θ (n) time, the\\ntotal time is Θ (n).\\n▪\\nAs in a comparison sort (see Section 8.1), SELECT and\\nRANDOMIZED-SELECT determine information about the relative\\norder of elements only by comparing elements. Recall from Chapter 8\\nthat sorting requires Ω(n lg n) time in the comparison model, even on\\naverage (see Problem 8-1). The linear-time sorting algorithms in\\nChapter 8 make assumptions about the type of the input. In contrast,\\nthe linear-time selection algorithms in this chapter do not require any\\nassumptions about the input’s type, only that the elements are distinct\\nand can be pairwise compared according to a linear order. The\\nalgorithms in this chapter are not subject to the Ω(n lg n) lower bound,\\nbecause they manage to solve the selection problem without sorting all\\nthe elements. Thus, solving the selection problem by sorting and\\nindexing, as presented in the introduction to this chapter, is\\nasymptotically inefﬁcient in the comparison model.\\nExercises\\n9.3-1\\nIn the algorithm SELECT, the input elements are divided into groups\\nof 5. Show that the algorithm works in linear time if the input elements\\nare divided into groups of 7 instead of 5.\\n9.3-2\\nSuppose that the preprocessing in lines 1–10 of SELECT is replaced by\\na base case for n ≥ n0, where n0 is a suitable constant; that g is chosen as\\n⌊r – p + 1)/5 ⌋; and that the elements in A[5g : n] belong to no group.\\nShow that although the recurrence for the running time becomes\\nmessier, it still solves to Θ (n).\\n9.3-3\\nShow how to use SELECT as a subroutine to make quicksort run in\\nO(n lg n) time in the worst case, assuming that all elements are distinct.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 329}),\n",
              " Document(page_content='Figure 9.4 Professor Olay needs to determine the position of the east-west oil pipeline that\\nminimizes the total length of the north-south spurs.\\n★ 9.3-4\\nSuppose that an algorithm uses only comparisons to ﬁnd the ith\\nsmallest element in a set of n elements. Show that it can also ﬁnd the i –\\n1 smaller elements and the n – i larger elements without performing any\\nadditional comparisons.\\n9.3-5\\nShow how to determine the median of a 5-element set using only 6\\ncomparisons.\\n9.3-6\\nYou have a “black-box” worst-case linear-time median subroutine. Give\\na simple, linear-time algorithm that solves the selection problem for an\\narbitrary order statistic.\\n9.3-7\\nProfessor Olay is consulting for an oil company, which is planning a\\nlarge pipeline running east to west through an oil ﬁeld of n wells. The\\ncompany wants to connect a spur pipeline from each well directly to the\\nmain pipeline along a shortest route (either north or south), as shown in\\nFigure 9.4. Given the x- and y-coordinates of the wells, how should the\\nprofessor pick an optimal location of the main pipeline to minimize the', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 330}),\n",
              " Document(page_content='total length of the spurs? Show how to determine an optimal location in\\nlinear time.\\n9.3-8\\nThe kth quantiles of an n-element set are the k – 1 order statistics that\\ndivide the sorted set into k equal-sized sets (to within 1). Give an O(n lg\\nk)-time algorithm to list the kth quantiles of a set.\\n9.3-9\\nDescribe an O(n)-time algorithm that, given a set S of n distinct\\nnumbers and a positive integer k ≤ n, determines the k numbers in S that\\nare closest to the median of S.\\n9.3-10\\nLet X[1 : n] and Y [1 : n] be two arrays, each containing n numbers\\nalready in sorted order. Give an O(lg n)-time algorithm to ﬁnd the\\nmedian of all 2n elements in arrays X and Y. Assume that all 2n\\nnumbers are distinct.\\nProblems\\n9-1\\xa0\\xa0\\xa0\\xa0\\xa0L argest i numbers in sorted order\\nYou are given a set of n numbers, and you wish to ﬁnd the i largest in\\nsorted order using a comparison-based algorithm. Describe the\\nalgorithm that implements each of the following methods with the best\\nasymptotic worst-case running time, and analyze the running times of\\nthe algorithms in terms of n and i.\\na. Sort the numbers, and list the i largest.\\nb. Build a max-priority queue from the numbers, and call EXTRACT-\\nMAX\\xa0i times.\\nc. Use an order-statistic algorithm to ﬁnd the ith largest number,\\npartition around that number, and sort the i largest numbers.\\n9-2\\xa0\\xa0\\xa0\\xa0\\xa0Variant of randomized selection', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 331}),\n",
              " Document(page_content='Professor Mendel has proposed simplifying RANDOMIZED-SELECT\\nby eliminating the check for whether i and k are equal. The simpliﬁed\\nprocedure is SIMPLER-RANDOMIZED-SELECT.\\nSIMPLER-RANDOMIZED-SELECT(A, p, r, i)\\n1if\\xa0p == r\\n2 return\\xa0A[p]// 1 ≤ i ≤ r – p + 1 means that i = 1\\n3q = RANDOMIZED-PARTITION(A, p, r)\\n4k = q – p + 1\\n5if\\xa0i ≤ k\\n6 return SIMPLER-RANDOMIZED-\\nSELECT(A, p, q, i)\\n7else return SIMPLER-RANDOMIZED-\\nSELECT(A, q + 1, r, i – k)\\na. Argue that in the worst case, SIMPLER-RANDOMIZED-SELECT\\nnever terminates.\\nb. Prove that the expected running time of SIMPLER-\\nRANDOMIZED-SELECT is still O(n).\\n9-3\\xa0\\xa0\\xa0\\xa0\\xa0W eighted median\\nConsider n elements x1, x2, … , xn with positive weights w1, w2, … , wn\\nsuch that \\n . The weighted (lower) median is an element xk\\nsatisfying\\nand\\nFor example, consider the following elements xi and weights wi:\\ni', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 332}),\n",
              " Document(page_content='1 2 3 4 5 6 7\\nxi3 8 2 5 4 1 6\\nwi0.120.350.0250.080.150.0750.2\\nFor these elements, the median is x5 = 4, but the weighted median is x7\\n= 6. To see why the weighted median is x7, observe that the elements\\nless than x7 are x1, x3, x4, x5, and x6, and the sum w1 + w3 + w4 + w5\\n+ w6 = 0.45, which is less than 1/2. Furthermore, only element x2 is\\ngreater than x7, and w2 = 0.35, which is no greater than 1/2.\\na. Argue that the median of x1, x2, … , xn is the weighted median of the\\nxi with weights wi = 1/n for i = 1, 2, … , n.\\nb. Show how to compute the weighted median of n elements in O(n lg n)\\nworst-case time using sorting.\\nc. Show how to compute the weighted median in Θ (n) worst-case time\\nusing a linear-time median algorithm such as SELECT from Section\\n9.3.\\nThe post-ofﬁce location problem is deﬁned as follows. The input is n\\npoints p1, p2, … , pn with associated weights w1, w2, … , wn. A solution\\nis a point p (not necessarily one of the input points) that minimizes the\\nsum \\n , where d(a, b) is the distance between points a and b.\\nd. Argue that the weighted median is a best solution for the one-\\ndimensional post-ofﬁce location problem, in which points are simply\\nreal numbers and the distance between points a and b is d(a, b) = |a –\\nb|.\\ne. Find the best solution for the two-dimensional post-ofﬁce location\\nproblem, in which the points are (x, y) coordinate pairs and the\\ndistance between points a = (x1, y1) and b = (x2, y2) is the Manhattan\\ndistance given by d(a, b) = |x1 – x2| + |y1 – y2|.\\n9-4\\xa0\\xa0\\xa0\\xa0\\xa0Small order statistics', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 333}),\n",
              " Document(page_content='Let’s denote by S(n) the worst-case number of comparisons used by\\nSELECT to select the ith order statistic from n numbers. Although S(n)\\n= Θ(n), the constant hidden by the Θ -notation is rather large. When i is\\nsmall relative to n, there is an algorithm that uses SELECT as a\\nsubroutine but makes fewer comparisons in the worst case.\\na. Describe an algorithm that uses Ui(n) comparisons to ﬁnd the ith\\nsmallest of n elements, where\\n(Hint: Begin with ⌊n/2 ⌋ disjoint pairwise comparisons, and recurse on\\nthe set containing the smaller element from each pair.)\\nb. Show that, if i < n/2, then Ui(n) = n + O(S(2i) lg(n/i)).\\nc. Show that if i is a constant less than n/2, then Ui(n) = n + O(lg n).\\nd. Show that if i = n/k for k ≥ 2, then Ui(n) = n + O(S(2n/k) lg k).\\n9-5\\xa0\\xa0\\xa0\\xa0\\xa0A lternative analysis of random ized selection\\nIn this problem, you will use indicator random variables to analyze the\\nprocedure RANDOMIZED-SELECT in a manner akin to our analysis\\nof RANDOMIZED-QUICKSORT in Section 7.4.2.\\nAs in the quicksort analysis, we assume that all elements are distinct,\\nand we rename the elements of the input array A as z1, z2, … , zn,\\nwhere zi is the ith smallest element. Thus the call RANDOMIZED-\\nSELECT(A, 1, n, i) returns zi.\\nFor 1 ≤ j < k ≤ n, let\\nXijk=I {zj is compared with zk sometime during the execution of the\\nalgorithm to ﬁnd zi}.\\na. Give an exact expression for E [Xijk]. (Hint: Your expression may\\nhave different values, depending on the values of i, j, and k.)', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 334}),\n",
              " Document(page_content='b. Let Xi denote the total number of comparisons between elements of\\narray A when ﬁnding zi. Show that\\nc. Show that E [Xi] ≤ 4n.\\nd. Conclude that, assuming all elements of array A are distinct,\\nRANDOMIZED-SELECT runs in O(n) expected time.\\n9-6\\xa0\\xa0\\xa0\\xa0\\xa0Select with groups of 3\\nExercise 9.3-1 asks you to show that the SELECT algorithm still runs in\\nlinear time if the elements are divided into groups of 7. This problem\\nasks about dividing into groups of 3.\\na. Show that SELECT runs in linear time if you divide the elements into\\ngroups whose size is any odd constant greater than 3.\\nb. Show that SELECT runs in O(n lg n) time if you divide the elements\\ninto groups of size 3.\\nBecause the bound in part (b) is just an upper bound, we do not\\nknow whether the groups-of-3 strategy actually runs in O(n) time. But\\nby repeating the groups-of-3 idea on the middle group of medians, we\\ncan pick a pivot that guarantees O(n) time. The SELECT3 algorithm on\\nthe next page determines the ith smallest of an input array of n > 1\\ndistinct elements.\\nc. Describe in English how the SELECT3 algorithm works. Include in\\nyour description one or more suitable diagrams.\\nd. Show that SELECT3 runs in O(n) time in the worst case.\\nChapter notes\\nThe worst-case linear-time median-ﬁnding algorithm was devised by\\nBlum, Floyd, Pratt, Rivest, and Tarjan [62]. The fast randomized', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 335}),\n",
              " Document(page_content='version is due to Hoare [218]. Floyd and Rivest [147] have developed an\\nimproved randomized version that partitions around an element\\nrecursively selected from a small sample of the elements.\\nSELECT3(A, p, r, i)\\n\\xa0\\xa01while (r – p + 1) mod 9 ≠ 0\\n\\xa0\\xa02for\\xa0j = p + 1 to\\xa0r // put the minimum into A[p]\\n\\xa0\\xa03 if\\xa0A[p] > A[j]\\n\\xa0\\xa04 exchange A[p] with A[j]\\n\\xa0\\xa05// If we want the minimum of A[p : r], we’re done.\\n\\xa0\\xa06if\\xa0i == 1\\n\\xa0\\xa07 return\\xa0A[p]\\n\\xa0\\xa08// Otherwise, we want the (i – 1)st element of A[p + 1 : r].\\n\\xa0\\xa09p = p + 1\\n10i = i – 1\\n11g = (r – p + 1)/3 // number of 3-element groups\\n12for\\xa0j = p\\xa0to\\xa0p + g – 1 // run through the groups\\n13sort 〈A[j], A[j + g], A[j + 2g]〉 in place\\n14// All group medians now lie in the middle third of A[p : r].\\n15g ′ = g/3 // number of 3-element\\nsubgroups\\n16for\\xa0j = p + g\\xa0to\\xa0p + g + g ′ – 1 // sort the subgroups\\n17sort 〈A[j], A[j + g ′], A[j + 2g ′]〉 in place\\n18// All subgroup medians now lie in the middle ninth of A[p : r].\\n19// Find the pivot x recursively as the median of the subgroup\\nmedians.\\n20x = SELECT3(A, p + 4g ′, p + 5g ′ – 1, ⌈g ′/2 ⌉)\\n21q = PARTITION-AROUND(A, p,\\nr, x)// partition around the pivot\\n22// The rest is just like lines 19–24 of SELECT.\\n23k = q – p + 1\\n24if\\xa0i == k\\n25return\\xa0A[q] // the pivot value is the answer\\n26elseif\\xa0i < k\\n27return SELECT3(A, p, q – 1, i)', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 336}),\n",
              " Document(page_content='28else return SELECT3(A, q + 1, r, i – k)\\nIt is still unknown exactly how many comparisons are needed to\\ndetermine the median. Bent and John [48] gave a lower bound of 2n\\ncomparisons for median ﬁnding, and Schönhage, Paterson, and\\nPippenger [397] gave an upper bound of 3n. Dor and Zwick have\\nimproved on both of these bounds. Their upper bound [123] is slightly\\nless than 2.95n, and their lower bound [124] is (2 + ϵ)n, for a small\\npositive constant ϵ, thereby improving slightly on related work by Dor\\net al. [122]. Paterson [354] describes some of these results along with\\nother related work.\\nProblem 9-6 was inspired by a paper by Chen and Dumitrescu [84].\\n1 As in the footnote on page 182, you can enforce the assumption that the numbers are distinct\\nby converting each input value A[i] to an ordered pair (A[i], i) with (A[i], i) < (A[j], j) if either\\nA[i] < A[j] or A[i] = A[j] and i < j.\\n2 We could also use the Akra-Bazzi method from Section 4.7, which involves calculus, to solve\\nthis recurrence. Indeed, a similar recurrence (4.24) on page 117 was used to illustrate that\\nmethod.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 337}),\n",
              " Document(page_content='Part III\\xa0\\xa0\\xa0\\xa0Data Structures', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 338}),\n",
              " Document(page_content='\\xa0\\n\\xa0\\nIntroduction\\nSets are as fundamental to computer science as they are to mathematics.\\nWhereas mathematical sets are unchanging, the sets manipulated by\\nalgorithms can grow, shrink, or otherwise change over time. We call\\nsuch sets dynamic. The next four chapters present some basic techniques\\nfor representing ﬁnite dynamic sets and manipulating them on a\\ncomputer.\\nAlgorithms may require several types of operations to be performed\\non sets. For example, many algorithms need only the ability to insert\\nelements into, delete elements from, and test membership in a set. We\\ncall a dynamic set that supports these operations a dictionary. Other\\nalgorithms require more complicated operations. For example, min-\\npriority queues, which Chapter 6 introduced in the context of the heap\\ndata structure, support the operations of inserting an element into and\\nextracting the smallest element from a set. The best way to implement a\\ndynamic set depends upon the operations that you need to support.\\nElements of a dynamic set\\nIn a typical implementation of a dynamic set, each element is\\nrepresented by an object whose attributes can be examined and\\nmanipulated given a pointer to the object. Some kinds of dynamic sets\\nassume that one of the object’s attributes is an identifying key. If the\\nkeys are all different, we can think of the dynamic set as being a set of\\nkey values. The object may contain satellite data, which are carried\\naround in other object attributes but are otherwise unused by the set', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 339}),\n",
              " Document(page_content='implementation. It may also have attributes that are manipulated by the\\nset operations. These attributes may contain data or pointers to other\\nobjects in the set.\\nSome dynamic sets presuppose that the keys are drawn from a totally\\nordered set, such as the real numbers, or the set of all words under the\\nusual alphabetic ordering. A total ordering allows us to deﬁne the\\nminimum element of the set, for example, or to speak of the next\\nelement larger than a given element in a set.\\nOperations on dynamic sets\\nOperations on a dynamic set can be grouped into two categories:\\nqueries, which simply return information about the set, and modifying\\noperations, which change the set. Here is a list of typical operations. Any\\nspeciﬁc application will usually require only a few of these to be\\nimplemented.\\nSEARCH(S, k)\\nA query that, given a set S and a key value k, returns a pointer x to\\nan element in S such that x.key = k, or NIL if no such element\\nbelongs to S.\\nINSERT(S, x)\\nA modifying operation that adds the element pointed to by x to the\\nset S. We usually assume that any attributes in element x needed by\\nthe set implementation have already been initialized.\\nDELETE(S, x)\\nA modifying operation that, given a pointer x to an element in the\\nset S, removes x from S. (Note that this operation takes a pointer to\\nan element x, not a key value.)\\nMINIMUM(S) and MAXIMUM(S)\\nQueries on a totally ordered set S that return a pointer to the\\nelement of S with the smallest (for MINIMUM) or largest (for\\nMAXIMUM) key.\\nSUCCESSOR(S, x)', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 340}),\n",
              " Document(page_content='A query that, given an element x whose key is from a totally ordered\\nset S, returns a pointer to the next larger element in S, or NIL if x is\\nthe maximum element.\\nPREDECESSOR(S, x)\\nA query that, given an element x whose key is from a totally ordered\\nset S, returns a pointer to the next smaller element in S, or NIL if x\\nis the minimum element.\\nIn some situations, we can extend the queries SUCCESSOR and\\nPREDECESSOR so that they apply to sets with nondistinct keys. For a\\nset on n keys, the normal presumption is that a call to MINIMUM\\nfollowed by n – 1 calls to SUCCESSOR enumerates the elements in the\\nset in sorted order.\\nWe usually measure the time taken to execute a set operation in\\nterms of the size of the set. For example, Chapter 13 describes a data\\nstructure that can support any of the operations listed above on a set of\\nsize n in O(lg n) time.\\nOf course, you can always choose to implement a dynamic set with\\nan array. The advantage of doing so is that the algorithms for the\\ndynamic-set operations are simple. The downside, however, is that many\\nof these operations have a worst-case running time of Θ (n). If the array\\nis not sorted, INSERT and DELETE can take Θ (1) time, but the\\nremaining operations take Θ (n) time. If instead the array is maintained\\nin sorted order, then MINIMUM, MAXIMUM, SUCCESSOR, and\\nPREDECESSOR take Θ (1) time; SEARCH takes O(lg n) time if\\nimplemented with binary search; but INSERT and DELETE take Θ (n)\\ntime in the worst case. The data structures studied in this part improve\\non the array implementation for many of the dynamic-set operations.\\nOverview of Part III\\nChapters 10–13 describe several data structures that we can use to\\nimplement dynamic sets. We’ll use many of these data structures later to\\nconstruct efﬁcient algorithms for a variety of problems. We already saw\\nanother important data structure—the heap—in Chapter 6.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 341}),\n",
              " Document(page_content='Chapter 10 presents the essentials of working with simple data\\nstructures such as arrays, matrices, stacks, queues, linked lists, and\\nrooted trees. If you have taken an introductory programming course,\\nthen much of this material should be familiar to you.\\nChapter 11 introduces hash tables, a widely used data structure\\nsupporting the dictionary operations INSERT, DELETE, and\\nSEARCH. In the worst case, hash tables require Θ (n) time to perform a\\nSEARCH operation, but the expected time for hash-table operations is\\nO(1). We rely on probability to analyze hash-table operations, but you\\ncan understand how the operations work even without probability.\\nBinary search trees, which are covered in Chapter 12, support all the\\ndynamic-set operations listed above. In the worst case, each operation\\ntakes Θ (n) time on a tree with n elements. Binary search trees serve as\\nthe basis for many other data structures.\\nChapter 13 introduces red-black trees, which are a variant of binary\\nsearch trees. Unlike ordinary binary search trees, red-black trees are\\nguaranteed to perform well: operations take O(lg n) time in the worst\\ncase. A red-black tree is a balanced search tree. Chapter 18 in Part V\\npresents another kind of balanced search tree, called a B-tree. Although\\nthe mechanics of red-black trees are somewhat intricate, you can glean\\nmost of their properties from the chapter without studying the\\nmechanics in detail. Nevertheless, you probably will ﬁnd walking\\nthrough the code to be instructive.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 342}),\n",
              " Document(page_content='10\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0Elementary Data Structures\\nIn this chapter, we examine the representation of dynamic sets by simple\\ndata structures that use pointers. Although you can construct many\\ncomplex data structures using pointers, we present only the rudimentary\\nones: arrays, matrices, stacks, queues, linked lists, and rooted trees.\\n10.1\\xa0\\xa0\\xa0\\xa0Simple array-based data structures: arrays, matrices,\\nstacks, queues\\n10.1.1\\xa0\\xa0\\xa0\\xa0Arrays\\nWe assume that, as in most programming languages, an array is stored\\nas a contiguous sequence of bytes in memory. If the ﬁrst element of an\\narray has index s (for example, in an array with 1-origin indexing, s = 1),\\nthe array starts at memory address a, and each array element occupies b\\nbytes, then the ith element occupies bytes a + b(i – s) through a + b(i – s\\n+ 1) – 1. Since most of the arrays in this book are indexed starting at 1,\\nand a few starting at 0, we can simplify these formulas a little. When s =\\n1, the ith element occupies bytes a + b(i – 1) through a + bi – 1, and\\nwhen s = 0, the ith element occupies bytes a + bi through a + b(i + 1) –\\n1. Assuming that the computer can access all memory locations in the\\nsame amount of time (as in the RAM model described in Section 2.2), it\\ntakes constant time to access any array element, regardless of the index.\\nMost programming languages require each element of a particular\\narray to be the same size. If the elements of a given array might occupy\\ndifferent numbers of bytes, then the above formulas fail to apply, since', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 343}),\n",
              " Document(page_content='the element size b is not a constant. In such cases, the array elements are\\nusually objects of varying sizes, and what actually appears in each array\\nelement is a pointer to the object. The number of bytes occupied by a\\npointer is typically the same, no matter what the pointer references, so\\nthat to access an object in an array, the above formulas give the address\\nof the pointer to the object and then the pointer must be followed to\\naccess the object itself.\\nFigure 10.1 Four ways to store the 2 × 3 matrix M from equation (10.1). (a) In row-major order,\\nin a single array. (b) In column-major order, in a single array. (c) In row-major order, with one\\narray per row (tan) and a single array (blue) of pointers to the row arrays. (d) In column-major\\norder, with one array per column (tan) and a single array (blue) of pointers to the column\\narrays.\\n10.1.2\\xa0\\xa0\\xa0\\xa0M atrices\\nWe typically represent a matrix or two-dimensional array by one or\\nmore one-dimensional arrays. The two most common ways to store a\\nmatrix are row-major and column-major order. Let’s consider an m × n\\nmatrix—a matrix with m rows and n columns. In row-major order, the\\nmatrix is stored row by row, and in column-major order, the matrix is\\nstored column by column. For example, consider the 2 × 3 matrix\\nRow-major order stores the two rows 1 2 3 and 4 5 6, whereas column-\\nmajor order stores the three columns 1 4; 2 5; and 3 6.\\nParts (a) and (b) of Figure 10.1 show how to store this matrix using a\\nsingle one-dimensional array. It’s stored in row-major order in part (a)\\nand in column-major order in part (b). If the rows, columns, and the\\nsingle array all are indexed starting at s, then M [i, j]—the element in\\nrow i and column j—is at array index s + (n(i – s)) + (j – s) with row-', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 344}),\n",
              " Document(page_content='major order and s + (m(j – s)) + (i – s) with column-major order. When\\ns = 1, the single-array indices are n(i – 1) + j with row-major order and i\\n+ m(j – 1) with column-major order. When s = 0, the single-array\\nindices are simpler: ni + j with row-major order and i + mj with column-\\nmajor order. For the example matrix M with 1-origin indexing, element\\nM [2, 1] is stored at index 3(2 – 1) + 1 = 4 in the single array using row-\\nmajor order and at index 2 + 2(1 – 1)  = 2 using column-major order.\\nParts (c) and (d) of Figure 10.1 show multiple-array strategies for\\nstoring the example matrix. In part (c), each row is stored in its own\\narray of length n, shown in tan. Another array, with m elements, shown\\nin blue, points to the m row arrays. If we call the blue array A, then A[i]\\npoints to the array storing the entries for row i of M, and array element\\nA[i] [j] stores matrix element M [i, j]. Part (d) shows the column-major\\nversion of the multiple-array representation, with n arrays, each of\\nlength m, representing the n columns. Matrix element M [i, j] is stored in\\narray element A[j] [i].\\nSingle-array representations are typically more efﬁcient on modern\\nmachines than multiple-array representations. But multiple-array\\nrepresentations can sometimes be more ﬂexible, for example, allowing\\nfor “ragged arrays,” in which the rows in the row-major version may\\nhave different lengths, or symmetrically for the column-major version,\\nwhere columns may have different lengths.\\nOccasionally, other schemes are used to store matrices. In the block\\nrepresentation, the matrix is divided into blocks, and each block is\\nstored contiguously. For example, a 4 × 4 matrix that is divided into 2 ×\\n2 blocks, such as\\nmight be stored in a single array in the order 〈1, 2, 5, 6, 3, 4, 7, 8, 9, 10,\\n13, 14, 11, 12, 15, 16〉.\\n10.1.3\\xa0\\xa0\\xa0\\xa0Stacks and queues', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 345}),\n",
              " Document(page_content='Stacks and queues are dynamic sets in which the element removed from\\nthe set by the DELETE operation is prespeciﬁed. In a stack, the\\nelement deleted from the set is the one most recently inserted: the stack\\nimplements a last-in, ﬁrst-out, or LIFO, policy. Similarly, in a queue, the\\nelement deleted is always the one that has been in the set for the longest\\ntime: the queue implements a ﬁrst-in, ﬁrst-out, or FIFO, policy. There\\nare several efﬁcient ways to implement stacks and queues on a\\ncomputer. Here, you will see how to use an array with attributes to store\\nthem.\\nStacks\\nThe INSERT operation on a stack is often called PUSH, and the\\nDELETE operation, which does not take an element argument, is often\\ncalled POP. These names are allusions to physical stacks, such as the\\nspring-loaded stacks of plates used in cafeterias. The order in which\\nplates are popped from the stack is the reverse of the order in which\\nthey were pushed onto the stack, since only the top plate is accessible.\\nFigure 10.2 shows how to implement a stack of at most n elements\\nwith an array S[1 : n]. The stack has attributes S.top, indexing the most\\nrecently inserted element, and S.size, equaling the size n of the array.\\nThe stack consists of elements S[1 : S.top], where S[1] is the element at\\nthe bottom of the stack and S[S.top] is the element at the top.\\nFigure 10.2 An array implementation of a stack S. Stack elements appear only in the tan\\npositions. (a) Stack S has 4 elements. The top element is 9. (b) Stack S after the calls PUSH(S,\\n17) and PUSH(S, 3). (c) Stack S after the call POP(S) has returned the element 3, which is the\\none most recently pushed. Although element 3 still appears in the array, it is no longer in the\\nstack. The top is element 17.\\nWhen S.top = 0, the stack contains no elements and is empty. We can\\ntest whether the stack is empty with the query operation STACK-', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 346}),\n",
              " Document(page_content='EMPTY. Upon an attempt to pop an empty stack, the stack underﬂows,\\nwhich is normally an error. If S.top exceeds S.size, the stack overﬂows.\\nThe procedures STACK-EMPTY, PUSH, and POP implement each\\nof the stack operations with just a few lines of code. Figure 10.2 shows\\nthe effects of the modifying operations PUSH and POP. Each of the\\nthree stack operations takes O(1) time.\\nSTACK-EMPTY(S)\\n1if\\xa0S.top == 0\\n2 return TRUE\\n3else return FALSE\\nPUSH(S, x)\\n1if\\xa0S.top == S.size\\n2 error “overﬂow”\\n3else\\xa0S.top = S.top + 1\\n4S[S.top] = x\\nPOP(S)\\n1if STACK-EMPTY(S)\\n2 error “underﬂow”\\n3else\\xa0S.top = S.top – 1\\n4 return\\xa0S[S.top + 1]', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 347}),\n",
              " Document(page_content='Figure 10.3 A queue implemented using an array Q[1 : 12]. Queue elements appear only in the\\ntan positions. (a) The queue has 5 elements, in locations Q[7 : 11]. (b) The conﬁguration of the\\nqueue after the calls ENQUEUE(Q, 17), ENQUEUE(Q, 3), and ENQUEUE(Q, 5). (c) The\\nconﬁguration of the queue after the call DEQUEUE(Q) returns the key value 15 formerly at the\\nhead of the queue. The new head has key 6.\\nQueues\\nWe call the INSERT operation on a queue ENQUEUE, and we call the\\nDELETE operation DEQUEUE. Like the stack operation POP,\\nDEQUEUE takes no element argument. The FIFO property of a queue\\ncauses it to operate like a line of customers waiting for service. The\\nqueue has a head and a tail. When an element is enqueued, it takes its\\nplace at the tail of the queue, just as a newly arriving customer takes a\\nplace at the end of the line. The element dequeued is always the one at\\nthe head of the queue, like the customer at the head of the line, who has\\nwaited the longest.\\nFigure 10.3 shows one way to implement a queue of at most n – 1\\nelements using an array Q[1 : n], with the attribute Q.size equaling the\\nsize n of the array. The queue has an attribute Q.head that indexes, or\\npoints to, its head. The attribute Q.tail indexes the next location at\\nwhich a newly arriving element will be inserted into the queue. The', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 348}),\n",
              " Document(page_content='elements in the queue reside in locations Q.head, Q.head + 1, … , Q.tail\\n– 1, where we “wrap around” in the sense that location 1 immediately\\nfollows location n in a circular order. When Q.head = Q.tail, the queue\\nis empty. Initially, we have Q.head = Q.tail = 1. An attempt to dequeue\\nan element from an empty queue causes the queue to underﬂow. When\\nQ.head = Q.tail + 1 or both Q.head = 1 and Q.tail = Q.size, the queue is\\nfull, and an attempt to enqueue an element causes the queue to\\noverﬂow.\\nIn the procedures ENQUEUE and DEQUEUE, we have omitted the\\nerror checking for underﬂow and overﬂow. (Exercise 10.1-5 asks you to\\nsupply these checks.) Figure 10.3 shows the effects of the ENQUEUE\\nand DEQUEUE operations. Each operation takes O(1) time.\\nENQUEUE(Q, x)\\n1Q[Q.tail] = x\\n2if\\xa0Q.tail == Q.size\\n3Q.tail = 1\\n4else\\xa0Q.tail = Q.tail + 1\\nDEQUEUE(Q)\\n1x = Q[Q.head]\\n2if\\xa0Q.head == Q.size\\n3Q.head = 1\\n4else\\xa0Q.head = Q.head + 1\\n5return\\xa0x\\nExercises\\n10.1-1\\nConsider an m × n matrix in row-major order, where both m and n are\\npowers of 2 and rows and columns are indexed from 0. We can represent\\na row index i in binary by the lg m bits 〈ilg m – 1, ilg m – 2, … , i0〉 and a\\ncolumn index j in binary by the lg n bits 〈jlg n – 1, jlg n – 2, … , j0〉.\\nSuppose that this matrix is a 2 × 2 block matrix, where each block has', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 349}),\n",
              " Document(page_content='m/2 rows and n/2 columns, and it is to be represented by a single array\\nwith 0-origin indexing. Show how to construct the binary representation\\nof the (lg m + lg n)-bit index into the single array from the binary\\nrepresentations of i and j.\\n10.1-2\\nUsing Figure 10.2 as a model, illustrate the result of each operation in\\nthe sequence PUSH(S, 4), PUSH(S, 1), PUSH(S, 3), POP(S), PUSH(S,\\n8), and POP(S) on an initially empty stack S stored in array S[1 : 6]\\n10.1-3\\nExplain how to implement two stacks in one array A[1 : n] in such a way\\nthat neither stack overﬂows unless the total number of elements in both\\nstacks together is n. The PUSH and POP operations should run in O(1)\\ntime.\\n10.1-4\\nUsing Figure 10.3 as a model, illustrate the result of each operation in\\nthe sequence ENQUEUE(Q, 4), ENQUEUE(Q, 1), ENQUEUE(Q, 3),\\nDEQUEUE(Q), ENQUEUE(Q, 8), and DEQUEUE(Q) on an initially\\nempty queue Q stored in array Q[1 : 6].\\n10.1-5\\nRewrite ENQUEUE and DEQUEUE to detect underﬂow and overﬂow\\nof a queue.\\n10.1-6\\nWhereas a stack allows insertion and deletion of elements at only one\\nend, and a queue allows insertion at one end and deletion at the other\\nend, a deque (double-ended queue, pronounced like “deck”) allows\\ninsertion and deletion at both ends. Write four O(1)-time procedures to\\ninsert elements into and delete elements from both ends of a deque\\nimplemented by an array.\\n10.1-7\\nShow how to implement a queue using two stacks. Analyze the running\\ntime of the queue operations.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 350}),\n",
              " Document(page_content='10.1-8\\nShow how to implement a stack using two queues. Analyze the running\\ntime of the stack operations.\\n10.2\\xa0\\xa0\\xa0\\xa0Linked lists\\nA linked list is a data structure in which the objects are arranged in a\\nlinear order. Unlike an array, however, in which the linear order is\\ndetermined by the array indices, the order in a linked list is determined\\nby a pointer in each object. Since the elements of linked lists often\\ncontain keys that can be searched for, linked lists are sometimes called\\nsearch lists. Linked lists provide a simple, ﬂexible representation for\\ndynamic sets, supporting (though not necessarily efﬁciently) all the\\noperations listed on page 250.\\nAs shown in Figure 10.4, each element of a doubly linked list\\xa0L is an\\nobject with an attribute key and two pointer attributes: next and prev.\\nThe object may also contain other satellite data. Given an element x in\\nthe list, x.next points to its successor in the linked list, and x.prev points\\nto its predecessor. If x.prev = NIL, the element x has no predecessor\\nand is therefore the ﬁrst element, or head, of the list. If x.next = NIL,\\nthe element x has no successor and is therefore the last element, or tail,\\nof the list. An attribute L.head points to the ﬁrst element of the list. If\\nL.head = NIL, the list is empty.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 351}),\n",
              " Document(page_content='Figure 10.4 (a) A doubly linked list L representing the dynamic set {1, 4, 9, 16}. Each element in\\nthe list is an object with attributes for the key and pointers (shown by arrows) to the next and\\nprevious objects. The next attribute of the tail and the prev attribute of the head are NIL,\\nindicated by a diagonal slash. The attribute L.head points to the head. (b) Following the\\nexecution of LIST-PREPEND(L, x), where x.key = 25, the linked list has an object with key 25\\nas the new head. This new object points to the old head with key 9. (c) The result of calling\\nLIST-INSERT(x, y), where x.key = 36 and y points to the object with key 9. (d) The result of\\nthe subsequent call LIST-DELETE(L, x), where x points to the object with key 4.\\nA list may have one of several forms. It may be either singly linked or\\ndoubly linked, it may be sorted or not, and it may be circular or not. If\\na list is singly linked, each element has a next pointer but not a prev\\npointer. If a list is sorted, the linear order of the list corresponds to the\\nlinear order of keys stored in elements of the list. The minimum element\\nis then the head of the list, and the maximum element is the tail. If the\\nlist is unsorted, the elements can appear in any order. In a circular list,\\nthe prev pointer of the head of the list points to the tail, and the next\\npointer of the tail of the list points to the head. You can think of a\\ncircular list as a ring of elements. In the remainder of this section, we\\nassume that the lists we are working with are unsorted and doubly\\nlinked.\\nSearching a linked list\\nThe procedure LIST-SEARCH(L, k) ﬁnds the ﬁrst element with key k\\nin list L by a simple linear search, returning a pointer to this element. If\\nno object with key k appears in the list, then the procedure returns NIL.\\nFor the linked list in Figure 10.4(a), the call LIST-SEARCH(L, 4)', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 352}),\n",
              " Document(page_content='returns a pointer to the third element, and the call LIST-SEARCH(L,\\n7) returns NIL. To search a list of n objects, the LIST-SEARCH\\nprocedure takes Θ (n) time in the worst case, since it may have to search\\nthe entire list.\\nLIST-SEARCH(L, k)\\n1x = L.head\\n2while\\xa0x ≠ NIL and x.key ≠ k\\n3x = x.next\\n4return\\xa0x\\nInserting into a linked list\\nGiven an element x whose key attribute has already been set, the LIST-\\nPREPEND procedure adds x to the front of the linked list, as shown in\\nFigure 10.4(b). (Recall that our attribute notation can cascade, so that\\nL.head.prev denotes the prev attribute of the object that L.head points\\nto.) The running time for LIST-PREPEND on a list of n elements is\\nO(1).\\nLIST-PREPEND(L, x)\\n1x.next = L.head\\n2x.prev = NIL\\n3if\\xa0L.head ≠ NIL\\n4L.head.prev = x\\n5L.head = x\\nYou can insert anywhere within a linked list. As Figure 10.4(c)\\nshows, if you have a pointer y to an object in the list, the LIST-INSERT\\nprocedure on the facing page “splices” a new element x into the list,\\nimmediately following y, in O(1) time. Since LIST-INSERT never\\nreferences the list object L, it is not supplied as a parameter.\\nLIST-INSERT(x, y)', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 353}),\n",
              " Document(page_content='1x.next = y.next\\n2x.prev = y\\n3if\\xa0y.next ≠ NIL\\n4y.next.prev = x\\n5y.next = x\\nDeleting from a linked list\\nThe procedure LIST-DELETE removes an element x from a linked list\\nL. It must be given a pointer to x, and it then “‘splices” x out of the list\\nby updating pointers. To delete an element with a given key, ﬁrst call\\nLIST-SEARCH to retrieve a pointer to the element. Figure 10.4(d)\\nshows how an element is deleted from a linked list. LIST-DELETE runs\\nin O(1) time, but to delete an element with a given key, the call to LIST-\\nSEARCH makes the worst-case running time be Θ (n).\\nLIST-DELETE(L, x)\\n1if\\xa0x.prev ≠ NIL\\n2x.prev.next = x.next\\n3else\\xa0L.head = x.next\\n4if\\xa0x.next ≠ NIL\\n5x.next.prev = x.prev\\nInsertion and deletion are faster operations on doubly linked lists\\nthan on arrays. If you want to insert a new ﬁrst element into an array or\\ndelete the ﬁrst element in an array, maintaining the relative order of all\\nthe existing elements, then each of the existing elements needs to be\\nmoved by one position. In the worst case, therefore, insertion and\\ndeletion take Θ (n) time in an array, compared with O(1) time for a\\ndoubly linked list. (Exercise 10.2-1 asks you to show that deleting an\\nelement from a singly linked list takes Θ (n) time in the worst case.) If,\\nhowever, you want to ﬁnd the kth element in the linear order, it takes\\njust O(1) time in an array regardless of k, but in a linked list, you’d have\\nto traverse k elements, taking Θ (k) time.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 354}),\n",
              " Document(page_content='Sentinels\\nThe code for LIST-DELETE is simpler if you ignore the boundary\\nconditions at the head and tail of the list:\\nFigure 10.5 A circular, doubly linked list with a sentinel. The sentinel L.nil, in blue, appears\\nbetween the head and tail. The attribute L.head is no longer needed, since the head of the list is\\nL.nil.next. (a) An empty list. (b) The linked list from Figure 10.4(a), with key 9 at the head and\\nkey 1 at the tail. (c) The list after executing LIST-INSERT′ (x, L.nil), where x.key = 25. The new\\nobject becomes the head of the list. (d) The list after deleting the object with key 1. The new tail\\nis the object with key 4. (e) The list after executing LIST-INSERT′ (x, y), where x.key = 36 and\\ny points to the object with key 9.\\nLIST-DELETE′ (x)\\n1x.prev.next = x.next\\n2x.next.prev = x.prev\\nA sentinel is a dummy object that allows us to simplify boundary\\nconditions. In a linked list L, the sentinel is an object L.nil that\\nrepresents NIL but has all the attributes of the other objects in the list.\\nReferences to NIL are replaced by references to the sentinel L.nil. As\\nshown in Figure 10.5, this change turns a regular doubly linked list into\\na circular, doubly linked list with a sentinel, in which the sentinel L.nil\\nlies between the head and tail. The attribute L.nil.next points to the\\nhead of the list, and L.nil.prev points to the tail. Similarly, both the next', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 355}),\n",
              " Document(page_content='attribute of the tail and the prev attribute of the head point to L.nil.\\nSince L.nil.next points to the head, the attribute L.head is eliminated\\naltogether, with references to it replaced by references to L.nil.next.\\nFigure 10.5(a) shows that an empty list consists of just the sentinel, and\\nboth L.nil.next and L.nil.prev point to L.nil.\\nTo delete an element from the list, just use the two-line procedure\\nLIST-DELETE′ from before. Just as LIST-INSERT never references\\nthe list object L, neither does LIST-DELETE′. You should never delete\\nthe sentinel L.nil unless you are deleting the entire list!\\nThe LIST-INSERT′ procedure inserts an element x into the list\\nfollowing object y. No separate procedure for prepending is necessary:\\nto insert at the head of the list, let y be L.nil; and to insert at the tail, let\\ny be L.nil.prev. Figure 10.5 shows the effects of LIST-INSERT′ and\\nLIST-DELETE′ on a sample list.\\nLIST-INSERT′ (x, y)\\n1x.next = y.next\\n2x.prev = y\\n3y.next.prev = x\\n4y.next = x\\nSearching a circular, doubly linked list with a sentinel has the same\\nasymptotic running time as without a sentinel, but it is possible to\\ndecrease the constant factor. The test in line 2 of LIST-SEARCH makes\\ntwo comparisons: one to check whether the search has run off the end\\nof the list and, if not, one to check whether the key resides in the current\\nelement x. Suppose that you know that the key is somewhere in the list.\\nThen you do not need to check whether the search runs off the end of\\nthe list, thereby eliminating one comparison in each iteration of the\\nwhile loop.\\nThe sentinel provides a place to put the key before starting the\\nsearch. The search starts at the head L.nil.next of list L, and it stops if it\\nﬁnds the key somewhere in the list. Now the search is guaranteed to ﬁnd\\nthe key, either in the sentinel or before reaching the sentinel. If the key is\\nfound before reaching the sentinel, then it really is in the element where', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 356}),\n",
              " Document(page_content='the search stops. If, however, the search goes through all the elements in\\nthe list and ﬁnds the key only in the sentinel, then the key is not really in\\nthe list, and the search returns NIL. The procedure LIST-SEARCH′\\nembodies this idea. (If your sentinel requires its key attribute to be NIL,\\nthen you might want to assign L.nil.key = NIL before line 5.)\\nLIST-SEARCH′ (L, k)\\n1L.nil.key = k// store the key in the sentinel to guarantee it is in list\\n2x = L.nil.next// start at the head of the list\\n3while\\xa0x.key ≠ k\\n4 x = x.next\\n5if\\xa0x == L.nil// found k in the sentinel\\n6 return\\xa0NIL//\\xa0k was not really in the list\\n7else return\\xa0x // found k in element x\\nSentinels often simplify code and, as in searching a linked list, they\\nmight speed up code by a small constant factor, but they don’t typically\\nimprove the asymptotic running time. Use them judiciously. When there\\nare many small lists, the extra storage used by their sentinels can\\nrepresent signiﬁcant wasted memory. In this book, we use sentinels only\\nwhen they signiﬁcantly simplify the code.\\nExercises\\n10.2-1\\nExplain why the dynamic-set operation INSERT on a singly linked list\\ncan be implemented in O(1) time, but the worst-case time for DELETE\\nis Θ(n).\\n10.2-2\\nImplement a stack using a singly linked list. The operations PUSH and\\nPOP should still take O(1) time. Do you need to add any attributes to\\nthe list?\\n10.2-3', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 357}),\n",
              " Document(page_content='Implement a queue using a singly linked list. The operations\\nENQUEUE and DEQUEUE should still take O(1) time. Do you need\\nto add any attributes to the list?\\n10.2-4\\nThe dynamic-set operation UNION takes two disjoint sets S1 and S2 as\\ninput, and it returns a set S = S1 ⋃ S2 consisting of all the elements of\\nS1 and S2. The sets S1 and S2 are usually destroyed by the operation.\\nShow how to support UNION in O(1) time using a suitable list data\\nstructure.\\n10.2-5\\nGive a Θ (n)-time nonrecursive procedure that reverses a singly linked\\nlist of n elements. The procedure should use no more than constant\\nstorage beyond that needed for the list itself.\\n★ 10.2-6\\nExplain how to implement doubly linked lists using only one pointer\\nvalue x.np per item instead of the usual two (next and prev). Assume\\nthat all pointer values can be interpreted as k-bit integers, and deﬁne\\nx.np = x.next XOR x.prev, the k-bit “exclusive-or” of x.next and x.prev.\\nThe value NIL is represented by 0. Be sure to describe what information\\nyou need to access the head of the list. Show how to implement the\\nSEARCH, INSERT, and DELETE operations on such a list. Also show\\nhow to reverse such a list in O(1) time.\\n10.3\\xa0\\xa0\\xa0\\xa0Representing rooted trees\\nLinked lists work well for representing linear relationships, but not all\\nrelationships are linear. In this section, we look speciﬁcally at the\\nproblem of representing rooted trees by linked data structures. We ﬁrst\\nlook at binary trees, and then we present a method for rooted trees in\\nwhich nodes can have an arbitrary number of children.\\nWe represent each node of a tree by an object. As with linked lists,\\nwe assume that each node contains a key attribute. The remaining', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 358}),\n",
              " Document(page_content='attributes of interest are pointers to other nodes, and they vary\\naccording to the type of tree.\\nBinary trees\\nFigure 10.6 shows how to use the attributes p, left, and right to store\\npointers to the parent, left child, and right child of each node in a\\nbinary tree T. If x.p = NIL, then x is the root. If node x has no left\\nchild, then x.left = NIL, and similarly for the right child. The root of\\nthe entire tree T is pointed to by the attribute T.root. If T.root = NIL,\\nthen the tree is empty.\\nRooted trees with unbounded branching\\nIt’s simple to extend the scheme for representing a binary tree to any\\nclass of trees in which the number of children of each node is at most\\nsome constant k: replace the left and right attributes by child1, child2, …\\n, childk. This scheme no longer works when the number of children of a\\nnode is unbounded, however, since we do not know how many\\nattributes to allocate in advance. Moreover, if k, the number of children,\\nis bounded by a large constant but most nodes have a small number of\\nchildren, we may waste a lot of memory.\\nFortunately, there is a clever scheme to represent trees with arbitrary\\nnumbers of children. It has the advantage of using only O(n) space for\\nany n-node rooted tree. The left-child, right-sibling representation\\nappears in Figure 10.7. As before, each node contains a parent pointer\\np, and T.root points to the root of tree T. Instead of having a pointer to\\neach of its children, however, each node x has only two pointers:\\n1. x.left-child points to the leftmost child of node x, and\\n2. x.right-sibling points to the sibling of x immediately to its right.\\nIf node x has no children, then x.left-child = NIL, and if node x is the\\nrightmost child of its parent, then x.right-sibling = NIL.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 359}),\n",
              " Document(page_content='Figure 10.6 The representation of a binary tree T. Each node x has the attributes x.p (top), x.left\\n(lower left), and x.right (lower right). The key attributes are not shown.\\nFigure 10.7 The left-child, right-sibling representation of a tree T. Each node x has attributes x.p\\n(top), x.left-child (lower left), and x.right-sibling (lower right). The key attributes are not shown.\\nOther tree representations', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 360}),\n",
              " Document(page_content='We sometimes represent rooted trees in other ways. In Chapter 6, for\\nexample, we represented a heap, which is based on a complete binary\\ntree, by a single array along with an attribute giving the index of the last\\nnode in the heap. The trees that appear in Chapter 19 are traversed only\\ntoward the root, and so only the parent pointers are present: there are\\nno pointers to children. Many other schemes are possible. Which\\nscheme is best depends on the application.\\nExercises\\n10.3-1\\nDraw the binary tree rooted at index 6 that is represented by the\\nfollowing attributes:\\nindexkeyleftright\\n1 178 9\\n2 14NILNIL\\n3 12NILNIL\\n4 2010NIL\\n5 332NIL\\n6 151 4\\n7 28NILNIL\\n8 22NILNIL\\n9 133 7\\n1025NIL5\\n10.3-2\\nWrite an O(n)-time recursive procedure that, given an n-node binary\\ntree, prints out the key of each node in the tree.\\n10.3-3\\nWrite an O(n)-time nonrecursive procedure that, given an n-node binary\\ntree, prints out the key of each node in the tree. Use a stack as an\\nauxiliary data structure.\\n10.3-4', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 361}),\n",
              " Document(page_content='Write an O(n)-time procedure that prints out all the keys of an arbitrary\\nrooted tree with n nodes, where the tree is stored using the left-child,\\nright-sibling representation.\\n★ 10.3-5\\nWrite an O(n)-time nonrecursive procedure that, given an n-node binary\\ntree, prints out the key of each node. Use no more than constant extra\\nspace outside of the tree itself and do not modify the tree, even\\ntemporarily, during the procedure.\\n★ 10.3-6\\nThe left-child, right-sibling representation of an arbitrary rooted tree\\nuses three pointers in each node: left-child, right-sibling, and parent.\\nFrom any node, its parent can be accessed in constant time and all its\\nchildren can be accessed in time linear in the number of children. Show\\nhow to use only two pointers and one boolean value in each node x so\\nthat x’s parent or all of x’s children can be accessed in time linear in the\\nnumber of x’s children.\\nProblems\\n10-1\\xa0\\xa0\\xa0\\xa0\\xa0C omparisons among lists\\nFor each of the four types of lists in the following table, what is the\\nasymptotic worst-case running time for each dynamic-set operation\\nlisted?\\n\\xa0unsorted,\\nsingly linkedsorted,\\nsingly\\nlinkedunsorted,\\ndoubly linkedsorted,\\ndoubly\\nlinked\\nSEARCH\\nINSERT\\nDELETE\\nSUCCESSOR\\nPREDECESSOR', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 362}),\n",
              " Document(page_content='MINIMUM\\nMAXIMUM\\n10-2\\xa0\\xa0\\xa0\\xa0\\xa0M ergeable heaps using linked lists\\nA mergeable heap supports the following operations: MAKE-HEAP\\n(which creates an empty mergeable heap), INSERT, MINIMUM,\\nEXTRACT-MIN, and UNION.1\\xa0Show how to implement mergeable\\nheaps using linked lists in each of the following cases. Try to make each\\noperation as efﬁcient as possible. Analyze the running time of each\\noperation in terms of the size of the dynamic set(s) being operated on.\\na. Lists are sorted.\\nb. Lists are unsorted.\\nc. Lists are unsorted, and dynamic sets to be merged are disjoint.\\n10-3\\xa0\\xa0\\xa0\\xa0\\xa0Searching a sorted compact list\\nWe can represent a singly linked list with two arrays, key and next.\\nGiven the index i of an element, its value is stored in key[i], and the\\nindex of its successor is given by next[i], where next[i] = NIL for the last\\nelement. We also need the index head of the ﬁrst element in the list. An\\nn-element list stored in this way is compact if it is stored only in\\npositions 1 through n of the key and next arrays.\\nLet’s assume that all keys are distinct and that the compact list is\\nalso sorted, that is, key[i] < key[next[i]] for all i = 1, 2, … , n such that\\nnext[i] ≠ NIL. Under these assumptions, you will show that the\\nrandomized algorithm COMPACT-LIST-SEARCH searches the list for\\nkey k in \\n  expected time.\\nCOMPACT-LIST-SEARCH(key, next, head, n, k)\\n\\xa0\\xa01i = head\\n\\xa0\\xa02while\\xa0i ≠ NIL and key[i] < k\\n\\xa0\\xa03j = RANDOM(1, n)\\n\\xa0\\xa04if\\xa0key[i] < key[j] and key[j] ≤ k\\n\\xa0\\xa05 i = j', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 363}),\n",
              " Document(page_content='\\xa0\\xa06 if\\xa0key[i] == k\\n\\xa0\\xa07 return\\xa0i\\n\\xa0\\xa08i = next[i]\\n\\xa0\\xa09if\\xa0i == NIL or key[i] > k\\n10return\\xa0NIL\\n11else return\\xa0i\\nIf you ignore lines 3–7 of the procedure, you can see that it’s an\\nordinary algorithm for searching a sorted linked list, in which index i\\npoints to each position of the list in turn. The search terminates once\\nthe index i “falls off” the end of the list or once key[i] ≥ k. In the latter\\ncase, if key[i] = k, the procedure has found a key with the value k. If,\\nhowever, key[i] > k, then the search will never ﬁnd a key with the value\\nk, so that terminating the search was the correct action.\\nLines 3–7 attempt to skip ahead to a randomly chosen position j.\\nSuch a skip helps if key[j] is larger than key[i] and no larger than k. In\\nsuch a case, j marks a position in the list that i would reach during an\\nordinary list search. Because the list is compact, we know that any\\nchoice of j between 1 and n indexes some element in the list.\\nInstead of analyzing the performance of COMPACT-LIST-\\nSEARCH directly, you will analyze a related algorithm, COMPACT-\\nLIST-SEARCH, which executes two separate loops. This algorithm\\ntakes an additional parameter t, which speciﬁes an upper bound on the\\nnumber of iterations of the ﬁrst loop.\\nCOMPACT-LIST-SEARCH′ (key, next, head, n, k, t)\\n\\xa0\\xa01i = head\\n\\xa0\\xa02for\\xa0q = 1 to\\xa0t\\n\\xa0\\xa03j = RANDOM(1, n)\\n\\xa0\\xa04if\\xa0key[i] < key[j] and key[j] ≤ k\\n\\xa0\\xa05 i = j\\n\\xa0\\xa06 if\\xa0key[i] == k\\n\\xa0\\xa07 return\\xa0i\\n\\xa0\\xa08while\\xa0i ≠ NIL and key[i] < k\\n\\xa0\\xa09i = next[i]', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 364}),\n",
              " Document(page_content='10if\\xa0i == NIL or key[i] > k\\n11return\\xa0NIL\\n12else return\\xa0i\\nTo compare the execution of the two algorithms, assume that the\\nsequence of calls of RANDOM(1, n) yields the same sequence of\\nintegers for both algorithms.\\na. Argue that for any value of t, COMPACT-LIST-SEARCH(key, next,\\nhead, n, k) and COMPACT-LIST-SEARCH′ (key, next, head, n, k, t)\\nreturn the same result and that the number of iterations of the while\\nloop of lines 2–8 in COMPACT-LIST-SEARCH is at most the total\\nnumber of iterations of both the for and while loops in COMPACT-\\nLIST-SEARCH′.\\nIn the call COMPACT-LIST-SEARCH′ (key, next, head, n, k, t), let Xt\\nbe the random variable that describes the distance in the linked list (that\\nis, through the chain of next pointers) from position i to the desired key\\nk after t iterations of the for loop of lines 2–7 have occurred.\\nb. Argue that COMPACT-LIST-SEARCH′ (key, next, head, n, k, t) has\\nan expected running time of O(t + E [Xt]).\\nc. Show that \\n . (Hint: Use equation (C.28) on page\\n1193.)\\nd. Show that \\n . (Hint: Use inequality (A.18) on page\\n1150.)\\ne. Prove that E [Xt] ≤ n/(t + 1).\\nf. Show that COMPACT-LIST-SEARCH′ (key, next, head, n, k, t) has\\nan expected running time of O(t + n/t).\\ng. Conclude that COMPACT-LIST-SEARCH runs in \\n  expected\\ntime.\\nh. Why do we assume that all keys are distinct in COMPACT-LIST-\\nSEARCH? Argue that random skips do not necessarily help', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 365}),\n",
              " Document(page_content='asymptotically when the list contains repeated key values.\\nChapter notes\\nAho, Hopcroft, and Ullman [6] and Knuth [259] are excellent references\\nfor elementary data structures. Many other texts cover both basic data\\nstructures and their implementation in a particular programming\\nlanguage. Examples of these types of textbooks include Goodrich and\\nTamassia [196], Main [311], Shaffer [406], and Weiss [452, 453, 454]. The\\nbook by Gonnet and Baeza-Yates [193] provides experimental data on\\nthe performance of many data-structure operations.\\nThe origin of stacks and queues as data structures in computer\\nscience is unclear, since corresponding notions already existed in\\nmathematics and paper-based business practices before the introduction\\nof digital computers. Knuth [259] cites A. M. Turing for the\\ndevelopment of stacks for subroutine linkage in 1947.\\nPointer-based data structures also seem to be a folk invention.\\nAccording to Knuth, pointers were apparently used in early computers\\nwith drum memories. The A-1 language developed by G. M. Hopper in\\n1951 represented algebraic formulas as binary trees. Knuth credits the\\nIPL-II language, developed in 1956 by A. Newell, J. C. Shaw, and H. A.\\nSimon, for recognizing the importance and promoting the use of\\npointers. Their IPL-III language, developed in 1957,  included explicit\\nstack operations.\\n1 Because we have deﬁned a mergeable heap to support MINIMUM and EXTRACT-MIN, we\\ncan also refer to it as a mergeable min-heap. Alternatively, if it supports MAXIMUM and\\nEXTRACT-MAX, it is a mergeable max-heap.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 366}),\n",
              " Document(page_content='11\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0Hash Tables\\nMany applications require a dynamic set that supports only the\\ndictionary operations INSERT, SEARCH, and DELETE. For example,\\na compiler that translates a programming language maintains a symbol\\ntable, in which the keys of elements are arbitrary character strings\\ncorresponding to identiﬁers in the language. A hash table is an effective\\ndata structure for implementing dictionaries. Although searching for an\\nelement in a hash table can take as long as searching for an element in a\\nlinked list— Θ (n) time in the worst case—in practice, hashing performs\\nextremely well. Under reasonable assumptions, the average time to\\nsearch for an element in a hash table is O(1). Indeed, the built-in\\ndictionaries of Python are implemented with hash tables.\\nA hash table generalizes the simpler notion of an ordinary array.\\nDirectly addressing into an ordinary array takes advantage of the O(1)\\naccess time for any array element. Section 11.1 discusses direct\\naddressing in more detail. To use direct addressing, you must be able to\\nallocate an array that contains a position for every possible key.\\nWhen the number of keys actually stored is small relative to the total\\nnumber of possible keys, hash tables become an effective alternative to\\ndirectly addressing an array, since a hash table typically uses an array of\\nsize proportional to the number of keys actually stored. Instead of using\\nthe key as an array index directly, we compute the array index from the\\nkey. Section 11.2 presents the main ideas, focusing on “chaining” as a\\nway to handle “collisions,” in which more than one key maps to the\\nsame array index. Section 11.3 describes how to compute array indices\\nfrom keys using hash functions. We present and analyze several', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 367}),\n",
              " Document(page_content='variations on the basic theme. Section 11.4 looks at “open addressing,”\\nwhich is another way to deal with collisions. The bottom line is that\\nhashing is an extremely effective and practical technique: the basic\\ndictionary operations require only O(1) time on the average. Section\\n11.5 discusses the hierarchical memory systems of modern computer\\nsystems have and illustrates how to design hash tables that work well in\\nsuch systems.\\n11.1\\xa0\\xa0\\xa0\\xa0Direct-address tables\\nDirect addressing is a simple technique that works well when the\\nuniverse U of keys is reasonably small. Suppose that an application\\nneeds a dynamic set in which each element has a distinct key drawn\\nfrom the universe U = {0, 1, …, m − 1}, where m is not too large.\\nTo represent the dynamic set, you can use an array, or direct-address\\ntable, denoted by T[0 : m − 1], in which each position, or slot,\\ncorresponds to a key in the universe U. Figure 11.1 illustrates this\\napproach. Slot k points to an element in the set with key k. If the set\\ncontains no element with key k, then T[k] = NIL.\\nThe dictionary operations DIRECT-ADDRESS-SEARCH,\\nDIRECT-ADDRESS-INSERT, and DIRECT-ADDRESS-DELETE\\non the following page are trivial to implement. Each takes only O(1)\\ntime.\\nFor some applications, the direct-address table itself can hold the\\nelements in the dynamic set. That is, rather than storing an element’s\\nkey and satellite data in an object external to the direct-address table,\\nwith a pointer from a slot in the table to the object, save space by\\nstoring the object directly in the slot. To indicate an empty slot, use a\\nspecial key. Then again, why store the key of the object at all? The index\\nof the object is its key! Of course, then you’d need some way to tell\\nwhether slots are empty.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 368}),\n",
              " Document(page_content='Figure 11.1 How to implement a dynamic set by a direct-address table T. Each key in the\\nuniverse U = {0, 1, …, 9} corresponds to an index into the table. The set K = {2, 3, 5, 8} of\\nactual keys determines the slots in the table that contain pointers to elements. The other slots, in\\nblue, contain NIL.\\nDIRECT-ADDRESS-SEARCH(T, k)\\n1return\\xa0T[k]\\nDIRECT-ADDRESS-INSERT(T, x)\\n1T[x.key] = x\\nDIRECT-ADDRESS-DELETE(T, x)\\n1T[x.key] = NIL\\nExercises\\n11.1-1\\nA dynamic set S is represented by a direct-address table T of length m.\\nDescribe a procedure that ﬁnds the maximum element of S. What is the\\nworst-case performance of your procedure?\\n11.1-2\\nA bit vector is simply an array of bits (each either 0 or 1). A bit vector of\\nlength m takes much less space than an array of m pointers. Describe', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 369}),\n",
              " Document(page_content='how to use a bit vector to represent a dynamic set of distinct elements\\ndrawn from the set {0, 1, …, m − 1} and with no satellite data.\\nDictionary operations should run in O(1) time.\\n11.1-3\\nSuggest how to implement a direct-address table in which the keys of\\nstored elements do not need to be distinct and the elements can have\\nsatellite data. All three dictionary operations (INSERT, DELETE, and\\nSEARCH) should run in O(1) time. (Don’t forget that DELETE takes\\nas an argument a pointer to an object to be deleted, not a key.)\\n★ 11.1-4\\nSuppose that you want to implement a dictionary by using direct\\naddressing on a huge array. That is, if the array size is m and the\\ndictionary contains at most n elements at any one time, then m ≫ n. At\\nthe start, the array entries may contain garbage, and initializing the\\nentire array is impractical because of its size. Describe a scheme for\\nimplementing a direct-address dictionary on a huge array. Each stored\\nobject should use O(1) space; the operations SEARCH, INSERT, and\\nDELETE should take O(1) time each; and initializing the data structure\\nshould take O(1) time. (Hint: Use an additional array, treated somewhat\\nlike a stack whose size is the number of keys actually stored in the\\ndictionary, to help determine whether a given entry in the huge array is\\nvalid or not.)\\n11.2\\xa0\\xa0\\xa0\\xa0Hash tables\\nThe downside of direct addressing is apparent: if the universe U is large\\nor inﬁnite, storing a table T of size |U| may be impractical, or even\\nimpossible, given the memory available on a typical computer.\\nFurthermore, the set K of keys actually stored may be so small relative\\nto U that most of the space allocated for T would be wasted.\\nWhen the set K of keys stored in a dictionary is much smaller than\\nthe universe U of all possible keys, a hash table requires much less\\nstorage than a direct-address table. Speciﬁcally, the storage requirement', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 370}),\n",
              " Document(page_content='reduces to Θ (|K|) while maintaining the beneﬁt that searching for an\\nelement in the hash table still requires only O(1) time. The catch is that\\nthis bound is for the average-case time,1 whereas for direct addressing it\\nholds for the worst-case time.\\nWith direct addressing, an element with key k is stored in slot k, but\\nwith hashing, we use a hash function\\xa0h to compute the slot number from\\nthe key k, so that the element goes into slot h(k). The hash function h\\nmaps the universe U of keys into the slots of a hash table\\xa0T[0 : m − 1]:\\nh : U → {0, 1, …, m − 1},\\nwhere the size m of the hash table is typically much less than |U|. We say\\nthat an element with key k\\xa0hashes to slot h(k), and we also say that h(k)\\nis the hash value of key k. Figure 11.2 illustrates the basic idea. The hash\\nfunction reduces the range of array indices and hence the size of the\\narray. Instead of a size of |U|, the array can have size m. An example of a\\nsimple, but not particularly good, hash function is h(k) = k mod m.\\nThere is one hitch, namely that two keys may hash to the same slot.\\nWe call this situation a collision. Fortunately, there are effective\\ntechniques for resolving the conﬂict created by collisions.\\nOf course, the ideal solution is to avoid collisions altogether. We\\nmight try to achieve this goal by choosing a suitable hash function h.\\nOne idea is to make h appear to be “random,” thus avoiding collisions\\nor at least minimizing their number. The very term “to hash,” evoking\\nimages of random mixing and chopping, captures the spirit of this\\napproach. (Of course, a hash function h must be deterministic in that a\\ngiven input k must always produce the same output h(k).) Because |U| >\\nm, however, there must be at least two keys that have the same hash\\nvalue, and avoiding collisions altogether is impossible. Thus, although a\\nwell-designed, “random”-looking hash function can reduce the number\\nof collisions, we still need a method for resolving the collisions that do\\noccur.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 371}),\n",
              " Document(page_content='Figure 11.2 Using a hash function h to map keys to hash-table slots. Because keys k2 and k5\\nmap to the same slot, they collide.\\nThe remainder of this section ﬁrst presents a deﬁnition of\\n“independent uniform hashing,” which captures the simplest notion of\\nwhat it means for a hash function to be “random.” It then presents and\\nanalyzes the simplest collision resolution technique, called chaining.\\nSection 11.4 introduces an alternative method for resolving collisions,\\ncalled open addressing.\\nIndependent uniform hashing\\nAn “ideal” hashing function h would have, for each possible input k in\\nthe domain U, an output h(k) that is an element randomly and\\nindependently chosen uniformly from the range {0, 1, …, m − 1}. Once\\na value h(k) is randomly chosen, each subsequent call to h with the same\\ninput k yields the same output h(k).\\nWe call such an ideal hash function an independent uniform hash\\nfunction. Such a function is also often called a random oracle [43]. When\\nhash tables are implemented with an independent uniform hash\\nfunction, we say we are using independent uniform hashing.\\nIndependent uniform hashing is an ideal theoretical abstraction, but\\nit is not something that can reasonably be implemented in practice.\\nNonetheless, we’ll analyze the efﬁciency of hashing under the', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 372}),\n",
              " Document(page_content='assumption of independent uniform hashing and then present ways of\\nachieving useful practical approximations to this ideal.\\nFigure 11.3 Collision resolution by chaining. Each nonempty hash-table slot T[j] points to a\\nlinked list of all the keys whose hash value is j. For example, h(k1) = h(k4) and h(k5) = h(k2) =\\nh(k7). The list can be either singly or doubly linked. We show it as doubly linked because\\ndeletion may be faster that way when the deletion procedure knows which list element (not just\\nwhich key) is to be deleted.\\nCollision resolution by chaining\\nAt a high level, you can think of hashing with chaining as a\\nnonrecursive form of divide-and-conquer: the input set of n elements is\\ndivided randomly into m subsets, each of approximate size n/m. A hash\\nfunction determines which subset an element belongs to. Each subset is\\nmanaged independently as a list.\\nFigure 11.3 shows the idea behind chaining: each nonempty slot\\npoints to a linked list, and all the elements that hash to the same slot go\\ninto that slot’s linked list. Slot j contains a pointer to the head of the list\\nof all stored elements with hash value j. If there are no such elements,\\nthen slot j contains NIL.\\nWhen collisions are resolved by chaining, the dictionary operations\\nare straightforward to implement. They appear on the next page and\\nuse the linked-list procedures from Section 10.2. The worst-case running\\ntime for insertion is O(1). The insertion procedure is fast in part because', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 373}),\n",
              " Document(page_content='it assumes that the element x being inserted is not already present in the\\ntable. To enforce this assumption, you can search (at additional cost) for\\nan element whose key is x.key before inserting. For searching, the worst-\\ncase running time is proportional to the length of the list. (We’ll analyze\\nthis operation more closely below.) Deletion takes O(1) time if the lists\\nare doubly linked, as in Figure 11.3. (Since CHAINED-HASH-\\nDELETE takes as input an element x and not its key k, no search is\\nneeded. If the hash table supports deletion, then its linked lists should\\nbe doubly linked in order to delete an item quickly. If the lists were only\\nsingly linked, then by Exercise 10.2-1, deletion could take time\\nproportional to the length of the list. With singly linked lists, both\\ndeletion and searching would have the same asymptotic running times.)\\nCHAINED-HASH-INSERT(T, x)\\n1LIST-PREPEND(T[h(x.key)], x)\\nCHAINED-HASH-SEARCH(T, k)\\n1return LIST-SEARCH(T[h(k)], k)\\nCHAINED-HASH-DELETE(T, x)\\n1LIST-DELETE(T[h(x.key)], x)\\nAnalysis of hashing with chaining\\nHow well does hashing with chaining perform? In particular, how long\\ndoes it take to search for an element with a given key?\\nGiven a hash table T with m slots that stores n elements, we deﬁne\\nthe load factor\\xa0 α for T as n/m, that is, the average number of elements\\nstored in a chain. Our analysis will be in terms of α, which can be less\\nthan, equal to, or greater than 1.\\nThe worst-case behavior of hashing with chaining is terrible: all n\\nkeys hash to the same slot, creating a list of length n. The worst-case\\ntime for searching is thus Θ (n) plus the time to compute the hash\\nfunction—no better than using one linked list for all the elements. We\\nclearly don’t use hash tables for their worst-case performance.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 374}),\n",
              " Document(page_content='The average-case performance of hashing depends on how well the\\nhash function h distributes the set of keys to be stored among the m\\nslots, on the average (meaning with respect to the distribution of keys to\\nbe hashed and with respect to the choice of hash function, if this choice\\nis randomized). Section 11.3 discusses these issues, but for now we\\nassume that any given element is equally likely to hash into any of the m\\nslots. That is, the hash function is uniform. We further assume that\\nwhere a given element hashes to is independent of where any other\\nelements hash to. In other words, we assume that we are using\\nindependent uniform hashing.\\nBecause hashes of distinct keys are assumed to be independent,\\nindependent uniform hashing is universal: the chance that any two\\ndistinct keys k1 and k2 collide is at most 1/m. Universality is important\\nin our analysis and also in the speciﬁcation of universal families of hash\\nfunctions, which we’ll see in Section 11.3.2.\\nFor j = 0, 1, …, m − 1, denote the length of the list T[j] by nj, so that\\nand the expected value of nj is E[nj] = α = n/m.\\nWe assume that O(1) time sufﬁces to compute the hash value h(k), so\\nthat the time required to search for an element with key k depends\\nlinearly on the length nh(k) of the list T[h(k)]. Setting aside the O(1)\\ntime required to compute the hash function and to access slot h(k), we’ll\\nconsider the expected number of elements examined by the search\\nalgorithm, that is, the number of elements in the list T[h(k)] that the\\nalgorithm checks to see whether any have a key equal to k. We consider\\ntwo cases. In the ﬁrst, the search is unsuccessful: no element in the table\\nhas key k. In the second, the search successfully ﬁnds an element with\\nkey k.\\nTheorem 11.1\\nIn a hash table in which collisions are resolved by chaining, an\\nunsuccessful search takes Θ (1 + α) time on average, under the\\nassumption of independent uniform hashing.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 375}),\n",
              " Document(page_content='Proof\\xa0\\xa0\\xa0Under the assumption of independent uniform hashing, any key\\nk not already stored in the table is equally likely to hash to any of the m\\nslots. The expected time to search unsuccessfully for a key k is the\\nexpected time to search to the end of list T[h(k)], which has expected\\nlength E[nh(k)] = α. Thus, the expected number of elements examined in\\nan unsuccessful search is α, and the total time required (including the\\ntime for computing h(k)) is Θ (1 + α).\\n▪\\nThe situation for a successful search is slightly different. An\\nunsuccessful search is equally likely to go to any slot of the hash table. A\\nsuccessful search, however, cannot go to an empty slot, since it is for an\\nelement that is present in one of the linked lists. We assume that the\\nelement searched for is equally likely to be any one of the elements in\\nthe table, so the longer the list, the more likely that the search is for one\\nof its elements. Even so, the expected search time still turns out to be\\nΘ(1 + α).\\nTheorem 11.2\\nIn a hash table in which collisions are resolved by chaining, a successful\\nsearch takes Θ (1 + α) time on average, under the assumption of\\nindependent uniform hashing.\\nProof\\xa0\\xa0\\xa0We assume that the element being searched for is equally likely\\nto be any of the n elements stored in the table. The number of elements\\nexamined during a successful search for an element x is 1 more than the\\nnumber of elements that appear before x in x’s list. Because new\\nelements are placed at the front of the list, elements before x in the list\\nwere all inserted after x was inserted. Let xi denote the ith element\\ninserted into the table, for i = 1, 2, …, n, and let ki = xi.key.\\nOur analysis uses indicator random variables extensively. For each\\nslot q in the table and for each pair of distinct keys ki and kj, we deﬁne\\nthe indicator random variable\\nXijq = I {the search is for xi, h(ki) = q, and h(kj) = q}.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 376}),\n",
              " Document(page_content='That is, Xijq = 1 when keys ki and kj collide at slot q and the search is\\nfor element xi. Because Pr{the search is for xi} = 1/n, Pr{h(ki) = q} =\\n1/m, Pr{h(kj) = q} = 1/m, and these events are all independent, we have\\nthat Pr{Xijq = 1} = 1/nm2. Lemma 5.1 on page 130 gives E[Xijq] =\\n1/nm2.\\nNext, we deﬁne, for each element xj, the indicator random variable\\nYj=I {xj appears in a list prior to the element being searched for}\\n=\\n,\\nsince at most one of the Xijq equals 1, namely when the element xi being\\nsearched for belongs to the same list as xj (pointed to by slot q), and i <\\nj (so that xi appears after xj in the list).\\nOur ﬁnal random variable is Z, which counts how many elements\\nappear in the list prior to the element being searched for:\\nBecause we must count the element being searched for as well as all\\nthose preceding it in its list, we wish to compute E[Z + 1]. Using\\nlinearity of expectation (equation (C.24) on page 1192) , we have\\n', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 377}),\n",
              " Document(page_content='Thus, the total time required for a successful search (including the time\\nfor computing the hash function) is Θ (2 + α/2 − α/2n) = Θ (1 + α).\\n▪\\nWhat does this analysis mean? If the number of elements in the table\\nis at most proportional to the number of hash-table slots, we have n =\\nO(m) and, consequently, α = n/m = O(m)/m = O(1). Thus, searching\\ntakes constant time on average. Since insertion takes O(1) worst-case\\ntime and deletion takes O(1) worst-case time when the lists are doubly\\nlinked (assuming that the list element to be deleted is known, and not\\njust its key), we can support all dictionary operations in O(1) time on\\naverage.\\nThe analysis in the preceding two theorems depends only on two\\nessential properties of independent uniform hashing: uniformity (each\\nkey is equally likely to hash to any one of the m slots), and\\nindependence (so any two distinct keys collide with probability 1/m).\\nExercises\\n11.2-1\\nYou use a hash function h to hash n distinct keys into an array T of\\nlength m. Assuming independent uniform hashing, what is the expected\\nnumber of collisions? More precisely, what is the expected cardinality of\\n{{k1, k2} : k1 ≠ k2 and h(k1) = h(k2)}?\\n11.2-2\\nConsider a hash table with 9 slots and the hash function h(k) = k mod 9.\\nDemonstrate what happens upon inserting the keys 5, 28, 19, 15, 20, 33,\\n12, 17, 10 with collisions resolved by chaining.\\n11.2-3\\nProfessor Marley hypothesizes that he can obtain substantial\\nperformance gains by modifying the chaining scheme to keep each list\\nin sorted order. How does the professor’s modiﬁcation affect the\\nrunning time for successful searches, unsuccessful searches, insertions,\\nand deletions?', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 378}),\n",
              " Document(page_content='11.2-4\\nSuggest how to allocate and deallocate storage for elements within the\\nhash table itself by creating a “free list”: a linked list of all the unused\\nslots. Assume that one slot can store a ﬂag and either one element plus a\\npointer or two pointers. All dictionary and free-list operations should\\nrun in O(1) expected time. Does the free list need to be doubly linked, or\\ndoes a singly linked free list sufﬁce?\\n11.2-5\\nYou need to store a set of n keys in a hash table of size m. Show that if\\nthe keys are drawn from a universe U with |U| > (n − 1)m, then U has a\\nsubset of size n consisting of keys that all hash to the same slot, so that\\nthe worst-case searching time for hashing with chaining is Θ (n).\\n11.2-6\\nYou have stored n keys in a hash table of size m, with collisions resolved\\nby chaining, and you know the length of each chain, including the\\nlength L of the longest chain. Describe a procedure that selects a key\\nuniformly at random from among the keys in the hash table and returns\\nit in expected time O(L · (1 + 1/ α)).\\n11.3\\xa0\\xa0\\xa0\\xa0Hash functions\\nFor hashing to work well, it needs a good hash function. Along with\\nbeing efﬁciently computable, what properties does a good hash function\\nhave? How do you design good hash functions?\\nThis section ﬁrst attempts to answer these questions based on two ad\\nhoc approaches for creating hash functions: hashing by division and\\nhashing by multiplication. Although these methods work well for some\\nsets of input keys, they are limited because they try to provide a single\\nﬁxed hash function that works well on any data—an  approach called\\nstatic hashing.\\nWe then see that provably good average-case performance for any\\ndata can be obtained by designing a suitable family of hash functions\\nand choosing a hash function at random from this family at runtime,\\nindependent of the data to be hashed. The approach we examine is', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 379}),\n",
              " Document(page_content='called random hashing. A particular kind of random hashing, universal\\nhashing, works well. As we saw with quicksort in Chapter 7,\\nrandomization is a powerful algorithmic design tool.\\nWhat makes a good hash function?\\nA good hash function satisﬁes (approximately) the assumption of\\nindependent uniform hashing: each key is equally likely to hash to any\\nof the m slots, independently of where any other keys have hashed to.\\nWhat does “equally likely” mean here? If the hash function is ﬁxed, any\\nprobabilities would have to be based on the probability distribution of\\nthe input keys.\\nUnfortunately, you typically have no way to check this condition,\\nunless you happen to know the probability distribution from which the\\nkeys are drawn. Moreover, the keys might not be drawn independently.\\nOccasionally you might know the distribution. For example, if you\\nknow that the keys are random real numbers k independently and\\nuniformly distributed in the range 0 ≤ k < 1, then the hash function\\nh(k) = ⌊km ⌋\\nsatisﬁes the condition of independent uniform hashing.\\nA good static hashing approach derives the hash value in a way that\\nyou expect to be independent of any patterns that might exist in the\\ndata. For example, the “division method” (discussed in Section 11.3.1)\\ncomputes the hash value as the remainder when the key is divided by a\\nspeciﬁed prime number. This method may give good results, if you\\n(somehow) choose a prime number that is unrelated to any patterns in\\nthe distribution of keys.\\nRandom hashing, described in Section 11.3.2, picks the hash\\nfunction to be used at random from a suitable family of hashing\\nfunctions. This approach removes any need to know anything about the\\nprobability distribution of the input keys, as the randomization\\nnecessary for good average-case behavior then comes from the (known)\\nrandom process used to pick the hash function from the family of hash\\nfunctions, rather than from the (unknown) process used to create the\\ninput keys. We recommend that you use random hashing.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 380}),\n",
              " Document(page_content='Keys are integers, vectors, or strings\\nIn practice, a hash function is designed to handle keys that are one of\\nthe following two types:\\nA short nonnegative integer that ﬁts in a w-bit machine word.\\nTypical values for w would be 32 or 64.\\nA short vector of nonnegative integers, each of bounded size. For\\nexample, each element might be an 8-bit byte, in which case the\\nvector is often called a (byte) string. The vector might be of\\nvariable length.\\nTo begin, we assume that keys are short nonnegative integers. Handling\\nvector keys is more complicated and discussed in Sections 11.3.5 and\\n11.5.2.\\n11.3.1\\xa0\\xa0\\xa0\\xa0Static hashing\\nStatic hashing uses a single, ﬁxed hash function. The only\\nrandomization available is through the (usually unknown) distribution\\nof input keys. This section discusses two standard approaches for static\\nhashing: the division method and the multiplication method. Although\\nstatic hashing is no longer recommended, the multiplication method\\nalso provides a good foundation for “nonstatic” hashing—b etter known\\nas random hashing—where the hash function is chosen at random from\\na suitable family of hash functions.\\nThe division method\\nThe division method for creating hash functions maps a key k into one of\\nm slots by taking the remainder of k divided by m. That is, the hash\\nfunction is\\nh(k) = k mod m.\\nFor example, if the hash table has size m = 12 and the key is k = 100,\\nthen h(k) = 4. Since it requires only a single division operation, hashing\\nby division is quite fast.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 381}),\n",
              " Document(page_content='The division method may work well when m is a prime not too close\\nto an exact power of 2. There is no guarantee that this method provides\\ngood average-case performance, however, and it may complicate\\napplications since it constrains the size of the hash tables to be prime.\\nThe multiplication method\\nThe general multiplication method for creating hash functions operates\\nin two steps. First, multiply the key k by a constant A in the range 0 < A\\n< 1 and extract the fractional part of kA. Then, multiply this value by m\\nand take the ﬂoor of the result. That is, the hash function is\\nh(k) = ⌊m (kA mod 1) ⌋,\\nwhere “kA mod 1” means the fractional part of kA, that is, kA − ⌊kA ⌋.\\nThe general multiplication method has the advantage that the value of\\nm is not critical and you can choose it independently of how you choose\\nthe multiplicative constant A.\\nFigure 11.4 The multiply-shift method to compute a hash function. The w-bit representation of\\nthe key k is multiplied by the w-bit value a = A · 2w. The ℓ highest-order bits of the lower w-bit\\nhalf of the product form the desired hash value ha(k).\\nThe multiply-shift method\\nIn practice, the multiplication method is best in the special case where\\nthe number m of hash-table slots is an exact power of 2, so that m = 2ℓ\\nfor some integer ℓ, where ℓ ≤ w and w is the number of bits in a machine', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 382}),\n",
              " Document(page_content='word. If you choose a ﬁxed w-bit positive integer a = A 2w, where 0 < A\\n< 1 as in the multiplication method so that a is in the range 0 < a < 2w,\\nyou can implement the function on most computers as follows. We\\nassume that a key k ﬁts into a single w-bit word.\\nReferring to Figure 11.4, ﬁrst multiply k by the w-bit integer a. The\\nresult is a 2w-bit value r12w + r0, where r1 is the high-order w-bit word\\nof the product and r0 is the low-order w-bit word of the product. The\\ndesired ℓ-bit hash value consists of the ℓ most signiﬁcant bits of r0.\\n(Since r1 is ignored, the hash function can be implemented on a\\ncomputer that produces only a w-bit product given two w-bit inputs,\\nthat is, where the multiplication operation computes modulo 2w.)\\nIn other words, you deﬁne the hash function h = ha, where\\nfor a ﬁxed nonzero w-bit value a. Since the product ka of two w-bit\\nwords occupies 2w bits, taking this product modulo 2w zeroes out the\\nhigh-order w bits (r1), leaving only the low-order w bits (r0). The ⋙\\noperator performs a logical right shift by w − ℓ bits, shifting zeros into\\nthe vacated positions on the left, so that the ℓ most signiﬁcant bits of r0\\nmove into the ℓ rightmost positions. (It’s the same as dividing by 2w−ℓ\\nand taking the ﬂoor of the result.) The resulting value equals the ℓ most\\nsigniﬁcant bits of r0. The hash function ha can be implemented with\\nthree machine instructions: multiplication, subtraction, and logical right\\nshift.\\nAs an example, suppose that k = 123456, ℓ = 14, m = 214 = 16384,\\nand w = 32. Suppose further that we choose a = 2654435769 (following\\na suggestion of Knuth [261]). Then ka = 327706022297664 = (76300 ·\\n232) + 17612864, and so r1 = 76300 and r0 = 17612864. The 14 most\\nsigniﬁcant bits of r0 yield the value ha(k) = 67.\\nEven though the multiply-shift method is fast, it doesn’t provide any\\nguarantee of good average-case performance. The universal hashing', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 383}),\n",
              " Document(page_content='approach presented in the next section provides such a guarantee. A\\nsimple randomized variant of the multiply-shift method works well on\\nthe average, when the program begins by picking a as a randomly\\nchosen odd integer.\\n11.3.2\\xa0\\xa0\\xa0\\xa0Random hashing\\nSuppose that a malicious adversary chooses the keys to be hashed by\\nsome ﬁxed hash function. Then the adversary can choose n keys that all\\nhash to the same slot, yielding an average retrieval time of Θ (n). Any\\nstatic hash function is vulnerable to such terrible worst-case behavior.\\nThe only effective way to improve the situation is to choose the hash\\nfunction randomly in a way that is independent of the keys that are\\nactually going to be stored. This approach is called random hashing. A\\nspecial case of this approach, called universal hashing, can yield provably\\ngood performance on average when collisions are handled by chaining,\\nno matter which keys the adversary chooses.\\nTo use random hashing, at the beginning of program execution you\\nselect the hash function at random from a suitable family of functions.\\nAs in the case of quicksort, randomization guarantees that no single\\ninput always evokes worst-case behavior. Because you randomly select\\nthe hash function, the algorithm can behave differently on each\\nexecution, even for the same set of keys to be hashed, guaranteeing\\ngood average-case performance.\\nLet H be a ﬁnite family of hash functions that map a given universe\\nU of keys into the range {0, 1, …, m − 1}. Such a family is said to be\\nuniversal if for each pair of distinct keys k1, k2 ∈ U, the number of hash\\nfunctions h ∈ H for which h(k1) = h(k2) is at most | H|/m. In other\\nwords, with a hash function randomly chosen from H, the chance of a\\ncollision between distinct keys k1 and k2 is no more than the chance\\n1/m of a collision if h(k1) and h(k2) were randomly and independently\\nchosen from the set {0, 1, …, m − 1}.\\nIndependent uniform hashing is the same as picking a hash function\\nuniformly at random from a family of mn hash functions, each member', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 384}),\n",
              " Document(page_content='of that family mapping the n keys to the m hash values in a different\\nway.\\nEvery independent uniform random family of hash function is\\nuniversal, but the converse need not be true: consider the case where U\\n= {0, 1, …, m − 1} and the only hash function in the family is the\\nidentity function. The probability that two distinct keys collide is zero,\\neven though each key is hashes to a ﬁxed value.\\nThe following corollary to Theorem 11.2 on page 279 says that\\nuniversal hashing provides the desired payoff: it becomes impossible for\\nan adversary to pick a sequence of operations that forces the worst-case\\nrunning time.\\nCorollary 11.3\\nUsing universal hashing and collision resolution by chaining in an\\ninitially empty table with m slots, it takes Θ (s) expected time to handle\\nany sequence of s INSERT, SEARCH, and DELETE operations\\ncontaining n = O(m) INSERT operations.\\nProof\\xa0\\xa0\\xa0The INSERT and DELETE operations take constant time.\\nSince the number n of insertions is O(m), we have that α = O(1).\\nFurthermore, the expected time for each SEARCH operation is O(1),\\nwhich can be seen by examining the proof of Theorem 11.2. That\\nanalysis depends only on collision probabilities, which are 1/m for any\\npair k1, k2 of keys by the choice of an independent uniform hash\\nfunction in that theorem. Using a universal family of hash functions\\nhere instead of using independent uniform hashing changes the\\nprobability of collision from 1/m to at most 1/m. By linearity of\\nexpectation, therefore, the expected time for the entire sequence of s\\noperations is O(s). Since each operation takes Ω(1) time, the Θ (s) bound\\nfollows.\\n▪\\n11.3.3\\xa0\\xa0\\xa0\\xa0Achievable properties of random  hashing\\nThere is a rich literature on the properties a family H of hash functions\\ncan have, and how they relate to the efﬁciency of hashing. We\\nsummarize a few of the most interesting ones here.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 385}),\n",
              " Document(page_content='Let H be a family of hash functions, each with domain U and range\\n{0, 1, …, m − 1}, and let h be any hash function that is picked uniformly\\nat random from H. The probabilities mentioned are probabilities over\\nthe picks of h.\\nThe family H is uniform if for any key k in U and any slot q in the\\nrange {0, 1, …, m − 1}, the probability that h(k) = q is 1/m.\\nThe family H is universal if for any distinct keys k1 and k2 in U,\\nthe probability that h(k1) = h(k2) is at most 1/m.\\nThe family H of hash functions is ϵ-universal if for any distinct\\nkeys k1 and k2 in U, the probability that h(k1) = h(k2) is at most\\nϵ. Therefore, a universal family of hash functions is also 1/m-\\nuniversal.2\\nThe family H is d-independent if for any distinct keys k1, k2, …,\\nkd in U and any slots q1, q2, …, qd, not necessarily distinct, in {0,\\n1, …, m − 1} the probability that h(ki) = qi for i = 1, 2, …, d is\\n1/md.\\nUniversal hash-function families are of particular interest, as they are\\nthe simplest type supporting provably efﬁcient hash-table operations for\\nany input data set. Many other interesting and desirable properties, such\\nas those noted above, are also possible and allow for efﬁcient specialized\\nhash-table operations.\\n11.3.4\\xa0\\xa0\\xa0\\xa0Designing a universal family of hash functions\\nThis section present two ways to design a universal (or ϵ-universal)\\nfamily of hash functions: one based on number theory and another\\nbased on a randomized variant of the multiply-shift method presented\\nin Section 11.3.1. The ﬁrst method is a bit easier to prove universal, but\\nthe second method is newer and faster in practice.\\nA universal family of hash functions based on num ber theory', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 386}),\n",
              " Document(page_content='We can design a universal family of hash functions using a little number\\ntheory. You may wish to refer to Chapter 31 if you are unfamiliar with\\nbasic concepts in number theory.\\nBegin by choosing a prime number p large enough so that every\\npossible key k lies in the range 0 to p − 1, inclusive. We assume here that\\np has a “reasonable” length. (See Section 11.3.5 for a discussion of\\nmethods for handling long input keys, such as variable-length strings.)\\nLet ℤp denote the set {0, 1, …, p − 1}, and let \\n denote the set {1, 2,\\n…, p − 1}. Since p is prime, we can solve equations modulo p with the\\nmethods given in Chapter 31. Because the size of the universe of keys is\\ngreater than the number of slots in the hash table (otherwise, just use\\ndirect addressing), we have p > m.\\nGiven any \\n  and any b ∈ ℤp, deﬁne the hash function hab as a\\nlinear transformation followed by reductions modulo p and then\\nmodulo m:\\nFor example, with p = 17 and m = 6, we have\\nh3,4(8)=((3 · 8 + 4) mod 17) mod 6\\n=(28 mod 17) mod 6\\n=11 mod 6\\n=5.\\nGiven p and m, the family of all such hash functions is\\nEach hash function hab maps ℤp to ℤm. This family of hash functions\\nhas the nice property that the size m of the output range (which is the\\nsize of the hash table) is arbitrary—it need not be prime. Since you can\\nchoose from among p − 1 values for a and p values for b, the family\\nHpm contains p(p − 1) hash functions.\\nTheorem 11.4', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 387}),\n",
              " Document(page_content='The family Hpm of hash functions deﬁned by equations (11.3) and\\n(11.4) is universal.\\nProof\\xa0\\xa0\\xa0Consider two distinct keys k1 and k2 from ℤp, so that k1 ≠ k2.\\nFor a given hash function hab, let\\nr1 = (ak1 + b) mod p,\\nr2 = (ak2 + b) mod p.\\nWe ﬁrst note that r1 ≠ r2. Why? Since we have r1 − r2 = a(k1 − k2) (mod\\np), it follows that r1 ≠ r2 because p is prime and both a and (k1 − k2) are\\nnonzero modulo p. By Theorem 31.6 on page 908, their product must\\nalso be nonzero modulo p. Therefore, when computing any hab ∈\\nHpm, distinct inputs k1 and k2 map to distinct values r1 and r2\\nmodulo p, and there are no collisions yet at the “mod p level.”\\nMoreover, each of the possible p(p − 1) choices for the pair (a, b) with a\\n≠ 0 yields a different resulting pair (r1, r2) with r1 ≠ r2, since we can\\nsolve for a and b given r1 and r2:\\na = ((r − r2)((k1 − k2)−1 mod p)) mod p,\\nb = (r1 − ak1) mod p,\\nwhere ((k1 − k2)−1 mod p) denotes the unique multiplicative inverse,\\nmodulo p, of k1 − k2. For each of the p possible values of r1, there are\\nonly p − 1 possible values of r2 that do not equal r1, making only p(p −\\n1) possible pairs (r1, r2) with r1 ≠ r2. Therefore, there is a one-to-one\\ncorrespondence between pairs (a, b) with a ≠ 0 and pairs (r1, r2) with r1\\n≠ r2. Thus, for any given pair of distinct inputs k1 and k2, if we pick (a,\\nb) uniformly at random from \\n , the resulting pair (r1, r2) is\\nequally likely to be any pair of distinct values modulo p.\\nTherefore, the probability that distinct keys k1 and k2 collide is equal\\nto the probability that r1 = r2 (mod m) when r1 and r2 are randomly', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 388}),\n",
              " Document(page_content='chosen as distinct values modulo p. For a given value of r1, of the p − 1\\npossible remaining values for r2, the number of values r2 such that r2 ≠\\nr1 and r2 = r1 (mod m) is at most\\nThe probability that r2 collides with r1 when reduced modulo m is at\\nmost ((p − 1)/m)/(p − 1) = 1/m, since r2 is equally likely to be any of the\\np − 1 values in Zp that are different from r1, but at most (p − 1)/m of\\nthose values are equivalent to r1 modulo m.\\nTherefore, for any pair of distinct values k1, k2 ∈ ℤp,\\nPr{hab(k1) = hab(k2)} ≤ 1/m,\\nso that Hpm is indeed universal.\\n▪\\nA 2/m-universal family of hash functions based on t he multiply-shift\\nmethod\\nWe recommend that in practice you use the following hash-function\\nfamily based on the multiply-shift method. It is exceptionally efﬁcient\\nand (although we omit the proof) provably 2/m-universal. Deﬁne H to\\nbe the family of multiply-shift hash functions with odd constants a:\\nTheorem 11.5\\nThe family of hash functions H given by equation (11.5) is 2/m-\\nuniversal.\\n▪\\nThat is, the probability that any two distinct keys collide is at most\\n2/m. In many practical situations, the speed of computing the hash', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 389}),\n",
              " Document(page_content='function more than compensates for the higher upper bound on the\\nprobability that two distinct keys collide when compared with a\\nuniversal hash function.\\n11.3.5\\xa0\\xa0\\xa0\\xa0Hashing long inputs such as vectors or strings\\nSometimes hash function inputs are so long that they cannot be easily\\nencoded modulo a reasonably sized prime number p or encoded within\\na single word of, say, 64 bits. As an example, consider the class of\\nvectors, such as vectors of 8-bit bytes (which is how strings in many\\nprogramming languages are stored). A vector might have an arbitrary\\nnonnegative length, in which case the length of the input to the hash\\nfunction may vary from input to input.\\nNumber-theoretic approaches\\nOne way to design good hash functions for variable-length inputs is to\\nextend the ideas used in Section 11.3.4 to design universal hash\\nfunctions. Exercise 11.3-6 explores one such approach.\\nCryptographic hashing\\nAnother way to design a good hash function for variable-length inputs\\nis to use a hash function designed for cryptographic applications.\\nCryptographic hash functions are complex pseudorandom functions,\\ndesigned for applications requiring properties beyond those needed\\nhere, but are robust, widely implemented, and usable as hash functions\\nfor hash tables.\\nA cryptographic hash function takes as input an arbitrary byte string\\nand returns a ﬁxed-length output. For example, the NIST standard\\ndeterministic cryptographic hash function SHA-256 [346] produces a\\n256-bit (32-byte) output for any input.\\nSome chip manufacturers include instructions in their CPU\\narchitectures to provide fast implementations of some cryptographic\\nfunctions. Of particular interest are instructions that efﬁciently\\nimplement rounds of the Advanced Encryption Standard (AES), the\\n“AES-NI” instructions. These instructions execute in a few tens of\\nnanoseconds, which is generally fast enough for use with hash tables. A', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 390}),\n",
              " Document(page_content='message authentication code such as CBC-MAC based on AES and the\\nuse of the AES-NI instructions could be a useful and efﬁcient hash\\nfunction. We don’t pursue the potential use of specialized instruction\\nsets further here.\\nCryptographic hash functions are useful because they provide a way\\nof implementing an approximate version of a random oracle. As noted\\nearlier, a random oracle is equivalent to an independent uniform hash\\nfunction family. From a theoretical point of view, a random oracle is an\\nunachievable ideal: a deterministic function that provides a randomly\\nselected output for each input. Because it is deterministic, it provides\\nthe same output if queried again for the same input. From a practical\\npoint of view, constructions of hash function families based on\\ncryptographic hash functions are sensible substitutes for random\\noracles.\\nThere are many ways to use a cryptographic hash function as a hash\\nfunction. For example, we could deﬁne\\nh(k) = SHA-256(k) mod m.\\nTo deﬁne a family of such hash functions one may prepend a “salt”\\nstring a to the input before hashing it, as in\\nha(k) = SHA-256(a ‖ k) mod m,\\nwhere a ‖ k denotes the string formed by concatenating the strings a and\\nk. The literature on message authentication codes (MACs) provides\\nadditional approaches.\\nCryptographic approaches to hash-function design are becoming\\nmore practical as computers arrange their memories in hierarchies of\\ndiffering capacities and speeds. Section 11.5 discusses one hash-function\\ndesign based on the RC6 encryption method.\\nExercises\\n11.3-1\\nYou wish to search a linked list of length n, where each element contains\\na key k along with a hash value h(k). Each key is a long character string.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 391}),\n",
              " Document(page_content='How might you take advantage of the hash values when searching the\\nlist for an element with a given key?\\n11.3-2\\nYou hash a string of r characters into m slots by treating it as a radix-\\n128 number and then using the division method. You can represent the\\nnumber m as a 32-bit computer word, but the string of r characters,\\ntreated as a radix-128 number, takes many words. How can you apply\\nthe division method to compute the hash value of the character string\\nwithout using more than a constant number of words of storage outside\\nthe string itself?\\n11.3-3\\nConsider a version of the division method in which h(k) = k mod m,\\nwhere m = 2p − 1 and k is a character string interpreted in radix 2p.\\nShow that if string x can be converted to string y by permuting its\\ncharacters, then x and y hash to the same value. Give an example of an\\napplication in which this property would be undesirable in a hash\\nfunction.\\n11.3-4\\nConsider a hash table of size m = 1000 and a corresponding hash\\nfunction h(k) = ⌊m (kA mod 1) ⌋ for \\n . Compute the\\nlocations to which the keys 61, 62, 63, 64, and 65 ar e mapped.\\n★ 11.3-5\\nShow that any ϵ-universal family H of hash functions from a ﬁnite set\\nU to a ﬁnite set Q has ϵ ≥ 1/|Q| − 1/|U|.\\n★ 11.3-6\\nLet U be the set of d-tuples of values drawn from ℤp, and let Q = ℤp,\\nwhere p is prime. Deﬁne the hash function hb : U → Q for b ∈ ℤp on an\\ninput d-tuple 〈a0, a1, …, ad−1〉 from U as', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 392}),\n",
              " Document(page_content='and let H = {hb : b ∈ ℤp}. Argue that H is ϵ-universal for ϵ = (d −\\n1)/p. (Hint: See Exercise 31.4-4.)\\n11.4\\xa0\\xa0\\xa0\\xa0Open addressing\\nThis section describes open addressing, a method for collision\\nresolution that, unlike chaining, does not make use of storage outside of\\nthe hash table itself. In open addressing, all elements occupy the hash\\ntable itself. That is, each table entry contains either an element of the\\ndynamic set or NIL. No lists or elements are stored outside the table,\\nunlike in chaining. Thus, in open addressing, the hash table can “ﬁll up”\\nso that no further insertions can be made. One consequence is that the\\nload factor α can never exceed 1.\\nCollisions are handled as follows: when a new element is to be\\ninserted into the table, it is placed in its “ﬁrst-choice” location if\\npossible. If that location is already occupied, the new element is placed\\nin its “second-choice” location. The process continues until an empty\\nslot is found in which to place the new element. Different elements have\\ndifferent preference orders for the locations.\\nTo search for an element, systematically examine the preferred table\\nslots for that element, in order of decreasing preference, until either you\\nﬁnd the desired element or you ﬁnd an empty slot and thus verify that\\nthe element is not in the table.\\nOf course, you could use chaining and store the linked lists inside the\\nhash table, in the otherwise unused hash-table slots (see Exercise 11.2-\\n4), but the advantage of open addressing is that it avoids pointers\\naltogether. Instead of following pointers, you compute the sequence of\\nslots to be examined. The memory freed by not storing pointers\\nprovides the hash table with a larger number of slots in the same\\namount of memory, potentially yielding fewer collisions and faster\\nretrieval.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 393}),\n",
              " Document(page_content='To perform insertion using open addressing, successively examine, or\\nprobe, the hash table until you ﬁnd an empty slot in which to put the\\nkey. Instead of being ﬁxed in the order 0, 1, …, m − 1 (which implies a\\nΘ(n) search time), the sequence of positions probed depends upon the\\nkey being inserted. To determine which slots to probe, the hash function\\nincludes the probe number (starting from 0) as a second input. Thus, the\\nhash function becomes\\nh : U × {0, 1, …, m − 1} →  {0, 1, …, m − 1}.\\nOpen addressing requires that for every key k, the probe sequence 〈h(k,\\n0), h(k, 1), …, h(k, m − 1)〉 be a permutation of 〈0, 1, …, m − 1〉, so that\\nevery hash-table position is eventually considered as a slot for a new key\\nas the table ﬁlls up. The HASH-INSERT procedure on the following\\npage assumes that the elements in the hash table T are keys with no\\nsatellite information: the key k is identical to the element containing key\\nk. Each slot contains either a key or NIL (if the slot is empty). The\\nHASH-INSERT procedure takes as input a hash table T and a key\\nk\\xa0that is assumed to be not already present in the hash table. It either\\nreturns the slot number where it stores key k or ﬂags an error because\\nthe hash table is already full.\\nHASH-INSERT(T, k)\\n1i = 0\\n2repeat\\n3 q = h(k, i)\\n4 if\\xa0T[q] == NIL\\n5 T[q] = k\\n6 return\\xa0q\\n7 else\\xa0i = i + 1\\n8until\\xa0i == m\\n9error “hash table overﬂow”\\nHASH-SEARCH(T, k)\\n1i = 0\\n2repeat', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 394}),\n",
              " Document(page_content='3 q = h(k, i)\\n4 if\\xa0T[q] == k\\n5 return\\xa0q\\n6 i = i + 1\\n7until\\xa0T[q] == NIL or i == m\\n8return\\xa0NIL\\nThe algorithm for searching for key k probes the same sequence of\\nslots that the insertion algorithm examined when key k was inserted.\\nTherefore, the search can terminate (unsuccessfully) when it ﬁnds an\\nempty slot, since k would have been inserted there and not later in its\\nprobe sequence. The procedure HASH-SEARCH takes as input a hash\\ntable T and a key k, returning q if it ﬁnds that slot q contains key k, or\\nNIL if key k is not present in table T.\\nDeletion from an open-address hash table is tricky. When you delete\\na key from slot q, it would be a mistake to mark that slot as empty by\\nsimply storing NIL in it. If you did, you might be unable to retrieve any\\nkey k for which slot q was probed and found occupied when k was\\ninserted. One way to solve this problem is by marking the slot, storing\\nin it the special value DELETED instead of NIL. The HASH-INSERT\\nprocedure then has to treat such a slot as empty so that it can insert a\\nnew key there. The HASH-SEARCH procedure passes over DELETED\\nvalues while searching, since slots containing DELETED were ﬁlled\\nwhen the key being searched for was inserted. Using the special value\\nDELETED, however, means that search times no longer depend on the\\nload factor α, and for this reason chaining is frequently selected as a\\ncollision resolution technique when keys must be deleted. There is a\\nsimple special case of open addressing, linear probing, that avoids the\\nneed to mark slots with DELETED. Section 11.5.1 shows how to delete\\nfrom a hash table when using linear probing.\\nIn our analysis, we assume independent uniform permutation hashing\\n(also confusingly known as uniform hashing in the literature): the probe\\nsequence of each key is equally likely to be any of the m! permutations\\nof 〈0, 1, …, m − 1〉. Independent uniform permutation hashing\\ngeneralizes the notion of independent uniform hashing deﬁned earlier to', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 395}),\n",
              " Document(page_content='a hash function that produces not just a single slot number, but a whole\\nprobe sequence. True independent uniform permutation hashing is\\ndifﬁcult to implement, however, and in practice suitable approximations\\n(such as double hashing, deﬁned below) are used.\\nWe’ll examine both double hashing and its special case, linear\\nprobing. These techniques guarantee that 〈h(k, 0), h(k, 1), …, h(k, m −\\n1)〉 is a permutation of 〈0, 1, …, m − 1〉 for each key k. (Recall that the\\nsecond parameter to the hash function h is the probe number.) Neither\\ndouble hashing nor linear probing meets the assumption of independent\\nuniform permutation hashing, however. Double hashing cannot\\ngenerate more than m2 different probe sequences (instead of the m! that\\nindependent uniform permutation hashing requires). Nonetheless,\\ndouble hashing has a large number of possible probe sequences and, as\\nyou might expect, seems to give good results. Linear probing is even\\nmore restricted, capable of generating only m different probe sequences.\\nDouble hashing\\nDouble hashing offers one of the best methods available for open\\naddressing because the permutations produced have many of the\\ncharacteristics of randomly chosen permutations. Double hashing uses a\\nhash function of the form\\nh(k, i) = (h1(k) + ih2(k)) mod m,\\nwhere both h1 and h2 are auxiliary hash functions. The initial probe goes\\nto position T[h1(k)], and successive probe positions are offset from\\nprevious positions by the amount h2(k), modulo m. Thus, the probe\\nsequence here depends in two ways upon the key k, since the initial\\nprobe position h1(k), the step size h2(k), or both, may vary. Figure 11.5\\ngives an example of insertion by double hashing.\\nIn order for the entire hash table to be searched, the value h2(k) must\\nbe relatively prime to the hash-table size m. (See Exercise 11.4-5.) A\\nconvenient way to ensure this condition is to let m be an exact power of\\n2 and to design h2 so that it always produces an odd number. Another', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 396}),\n",
              " Document(page_content='way is to let m be prime and to design h2 so that it always returns a\\npositive integer less than m. For example, you could choose m prime\\nand let\\nFigure 11.5 Insertion by double hashing. The hash table has size 13 with h1(k) = k mod 13 and\\nh2(k) = 1 + (k mod 11). Since 14 = 1 (mod 13) and 14 = 3 (mod 11), the key 14 goes into empty\\nslot 9, after slots 1 and 5 are examined and found to be occupied.\\nh1(k) = k mod m,\\nh2(k) = 1 + (k mod m′),\\nwhere m′ is chosen to be slightly less than m (say, m − 1). For example, if\\nk = 123456, m = 701, and m′ = 700, then h1(k) = 80 and h2(k) = 257, so\\nthat the ﬁrst probe goes to position 80, and successive probes examine\\nevery 257th slot (modulo m) until the key has been found or every slot\\nhas been examined.\\nAlthough values of m other than primes or exact powers of 2 can in\\nprinciple be used with double hashing, in practice it becomes more\\ndifﬁcult to efﬁciently generate h2(k) (other than choosing h2(k) = 1,\\nwhich gives linear probing) in a way that ensures that it is relatively\\nprime to m, in part because the relative density ϕ(m)/m of such numbers\\nfor general m may be small (see equation (31.25) on page 921) .', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 397}),\n",
              " Document(page_content='When m is prime or an exact power of 2, double hashing produces\\nΘ(m2) probe sequences, since each possible (h1(k), h2(k)) pair yields a\\ndistinct probe sequence. As a result, for such values of m, double\\nhashing appears to perform close to the “ideal” scheme of independent\\nuniform permutation hashing.\\nLinear probing\\nLinear probing, a special case of double hashing, is the simplest open-\\naddressing approach to resolving collisions. As with double hashing, an\\nauxiliary hash function h1 determines the ﬁrst probe position h1(k) for\\ninserting an element. If slot T[h1(k)] is already occupied, probe the next\\nposition T[h1(k) + 1]. Keep going as necessary, on up to slot T[m − 1],\\nand then wrap around to slots T[0], T[1], and so on, but never going\\npast slot T[h1(k) − 1]. To view linear probing as a special case of double\\nhashing, just set the double-hashing step function h2 to be ﬁxed at 1:\\nh2(k) = 1 for all k. That is, the hash function is\\nfor i = 0, 1, …, m − 1. The value of h1(k) determines the entire probe\\nsequence, and so assuming that h1(k) can take on any value in {0, 1, …,\\nm − 1}, linear probing allows only m distinct probe sequences.\\nWe’ll revisit linear probing in Section 11.5.1.\\nAnalysis of open-address hashing\\nAs in our analysis of chaining in Section 11.2, we analyze open\\naddressing in terms of the load factor α = n/m of the hash table. With\\nopen addressing, at most one element occupies each slot, and thus n ≤\\nm, which implies α ≤ 1. The analysis below requires α to be strictly less\\nthan 1, and so we assume that at least one slot is empty. Because\\ndeleting from an open-address hash table does not really free up a slot,\\nwe assume as well that no deletions occur.\\nFor the hash function, we assume independent uniform permutation\\nhashing. In this idealized scheme, the probe sequence 〈h(k, 0), h(k, 1),', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 398}),\n",
              " Document(page_content='…, h(k, m − 1)〉 used to insert or search for each key k is equally likely\\nto be any permutation of 〈0, 1, …, m − 1〉. Of course, any given key has\\na unique ﬁxed probe sequence associated with it. What we mean here is\\nthat, considering the probability distribution on the space of keys and\\nthe operation of the hash function on the keys, each possible probe\\nsequence is equally likely.\\nWe now analyze the expected number of probes for hashing with\\nopen addressing under the assumption of independent uniform\\npermutation hashing, beginning with the expected number of probes\\nmade in an unsuccessful search (assuming, as stated above, that α < 1).\\nThe bound proven, of 1/(1 − α) = 1 + α + α2 + α3 + ⋯, has an\\nintuitive interpretation. The ﬁrst probe always occurs. With probability\\napproximately α, the ﬁrst probe ﬁnds an occupied slot, so that a second\\nprobe happens. With probability approximately α2, the ﬁrst two slots\\nare occupied so that a third probe ensues, and so on.\\nTheorem 11.6\\nGiven an open-address hash table with load factor α = n/m < 1, the\\nexpected number of probes in an unsuccessful search is at most 1/(1 −\\nα), assuming independent uniform permutation hashing and no\\ndeletions.\\nProof\\xa0\\xa0\\xa0In an unsuccessful search, every probe but the last accesses an\\noccupied slot that does not contain the desired key, and the last slot\\nprobed is empty. Let the random variable X denote the number of\\nprobes made in an unsuccessful search, and deﬁne the event Ai, for i =\\n1, 2, …, as the event that an ith probe occurs and it is to an occupied\\nslot. Then the event {X ≥ i} is the intersection of events A1 ⋂ A2 ⋂ ⋯\\n⋂ Ai−1. We bound Pr{X ≥ i} by bounding Pr{A1 ⋂ A2 ⋂ ⋯ ⋂ Ai−1}.\\nBy Exercise C.2-5 on page 1190,\\nPr{A1 ⋂ A2 ⋂ ⋯ ⋂\\nAi−1}=Pr{A1} · Pr{A2 | A1} · Pr {A3 | A1 ⋂ A2}\\n⋯\\nPr{Ai−1 | A1 ⋂ A2 ⋂ ⋯ ⋂ Ai−2}.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 399}),\n",
              " Document(page_content='Since there are n elements and m slots, Pr{A1} = n/m. For j > 1, the\\nprobability that there is a jth probe and it is to an occupied slot, given\\nthat the ﬁrst j − 1 probes were to occupied slots, is (n − j + 1)/(m − j +\\n1). This probability follows because the jth probe would be ﬁnding one\\nof the remaining (n − (j − 1)) elements in one of the (m − (j − 1))\\nunexamined slots, and by the assumption of independent uniform\\npermutation hashing, the probability is the ratio of these quantities.\\nSince n < m implies that (n − j)/(m − j) ≤ n/m for all j in the range 0 ≤ j <\\nm, it follows that for all i in the range 1 ≤ i ≤ m, we have\\nThe product in the ﬁrst line has i − 1 factors. When i = 1, the product is\\n1, the identity for multiplication, and we get Pr{X ≥ 1} = 1, which\\nmakes sense, since there must always be at least 1 probe. If each of the\\nﬁrst n probes is to an occupied slot, then all occupied slots have been\\nprobed. Then, the (n + 1)st probe must be to an empty slot, which gives\\nPr{X ≥ i} = 0 for i > n + 1. Now, we use equation (C.28) on page 1193 t o\\nbound the expected number of probes:\\n▪', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 400}),\n",
              " Document(page_content='If α is a constant, Theorem 11.6 predicts that an unsuccessful search\\nruns in O(1) time. For example, if the hash table is half full, the average\\nnumber of probes in an unsuccessful search is at most 1/(1 − .5) = 2. If it\\nis 90% full, the average number of probes is at most 1/(1 − .9) = 10.\\nTheorem 11.6 yields almost immediately how well the HASH-\\nINSERT procedure performs.\\nCorollary 11.7\\nInserting an element into an open-address hash table with load factor α,\\nwhere α < 1, requires at most 1/(1 − α) probes on average, assuming\\nindependent uniform permutation hashing and no deletions.\\nProof\\xa0\\xa0\\xa0An element is inserted only if there is room in the table, and thus\\nα < 1. Inserting a key requires an unsuccessful search followed by\\nplacing the key into the ﬁrst empty slot found. Thus, the expected\\nnumber of probes is at most 1/(1 − α).\\n▪\\nIt takes a little more work to compute the expected number of\\nprobes for a successful search.\\nTheorem 11.8\\nGiven an open-address hash table with load factor α < 1, the expected\\nnumber of probes in a successful search is at most\\nassuming independent uniform permutation hashing with no deletions\\nand assuming that each key in the table is equally likely to be searched\\nfor.\\nProof\\xa0\\xa0\\xa0A search for a key k reproduces the same probe sequence as\\nwhen the element with key k was inserted. If k was the (i + 1)st key\\ninserted into the hash table, then the load factor at the time it was\\ninserted was i/m, and so by Corollary 11.7, the expected number of\\nprobes made in a search for k is at most 1/(1 − i/m) = m/(m − i).\\nAveraging over all n keys in the hash table gives us the expected number\\nof probes in a successful search:', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 401}),\n",
              " Document(page_content='▪\\nIf the hash table is half full, the expected number of probes in a\\nsuccessful search is less than 1.387. If the hash table is 90% full, the\\nexpected number of probes is less than 2.559. If α = 1, then in an\\nunsuccessful search, all m slots must be probed. Exercise 11.4-4 asks\\nyou to analyze a successful search when α = 1.\\nExercises\\n11.4-1\\nConsider inserting the keys 10, 22, 31, 4, 15, 28, 17, 88, 59 into a hash\\ntable of length m = 11 using open addressing. Illustrate the result of\\ninserting these keys using linear probing with h(k, i) = (k + i) mod m\\nand using double hashing with h1(k) = k and h2(k) = 1 + (k mod (m −\\n1)).\\n11.4-2\\nWrite pseudocode for HASH-DELETE that ﬁlls the deleted key’s slot\\nwith the special value DELETED, and modify HASH-SEARCH and\\nHASH-INSERT as needed to handle DELETED.\\n11.4-3', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 402}),\n",
              " Document(page_content='Consider an open-address hash table with independent uniform\\npermutation hashing and no deletions. Give upper bounds on the\\nexpected number of probes in an unsuccessful search and on the\\nexpected number of probes in a successful search when the load factor is\\n3/4 and when it is 7/8.\\n11.4-4\\nShow that the expected number of probes required for a successful\\nsearch when α = 1 (that is, when n = m), is Hm, the mth harmonic\\nnumber.\\n★ 11.4-5\\nShow that, with double hashing, if m and h2(k) have greatest common\\ndivisor d ≥ 1 for some key k, then an unsuccessful search for key k\\nexamines (1/d)th of the hash table before returning to slot h1(k). Thus,\\nwhen d = 1, so that m and h2(k) are relatively prime, the search may\\nexamine the entire hash table. (Hint: See Chapter 31.)\\n★ 11.4-6\\nConsider an open-address hash table with a load factor α. Approximate\\nthe nonzero value α for which the expected number of probes in an\\nunsuccessful search equals twice the expected number of probes in a\\nsuccessful search. Use the upper bounds given by Theorems 11.6 and\\n11.8 for these expected numbers of probes.\\n11.5\\xa0\\xa0\\xa0\\xa0Practical considerations\\nEfﬁcient hash table algorithms are not only of theoretical interest, but\\nalso of immense practical importance. Constant factors can matter. For\\nthis reason, this section discusses two aspects of modern CPUs that are\\nnot included in the standard RAM model presented in Section 2.2:\\nMemory hierarchies: The memory of modern CPUs has a number of\\nlevels, from the fast registers, through one or more levels of cache\\nmemory, to the main-memory level. Each successive level stores more', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 403}),\n",
              " Document(page_content='data than the previous level, but access is slower. As a consequence, a\\ncomplex computation (such as a complicated hash function) that\\nworks entirely within the fast registers can take less time than a single\\nread operation from main memory. Furthermore, cache memory is\\norganized in cache blocks of (say) 64 bytes each, which are always\\nfetched together from main memory. There is a substantial beneﬁt for\\nensuring that memory usage is local: reusing the same cache block is\\nmuch more efﬁcient than fetching a different cache block from main\\nmemory.\\nThe standard RAM model measures efﬁciency of a hash-table\\noperation by counting the number of hash-table slots probed. In\\npractice, this metric is only a crude approximation to the truth, since\\nonce a cache block is in the cache, successive probes to that cache\\nblock are much faster than probes that must access main memory.\\nAdvanced instruction sets: Modern CPUs may have sophisticated\\ninstruction sets that implement advanced primitives useful for\\nencryption or other forms of cryptography. These instructions may be\\nuseful in the design of exceptionally efﬁcient hash functions.\\nSection 11.5.1 discusses linear probing, which becomes the collision-\\nresolution method of choice in the presence of a memory hierarchy.\\nSection 11.5.2 suggests how to construct “advanced” hash functions\\nbased on cryptographic primitives, suitable for use on computers with\\nhierarchical memory models.\\n11.5.1\\xa0\\xa0\\xa0\\xa0Linear probing\\nLinear probing is often disparaged because of its poor performance in\\nthe standard RAM model. But linear probing excels for hierarchical\\nmemory models, because successive probes are usually to the same\\ncache block of memory.\\nDeletion with linear probing\\nAnother reason why linear probing is often not used in practice is that\\ndeletion seems complicated or impossible without using the special\\nDELETED value. Yet we’ll now see that deletion from a hash table', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 404}),\n",
              " Document(page_content='based on linear probing is not all that difﬁcult, even without the\\nDELETED marker. The deletion procedure works for linear probing,\\nbut not for open-address probing in general, because with linear\\nprobing keys all follow the same simple cyclic probing sequence (albeit\\nwith different starting points).\\nThe deletion procedure relies on an “inverse” function to the linear-\\nprobing hash function h(k, i) = (h1(k) + i) mod m, which maps a key k\\nand a probe number i to a slot number in the hash table. The inverse\\nfunction g maps a key k and a slot number q, where 0 ≤ q < m, to the\\nprobe number that reaches slot q:\\ng(k, q) = (q − h1(k)) mod m.\\nIf h(k, i) = q, then g(k, q) = i, and so h(k, g(k, q)) = q.\\nThe procedure LINEAR-PROBING-HASH-DELETE on the facing\\npage deletes the key stored in position q from hash table T. Figure 11.6\\nshows how it works. The procedure ﬁrst deletes the key in position q by\\nsetting T[q] to NIL in line 2. It then searches for a slot q′ (if any) that\\ncontains a key that should be moved to the slot q just vacated by key k.\\nLine 9 asks the critical question: does the key k′ in slot q′ need to be\\nmoved to the vacated slot q in order to preserve the accessibility of k′? If\\ng(k′, q) < g(k′, q′), then during the insertion of k′ into the table, slot q\\nwas examined but found to be already occupied. But now slot q, where a\\nsearch will look for k′, is empty. In this case, key k′ moves to slot q in\\nline 10, and the search continues, to see whether any later key also needs\\nto be moved to the slot q′ that was just freed up when k′ moved.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 405}),\n",
              " Document(page_content='Figure 11.6 Deletion in a hash table that uses linear probing. The hash table has size 10 with\\nh1(k) = k mod 10. (a) The hash table after inserting keys in the order 74, 43, 93, 18, 82, 38, 92.\\n(b) The hash table after deleting the key 43 from slot 3. Key 93 moves up to slot 3 to keep it\\naccessible, and then key 92 moves up to slot 5 just vacated by key 93. No other keys need to be\\nmoved.\\nLINEAR-PROBING-HASH-DELETE(T, q)\\n\\xa0\\xa01while\\xa0TRUE\\n\\xa0\\xa02T[q] = NIL // make slot q empty\\n\\xa0\\xa03q′ = q // starting point for search\\n\\xa0\\xa04repeat\\n\\xa0\\xa05 q′ = (q′ + 1) mod m // next slot number with linear probing\\n\\xa0\\xa06 k′ = T[q′] // next key to try to move\\n\\xa0\\xa07 if\\xa0k′ == NIL\\n\\xa0\\xa08 return // return when an empty slot is found\\n\\xa0\\xa09until\\xa0g(k′, q) < g(k′, q′)// was empty slot q probed before q′?\\n10T[q] = k′ // move k′ into slot q\\n11q = q′ // free up slot q′\\nAnalysis of linear probing\\nLinear probing is popular to implement, but it exhibits a phenomenon\\nknown as primary clustering. Long runs of occupied slots build up,', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 406}),\n",
              " Document(page_content='increasing the average search time. Clusters arise because an empty slot\\npreceded by i full slots gets ﬁlled next with probability (i + 1)/m. Long\\nruns of occupied slots tend to get longer, and the average search time\\nincreases.\\nIn the standard RAM model, primary clustering is a problem, and\\ngeneral double hashing usually performs better than linear probing. By\\ncontrast, in a hierarchical memory model, primary clustering is a\\nbeneﬁcial property, as elements are often stored together in the same\\ncache block. Searching proceeds through one cache block before\\nadvancing to search the next cache block. With linear probing, the\\nrunning time for a key k of HASH-INSERT, HASH-SEARCH, or\\nLINEAR-PROBING-HASH-DELETE is at most proportional to the\\ndistance from h1(k) to the next empty slot.\\nThe following theorem is due to Pagh et al. [351]. A more recent\\nproof is given by Thorup [438]. We omit the proof here. The need for 5-\\nindependence is by no means obvious; see the cited proofs.\\nTheorem 11.9\\nIf h1 is 5-independent and α ≤ 2/3, then it takes expected constant time\\nto search for, insert, or delete a key in a hash table using linear probing.\\n▪\\n(Indeed, the expected operation time is O(1/ ϵ\\xa02) for α = 1 − ϵ.)\\n★ 11.5.2 Hash functions for hierarchical memory models\\nThis section illustrates an approach for designing efﬁcient hash tables in\\na modern computer system having a m emory hierarchy.\\nBecause of the memory hierarchy, linear probing is a good choice for\\nresolving collisions, as probe sequences are sequential and tend to stay\\nwithin cache blocks. But linear probing is most efﬁcient when the hash\\nfunction is complex (for example, 5-independent as in Theorem 11.9).\\nFortunately, having a memory hierarchy means that complex hash\\nfunctions can be implemented efﬁciently.\\nAs noted in Section 11.3.5, one approach is to use a cryptographic\\nhash function such as SHA-256. Such functions are complex and', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 407}),\n",
              " Document(page_content='sufﬁciently random for hash table applications. On machines with\\nspecialized instructions, cryptographic functions can be quite efﬁcient.\\nInstead, we present here a simple hash function based only on\\naddition, multiplication, and swapping the halves of a word. This\\nfunction can be implemented entirely within the fast registers, and on a\\nmachine with a memory hierarchy, its latency is small compared with\\nthe time taken to access a random slot of the hash table. It is related to\\nthe RC6 encryption algorithm and can for practical purposes be\\nconsidered a “random oracle.”\\nThe wee hash function\\nLet w denote the word size of the machine (e.g., w = 64), assumed to be\\neven, and let a and b be w-bit unsigned (nonnegative) integers such that\\na is odd. Let swap(x) denote the w-bit result of swapping the two w/2-bit\\nhalves of w-bit input x. That is,\\nswap(x) = (x ⋙ (w/2)) + (x ⋘ (w/2))\\nwhere “ ⋙” is “logical right shift” (as in equation (11.2)) and “ ⋘ is\\n“left shift.” Deﬁne\\nfa(k) = swap((2k2 + ak) mod 2w).\\nThus, to compute fa(k), evaluate the quadratic function 2k2 + ak\\nmodulo 2w and then swap the left and right halves of the result.\\nLet r denote a desired number of “rounds” for the computation of\\nthe hash function. We’ll use r = 4, but the hash function is well deﬁned\\nfor any nonnegative r. Denote by \\n  the result of iterating fa a total\\nof r times (that is, r rounds) starting with input value k. For any odd a\\nand any r ≥ 0, the function \\n, although complicated, is one-to-one (see\\nExercise 11.5-1). A cryptographer would view \\n as a simple block\\ncipher operating on w-bit input blocks, with r rounds and key a.\\nWe ﬁrst deﬁne the wee hash function h for short inputs, where by\\n“short” we means “whose length t is at most w-bits,” so that the input\\nﬁts within one computer word. We would like inputs of different lengths', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 408}),\n",
              " Document(page_content='to be hashed differently. The wee hash function\\xa0ha,b,t,r(k) for parameters\\na, b, and r on t-bit input k is deﬁned as\\nThat is, the hash value for t-bit input k is obtained by applying \\n  to k\\n+ b, then taking the ﬁnal result modulo m. Adding the value b provides\\nhash-dependent randomization of the input, in a way that ensures that\\nfor variable-length inputs the 0-length input does not have a ﬁxed hash\\nvalue. Adding the value 2t to a ensures that the hash function acts\\ndifferently for inputs of different lengths. (We use 2t rather than t to\\nensure that the key a + 2t is odd if a is odd.) We call this hash function\\n“wee” because it uses a tiny amount of memory—m ore precisely, it can\\nbe implemented efﬁciently using only the computer’s fast registers. (This\\nhash function does not have a name in the literature; it is a variant we\\ndeveloped for this textbook.)\\nSpeed of the wee hash function\\nIt is surprising how much efﬁciency can be bought with locality.\\nExperiments (unpublished, by the authors) suggest that evaluating the\\nwee hash function takes less time than probing a single randomly chosen\\nslot in a hash table. These experiments were run on a laptop (2019\\nMacBook Pro) with w = 64 and a = 123. For large hash tables,\\nevaluating the wee hash function was 2 to 10 times faster than\\nperforming a single probe of the hash table.\\nThe wee hash function for variable-length inputs\\nSometimes inputs are long—more than one w-bit word in length—or\\nhave variable length, as discussed in Section 11.3.5. We can extend the\\nwee hash function, deﬁned above for inputs that are at most single w-bit\\nword in length, to handle long or variable-length inputs. Here is one\\nmethod for doing so.\\nSuppose that an input k has length t (measured in bits). Break k into\\na sequence 〈k1, k2, …, ku〉 of w-bit words, where u = ⌈t/w ⌉, k1 contains\\nthe least-signiﬁcant w bits of k, and ku contains the most signiﬁcant', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 409}),\n",
              " Document(page_content='bits. If t is not a multiple of w, then ku contains fewer than w bits, in\\nwhich case, pad out the unused high-order bits of ku with 0-bits. Deﬁne\\nthe function chop to return a sequence of the w-bit words in k:\\nchop(k) = 〈k1, k2, …, ku〉.\\nThe most important property of the chop operation is that it is one-to-\\none, given t: for any two t-bit keys k and k′, if k ≠ k′ then chop(k) ≠\\nchop(k′), and k can be derived from chop(k) and t. The chop operation\\nalso has the useful property that a single-word input key yields a single-\\nword output sequence: chop(k) = 〈k〉.\\nWith the chop function in hand, we specify the wee hash function\\nha,b,t,r(k) for an input k of length t bits as follows:\\nha,b,t,r(k) = WEE(k, a, b, t, r, m),\\nwhere the procedure WEE deﬁned on the facing page iterates through\\nthe elements of the w-bit words returned by chop(k), applying \\n to the\\nsum of the current word ki and the previously computed hash value so\\nfar, ﬁnally returning the result obtained modulo m. This deﬁnition for\\nvariable-length and long (multiple-word) inputs is a consistent extension\\nof the deﬁnition in equation (11.7) for short (single-word) inputs. For\\npractical use, we recommend that a be a randomly chosen odd w-bit\\nword, b be a randomly chosen w-bit word, and that r = 4.\\nNote that the wee hash function is really a hash function family, with\\nindividual hash functions determined by parameters a, b, t, r, and m.\\nThe (approximate) 5-independence of the wee hash function family for\\nvariable-length inputs can be argued based on the assumption that the\\n1-word wee hash function is a random oracle and on the security of the\\ncipher-block-chaining message authentication code (CBC-MAC), as\\nstudied by Bellare et al. [42]. The case here is actually simpler than that\\nstudied in the literature, since if two messages have different lengths t\\nand t′, then their “keys” are different: a + 2t ≠ a + 2t′. We omit the\\ndetails.\\nWEE(k, a, b, t, r, m)', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 410}),\n",
              " Document(page_content='1u = ⌈t/w ⌉\\n2〈k1, k2, …, ku〉 = chop(k)\\n3q = b\\n4for\\xa0i = 1 to\\xa0u\\n5\\n6return\\xa0q mod m\\nThis deﬁnition of a cryptographically inspired hash-function family\\nis meant to be realistic, yet only illustrative, and many variations and\\nimprovements are possible. See the chapter notes for suggestions.\\nIn summary, we see that when the memory system is hierarchical, it\\nbecomes advantageous to use linear probing (a special case of double\\nhashing), since successive probes tend to stay in the same cache block.\\nFurthermore, hash functions that can be implemented using only the\\ncomputer’s fast registers are exceptionally efﬁcient, so they can be quite\\ncomplex and even cryptographically inspired, providing the high degree\\nof independence needed for linear probing to work most efﬁciently.\\nExercises\\n★ 11.5-1\\nComplete the argument that for any odd positive integer a and any\\ninteger r ≥ 0, the function \\n  is one-to-one. Use a proof by\\ncontradiction and make use of the fact that the function fa works\\nmodulo 2w.\\n★ 11.5-2\\nArgue that a random oracle is 5-independent.\\n★ 11.5-3\\nConsider what happens to the value \\n  as we ﬂip a single bit ki of\\nthe input value k, for various values of r. Let \\n  and \\n deﬁne the bit values ki in the input (with k0 the least-', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 411}),\n",
              " Document(page_content='signiﬁcant bit) and the bit values bj in ga(k) = (2k2 + ak) mod 2w\\n(where ga(k) is the value that, when its halves are swapped, becomes\\nfa(k)). Suppose that ﬂipping a single bit ki of the input k may cause any\\nbit bj of ga(k) to ﬂip, for j ≥ i. What is the least value of r for which\\nﬂipping the value of any single bit ki may cause any bit of the output \\n to ﬂip? Explain.\\nProblems\\n11-1\\xa0\\xa0\\xa0\\xa0\\xa0L ongest-probe bound for hashing\\nSuppose you are using an open-addressed hash table of size m to store n\\n≤ m/2 items.\\na. Assuming independent uniform permutation hashing, show that for i\\n= 1, 2, …, n, the probability is at most 2−p that the ith insertion\\nrequires strictly more than p probes.\\nb. Show that for i = 1, 2, …, n, the probability is O(1/n2) that the ith\\ninsertion requires more than 2 lg n probes.\\nLet the random variable Xi denote the number of probes required by the\\nith insertion. You have shown in part (b) that Pr{Xi > 2 lg n} = O(1/n2).\\nLet the random variable X = max {Xi : 1 ≤ i ≤ n} denote the maximum\\nnumber of probes required by any of the n insertions.\\nc. Show that Pr{X > 2 lg n} = O(1/n).\\nd. Show that the expected length E[X] of the longest probe sequence is\\nO(lg n).\\n11-2\\xa0\\xa0\\xa0\\xa0\\xa0Searching a static set\\nYou are asked to implement a searchable set of n elements in which the\\nkeys are numbers. The set is static (no INSERT or DELETE\\noperations), and the only operation required is SEARCH. You are given', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 412}),\n",
              " Document(page_content='an arbitrary amount of time to preprocess the n elements so that\\nSEARCH operations run quickly.\\na. Show how to implement SEARCH in O(lg n) worst-case time using\\nno extra storage beyond what is needed to store the elements of the set\\nthemselves.\\nb. Consider implementing the set by open-address hashing on m slots,\\nand assume independent uniform permutation hashing. What is the\\nminimum amount of extra storage m − n required to make the average\\nperformance of an unsuccessful SEARCH operation be at least as\\ngood as the bound in part (a)? Your answer should be an asymptotic\\nbound on m − n in terms of n.\\n11-3\\xa0\\xa0\\xa0\\xa0\\xa0Slot-size bound for chaining\\nGiven a hash table with n slots, with collisions resolved by chaining,\\nsuppose that n keys are inserted into the table. Each key is equally likely\\nto be hashed to each slot. Let M be the maximum number of keys in\\nany slot after all the keys have been inserted. Your mission is to prove\\nan O(lg n / lg lg n) upper bound on E[M], the expected value of M.\\na. Argue that the probability Qk that exactly k keys hash to a particular\\nslot is given by\\nb. Let Pk be the probability that M = k, that is, the probability that the\\nslot containing the most keys contains k keys. Show that Pk ≤ nQk.\\nc. Show that Qk < ek/kk. Hint: Use Stirling’s approximation, equation\\n(3.25) on page 67.\\nd. Show that there exists a constant c > 1 such that \\n  for k0 =\\nc lg n / lg lg n. Conclude that Pk < 1/n2 for k ≥ k0 = c lg n / lg lg n.\\ne. Argue that', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 413}),\n",
              " Document(page_content='Conclude that E[M] = O(lg n / lg lg n).\\n11-4\\xa0\\xa0\\xa0\\xa0\\xa0H ashing and authentication\\nLet H be a family of hash functions in which each hash function h ∈\\nH maps the universe U of keys to {0, 1, …, m − 1}.\\na. Show that if the family H of hash functions is 2-independent, then it\\nis universal.\\nb. Suppose that the universe U is the set of n-tuples of values drawn\\nfrom ℤp = {0, 1, …, p − 1}, where p is prime. Consider an element x =\\n〈x0, x1, …, xn−1〉 ∈ U. For any n-tuple a = 〈a0, a1, …, an−1〉 ∈ U,\\ndeﬁne the hash function ha by\\nLet H = {ha : a ∈ U}. Show that H is universal, but not 2-\\nindependent. (Hint: Find a key for which all hash functions in H\\nproduce the same value.)\\nc. Suppose that we modify H slightly from part (b): for any a ∈ U and\\nfor any b ∈ ℤp, deﬁne\\nand \\n . Argue that H′ is 2-independent.\\n(Hint: Consider ﬁxed n-tuples x ∈ U and y ∈ U, with xi ≠ yi for some\\ni. What happens to \\n  and \\n  as ai and b range over ℤp?)\\nd. Alice and Bob secretly agree on a hash function h from a 2-\\nindependent family H of hash functions. Each h ∈ H maps from a', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 414}),\n",
              " Document(page_content='universe of keys U to ℤp, where p is prime. Later, Alice sends a\\nmessage m to Bob over the internet, where m ∈ U. She authenticates\\nthis message to Bob by also sending an authentication tag t = h(m),\\nand Bob checks that the pair (m, t) he receives indeed satisﬁes t =\\nh(m). Suppose that an adversary intercepts (m, t) en route and tries to\\nfool Bob by replacing the pair (m, t) with a different pair (m′, t′).\\nArgue that the probability that the adversary succeeds in fooling Bob\\ninto accepting (m′, t′) is at most 1/p, no matter how much computing\\npower the adversary has, even if the adversary knows the family H of\\nhash functions used.\\nChapter notes\\nThe books by Knuth [261] and Gonnet and Baeza-Yates [193] are\\nexcellent references for the analysis of hashing algorithms. Knuth\\ncredits H. P. Luhn (1953) for inventing hash tables, along with the\\nchaining method for resolving collisions. At about the same time, G. M.\\nAmdahl originated the idea of open addressing. The notion of a\\nrandom oracle was introduced by Bellare et al. [43]. Carter and\\nWegman [80] introduced the notion of universal families of hash\\nfunctions in 1979.\\nDietzfelbinger et al. [113] invented the multiply-shift hash function\\nand gave a proof of Theorem 11.5. Thorup [437] provides extensions\\nand additional analysis. Thorup [438] gives a simple proof that linear\\nprobing with 5-independent hashing takes constant expected time per\\noperation. Thorup also describes the method for deletion in a hash table\\nusing linear probing.\\nFredman, Komlós, and Szemerédi [154] developed a perfect hashing\\nscheme for static sets—“perfect” because all collisions are avoided. An\\nextension of their method to dynamic sets, handling insertions and\\ndeletions in amortized expected time O(1), has been given by\\nDietzfelbinger et al. [114].\\nThe wee hash function is based on the RC6 encryption algorithm\\n[379]. Leiserson et al. [292] propose an “RC6MIX” function that is\\nessentially the same as the wee hash function. They give experimental', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 415}),\n",
              " Document(page_content='evidence that it has good randomness, and they also give a “DOTMIX”\\nfunction for dealing with variable-length inputs. Bellare et al. [42]\\nprovide an analysis of the security of the cipher-block-chaining message\\nauthentication code. This analysis implies that the wee hash function\\nhas the desired pseudorandomness properties.\\n1 The deﬁnition of “average-case” requires care—are we assuming an input distribution over the\\nkeys, or are we randomizing the choice of hash function itself? We’ll consider both approaches,\\nbut with an emphasis on the use of a randomly chosen hash function.\\n2 In the literature, a (c/m)-universal hash function is sometimes called c-universal or c-\\napproximately universal. We’ll stick with the notation (c/m)-universal.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 416}),\n",
              " Document(page_content='12\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0Binary Search Trees\\nThe search tree data structure supports each of the dynamic-set\\noperations listed on page 250: SEARCH, MINIMUM, MAXIMUM,\\nPREDECESSOR, SUCCESSOR, INSERT, and DELETE. Thus, you\\ncan use a search tree both as a dictionary and as a priority queue.\\nBasic operations on a binary search tree take time proportional to\\nthe height of the tree. For a complete binary tree with n nodes, such\\noperations run in Θ (lg n) worst-case time. If the tree is a linear chain of\\nn nodes, however, the same operations take Θ (n) worst-case time. In\\nChapter 13, we’ll see a variation of binary search trees, red-black trees,\\nwhose operations guarantee a height of O(lg n). We won’t prove it here,\\nbut if you build a binary search tree on a random set of n keys, its\\nexpected height is O(lg n) even if you don’t try to limit its height.\\nAfter presenting the basic properties of binary search trees, the\\nfollowing sections show how to walk a binary search tree to print its\\nvalues in sorted order, how to search for a value in a binary search tree,\\nhow to ﬁnd the minimum or maximum element, how to ﬁnd the\\npredecessor or successor of an element, and how to insert into or delete\\nfrom a binary search tree. The basic mathematical properties of trees\\nappear in Appendix B.\\n12.1\\xa0\\xa0\\xa0\\xa0What is a binary search tree?\\nA binary search tree is organized, as the name suggests, in a binary tree,\\nas shown in Figure 12.1. You can represent such a tree with a linked', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 417}),\n",
              " Document(page_content='data structure, as in Section 10.3. In addition to a key and satellite data,\\neach node object contains attributes left, right, and p that point to the\\nnodes corresponding to its left child, its right child, and its parent,\\nrespectively. If a child or the parent is missing, the appropriate attribute\\ncontains the value NIL. The tree itself has an attribute root\\xa0that points\\nto the root node, or NIL if the tree is empty. The root node T.root is the\\nonly node in a tree T whose parent is NIL.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 418}),\n",
              " Document(page_content='Figure 12.1 Binary search trees. For any node x, the keys in the left subtree of x are at most\\nx.key, and the keys in the right subtree of x are at least x.key. Different binary search trees can\\nrepresent the same set of values. The worst-case running time for most search-tree operations is\\nproportional to the height of the tree. (a) A binary search tree on 6 nodes with height 2. The top\\nﬁgure shows how to view the tree conceptually, and the bottom ﬁgure shows the left, right, and p\\nattributes in each node, in the style of Figure 10.6 on page 266. (b) A less efﬁcient binary search\\ntree, with height 4, that contains the same keys.\\nThe keys in a binary search tree are always stored in such a way as to\\nsatisfy the binary-search-tree property:\\nLet x be a node in a binary search tree. If y is a node in the left\\nsubtree of x, then y.key ≤ x.key. If y is a node in the right\\nsubtree of x, then y.key ≥ x.key.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 419}),\n",
              " Document(page_content='Thus, in Figure 12.1(a), the key of the root is 6, the keys 2, 5, and 5\\nin its left subtree are no larger than 6, and the keys 7 and 8 in its right\\nsubtree are no smaller than 6. The same property holds for every node\\nin the tree. For example, looking at the root’s left child as the root of a\\nsubtree, this subtree root has the key 5, the key 2 in its left subtree is no\\nlarger than 5, and the key 5 in its right subtree is no smaller than 5.\\nBecause of the binary-search-tree property, you can print out all the\\nkeys in a binary search tree in sorted order by a simple recursive\\nalgorithm, called an inorder tree walk, given by the procedure\\nINORDER-TREE-WALK. This algorithm is so named because it\\nprints the key of the root of a subtree between printing the values in its\\nleft subtree and printing those in its right subtree. (Similarly, a preorder\\ntree walk prints the root before the values in either subtree, and a\\npostorder tree walk prints the root after the values in its subtrees.) To\\nprint all the elements in a binary search tree T, call INORDER-TREE-\\nWALK(T.root). For example, the inorder tree walk prints the keys in\\neach of the two binary search trees from Figure 12.1 in the order 2, 5, 5,\\n6, 7, 8. The correctness of the algorithm follows by induction directly\\nfrom the binary-search-tree property.\\nINORDER-TREE-WALK(x)\\n1if\\xa0x ≠ NIL\\n2 INORDER-TREE-WALK(x.left)\\n3 print x.key\\n4 INORDER-TREE-WALK(x.right)\\nIt takes Θ (n) time to walk an n-node binary search tree, since after\\nthe initial call, the procedure calls itself recursively exactly twice for\\neach node in the tree—once for its left child and once for its right child.\\nThe following theorem gives a formal proof that it takes linear time to\\nperform an inorder tree walk.\\nTheorem 12.1\\nIf x is the root of an n-node subtree, then the call INORDER-TREE-\\nWALK(x) takes Θ (n) time.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 420}),\n",
              " Document(page_content='Proof\\xa0\\xa0\\xa0Let T(n) denote the time taken by INORDER-TREE-WALK\\nwhen it is called on the root of an n-node subtree. Since INORDER-\\nTREE-WALK visits all n nodes of the subtree, we have T(n) = Ω(n). It\\nremains to show that T(n) = O(n).\\nSince INORDER-TREE-WALK takes a small, constant amount of\\ntime on an empty subtree (for the test x ≠ NIL), we have T(0) = c for\\nsome constant c > 0.\\nFor n > 0, suppose that INORDER-TREE-WALK is called on a\\nnode x whose left subtree has k nodes and whose right subtree has n − k\\n− 1 nodes. The time to perform INORDER-TREE-WALK(x) is\\nbounded by T(n) ≤ T(k) + T(n − k − 1) + d for some constant d > 0 that\\nreﬂects an upper bound on the time to execute the body of INORDER-\\nTREE-WALK(x), exclusive of the time spent in recursive calls.\\nWe use the substitution method to show that T(n) = O(n) by proving\\nthat T(n) ≤ (c + d)n + c. For n = 0, we have (c + d) · 0 + c = c = T(0). For\\nn > 0, we have\\nT(n)≤T(k) + T(n − k − 1) + d\\n≤((c + d)k + c) + ((c + d)(n − k − 1) + c) + d\\n=(c + d)n + c − (c + d) + c + d\\n=(c + d)n + c,\\nwhich completes the proof.\\n▪\\nExercises\\n12.1-1\\nFor the set {1, 4, 5, 10, 16, 17, 21} of keys, draw binary search trees of\\nheights 2, 3, 4, 5, and 6.\\n12.1-2\\nWhat is the difference between the binary-search-tree property and the\\nmin-heap property on page 163? Can the min-heap property be used to\\nprint out the keys of an n-node tree in sorted order in O(n) time? Show\\nhow, or explain why not.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 421}),\n",
              " Document(page_content='12.1-3\\nGive a nonrecursive algorithm that performs an inorder tree walk.\\n(Hint: An easy solution uses a stack as an auxiliary data structure. A\\nmore complicated, but elegant, solution uses no stack but assumes that\\nyou can test two pointers for equality.)\\n12.1-4\\nGive recursive algorithms that perform preorder and postorder tree\\nwalks in Θ (n) time on a tree of n nodes.\\n12.1-5\\nArgue that since sorting n elements takes Ω(n lg n) time in the worst case\\nin the comparison model, any comparison-based algorithm for\\nconstructing a binary search tree from an arbitrary list of n elements\\ntakes Ω(n lg n) time in the worst case.\\n12.2\\xa0\\xa0\\xa0\\xa0Querying a binary search tree\\nBinary search trees can support the queries MINIMUM, MAXIMUM,\\nSUCCESSOR, and PREDECESSOR, as well as SEARCH. This\\nsection examines these operations and shows how to support each one\\nin O(h) time on any binary search tree of height h.\\nSearching\\nTo search for a node with a given key in a binary search tree, call the\\nTREE-SEARCH procedure. Given a pointer x to the root of a subtree\\nand a key k, TREE-SEARCH(x, k) returns a pointer to a node with key\\nk if one exists in the subtree; otherwise, it returns NIL. To search for key\\nk in the entire binary search tree T, call TREE-SEARCH(T.root, k).\\nTREE-SEARCH(x, k)\\n1if\\xa0x == NIL or k == x.key\\n2 return\\xa0x\\n3if\\xa0k < x.key\\n4 return TREE-SEARCH(x.left, k)', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 422}),\n",
              " Document(page_content='5else return TREE-SEARCH(x.right, k)\\nITERATIVE-TREE-SEARCH(x, k)\\n1while\\xa0x ≠ NIL and k ≠ x.key\\n2 if\\xa0k < x.key\\n3 x = x.left\\n4 else\\xa0x = x.right\\n5return\\xa0x\\nThe TREE-SEARCH procedure begins its search at the root and\\ntraces a simple path downward in the tree, as shown in Figure 12.2(a).\\nFor each node x it encounters, it compares the key k with x.key. If the\\ntwo keys are equal, the search terminates. If k is smaller than x.key, the\\nsearch continues in the left subtree of x, since the binary-search-tree\\nproperty implies that k cannot reside in the right subtree. Symmetrically,\\nif k is larger than x.key, the search continues in the right subtree. The\\nnodes encountered during the recursion form a simple path downward\\nfrom the root of the tree, and thus the running time of TREE-SEARCH\\nis O(h), where h is the height of the tree.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 423}),\n",
              " Document(page_content='Figure 12.2 Queries on a binary search tree. Nodes and paths followed in each query are colored\\nblue. (a) A search for the key 13 in the tree follows the path 15 →  6 →  7 →  13 from the root. (b)\\nThe minimum key in the tree is 2, which is found by following left pointers from the root. The\\nmaximum key 20 is found by following right pointers from the root. (c) The successor of the\\nnode with key 15 is the node with key 17, since it is the minimum key in the right subtree of 15.\\n(d) The node with key 13 has no right subtree, and thus its successor is its lowest ancestor whose\\nleft child is also an ancestor. In this case, the node with key 15 is its successor.\\nSince the TREE-SEARCH procedure recurses on either the left\\nsubtree or the right subtree, but not both, we can rewrite the algorithm\\nto “unroll” the recursion into a while loop. On most computers, the\\nITERATIVE-TREE-SEARCH procedure on the facing page is more\\nefﬁcient.\\nMinimum and maximum', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 424}),\n",
              " Document(page_content='To ﬁnd an element in a binary search tree whose key is a minimum, just\\nfollow left child pointers from the root until you encounter a NIL, as\\nshown in Figure 12.2(b). The TREE-MINIMUM procedure returns a\\npointer to the minimum element in the subtree rooted at a given node x,\\nwhich we assume to be non-NIL.\\nTREE-MINIMUM(x)\\n1while\\xa0x.left ≠ NIL\\n2x = x.left\\n3return\\xa0x\\nTREE-MAXIMUM(x)\\n1while\\xa0x.right ≠ NIL\\n2x = x.right\\n3return\\xa0x\\nThe binary-search-tree property guarantees that TREE-MINIMUM\\nis correct. If node x has no left subtree, then since every key in the right\\nsubtree of x is at least as large as x.key, the minimum key in the subtree\\nrooted at x is x.key. If node x has a left subtree, then since no key in the\\nright subtree is smaller than x.key and every key in the left subtree is not\\nlarger than x.key, the minimum key in the subtree rooted at x resides in\\nthe subtree rooted at x.left.\\nThe pseudocode for TREE-MAXIMUM is symmetric. Both TREE-\\nMINIMUM and TREE-MAXIMUM run in O(h) time on a tree of\\nheight h since, as in TREE-SEARCH, the sequence of nodes\\nencountered forms a simple path downward from the root.\\nSuccessor and predecessor\\nGiven a node in a binary search tree, how can you ﬁnd its successor in\\nthe sorted order determined by an inorder tree walk? If all keys are\\ndistinct, the successor of a node x is the node with the smallest key\\ngreater than x.key. Regardless of whether the keys are distinct, we deﬁne\\nthe successor of a node as the next node visited in an inorder tree walk.\\nThe structure of a binary search tree allows you to determine the', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 425}),\n",
              " Document(page_content='successor of a node without comparing keys. The TREE-SUCCESSOR\\nprocedure on the facing page returns the successor of a node x in a\\nbinary search tree if it exists, or NIL if x is the last node that would be\\nvisited during an inorder walk.\\nThe code for TREE-SUCCESSOR has two cases. If the right subtree\\nof node x is nonempty, then the successor of x is just the leftmost node\\nin x’s right subtree, which line 2 ﬁnds by calling TREE-\\nMINIMUM(x.right). For example, the successor of the node with key\\n15 in Figure 12.2(c) is the node with key 17.\\nOn the other hand, as Exercise 12.2-6 asks you to show, if the right\\nsubtree of node x is empty and x has a successor y, then y is the lowest\\nancestor of x whose left child is also an ancestor of x. In Figure 12.2(d),\\nthe successor of the node with key 13 is the node with key 15. To ﬁnd y,\\ngo up the tree from x until you encounter either the root or a node that\\nis the left child of its parent. Lines 4–8 of TREE-SUCCESSOR handle\\nthis case.\\nTREE-SUCCESSOR(x)\\n1if\\xa0x.right ≠ NIL\\n2 return TREE-MINIMUM(x.right)\\xa0\\xa0// leftmost node in right\\nsubtree\\n3else\\xa0// ﬁnd the lowest ancestor of x whose left child is an ancestor of\\nx\\n4 y = x.p\\n5 while\\xa0y ≠ NIL and x == y.right\\n6 x = y\\n7 y = y.p\\n8 return\\xa0y\\nThe running time of TREE-SUCCESSOR on a tree of height h is\\nO(h), since it either follows a simple path up the tree or follows a simple\\npath down the tree. The procedure TREE-PREDECESSOR, which is\\nsymmetric to TREE-SUCCESSOR, also runs in O(h) time.\\nIn summary, we have proved the following theorem.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 426}),\n",
              " Document(page_content='Theorem 12.2\\nThe dynamic-set operations SEARCH, MINIMUM, MAXIMUM,\\nSUCCESSOR, and PREDECESSOR can be implemented so that each\\none runs in O(h) time on a binary search tree of height h.\\n▪\\nExercises\\n12.2-1\\nYou are searching for the number 363 in a binary search tree containing\\nnumbers between 1 and 1000. Which of the following sequences cannot\\nbe the sequence of nodes examined?\\na. 2, 252, 401, 398, 330, 344, 397, 363.\\nb. 924, 220, 911, 244, 898, 258, 362,  363.\\nc. 925, 202, 911, 240, 912, 245, 363.\\nd. 2, 399, 387, 219, 266, 382, 381, 278,  363.\\ne. 935, 278, 347, 621, 299, 392, 358,  363.\\n12.2-2\\nWrite recursive versions of TREE-MINIMUM and TREE-\\nMAXIMUM.\\n12.2-3\\nWrite the TREE-PREDECESSOR procedure.\\n12.2-4\\nProfessor Kilmer claims to have discovered a remarkable property of\\nbinary search trees. Suppose that the search for key k in a binary search\\ntree ends up at a leaf. Consider three sets: A, the keys to the left of the\\nsearch path; B, the keys on the search path; and C, the keys to the right\\nof the search path. Professor Kilmer claims that any three keys a ∈ A, b\\n∈ B, and c ∈ C must satisfy a ≤ b ≤ c. Give a smallest possible\\ncounterexample to the professor’s claim.\\n12.2-5', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 427}),\n",
              " Document(page_content='Show that if a node in a binary search tree has two children, then its\\nsuccessor has no left child and its predecessor has no right child.\\n12.2-6\\nConsider a binary search tree T whose keys are distinct. Show that if the\\nright subtree of a node x in T is empty and x has a successor y, then y is\\nthe lowest ancestor of x whose left child is also an ancestor of x. (Recall\\nthat every node is its own ancestor.)\\n12.2-7\\nAn alternative method of performing an inorder tree walk of an n-node\\nbinary search tree ﬁnds the minimum element in the tree by calling\\nTREE-MINIMUM and then making n − 1 calls to TREE-\\nSUCCESSOR. Prove that this algorithm runs in Θ (n) time.\\n12.2-8\\nProve that no matter what node you start at in a height-h binary search\\ntree, k successive calls to TREE-SUCCESSOR take O(k + h) time.\\n12.2-9\\nLet T be a binary search tree whose keys are distinct, let x be a leaf\\nnode, and let y be its parent. Show that y.key is either the smallest key in\\nT larger than x.key or the largest key in T smaller than x.key.\\n12.3\\xa0\\xa0\\xa0\\xa0Insertion and deletion\\nThe operations of insertion and deletion cause the dynamic set\\nrepresented by a binary search tree to change. The data structure must\\nbe modiﬁed to reﬂect this change, but in such a way that the binary-\\nsearch-tree property continues to hold. We’ll see that modifying the tree\\nto insert a new element is relatively straightforward, but deleting a node\\nfrom a binary search tree is more complicated.\\nInsertion\\nThe TREE-INSERT procedure inserts a new node into a binary search\\ntree. The procedure takes a binary search tree T and a node z for which', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 428}),\n",
              " Document(page_content='z.key has already been ﬁlled in, z.left = NIL, and z.right = NIL. It\\nmodiﬁes T and some of the attributes of z so as to insert z into an\\nappropriate position in the tree.\\nTREE-INSERT(T, z)\\n\\xa0\\xa01x = T.root // node being compared with z\\n\\xa0\\xa02y = NIL //\\xa0y will be parent of z\\n\\xa0\\xa03while\\xa0x ≠ NIL // descend until reaching a leaf\\n\\xa0\\xa04y = x\\n\\xa0\\xa05if\\xa0z.key < x.key\\n\\xa0\\xa06 x = x.left\\n\\xa0\\xa07else\\xa0x = x.right\\n\\xa0\\xa08z.p = y // found the location—insert z with parent y\\n\\xa0\\xa09if\\xa0y == NIL\\n10T.root = z // tree T was empty\\n11elseif\\xa0z.key < y.key\\n12y.left = z\\n13else\\xa0y.right = z\\nFigure 12.3 shows how TREE-INSERT works. Just like the\\nprocedures TREE-SEARCH and ITERATIVE-TREE-SEARCH,\\nTREE-INSERT begins at the root of the tree and the pointer x traces a\\nsimple path downward looking for a NIL to replace with the input node\\nz. The procedure maintains the trailing pointer\\xa0y as the parent of x.\\nAfter initialization, the while loop in lines 3–7 causes these two pointers\\nto move down the tree, going left or right depending on the comparison\\nof z.key with x.key, until x becomes NIL. This NIL occupies the\\nposition where node z will go. More precisely, this NIL is a left or right\\nattribute of the node that will become z’s parent, or it is T.root if tree T\\nis currently empty. The procedure needs the trailing pointer y, because\\nby the time it ﬁnds the NIL where z belongs, the search has proceeded\\none step beyond the node that needs to be changed. Lines 8–13 set the\\npointers that cause z to be inserted.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 429}),\n",
              " Document(page_content='Figure 12.3 Inserting a node with key 13 into a binary search tree. The simple path from the root\\ndown to the position where the node is inserted is shown in blue. The new node and the link to\\nits parent are highlighted in orange.\\nLike the other primitive operations on search trees, the procedure\\nTREE-INSERT runs in O(h) time on a tree of height h.\\nDeletion\\nThe overall strategy for deleting a node z from a binary search tree T\\nhas three basic cases and, as we’ll see, one of the cases is a bit tricky.\\nIf z has no children, then simply remove it by modifying its parent\\nto replace z with NIL as its child.\\nIf z has just one child, then elevate that child to take z’s position\\nin the tree by modifying z’s parent to replace z by z’s child.\\nIf z has two children, ﬁnd z’s successor y—which must belong to\\nz’s right subtree—and move y to take z’s position in the tree. The\\nrest of z’s original right subtree becomes y’s new right subtree, and\\nz’s left subtree becomes y’s new left subtree. Because y is z’s\\nsuccessor, it cannot have a left child, and y’s original right child\\nmoves into y’s original position, with the rest of y’s original right\\nsubtree following automatically. This case is the tricky one\\nbecause, as we’ll see, it matters whether y is z’s right child.\\nThe procedure for deleting a given node z from a binary search tree\\nT takes as arguments pointers to T and z. It organizes its cases a bit\\ndifferently from the three cases outlined previously by considering the\\nfour cases shown in Figure 12.4.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 430}),\n",
              " Document(page_content='Figure 12.4 Deleting a node z, in blue, from a binary search tree. Node z may be the root, a left\\nchild of node q, or a right child of q. The node that will replace node z in its position in the tree\\nis colored orange. (a) Node z has no left child. Replace z by its right child r, which may or may\\nnot be NIL. (b) Node z has a left child l but no right child. Replace z by l. (c) Node z has two\\nchildren. Its left child is node l, its right child is its successor y (which has no left child), and y’s\\nright child is node x. Replace z by y, updating y’s left child to become l, but leaving x as y’s right\\nchild. (d) Node z has two children (left child l and right child r), and its successor y ≠ r lies\\nwithin the subtree rooted at r. First replace y by its own right child x, and set y to be r’s parent.\\nThen set y to be q’s child and the parent of l.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 431}),\n",
              " Document(page_content='If z has no left child, then as in part (a) of the ﬁgure, replace z by\\nits right child, which may or may not be NIL. When z’s right child\\nis NIL, this case deals with the situation in which z has no\\nchildren. When z’s right child is non-NIL, this case handles the\\nsituation in which z has just one child, which is its right child.\\nOtherwise, if z has just one child, then that child is a left child. As\\nin part (b) of the ﬁgure, replace z by its left child.\\nOtherwise, z has both a left and a right child. Find z’s successor y,\\nwhich lies in z’s right subtree and has no left child (see Exercise\\n12.2-5). Splice node y out of its current location and replace z by y\\nin the tree. How to do so depends on whether y is z’s right child:\\nIf y is z’s right child, then as in part (c) of the ﬁgure, replace\\nz by y, leaving y’s right child alone.\\nOtherwise, y lies within z’s right subtree but is not z’s right\\nchild. In this case, as in part (d) of the ﬁgure, ﬁrst replace y\\nby its own right child, and then replace z by y.\\nAs part of the process of deleting a node, subtrees need to move\\naround within the binary search tree. The subroutine TRANSPLANT\\nreplaces one subtree as a child of its parent with another subtree. When\\nTRANSPLANT replaces the subtree rooted at node u with the subtree\\nrooted at node v, node u’s parent becomes node v’s parent, and u’s\\nparent ends up having v as its appropriate child. TRANSPLANT allows\\nv to be NIL instead of a pointer to a node.\\nTRANSPLANT(T, u, v)\\n1if\\xa0u.p == NIL\\n2 T.root = v\\n3elseif\\xa0u == u.p.left\\n4 u.p.left = v\\n5else\\xa0u.p.right = v\\n6if\\xa0v ≠ NIL\\n7 v.p = u.p', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 432}),\n",
              " Document(page_content='Here is how TRANSPLANT works. Lines 1–2 handle the case in\\nwhich u is the root of T. Otherwise, u is either a left child or a right child\\nof its parent. Lines 3–4 take care of updating u.p.left if u is a left child,\\nand line 5 updates u.p.right if u is a right child. Because v may be NIL,\\nlines 6–7 update v.p only if v is non-NIL. The procedure\\nTRANSPLANT does not attempt to update v.left and v.right. Doing so,\\nor not doing so, is the responsibility of TRANSPLANT’s caller.\\nThe procedure TREE-DELETE on the facing page uses\\nTRANSPLANT to delete node z from binary search tree T. It executes\\nthe four cases as follows. Lines 1–2 handle the case in which node z has\\nno left child (Figure 12.4(a)), and lines 3–4 handle the case in which z\\nhas a left child but no right child (Figure 12.4(b)). Lines 5–12 deal with\\nthe remaining two cases, in which z has two children. Line 5 ﬁnds node\\ny, which is the successor of z. Because z has a nonempty right subtree,\\nits successor must be the node in that subtree with the smallest key;\\nhence the call to TREE-MINIMUM(z.right). As we noted before, y has\\nno left child. The procedure needs to splice y out of its current location\\nand replace z by y in the tree. If y is z’s right child (Figure 12.4(c)), then\\nlines 10–12 replace z as a child of its parent by y and replace y’s left\\nchild by z’s left child. Node y retains its right child (x in Figure 12.4(c)),\\nand so no change to y.right needs to occur. If y is not z’s right child\\n(Figure 12.4(d)), then two nodes have to move. Lines 7–9 replace y as a\\nchild of its parent by y’s right child (x in Figure 12.4(c)) and make z’s\\nright child (r in the ﬁgure) become y’s right child instead. Finally, lines\\n10–12 replace z as a child of its parent by y and replace y’s left child by\\nz’s left child.\\nTREE-DELETE(T, z)\\n\\xa0\\xa01if\\xa0z.left == NIL\\n\\xa0\\xa02TRANSPLANT(T, z, z.right) // replace z by its right child\\n\\xa0\\xa03elseif\\xa0z.right == NIL\\n\\xa0\\xa04TRANSPLANT(T, z, z.left) // replace z by its left child\\n\\xa0\\xa05else\\xa0y = TREE-MINIMUM(z.right)//\\xa0y is z’s successor\\n\\xa0\\xa06if\\xa0y ≠ z.right // is y farther down the tree?\\n\\xa0\\xa07 TRANSPLANT(T, y, y.right)// replace y by its right child', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 433}),\n",
              " Document(page_content='\\xa0\\xa08 y.right = z.right //\\xa0z’s right child becomes\\n\\xa0\\xa09 y.right.p = y //\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0y’s right child\\n10TRANSPLANT(T, z, y) // replace z by its successor y\\n11y.left = z.left // and give z’s left child to y,\\n12y.left.p = y //\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0w hich had no left child\\nEach line of TREE-DELETE, including the calls to\\nTRANSPLANT, takes constant time, except for the call to TREE-\\nMINIMUM in line 5. Thus, TREE-DELETE runs in O(h) time on a\\ntree of height h.\\nIn summary, we have proved the following theorem.\\nTheorem 12.3\\nThe dynamic-set operations INSERT and DELETE can be\\nimplemented so that each one runs in O(h) time on a binary search tree\\nof height h.\\n▪\\nExercises\\n12.3-1\\nGive a recursive version of the TREE-INSERT procedure.\\n12.3-2\\nSuppose that you construct a binary search tree by repeatedly inserting\\ndistinct values into the tree. Argue that the number of nodes examined\\nin searching for a value in the tree is 1 plus the number of nodes\\nexamined when the value was ﬁrst inserted into the tree.\\n12.3-3\\nYou can sort a given set of n numbers by ﬁrst building a binary search\\ntree containing these numbers (using TREE-INSERT repeatedly to\\ninsert the numbers one by one) and then printing the numbers by an\\ninorder tree walk. What are the worst-case and best-case running times\\nfor this sorting algorithm?\\n12.3-4', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 434}),\n",
              " Document(page_content='When TREE-DELETE calls TRANSPLANT, under what\\ncircumstances can the parameter v of TRANSPLANT be NIL?\\n12.3-5\\nIs the operation of deletion “commutative” in the sense that deleting x\\nand then y from a binary search tree leaves the same tree as deleting y\\nand then x? Argue why it is or give a counterexample.\\n12.3-6\\nSuppose that instead of each node x keeping the attribute x.p, pointing\\nto x’s parent, it keeps x.succ, pointing to x’s successor. Give pseudocode\\nfor TREE-SEARCH, TREE-INSERT, and TREE-DELETE on a\\nbinary search tree T using this representation. These procedures should\\noperate in O(h) time, where h is the height of the tree T. You may\\nassume that all keys in the binary search tree are distinct. (Hint: You\\nmight wish to implement a subroutine that returns the parent of a\\nnode.)\\n12.3-7\\nWhen node z in TREE-DELETE has two children, you can choose\\nnode y to be its predecessor rather than its successor. What other\\nchanges to TREE-DELETE are necessary if you do so? Some have\\nargued that a fair strategy, giving equal priority to predecessor and\\nsuccessor, yields better empirical performance. How might TREE-\\nDELETE be minimally changed to implement such a fair strategy?\\nProblems\\n12-1\\xa0\\xa0\\xa0\\xa0\\xa0B inary search trees with equal keys\\nEqual keys pose a problem for the implementation of binary search\\ntrees.\\na. What is the asymptotic performance of TREE-INSERT when used to\\ninsert n items with identical keys into an initially empty binary search\\ntree?', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 435}),\n",
              " Document(page_content='Consider changing TREE-INSERT to test whether z.key = x.key before\\nline 5 and to test whether z.key = y.key before line 11. If equality holds,\\nimplement one of the following strategies. For each strategy, ﬁnd the\\nasymptotic performance of inserting n items with identical keys into an\\ninitially empty binary search tree. (The strategies are described for line\\n5, which compares the keys of z and x. Substitute y for x to arrive at the\\nstrategies for line 11.)\\nb. Keep a boolean ﬂag x.b at node x, and set x to either x.left or x.right\\nbased on the value of x.b, which alternates between FALSE and\\nTRUE each time TREE-INSERT visits x while inserting a node with\\nthe same key as x.\\nc. Keep a list of nodes with equal keys at x, and insert z into the list.\\nd. Randomly set x to either x.left or x.right. (Give the worst-case\\nperformance and informally derive the expected running time.)\\n12-2\\xa0\\xa0\\xa0\\xa0\\xa0R adix trees\\nGiven two strings a = a0a1 … ap and b = b0b1 … bq, where each ai and\\neach bj belongs to some ordered set of characters, we say that string a is\\nlexicographically less than string b if either\\n1. there exists an integer j, where 0 ≤ j ≤ min {p, q}, such that ai = bi\\nfor all i = 0, 1, …, j − 1 and aj < bj, or\\n2. p < q and ai = bi for all i = 0, 1, …, p.\\nFor example, if a and b are bit strings, then 10100 < 10110 by rule 1\\n(letting j = 3) and 10100 < 101000 by rule 2. This ordering is similar to\\nthat used in English-language dictionaries.\\nThe radix tree data structure shown in Figure 12.5 (also known as a\\ntrie) stores the bit strings 1011, 10, 011, 100, and 0. When searching for\\na key a = a0a1 … ap, go left at a node of depth i if ai = 0 and right if ai\\n= 1. Let S be a set of distinct bit strings whose lengths sum to n. Show\\nhow to use a radix tree to sort S lexicographically in Θ (n) time. For the', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 436}),\n",
              " Document(page_content='example in Figure 12.5, the output of the sort should be the sequence 0,\\n011, 10, 100, 1011.\\nFigure 12.5 A radix tree storing the bit strings 1011, 10, 011, 100, and 0. To determine each\\nnode’s key, traverse the simple path from the root to that node. There is no need, therefore, to\\nstore the keys in the nodes. The keys appear here for illustrative purposes only. Keys\\ncorresponding to blue nodes are not in the tree. Such nodes are present only to establish a path\\nto other nodes.\\n12-3\\xa0\\xa0\\xa0\\xa0\\xa0A verage node depth in a random ly bui lt binary search tree\\nA randomly built binary search tree on n keys is a binary search tree\\ncreated by starting with an empty tree and inserting the keys in random\\norder, where each of the n! permutations of the keys is equally likely. In\\nthis problem, you will prove that the average depth of a node in a\\nrandomly built binary search tree with n nodes is O(lg n). The technique\\nreveals a surprising similarity between the building of a binary search\\ntree and the execution of RANDOMIZED-QUICKSORT from Section\\n7.3.\\nDenote the depth of any node x in tree T by d(x, T). Then the total\\npath length\\xa0P(T) of a tree T is the sum, over all nodes x in T, of d(x, T).\\na. Argue that the average depth of a node in T is\\nThus, you need to show that the expected value of P(T) is O(n lg n).', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 437}),\n",
              " Document(page_content='b. Let TL and TR denote the left and right subtrees of tree T,\\nrespectively. Argue that if T has n nodes, then\\nP(T) = P(TL) + P(TR) + n − 1.\\nc. Let P(n) denote the average total path length of a randomly built\\nbinary search tree with n nodes. Show that\\nd. Show how to rewrite P(n) as\\ne. Recalling the alternative analysis of the randomized version of\\nquicksort given in Problem 7-3, conclude that P(n) = O(n lg n).\\nEach recursive invocation of randomized quicksort chooses a random\\npivot element to partition the set of elements being sorted. Each node of\\na binary search tree partitions the set of elements that fall into the\\nsubtree rooted at that node.\\nf. Describe an implementation of quicksort in which the comparisons to\\nsort a set of elements are exactly the same as the comparisons to insert\\nthe elements into a binary search tree. (The order in which\\ncomparisons are made may differ, but the same comparisons must\\noccur.)\\n12-4\\xa0\\xa0\\xa0\\xa0\\xa0N umber of different binary trees\\nLet bn denote the number of different binary trees with n nodes. In this\\nproblem, you will ﬁnd a formula for bn, as well as an asymptotic\\nestimate.\\na. Show that b0 = 1 and that, for n ≥ 1,', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 438}),\n",
              " Document(page_content='b. Referring to Problem 4-5 on page 121 f or the deﬁnition of a\\ngenerating function, let B(x) be the generating function\\nShow that B(x) = xB(x)2 + 1, and hence one way to express B(x) in\\nclosed form is\\nThe Taylor expansion of f(x) around the point x = a is given by\\nwhere f(k)(x) is the kth derivative of f evaluated at x.\\nc. Show that\\n(the nth Catalan number) by using the Taylor expansion of \\naround x = 0. (If you wish, instead of using the Taylor expansion, you\\nmay use the generalization of the binomial theorem, equation (C.4) on\\npage 1181, to noninteger exponents n, where for any real number n\\nand for any integer k, you can interpret \\n to be n(n − 1) … (n − k +\\n1)/k! if k ≥ 0, and 0 otherwise.)\\nd. Show that\\n', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 439}),\n",
              " Document(page_content='Chapter notes\\nKnuth [261] contains a good discussion of simple binary search trees as\\nwell as many variations. Binary search trees seem to have been\\nindependently discovered by a number of people in the late 1950s. Radix\\ntrees are often called “tries,” which comes from the middle letters in the\\nword retrieval. Knuth [261] also discusses them.\\nMany texts, including the ﬁrst two editions of this book, describe a\\nsomewhat simpler method of deleting a node from a binary search tree\\nwhen both of its children are present. Instead of replacing node z by its\\nsuccessor y, delete node y but copy its key and satellite data into node z.\\nThe downside of this approach is that the node actually deleted might\\nnot be the node passed to the delete procedure. If other components of\\na program maintain pointers to nodes in the tree, they could mistakenly\\nend up with “stale” pointers to nodes that have been deleted. Although\\nthe deletion method presented in this edition of this book is a bit more\\ncomplicated, it guarantees that a call to delete node z deletes node z and\\nonly node z.\\nSection 14.5 will show how to construct an optimal binary search\\ntree when you know the search frequencies before constructing the tree.\\nThat is, given the frequencies of searching for each key and the\\nfrequencies of searching for values that fall between keys in the tree, a\\nset of searches in the constructed binary search tree examines the\\nminimum number of nodes.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 440}),\n",
              " Document(page_content='13\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0Red-Black Trees\\nChapter 12 showed that a binary search tree of height h can support any\\nof the basic dynamic-set operations—such as SEARCH,\\nPREDECESSOR, SUCCESSOR, MINIMUM, MAXIMUM,\\nINSERT, and DELETE—in O(h) time. Thus, the set operations are fast\\nif the height of the search tree is small. If its height is large, however, the\\nset operations may run no faster than with a linked list. Red-black trees\\nare one of many search-tree schemes that are “balanced” in order to\\nguarantee that basic dynamic-set operations take O(lg n) time in the\\nworst case.\\n13.1\\xa0\\xa0\\xa0\\xa0Properties of red-black trees\\nA red-black tree is a binary search tree with one extra bit of storage per\\nnode: its color, which can be either RED or BLACK. By constraining\\nthe node colors on any simple path from the root to a leaf, red-black\\ntrees ensure that no such path is more than twice as long as any other, so\\nthat the tree is approximately balanced. Indeed, as we’re about to see, the\\nheight of a red-black tree with n keys is at most 2 lg(n + 1), which is O(lg\\nn).\\nEach node of the tree now contains the attributes color, key, left,\\nright, and p. If a child or the parent of a node does not exist, the\\ncorresponding pointer attribute of the node contains the value NIL.\\nThink of these NILs as pointers to leaves (external nodes) of the binary', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 441}),\n",
              " Document(page_content='search tree and the normal, key-bearing nodes as internal nodes of the\\ntree.\\nA red-black tree is a binary search tree that satisﬁes the following red-\\nblack properties:\\n1. Every node is either red or black.\\n2. The root is black.\\n3. Every leaf (NIL) is black.\\n4. If a node is red, then both its children are black.\\n5. For each node, all simple paths from the node to descendant\\nleaves contain the same number of black nodes.\\nFigure 13.1(a) shows an example of a red-black tree.\\nAs a matter of convenience in dealing with boundary conditions in\\nred-black tree code, we use a single sentinel to represent NIL (see page\\n262). For a red-black tree T, the sentinel T.nil is an object with the same\\nattributes as an ordinary node in the tree. Its color attribute is BLACK,\\nand its other attributes—p, left, right, and key—can take on arbitrary\\nvalues. As Figure 13.1(b) shows, all pointers to NIL are replaced by\\npointers to the sentinel T.nil.\\nWhy use the sentinel? The sentinel makes it possible to treat a NIL\\nchild of a node x as an ordinary node whose parent is x. An alternative\\ndesign would use a distinct sentinel node for each NIL in the tree, so\\nthat the parent of each NIL is well deﬁned. That approach needlessly\\nwastes space, however. Instead, just the one sentinel T.nil represents all\\nthe NILs—all leaves and the root’s parent. The values of the attributes p,\\nleft, right, and key of the sentinel are immaterial. The red-black tree\\nprocedures can place whatever values in the sentinel that yield simpler\\ncode.\\nWe generally conﬁne our interest to the internal nodes of a red-black\\ntree, since they hold the key values. The remainder of this chapter omits\\nthe leaves in drawings of red-black trees, as shown in Figure 13.1(c).\\nWe call the number of black nodes on any simple path from, but not\\nincluding, a node x down to a leaf the black-height of the node, denoted\\nbh(x). By property 5, the notion of black-height is well deﬁned, since all', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 442}),\n",
              " Document(page_content='descending simple paths from the node have the same number of black\\nnodes. The black-height of a red-black tree is the black-height of its\\nroot.\\nThe following lemma shows why red-black trees make good search\\ntrees.\\nLemma 13.1\\nA red-black tree with n internal nodes has height at most 2 lg(n + 1).\\nProof\\xa0\\xa0\\xa0We start by showing that the subtree rooted at any node x\\ncontains at least 2bh(x) − 1 internal nodes. We prove this claim by\\ninduction on the height of x. If the height of x is 0, then x must be a leaf\\n(T.nil), and the subtree rooted at x indeed contains at least 2bh(x) − 1 =\\n20 − 1 = 0 internal nodes. For the inductive step, consider a node x that\\nhas positive height and is an internal node. Then node x has two\\nchildren, either or both of which may be a leaf. If a child is black, then it\\ncontributes 1 to x’s black-height but not to its own. If a child is red, then\\nit contributes to neither x’s black-height nor its own. Therefore, each\\nchild has a black-height of either bh(x) − 1 (if it’s black) or bh(x) (if it’s\\nred). Since the height of a child of x is less than the height of x itself, we\\ncan apply the inductive hypothesis to conclude that each child has at\\nleast 2bh(x)−1 − 1 internal nodes. Thus, the subtree rooted at x contains\\nat least (2bh(x)−1 − 1) + (2bh(x)−1 − 1) + 1 = 2bh(x) − 1 internal\\nnodes, which proves the claim.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 443}),\n",
              " Document(page_content='Figure 13.1 A red-black tree. Every node in a red-black tree is either red or black, the children of\\na red node are both black, and every simple path from a node to a descendant leaf contains the\\nsame number of black nodes. (a) Every leaf, shown as a NIL, is black. Each non-NIL node is\\nmarked with its black-height, where NILs have black-height 0. (b) The same red-black tree but\\nwith each NIL replaced by the single sentinel T.nil, which is always black, and with black-heights\\nomitted. The root’s parent is also the sentinel. (c) The same red-black tree but with leaves and the\\nroot’s parent omitted entirely. The remainder of this chapter uses this drawing style.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 444}),\n",
              " Document(page_content='To complete the proof of the lemma, let h be the height of the tree.\\nAccording to property 4, at least half the nodes on any simple path from\\nthe root to a leaf, not including the root, must be black. Consequently,\\nthe black-height of the root must be at least h/2, and thus,\\nn ≥ 2h/2 − 1.\\nMoving the 1 to the left-hand side and taking logarithms on both sides\\nyields lg(n + 1) ≥ h/2, or h ≤ 2 lg(n + 1).\\n▪\\nAs an immediate consequence of this lemma, each of the dynamic-set\\noperations SEARCH, MINIMUM, MAXIMUM, SUCCESSOR, and\\nPREDECESSOR runs in O(lg n) time on a red-black tree, since each\\ncan run in O(h) time on a binary search tree of height h (as shown in\\nChapter 12) and any red-black tree on n nodes is a binary search tree\\nwith height O(lg n). (Of course, references to NIL in the algorithms of\\nChapter 12 have to be replaced by T.nil.) Although the procedures\\nTREE-INSERT and TREE-DELETE from Chapter 12 run in O(lg n)\\ntime when given a red-black tree as input, you cannot just use them to\\nimplement the dynamic-set operations INSERT and DELETE. They do\\nnot necessarily maintain the red-black properties, so you might not end\\nup with a legal red-black tree. The remainder of this chapter shows how\\nto insert into and delete from a red-black tree in O(lg n) time.\\nExercises\\n13.1-1\\nIn the style of Figure 13.1(a), draw the complete binary search tree of\\nheight 3 on the keys {1, 2, …, 15}. Add the NIL leaves and color the\\nnodes in three different ways such that the black-heights of the resulting\\nred-black trees are 2, 3, and 4.\\n13.1-2\\nDraw the red-black tree that results after TREE-INSERT is called on\\nthe tree in Figure 13.1 with key 36. If the inserted node is colored red, is\\nthe resulting tree a red-black tree? What if it is colored black?', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 445}),\n",
              " Document(page_content='13.1-3\\nDeﬁne a relaxed red-black tree as a binary search tree that satisﬁes red-\\nblack properties 1, 3, 4, and 5, but whose root may be either red or\\nblack. Consider a relaxed red-black tree T whose root is red. If the root\\nof T is changed to black but no other changes occur, is the resulting tree\\na red-black tree?\\n13.1-4\\nSuppose that every black node in a red-black tree “absorbs” all of its red\\nchildren, so that the children of any red node become children of the\\nblack parent. (Ignore what happens to the keys.) What are the possible\\ndegrees of a black node after all its red children are absorbed? What can\\nyou say about the depths of the leaves of the resulting tree?\\n13.1-5\\nShow that the longest simple path from a node x in a red-black tree to a\\ndescendant leaf has length at most twice that of the shortest simple path\\nfrom node x to a descendant leaf.\\n13.1-6\\nWhat is the largest possible number of internal nodes in a red-black tree\\nwith black-height k? What is the smallest possible number?\\n13.1-7\\nDescribe a red-black tree on n keys that realizes the largest possible ratio\\nof red internal nodes to black internal nodes. What is this ratio? What\\ntree has the smallest possible ratio, and what is the ratio?\\n13.1-8\\nArgue that in a red-black tree, a red node cannot have exactly one non-\\nNIL child.\\n13.2\\xa0\\xa0\\xa0\\xa0Rotations\\nThe search-tree operations TREE-INSERT and TREE-DELETE, when\\nrun on a red-black tree with n keys, take O(lg n) time. Because they\\nmodify the tree, the result may violate the red-black properties', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 446}),\n",
              " Document(page_content='enumerated in Section 13.1. To restore these properties, colors and\\npointers within nodes need to change.\\nThe pointer structure changes through rotation, which is a local\\noperation in a search tree that preserves the binary-search-tree property.\\nFigure 13.2 shows the two kinds of rotations: left rotations and right\\nrotations. Let’s look at a left rotation on a node x, which transforms the\\nstructure on the right side of the ﬁgure to the structure on the left. Node\\nx has a right child y, which must not be T.nil. The left rotation changes\\nthe subtree originally rooted at x by “twisting” the link between x and y\\nto the left. The new root of the subtree is node y, with x as y’s left child\\nand y’s original left child (the subtree represented by β in the ﬁgure) as\\nx’s right child.\\nThe pseudocode for LEFT-ROTATE appearing on the following\\npage assumes that x.right ≠ T.nil and that the root’s parent is T.nil.\\nFigure 13.3 shows an example of how LEFT-ROTATE modiﬁes a\\nbinary search tree. The code for RIGHT-ROTATE is symmetric. Both\\nLEFT-ROTATE and RIGHT-ROTATE run in O(1) time. Only pointers\\nare changed by a rotation, and all other attributes in a node remain the\\nsame.\\nFigure 13.2 The rotation operations on a binary search tree. The operation LEFT-ROTATE(T,\\nx) transforms the conﬁguration of the two nodes on the right into the conﬁguration on the left\\nby changing a constant number of pointers. The inverse operation RIGHT-ROTATE(T, y)\\ntransforms the conﬁguration on the left into the conﬁguration on the right. The letters α, β, and γ\\nrepresent arbitrary subtrees. A rotation operation preserves the binary-search-tree property: the\\nkeys in α precede x.key, which precedes the keys in β, which precede y.key, which precedes the\\nkeys in γ.\\nLEFT-ROTATE(T, x)\\n\\xa0\\xa01y = x.right\\n\\xa0\\xa02x.right = y.left// turn y’s left subtree into x’s right subtree', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 447}),\n",
              " Document(page_content='\\xa0\\xa03if\\xa0y.left ≠ T.nil // if y’s left subtree is not empty …\\n\\xa0\\xa04y.left.p = x // … then x becomes the parent of the subtree’s\\nroot\\n\\xa0\\xa05y.p = x.p //\\xa0x’s parent becomes y’s parent\\n\\xa0\\xa06if\\xa0x.p == T.nil // if x was the root …\\n\\xa0\\xa07T.root = y // … then y becomes the root\\n\\xa0\\xa08elseif\\xa0x ==\\nx.p.left// otherwise, if x was a left child …\\n\\xa0\\xa09x.p.left = y // … then y becomes a left child\\n10else\\xa0x.p.right = y// otherwise, x was a right child, and now y is\\n11y.left = x // make x become y’s left child\\n12x.p = y\\nExercises\\n13.2-1\\nWrite pseudocode for RIGHT-ROTATE.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 448}),\n",
              " Document(page_content='Figure 13.3 An example of how the procedure LEFT-ROTATE(T, x) modiﬁes a binary search\\ntree. Inorder tree walks of the input tree and the modiﬁed tree produce the same listing of key\\nvalues.\\n13.2-2\\nArgue that in every n-node binary search tree, there are exactly n − 1\\npossible rotations.\\n13.2-3\\nLet a, b, and c be arbitrary nodes in subtrees α, β, and γ, respectively, in\\nthe right tree of Figure 13.2. How do the depths of a, b, and c change\\nwhen a left rotation is performed on node x in the ﬁgure?\\n13.2-4\\nShow that any arbitrary n-node binary search tree can be transformed\\ninto any other arbitrary n-node binary search tree using O(n) rotations.\\n(Hint: First show that at most n − 1 right rotations sufﬁce to transform\\nthe tree into a right-going chain.)\\n★ 13.2-5', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 449}),\n",
              " Document(page_content='We say that a binary search tree T1 can be right-converted to binary\\nsearch tree T2 if it is possible to obtain T2 from T1 via a series of calls to\\nRIGHT-ROTATE. Give an example of two trees T1 and T2 such that\\nT1 cannot be right-converted to T2. Then, show that if a tree T1 can be\\nright-converted to T2, it can be right-converted using O(n2) calls to\\nRIGHT-ROTATE.\\n13.3\\xa0\\xa0\\xa0\\xa0Insertion\\nIn order to insert a node into a red-black tree with n internal nodes in\\nO(lg n) time and maintain the red-black properties, we’ll need to slightly\\nmodify the TREE-INSERT procedure on page 321. The procedure RB-\\nINSERT starts by inserting node z into the tree T as if it were an\\nordinary binary search tree, and then it colors z red. (Exercise 13.3-1\\nasks you to explain why to make node z red rather than black.) To\\nguarantee that the red-black properties are preserved, an auxiliary\\nprocedure RB-INSERT-FIXUP on the facing page recolors nodes and\\nperforms rotations. The call RB-INSERT(T, z) inserts node z, whose key\\nis assumed to have already been ﬁlled in, into the red-black tree T.\\nRB-INSERT(T, z)\\n\\xa0\\xa01x = T.root // node being compared with z\\n\\xa0\\xa02y = T.nil //\\xa0y will be parent of z\\n\\xa0\\xa03while\\xa0x ≠ T.nil // descend until reaching the sentinel\\n\\xa0\\xa04y = x\\n\\xa0\\xa05if\\xa0z.key < x.key\\n\\xa0\\xa06 x = x.left\\n\\xa0\\xa07else\\xa0x = x.right\\n\\xa0\\xa08z.p = y // found the location—insert z with parent\\ny\\n\\xa0\\xa09if\\xa0y == T.nil\\n10T.root = z // tree T was empty\\n11elseif\\xa0z.key < y.key\\n12y.left = z', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 450}),\n",
              " Document(page_content='13else\\xa0y.right = z\\n14z.left = T.nil // both of z’s children are the sentinel\\n15z.right = T.nil\\n16z.color = RED // the new node starts out red\\n17RB-INSERT-FIXUP(T,\\nz)// correct any violations of red-black\\nproperties\\nThe procedures TREE-INSERT and RB-INSERT differ in four\\nways. First, all instances of NIL in TREE-INSERT are replaced by T.nil.\\nSecond, lines 14–15 of RB-INSERT set z.left and z.right to T.nil, in\\norder to maintain the proper tree structure. (TREE-INSERT assumed\\nthat z’s children were already NIL.) Third, line 16 colors z red. Fourth,\\nbecause coloring z red may cause a violation of one of the red-black\\nproperties, line 17 of RB-INSERT calls RB-INSERT-FIXUP(T, z) in\\norder to restore the red-black properties.\\nRB-INSERT-FIXUP(T, z)\\n\\xa0\\xa01while\\xa0z.p.color == RED\\n\\xa0\\xa02if\\xa0z.p == z.p.p.left // is z’s parent a left child?\\n\\xa0\\xa03 y = z.p.p.right //\\xa0y is z’s uncle\\n\\xa0\\xa04 if\\xa0y.color == RED // are z’s parent and uncle both\\nred?\\n\\xa0\\xa05 z.p.color = BLACK\\n\\xa0\\xa06 y.color = BLACK\\n\\xa0\\xa07 z.p.p.color = RED\\n\\xa0\\xa08 z = z.p.p\\n\\xa0\\xa09 else\\n10 if\\xa0z == z.p.right\\n11 z = z.p\\n12 LEFT-ROTATE(T, z)\\n13 z.p.color = BLACK\\n14 z.p.p.color = RED\\n15 RIGHT-ROTATE(T,\\nz.p.p)\\n16else\\xa0// same as lines 3–15, but with “right” and “left” exchanged', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 451}),\n",
              " Document(page_content='17 y = z.p.p.left\\n18 if\\xa0y.color == RED\\n19 z.p.color = BLACK\\n20 y.color = BLACK\\n21 z.p.p.color = RED\\n22 z = z.p.p\\n23 else\\n24 if\\xa0z == z.p.left\\n25 z = z.p\\n26 RIGHT-ROTATE(T,\\nz)\\n27 z.p.color = BLACK\\n28 z.p.p.color = RED\\n29 LEFT-ROTATE(T, z.p.p)\\n30T.root.color = BLACK\\nTo understand how RB-INSERT-FIXUP works, let’s examine the\\ncode in three major steps. First, we’ll determine which violations of the\\nred-black properties might arise in RB-INSERT upon inserting node z\\nand coloring it red. Second, we’ll consider the overall goal of the while\\nloop in lines 1–29. Finally, we’ll explore each of the three cases within\\nthe while loop’s body (case 2 falls through into case 3, so these two cases\\nare not mutually exclusive) and see how they accomplish the goal.\\nIn describing the structure of a red-black tree, we’ll often need to\\nrefer to the sibling of a node’s parent. We use the term uncle for such a\\nnode.1\\xa0Figure 13.4 shows how RB-INSERT-FIXUP operates on a\\nsample red-black tree, with cases depending in part on the colors of a\\nnode, its parent, and its uncle.\\nWhat violations of the red-black properties might occur upon the call\\nto RB-INSERT-FIXUP? Property 1 certainly continues to hold (every\\nnode is either red or black), as does property 3 (every leaf is black), since\\nboth children of the newly inserted red node are the sentinel T.nil.\\nProperty 5, which says that the number of black nodes is the same on\\nevery simple path from a given node, is satisﬁed as well, because node z\\nreplaces the (black) sentinel, and node z is red with sentinel children.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 452}),\n",
              " Document(page_content='Thus, the only properties that might be violated are property 2, which\\nrequires the root to be black, and property 4, which says that a red node\\ncannot have a red child. Both possible violations may arise because z is\\ncolored red. Property 2 is violated if z is the root, and property 4 is\\nviolated if z’s parent is red. Figure 13.4(a) shows a violation of property\\n4 after the node z has been inserted.\\nThe while loop of lines 1–29 has two symmetric possibilities: lines 3–\\n15 deal with the situation in which node z’s parent z.p is a left child of z’s\\ngrandparent z.p.p, and lines 17–29 apply when z’s parent is a right child.\\nOur proof will focus only on lines 3–15,  relying on the symmetry in lines\\n17–29.\\nWe’ll show that the while loop maintains the following three-part\\ninvariant at the start of each iteration of the loop:\\na. Node z is red.\\nb. If z.p is the root, then z.p is black.\\nc. If the tree violates any of the red-black properties, then it violates\\nat most one of them, and the violation is of either property 2 o r\\nproperty 4, but not both. If the tree violates property 2, it is\\nbecause z is the root and is red. If the tree violates property 4, it is\\nbecause both z and z.p are red.\\nPart (c), which deals with violations of red-black properties, is more\\ncentral to showing that RB-INSERT-FIXUP restores the red-black\\nproperties than parts (a) and (b), which we’ll use along the way to\\nunderstand situations in the code. Because we’ll be focusing on node z\\nand nodes near it in the tree, it helps to know from part (a) that z is red.\\nPart (b) will help show that z’s grandparent z.p.p exists when it’s\\nreferenced in lines 2, 3, 7, 8, 14, and 15 (recall that we’re focusing only\\non lines 3–15).', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 453}),\n",
              " Document(page_content='Figure 13.4 The operation of RB-INSERT-FIXUP. (a) A node z after insertion. Because both z\\nand its parent z.p are red, a violation of property 4 occurs. Since z’s uncle y is red, case 1 in the\\ncode applies. Node z’s grandparent z.p.p must be black, and its blackness transfers down one\\nlevel to z’s parent and uncle. Once the pointer z moves up two levels in the tree, the tree shown in\\n(b) results. Once again, z and its parent are both red, but this time z’s uncle y is black. Since z is\\nthe right child of z.p, case 2 applies. Performing a left rotation results in the tree in (c). Now z is\\nthe left child of its parent, and case 3 applies. Recoloring and right rotation yield the tree in (d),\\nwhich is a legal red-black tree.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 454}),\n",
              " Document(page_content='Recall that to use a loop invariant, we need to show that the invariant\\nis true upon entering the ﬁrst iteration of the loop, that each iteration\\nmaintains it, that the loop terminates, and that the loop invariant gives\\nus a useful property at loop termination. We’ll see that each iteration of\\nthe loop has two possible outcomes: either the pointer z moves up the\\ntree, or some rotations occur and then the loop terminates.\\nInitialization: Before RB-INSERT is called, the red-black tree has no\\nviolations. RB-INSERT adds a red node z and calls RB-INSERT-\\nFIXUP. We’ll show that each part of the invariant holds at the time\\nRB-INSERT-FIXUP is called:\\na. When RB-INSERT-FIXUP is called, z is the red node that was\\nadded.\\nb. If z.p is the root, then z.p started out black and did not change\\nbefore the call of RB-INSERT-FIXUP.\\nc. We have already seen that properties 1, 3, and 5 hold when RB-\\nINSERTFIXUP is called.\\nIf the tree violates property 2 (the root must be black), then the red\\nroot must be the newly added node z, which is the only internal node\\nin the tree. Because the parent and both children of z are the\\nsentinel, which is black, the tree does not also violate property 4\\n(both children of a red node are black). Thus this violation of\\nproperty 2 is the only violation of red-black properties in the entire\\ntree.\\nIf the tree violates property 4, then, because the children of node z\\nare black sentinels and the tree had no other violations prior to z\\nbeing added, the violation must be because both z and z.p are red.\\nMoreover, the tree violates no other red-black properties.\\nMaintenance: There are six cases within the while loop, but we’ll examine\\nonly the three cases in lines 3–15, when node z’s parent z.p is a left\\nchild of z’s grandparent z.p.p. The proof for lines 17–29 is symmetric.\\nThe node z.p.p exists, since by part (b) of the loop invariant, if z.p is\\nthe root, then z.p is black. Since RB-INSERT-FIXUP enters a loop', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 455}),\n",
              " Document(page_content='iteration only if z.p is red, we know that z.p cannot be the root. Hence,\\nz.p.p exists.\\nCase 1 differs from cases 2 and 3 by the color of z’s uncle y. Line 3\\nmakes y point to z’s uncle z.p.p.right, and line 4 tests y’s color. If y is\\nred, then case 1 executes. Otherwise, control passes to cases 2 and 3. In\\nall three cases, z’s grandparent z.p.p is black, since its parent z.p is red,\\nand property 4 is violated only between z and z.p.\\nFigure 13.5 Case 1 of the procedure RB-INSERT-FIXUP. Both z and its parent z.p are red,\\nviolating property 4. In case 1, z’s uncle y is red. The same action occurs regardless of whether\\n(a)\\xa0z is a right child or (b)\\xa0z is a left child. Each of the subtrees α, β, γ, δ, and ϵ has a black root—\\npossibly the sentinel—and each has the same black-height. The code for case 1 moves the\\nblackness of z’s grandparent down to z’s parent and uncle, preserving property 5: all downward\\nsimple paths from a node to a leaf have the same number of blacks. The while loop continues\\nwith node z’s grandparent z.p.p as the new z. If the action of case 1 causes a new violation of\\nproperty 4 to occur, it must be only between the new z, which is red, and its parent, if it is red as\\nwell.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 456}),\n",
              " Document(page_content='Figure 13.6 Cases 2 and 3 of the procedure RB-INSERT-FIXUP. As in case 1, property 4 is\\nviolated in either case 2 or case 3 because z and its parent z.p are both red. Each of the subtrees\\nα, β, γ, and δ has a black root ( α, β, and γ from property 4, and δ because otherwise case 1 would\\napply), and each has the same black-height. Case 2 transforms into case 3 by a left rotation,\\nwhich preserves property 5: all downward simple paths from a node to a leaf have the same\\nnumber of blacks. Case 3 causes some color changes and a right rotation, which also preserve\\nproperty 5. The while loop then terminates, because property 4 is satisﬁed: there are no longer\\ntwo red nodes in a row.\\nCase 1. z’s uncle y is red\\nFigure 13.5 shows the situation for case 1 (lines 5–8), which\\noccurs when both z.p and y are red. Because z’s grandparent\\nz.p.p is black, its blackness can transfer down one level to both\\nz.p and y, thereby ﬁxing the problem of z and z.p both being\\nred. Having had its blackness transferred down one level, z’s\\ngrandparent becomes red, thereby maintaining property 5. The\\nwhile loop repeats with z.p.p as the new node z, so that the\\npointer z moves up two levels in the tree.\\nNow, we show that case 1 maintains the loop invariant at the\\nstart of the next iteration. We use z to denote node z in the\\ncurrent iteration, and z′ = z.p.p to denote the node that will be\\ncalled node z at the test in line 1 upon the next iteration.\\na. Because this iteration colors z.p.p red, node z′ is red at the\\nstart of the next iteration.\\nb. The node z′.p is z.p.p.p in this iteration, and the color of this\\nnode does not change. If this node is the root, it was black\\nprior to this iteration, and it remains black at the start of the\\nnext iteration.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 457}),\n",
              " Document(page_content='c. We have already argued that case 1 maintains property 5, and\\nit does not introduce a violation of properties 1 or 3.\\nIf node z′ is the root at the start of the next iteration, then case\\n1 corrected the lone violation of property 4 in this iteration.\\nSince z′ is red and it is the root, property 2 becomes the only\\none that is violated, and this violation is due to z′.\\nIf node z′ is not the root at the start of the next iteration, then\\ncase 1 has not created a violation of property 2. Case 1\\ncorrected the lone violation of property 4 that existed at the\\nstart of this iteration. It then made z′ red and left z′.p alone. If\\nz′.p was black, there is no violation of property 4. If z′.p was\\nred, coloring z′ red created one violation of property 4,\\nbetween z′ and z′.p.\\nCase 2. z’s uncle y is black and z is a right child\\nCase 3. z’s uncle y is black and z is a left child\\nIn cases 2 and 3, the color of z’s uncle y is black. We distinguish\\nthe two cases, which assume that z’s parent z.p is red and a left\\nchild, according to whether z is a right or left child of z.p. Lines\\n11–12 constitute case 2, which is shown in Figure 13.6 together\\nwith case 3. In case 2, node z is a right child of its parent. A left\\nrotation immediately transforms the situation into case 3 (lines\\n13–15), in which node z is a left child. Because both z and z.p\\nare red, the rotation affects neither the black-heights of nodes\\nnor property 5. Whether case 3 executes directly or through case\\n2, z’s uncle y is black, since otherwise case 1 would have run.\\nAdditionally, the node z.p.p exists, since we have argued that this\\nnode existed at the time that lines 2 and 3 were executed, and\\nafter moving z up one level in line 11 and then down one level in\\nline 12, the identity of z.p.p remains unchanged. Case 3\\nperforms some color changes and a right rotation, which\\npreserve property 5. At this point, there are no longer two red\\nnodes in a row. The while loop terminates upon the next test in\\nline 1, since z.p is now black.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 458}),\n",
              " Document(page_content='We now show that cases 2 and 3 maintain the loop invariant.\\n(As we have just argued, z.p will be black upon the next test in\\nline 1, and the loop body will not execute again.)\\na. Case 2 makes z point to z.p, which is red. No further change\\nto z or its color occurs in cases 2 and 3.\\nb. Case 3 makes z.p black, so that if z.p is the root at the start of\\nthe next iteration, it is black.\\nc. As in case 1, properties 1, 3, and 5 are maintained in cases 2\\nand 3.\\nSince node z is not the root in cases 2 and 3, we know that\\nthere is no violation of property 2. Cases 2 and 3 do not\\nintroduce a violation of property 2, since the only node that is\\nmade red becomes a child of a black node by the rotation in\\ncase 3.\\nCases 2 and 3 correct the lone violation of property 4, and\\nthey do not introduce another violation.\\nTermination: To see that the loop terminates, observe that if only case 1\\noccurs, then the node pointer z moves toward the root in each\\niteration, so that eventually z.p is black. (If z is the root, then z.p is the\\nsentinel T.nil, which is black.) If either case 2 or case 3 occurs, then\\nwe’ve seen that the loop terminates. Since the loop terminates because\\nz.p is black, the tree does not violate property 4 at loop termination.\\nBy the loop invariant, the only property that might fail to hold is\\nproperty 2. Line 30 restores this property by coloring the root black,\\nso that when RB-INSERT-FIXUP terminates, all the red-black\\nproperties hold.\\nThus, we have shown that RB-INSERT-FIXUP correctly restores the\\nred-black properties.\\nAnalysis\\nWhat is the running time of RB-INSERT? Since the height of a red-\\nblack tree on n nodes is O(lg n), lines 1–16 of RB-INSERT take O(lg n)', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 459}),\n",
              " Document(page_content='time. In RB-INSERTFIXUP, the while loop repeats only if case 1\\noccurs, and then the pointer z moves two levels up the tree. The total\\nnumber of times the while loop can be executed is therefore O(lg n).\\nThus, RB-INSERT takes a total of O(lg n) time. Moreover, it never\\nperforms more than two rotations, since the while loop terminates if case\\n2 or case 3 is executed.\\nExercises\\n13.3-1\\nLine 16 of RB-INSERT sets the color of the newly inserted node z to\\nred. If instead z’s color were set to black, then property 4 of a red-black\\ntree would not be violated. Why not set z’s color to black?\\n13.3-2\\nShow the red-black trees that result after successively inserting the keys\\n41, 38, 31, 12, 19, 8 into an initially empty red-black tree.\\n13.3-3\\nSuppose that the black-height of each of the subtrees α, β, γ, δ, ϵ in\\nFigures 13.5 and 13.6 is k. Label each node in each ﬁgure with its black-\\nheight to verify that the indicated transformation preserves property 5.\\n13.3-4\\nProfessor Teach is concerned that RB-INSERT-FIXUP might set\\nT.nil.color to RED, in which case the test in line 1 would not cause the\\nloop to terminate when z is the root. Show that the professor’s concern is\\nunfounded by arguing that RB-INSERT-FIXUP never sets T.nil.color to\\nRED.\\n13.3-5\\nConsider a red-black tree formed by inserting n nodes with RB-\\nINSERT. Argue that if n > 1, the tree has at least one red node.\\n13.3-6\\nSuggest how to implement RB-INSERT efﬁciently if the representation\\nfor red-black trees includes no storage for parent pointers.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 460}),\n",
              " Document(page_content='13.4\\xa0\\xa0\\xa0\\xa0Deletion\\nLike the other basic operations on an n-node red-black tree, deletion of\\na node takes O(lg n) time. Deleting a node from a red-black tree is more\\ncomplicated than inserting a node.\\nThe procedure for deleting a node from a red-black tree is based on\\nthe TREE-DELETE procedure on page 325. First, we need to\\ncustomize the TRANSPLANT\\xa0subroutine on page 324 that TREE-\\nDELETE calls so that it applies to a red-black tree. Like\\nTRANSPLANT, the new procedure RB-TRANSPLANT replaces the\\nsubtree rooted at node u by the subtree rooted at node v. The RB-\\nTRANSPLANT procedure differs from TRANSPLANT in two ways.\\nFirst, line 1 references the sentinel T.nil instead of NIL. Second, the\\nassignment to v.p in line 6 occurs unconditionally: the procedure can\\nassign to v.p even if v points to the sentinel. We’ll take advantage of the\\nability to assign to v.p when v = T.nil.\\nRB-TRANSPLANT(T, u, v)\\n1if\\xa0u.p == T.nil\\n2T.root = v\\n3elseif\\xa0u == u.p.left\\n4u.p.left = v\\n5else\\xa0u.p.right = v\\n6v.p = u.p\\nThe procedure RB-DELETE on the next page is like the TREE-\\nDELETE procedure, but with additional lines of pseudocode. The\\nadditional lines deal with nodes x and y that may be involved in\\nviolations of the red-black properties. When the node z being deleted has\\nat most one child, then y will be z. When z has two children, then, as in\\nTREE-DELETE, y will be z’s successor, which has no left child and\\nmoves into z’s position in the tree. Additionally, y takes on z’s color. In\\neither case, node y has at most one child: node x, which takes y’s place in\\nthe tree. (Node x will be the sentinel T.nil if y has no children.) Since\\nnode y will be either removed from the tree or moved within the tree, the', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 461}),\n",
              " Document(page_content='procedure needs to keep track of y’s original color. If the red-black\\nproperties might be violated after deleting node z, RB-DELETE calls\\nthe auxiliary procedure RB-DELETE-FIXUP, which changes colors\\nand performs rotations to restore the red-black properties.\\nAlthough RB-DELETE contains almost twice as many lines of\\npseudocode as TREE-DELETE, the two procedures have the same basic\\nstructure. You can ﬁnd each line of TREE-DELETE within RB-\\nDELETE (with the changes of replacing NIL by T.nil and replacing calls\\nto TRANSPLANT by calls to RB-TRANSPLANT), executed under\\nthe same conditions.\\nIn detail, here are the other differences between the two procedures:\\nLines 1 and 9 set node y as described above: line 1 when node z\\nhas at most one child and line 9 when z has two children.\\nBecause node y’s color might change, the variable y-original-color\\nstores y’s color before any changes occur. Lines 2 and 10 set this\\nvariable immediately after assignments to y. When node z has two\\nchildren, then nodes y and z are distinct. In this case, line 17\\nmoves y into z’s original position in the tree (that is, z’s location in\\nthe tree at the time RB-DELETE was called), and line 20 gives y\\nthe same color as z. When node y was originally black, removing\\nor moving it could cause violations of the red-black properties,\\nwhich are corrected by the call of RB-DELETE-FIXUP in line 22.\\nRB-DELETE(T, z)\\n\\xa0\\xa01y = z \\xa0\\n\\xa0\\xa02y-original-color = y.color \\xa0\\n\\xa0\\xa03if\\xa0z.left == T.nil \\xa0\\n\\xa0\\xa04x = z.right \\xa0\\n\\xa0\\xa05RB-TRANSPLANT(T, z, z.right)// replace z by its right child\\n\\xa0\\xa06elseif\\xa0z.right == T.nil \\xa0\\n\\xa0\\xa07x = z.left \\xa0\\n\\xa0\\xa08RB-TRANSPLANT(T, z, z.left)// replace z by its left child\\n\\xa0\\xa09else\\xa0y = TREE-MINIMUM(z.right)//\\xa0y is z’s successor\\n10y-original-color = y.color \\xa0', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 462}),\n",
              " Document(page_content='11x = y.right \\xa0\\n12if\\xa0y ≠ z.right \\xa0// is y farther down the tree?\\n13 RB-TRANSPLANT(T, y,\\ny.right)// replace y by its right child\\n14 y.right = z.right \\xa0//\\xa0z’s right child becomes\\n15 y.right.p = y \\xa0//\\xa0\\xa0\\xa0y’s right child\\n16else\\xa0x.p = y \\xa0// in case x is T.nil\\n17RB-TRANSPLANT(T, z, y) \\xa0// replace z by its successor y\\n18y.left = z.left \\xa0// and give z’s left child to y,\\n19y.left.p = y \\xa0//\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0w hich had no left child\\n20y.color = z.color \\xa0\\n21if\\xa0y-original-color == BLACK // if any red-black violations\\noccurred,\\n22RB-DELETE-FIXUP(T, x)//\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0c orrect them\\nAs discussed, the procedure keeps track of the node x that moves\\ninto node y’s original position at the time of call. The assignments\\nin lines 4, 7, and 11 set x to point to either y’s only child or, if y\\nhas no children, the sentinel T.nil.\\nSince node x moves into node y’s original position, the attribute\\nx.p must be set correctly. If node z has two children and y is z’s\\nright child, then y just moves into z’s position, with x remaining a\\nchild of y. Line 12 checks for this case. Although you might think\\nthat setting x.p to y in line 16 is unnecessary since x is a child of y,\\nthe call of RB-DELETE-FIXUP relies on x.p being y even if x is\\nT.nil. Thus, when z has two children and y is z’s right child,\\nexecuting line 16 is necessary if y’s right child is T.nil, and\\notherwise it does not change anything.\\nOtherwise, node z is either the same as node y or it is a proper\\nancestor of y’s original parent. In these cases, the calls of RB-\\nTRANSPLANT in lines 5, 8, and 13 set x.p correctly in line 6 of\\nRB-TRANSPLANT. (In these calls of RB-TRANSPLANT, the\\nthird parameter passed is the same as x.)\\nFinally, if node y was black, one or more violations of the red-\\nblack properties might arise. The call of RB-DELETE-FIXUP in', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 463}),\n",
              " Document(page_content='line 22 restores the red-black properties. If y was red, the red-black\\nproperties still hold when y is removed or moved, for the following\\nreasons:\\n1. No black-heights in the tree have changed. (See Exercise\\n13.4-1.)\\n2. No red nodes have been made adjacent. If z has at most\\none child, then y and z are the same node. That node is\\nremoved, with a child taking its place. If the removed node\\nwas red, then neither its parent nor its children can also be\\nred, so moving a child to take its place cannot cause two\\nred nodes to become adjacent. If, on the other hand, z has\\ntwo children, then y takes z’s place in the tree, along with\\nz’s color, so there cannot be two adjacent red nodes at y’s\\nnew position in the tree. In addition, if y was not z’s right\\nchild, then y’s original right child x replaces y in the tree.\\nSince y is red, x must be black, and so replacing y by x\\ncannot cause two red nodes to become adjacent.\\n3. Because y could not have been the root if it was red, the\\nroot remains black.\\nIf node y was black, three problems may arise, which the call of RB-\\nDELETE-FIXUP will remedy. First, if y was the root and a red child of\\ny became the new root, property 2 is violated. Second, if both x and its\\nnew parent are red, then a violation of property 4 occurs. Third, moving\\ny within the tree causes any simple path that previously contained y to\\nhave one less black node. Thus, property 5 is now violated by any\\nancestor of y in the tree. We can correct the violation of property 5 by\\nsaying that when the black node y is removed or moved, its blackness\\ntransfers to the node x that moves into y’s original position, giving x an\\n“extra” black. That is, if we add 1 to the count of black nodes on any\\nsimple path that contains x, then under this interpretation, property 5\\nholds. But now another problem emerges: node x is neither red nor\\nblack, thereby violating property 1. Instead, node x is either “doubly\\nblack” or “red-and-black,” and it contributes either 2 or 1, respectively,\\nto the count of black nodes on simple paths containing x. The color', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 464}),\n",
              " Document(page_content='attribute of x will still be either RED (if x is red-and-black) or BLACK\\n(if x is doubly black). In other words, the extra black on a node is\\nreﬂected in x’s pointing to the node rather than in the color attribute.\\nThe procedure RB-DELETE-FIXUP on the next page restores\\nproperties 1, 2, and 4. Exercises 13.4-2 and 13.4-3 ask you to show that\\nthe procedure restores properties 2 and 4, and so in the remainder of this\\nsection, we focus on property 1. The goal of the while loop in lines 1–43\\nis to move the extra black up the tree until\\n1. x points to a red-and-black node, in which case line 44 colors x\\n(singly) black;\\n2. x points to the root, in which case the extra black simply\\nvanishes; or\\n3. having performed suitable rotations and recolorings, the loop\\nexits.\\nLike RB-INSERT-FIXUP, the RB-DELETE-FIXUP procedure\\nhandles two symmetric situations: lines 3–22 for when node x is a left\\nchild, and lines 24–43 for when x is a right child. Our proof focuses on\\nthe four cases shown in lines 3–22.\\nWithin the while loop, x always points to a nonroot doubly black\\nnode. Line 2 determines whether x is a left child or a right child of its\\nparent x.p so that either lines 3–22 or 24–43 will execute in a given\\niteration. The sibling of x is always denoted by a pointer w. Since node x\\nis doubly black, node w cannot be T.nil, because otherwise, the number\\nof blacks on the simple path from x.p to the (singly black) leaf w would\\nbe smaller than the number on the simple path from x.p to x.\\nRecall that the RB-DELETE procedure always assigns to x.p before\\ncalling RB-DELETE-FIXUP (either within the call of RB-\\nTRANSPLANT in line 13 or the assignment in line 16), even when node\\nx is the sentinel T.nil. That is because RB-DELETE-FIXUP references\\nx’s parent x.p in several places, and this attribute must point to the node\\nthat became x’s parent in RB-DELETE—even if x is T.nil.\\nFigure 13.7 demonstrates the four cases in the code when node x is a\\nleft child. (As in RB-INSERT-FIXUP, the cases in RB-DELETE-\\nFIXUP are not mutually exclusive.) Before examining each case in', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 465}),\n",
              " Document(page_content='detail, let’s look more generally at how we can verify that the\\ntransformation in each of the cases preserves property 5. The key idea is\\nthat in each case, the transformation applied preserves the number of\\nblack nodes (including x’s extra black) from (and including) the root of\\nthe subtree shown to the roots of each of the subtrees α, β, …, ζ. Thus, if\\nproperty 5 holds prior to the transformation, it continues to hold\\nafterward. For example, in Figure 13.7(a), which illustrates case 1, the\\nnumber of black nodes from the root to the root of either subtree α or β\\nis 3, both before and after the transformation. (Again, remember that\\nnode x adds an extra black.) Similarly, the number of black nodes from\\nthe root to the root of any of γ, δ, ϵ, and ζ is 2, both before and after the\\ntransformation.2 In Figure 13.7(b), the counting must involve the value\\nc of the color attribute of the root of the subtree shown, which can be\\neither RED or BLACK.\\nRB-DELETE-FIXUP(T, x)\\n\\xa0\\xa01while\\xa0x ≠ T.root and x.color == BLACK\\n\\xa0\\xa02if\\xa0x == x.p.left // is x a left\\nchild?\\n\\xa0\\xa03 w = x.p.right //\\xa0w is x’s\\nsibling\\n\\xa0\\xa04 if\\xa0w.color == RED\\n\\xa0\\xa05 w.color = BLACK\\n\\xa0\\xa06 x.p.color = RED\\n\\xa0\\xa07 LEFT-ROTATE(T, x.p)\\n\\xa0\\xa08 w = x.p.right\\n\\xa0\\xa09 if\\xa0w.left.color == BLACK and w.right.color ==\\nBLACK\\n10 w.color = RED\\n11 x = x.p\\n12 else\\n13 if\\xa0w.right.color == BLACK\\n14 w.left.color = BLACK\\n15 w.color = RED\\n16 RIGHT-ROTATE(T, w)', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 466}),\n",
              " Document(page_content='17 w = x.p.right\\n18 w.color = x.p.color\\n19 x.p.color = BLACK\\n20 w.right.color = BLACK\\n21 LEFT-ROTATE(T, x.p)\\n22 x = T.root\\n23else\\xa0// same as lines 3–22, but with “right” and “left” exchanged\\n24 w = x.p.left\\n25 if\\xa0w.color == RED\\n26 w.color = BLACK\\n27 x.p.color = RED\\n28 RIGHT-ROTATE(T, x.p)\\n29 w = x.p.left\\n30 if\\xa0w.right.color == BLACK and w.left.color == BLACK\\n31 w.color = RED\\n32 x = x.p\\n33 else\\n34 if\\xa0w.left.color == BLACK\\n35 w.right.color = BLACK\\n36 w.color = RED\\n37 LEFT-ROTATE(T, w)\\n38 w = x.p.left\\n39 w.color = x.p.color\\n40 x.p.color = BLACK\\n41 w.left.color = BLACK\\n42 RIGHT-ROTATE(T, x.p)\\n43 x = T.root\\n44x.color = BLACK', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 467}),\n",
              " Document(page_content='Figure 13.7 The cases in lines 3–22 of the procedure RB-DELETE-FIXUP. Brown nodes have\\ncolor attributes represented by c and c′, which may be either RED or BLACK. The letters α, β,\\n…, ζ represent arbitrary subtrees. Each case transforms the conﬁguration on the left into the\\nconﬁguration on the right by changing some colors and/or performing a rotation. Any node\\npointed to by x has an extra black and is either doubly black or red-and-black. Only case 2\\ncauses the loop to repeat. (a) Case 1 is transformed into case 2, 3, or 4 by exchanging the colors\\nof nodes B and D and performing a left rotation. (b) In case 2, the extra black represented by the\\npointer x moves up the tree by coloring node D red and setting x to point to node B. If case 2 is\\nentered through case 1, the while loop terminates because the new node x is red-and-black, and\\ntherefore the value c of its color attribute is RED. (c) Case 3 is transformed to case 4 by\\nexchanging the colors of nodes C and D and performing a right rotation. (d) Case 4 removes the\\nextra black represented by x by changing some colors and performing a left rotation (without\\nviolating the red-black properties), and then the loop terminates.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 468}),\n",
              " Document(page_content='If we deﬁne count(RED) = 0 and count(BLACK) = 1, then the number\\nof black nodes from the root to α is 2 + count(c), both before and after\\nthe transformation. In this case, after the transformation, the new node\\nx has color attribute c, but this node is really either red-and-black (if c =\\nRED) or doubly black (if c = BLACK). You can verify the other cases\\nsimilarly (see Exercise 13.4-6).\\nCase 1. x’s sibling w is red\\nCase 1 (lines 5–8 and Figure 13.7(a)) occurs when node w, the sibling of\\nnode x, is red. Because w is red, it must have black children. This case\\nswitches the colors of w and x.p and then performs a left-rotation on x.p\\nwithout violating any of the red-black properties. The new sibling of x,\\nwhich is one of w’s children prior to the rotation, is now black, and thus\\ncase 1 converts into one of cases 2, 3, or 4.\\nCases 2, 3, and 4 occur when node w is black and are distinguished\\nby the colors of w’s children.\\nCase 2. x’s sibling w is black, and bot h of w’s children are black\\nIn case 2 (lines 10–11 and Figure 13.7(b)), both of w’s children are black.\\nSince w is also black, this case removes one black from both x and w,\\nleaving x with only one black and leaving w red. To compensate for x\\nand w each losing one black, x’s parent x.p can take on an extra black.\\nLine 11 does so by moving x up one level, so that the while loop repeats\\nwith x.p as the new node x. If case 2 enters through case 1, the new node\\nx is red-and-black, since the original x.p was red. Hence, the value c of\\nthe color attribute of the new node x is RED, and the loop terminates\\nwhen it tests the loop condition. Line 44 then colors the new node x\\n(singly) black.\\nCase 3. x’s sibling w is black, w’s left child is red, and w’s right child is\\nblack\\nCase 3 (lines 14–17 and Figure 13.7(c)) occurs when w is black, its left\\nchild is red, and its right child is black. This case switches the colors of w\\nand its left child w.left and then performs a right rotation on w without\\nviolating any of the red-black properties. The new sibling w of x is now a', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 469}),\n",
              " Document(page_content='black node with a red right child, and thus case 3 falls through into case\\n4.\\nCase 4. x’s sibling w is black, and w ’s right child is red\\nCase 4 (lines 18–22 and Figure 13.7(d)) occurs when node x’s sibling w is\\nblack and w’s right child is red. Some color changes and a left rotation\\non x.p allow the extra black on x to vanish, making it singly black,\\nwithout violating any of the red-black properties. Line 22 s ets x to be the\\nroot, and the while loop terminates when it next tests the loop condition.\\nAnalysis\\nWhat is the running time of RB-DELETE? Since the height of a red-\\nblack tree of n nodes is O(lg n), the total cost of the procedure without\\nthe call to RB-DELETE-FIXUP takes O(lg n) time. Within RB-\\nDELETE-FIXUP, each of cases 1, 3, and 4 lead to termination after\\nperforming a constant number of color changes and at most three\\nrotations. Case 2 is the only case in which the while loop can be repeated,\\nand then the pointer x moves up the tree at most O(lg n) times,\\nperforming no rotations. Thus, the procedure RB-DELETE-FIXUP\\ntakes O(lg n) time and performs at most three rotations, and the overall\\ntime for RB-DELETE is therefore also O(lg n).\\nExercises\\n13.4-1\\nShow that if node y in RB-DELETE is red, then no black-heights\\nchange.\\n13.4-2\\nArgue that after RB-DELETE-FIXUP executes, the root of the tree\\nmust be black.\\n13.4-3\\nArgue that if in RB-DELETE both x and x.p are red, then property 4 is\\nrestored by the call to RB-DELETE-FIXUP(T, x).\\n13.4-4', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 470}),\n",
              " Document(page_content='In Exercise 13.3-2 on page 346, you found the red-black tree that results\\nfrom successively inserting the keys 41, 38, 31, 12, 19, 8 into an initially\\nempty tree. Now show the red-black trees that result from the successive\\ndeletion of the keys in the order 8, 12, 19, 31, 38, 41.\\n13.4-5\\nWhich lines of the code for RB-DELETE-FIXUP might examine or\\nmodify the sentinel T.nil?\\n13.4-6\\nIn each of the cases of Figure 13.7, give the count of black nodes from\\nthe root of the subtree shown to the roots of each of the subtrees α, β,\\n…, ζ, and verify that each count remains the same after the\\ntransformation. When a node has a color attribute c or c′, use the\\nnotation count(c) or count(c′) symbolically in your count.\\n13.4-7\\nProfessors Skelton and Baron worry that at the start of case 1 of RB-\\nDELETE-FIXUP, the node x.p might not be black. If x.p is not black,\\nthen lines 5–6 are wrong. Show that x.p must be black at the start of case\\n1, so that the professors need not be concerned.\\n13.4-8\\nA node x is inserted into a red-black tree with RB-INSERT and then is\\nimmediately deleted with RB-DELETE. Is the resulting red-black tree\\nalways the same as the initial red-black tree? Justify your answer.\\n★ 13.4-9\\nConsider the operation RB-ENUMERATE(T, r, a, b), which outputs all\\nthe keys k such that a ≤ k ≤ b in a subtree rooted at node r in an n-node\\nred-black tree T. Describe how to implement RB-ENUMERATE in\\nΘ(m + lg n) time, where m is the number of keys that are output. Assume\\nthat the keys in T are unique and that the values a and b appear as keys\\nin T. How does your solution change if a and b might not appear in T?\\nProblems', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 471}),\n",
              " Document(page_content='13-1\\xa0\\xa0\\xa0\\xa0\\xa0Persistent dynamic sets\\nDuring the course of an algorithm, you sometimes ﬁnd that you need to\\nmaintain past versions of a dynamic set as it is updated. We call such a\\nset persistent. One way to implement a persistent set is to copy the entire\\nset whenever it is modiﬁed, but this approach can slow down a program\\nand also consume a lot of space. Sometimes, you can do much better.\\nConsider a persistent set S with the operations INSERT, DELETE,\\nand SEARCH, which you implement using binary search trees as shown\\nin Figure 13.8(a). Maintain a separate root for every version of the set.\\nIn order to insert the key 5 into the set, create a new node with key 5.\\nThis node becomes the left child of a new node with key 7, since you\\ncannot modify the existing node with key 7. Similarly, the new node with\\nkey 7 becomes the left child of a new node with key 8 whose right child\\nis the existing node with key 10. The new node with key 8 becomes, in\\nturn, the right child of a new root r′ with key 4 whose left child is the\\nexisting node with key 3. Thus, you copy only part of the tree and share\\nsome of the nodes with the original tree, as shown in Figure 13.8(b).\\nAssume that each tree node has the attributes key, left, and right but\\nno parent. (See also Exercise 13.3-6 on page 346. )\\na. For a persistent binary search tree (not a red-black tree, just a binary\\nsearch tree), identify the nodes that need to change to insert or delete a\\nnode.\\nFigure 13.8 (a) A binary search tree with keys 2, 3, 4, 7, 8, 10. (b) The persistent binary search\\ntree that results from the insertion of key 5. The most recent version of the set consists of the\\nnodes reachable from the root r′, and the previous version consists of the nodes reachable from r.\\nBlue nodes are added when key 5 is inserted.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 472}),\n",
              " Document(page_content='b. Write a procedure PERSISTENT-TREE-INSERT(T, z) that, given a\\npersistent binary search tree T and a node z to insert, returns a new\\npersistent tree T′ that is the result of inserting z into T. Assume that\\nyou have a procedure COPY-NODE(x) that makes a copy of node x,\\nincluding all of its attributes.\\nc. If the height of the persistent binary search tree T is h, what are the\\ntime and space requirements of your implementation of\\nPERSISTENT-TREE-INSERT? (The space requirement is\\nproportional to the number of nodes that are copied.)\\nd. Suppose that you include the parent attribute in each node. In this\\ncase, the PERSISTENT-TREE-INSERT procedure needs to perform\\nadditional copying. Prove that PERSISTENT-TREE-INSERT then\\nrequires Ω(n) time and space, where n is the number of nodes in the\\ntree.\\ne. Show how to use red-black trees to guarantee that the worst-case\\nrunning time and space are O(lg n) per insertion or deletion. You may\\nassume that all keys are distinct.\\n13-2\\xa0\\xa0\\xa0\\xa0\\xa0Join operation on red-black trees\\nThe join operation takes two dynamic sets S1 and S2 and an element x\\nsuch that for any x1 ∈ S1 and x2 ∈ S2, we have x1.key ≤ x.key ≤ x2.key.\\nIt returns a set S = S1 ⋃ {x} ⋃ S2. In this problem, we investigate how\\nto implement the join operation on red-black trees.\\na. Suppose that you store the black-height of a red-black tree T as the\\nnew attribute T.bh. Argue that RB-INSERT and RB-DELETE can\\nmaintain the bh\\xa0attribute without requiring extra storage in the nodes\\nof the tree and without increasing the asymptotic running times. Show\\nhow to determine the black-height of each node visited while\\ndescending through T, using O(1) time per node visited.\\nLet T1 and T2 be red-black trees and x be a key value such that for any\\nnodes x1 in T1 and x2 in T2, we have x1.key ≤ x.key ≤ x2.key. You will\\nshow how to implement the operation RB-JOIN(T1, x, T2), which', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 473}),\n",
              " Document(page_content='destroys T1 and T2 and returns a red-black tree T = T1 ⋃ {x} ⋃ T2. Let\\nn be the total number of nodes in T1 and T2.\\nb. Assume that T1.bh ≥ T2.bh. Describe an O(lg n)-time algorithm that\\nﬁnds a black node y in T1 with the largest key from among those\\nnodes whose black-height is T2.bh.\\nc. Let Ty be the subtree rooted at y. Describe how Ty ⋃ {x} ⋃ T2 can\\nreplace Ty in O(1) time without destroying the binary-search-tree\\nproperty.\\nd. What color should you make x so that red-black properties 1, 3, and 5\\nare maintained? Describe how to enforce properties 2 an d 4 in O(lg n)\\ntime.\\ne. Argue that no generality is lost by making the assumption in part (b).\\nDescribe the symmetric situation that arises when T1.bh ≤ T2.bh.\\nf. Argue that the running time of RB-JOIN is O(lg n).\\n13-3\\xa0\\xa0\\xa0\\xa0\\xa0A VL trees\\nAn AVL tree is a binary search tree that is height balanced: for each node\\nx, the heights of the left and right subtrees of x differ by at most 1. To\\nimplement an AVL tree, maintain an extra attribute h in each node such\\nthat x.h is the height of node x. As for any other binary search tree T,\\nassume that T.root points to the root node.\\na. Prove that an AVL tree with n nodes has height O(lg n). (Hint: Prove\\nthat an AVL tree of height h has at least Fh nodes, where Fh is the hth\\nFibonacci number.)\\nb. To insert into an AVL tree, ﬁrst place a node into the appropriate\\nplace in binary search tree order. Afterward, the tree might no longer\\nbe height balanced. Speciﬁcally, the heights of the left and right\\nchildren of some node might differ by 2. Describe a procedure\\nBALANCE(x), which takes a subtree rooted at x whose left and right\\nchildren are height balanced and have heights that differ by at most 2,', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 474}),\n",
              " Document(page_content='so that |x.right.h − x.left.h| ≤ 2, and alters the subtree rooted at x to be\\nheight balanced. The procedure should return a pointer to the node\\nthat is the root of the subtree after alterations occur. (Hint: Use\\nrotations.)\\nc. Using part (b), describe a recursive procedure AVL-INSERT(T, z)\\nthat takes an AVL tree T and a newly created node z (whose key has\\nalready been ﬁlled in), and adds z into T, maintaining the property\\nthat T is an AVL tree. As in TREE-INSERT from Section 12.3,\\nassume that z.key has already been ﬁlled in and that z.left = NIL and\\nz.right = NIL. Assume as well that z.h = 0.\\nd. Show that AVL-INSERT, run on an n-node AVL tree, takes O(lg n)\\ntime and performs O(lg n) rotations.\\nChapter notes\\nThe idea of balancing a search tree is due to Adel’son-Vel’ski ĭ and\\nLandis [2], who introduced a class of balanced search trees called “AVL\\ntrees” in 1962, described in Problem 13-3. Another class of search trees,\\ncalled “2-3 trees,” was introduced by J. E. Hopcroft (unpublished) in\\n1970. A 2-3 tree maintains balance by manipulating the degrees of nodes\\nin the tree, where each node has either two or three children. Chapter 18\\ncovers a generalization of 2-3 trees introduced by Bayer and McCreight\\n[39], called “B-trees.”\\nRed-black trees were invented by Bayer [38] under the name\\n“symmetric binary B-trees.” Guibas and Sedgewick [202] studied their\\nproperties at length and introduced the red/black color convention.\\nAndersson [16] gives a simpler-to-code variant of red-black trees. Weiss\\n[451] calls this variant AA-trees. An AA-tree is similar to a red-black\\ntree except that left children can never be red.\\nSedgewick and Wayne [402] present red-black trees as a modiﬁed\\nversion of 2-3 trees in which a node with three children is split into two\\nnodes with two children each. One of these nodes becomes the left child\\nof the other, and only left children can be red. They call this structure a\\n“left-leaning red-black binary search tree.” Although the code for left-', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 475}),\n",
              " Document(page_content='leaning red-black binary search trees is more concise than the red-black\\ntree pseudocode in this chapter, operations on left-leaning red-black\\nbinary search trees do not limit the number of rotations per operation to\\na constant. This distinction will matter in Chapter 17.\\nTreaps, a hybrid of binary search trees and heaps, were proposed by\\nSeidel and Aragon [404]. They are the default implementation of a\\ndictionary in LEDA [324], which is a well-implemented collection of\\ndata structures and algorithms.\\nThere are many other variations on balanced binary trees, including\\nweight-balanced trees [344], k-neighbor trees [318], and scapegoat trees\\n[174]. Perhaps the most intriguing are the “splay trees” introduced by\\nSleator and Tarjan [418], which are “self-adjusting.” (See Tarjan [429]\\nfor a good description of splay trees.) Splay trees maintain balance\\nwithout any explicit balance condition such as color. Instead, “splay\\noperations” (which involve rotations) are performed within the tree\\nevery time an access is made. The amortized cost (see Chapter 16) of\\neach operation on an n-node tree is O(lg n). Splay trees have been\\nconjectured to perform within a constant factor of the best ofﬂine\\nrotation-based tree. The best known competitive ratio (see Chapter 27)\\nfor a rotation-based tree is the Tango Tree of Demaine et al. [109].\\nSkip lists [369] provide an alternative to balanced binary trees. A skip\\nlist is a linked list that is augmented with a number of additional\\npointers. Each dictionary operation runs in O(lg n) expected time on a\\nskip list of n items.\\n1 Although we try to avoid gendered language in this book, the English language lacks a gender-\\nneutral word for a parent’s sibling.\\n2 If property 5 holds, we can assume that paths from the roots of γ, δ, ϵ, and ζ down to leaves\\ncontain one more black than do paths from the roots of α and β down to leaves.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 476}),\n",
              " Document(page_content='Part IV\\xa0\\xa0\\xa0\\xa0Advanced Design and Analysis\\nTechniques', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 477}),\n",
              " Document(page_content='\\xa0\\n\\xa0\\nIntroduction\\nThis part covers three important techniques used in designing and\\nanalyzing efﬁcient algorithms: dynamic programming (Chapter 14),\\ngreedy algorithms (Chapter 15), and amortized analysis (Chapter 16).\\nEarlier parts have presented other widely applicable techniques, such as\\ndivide-and-conquer, randomization, and how to solve recurrences. The\\ntechniques in this part are somewhat more sophisticated, but you will be\\nable to use them solve many computational problems. The themes\\nintroduced in this part will recur later in this book.\\nDynamic programming typically applies to optimization problems in\\nwhich you make a set of choices in order to arrive at an optimal\\nsolution, each choice generates subproblems of the same form as the\\noriginal problem, and the same subproblems arise repeatedly. The key\\nstrategy is to store the solution to each such subproblem rather than\\nrecompute it. Chapter 14 shows how this simple idea can sometimes\\ntransform exponential-time algorithms into polynomial-time\\nalgorithms.\\nLike dynamic-programming algorithms, greedy algorithms typically\\napply to optimization problems in which you make a set of choices in\\norder to arrive at an optimal solution. The idea of a greedy algorithm is\\nto make each choice in a locally optimal manner, resulting in a faster\\nalgorithm than you get with dynamic programming. Chapter 15 will\\nhelp you determine when the greedy approach works.\\nThe technique of amortized analysis applies to certain algorithms\\nthat perform a sequence of similar operations. Instead of bounding the', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 478}),\n",
              " Document(page_content='cost of the sequence of operations by bounding the actual cost of each\\noperation separately, an amortized analysis provides a worst-case bound\\non the actual cost of the entire sequence. One advantage of this\\napproach is that although some operations might be expensive, many\\nothers might be cheap. You can use amortized analysis when designing\\nalgorithms, since the design of an algorithm and the analysis of its\\nrunning time are often closely intertwined. Chapter 16 introduces three\\nways to perform an amortized analysis of an algorithm.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 479}),\n",
              " Document(page_content='14\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0Dynamic Programming\\nDynamic programming, like the divide-and-conquer method, solves\\nproblems by combining the solutions to subproblems. (“Programming”\\nin this context refers to a tabular method, not to writing computer\\ncode.) As we saw in Chapters 2 and 4, divide-and-conquer algorithms\\npartition the problem into disjoint subproblems, solve the subproblems\\nrecursively, and then combine their solutions to solve the original\\nproblem. In contrast, dynamic programming applies when the\\nsubproblems overlap—that is, when subproblems share\\nsubsubproblems. In this context, a divide-and-conquer algorithm does\\nmore work than necessary, repeatedly solving the common\\nsubsubproblems. A dynamic-programming algorithm solves each\\nsubsubproblem just once and then saves its answer in a table, thereby\\navoiding the work of recomputing the answer every time it solves each\\nsubsubproblem.\\nDynamic programming typically applies to optimization problems.\\nSuch problems can have many possible solutions. Each solution has a\\nvalue, and you want to ﬁnd a solution with the optimal (minimum or\\nmaximum) value. We call such a solution an optimal solution to the\\nproblem, as opposed to the optimal solution, since there may be several\\nsolutions that achieve the optimal value.\\nTo develop a dynamic-programming algorithm, follow a sequence of\\nfour steps:\\n1. Characterize the structure of an optimal solution.\\n2. Recursively deﬁne the value of an optimal solution.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 480}),\n",
              " Document(page_content='3. Compute the value of an optimal solution, typically in a bottom-\\nup fashion.\\n4. Construct an optimal solution from computed information.\\nSteps 1–3 form the basis of a dynamic-programming solution to a\\nproblem. If you need only the value of an optimal solution, and not the\\nsolution itself, then you can omit step 4. When you do perform step 4, it\\noften pays to maintain additional information during step 3 so that you\\ncan easily construct an optimal solution.\\nThe sections that follow use the dynamic-programming method to\\nsolve some optimization problems. Section 14.1 examines the problem\\nof cutting a rod into rods of smaller length in a way that maximizes\\ntheir total value. Section 14.2 shows how to multiply a chain of matrices\\nwhile performing the fewest total scalar multiplications. Given these\\nexamples of dynamic programming, Section 14.3 discusses two key\\ncharacteristics that a problem must have for dynamic programming to\\nbe a viable solution technique. Section 14.4 then shows how to ﬁnd the\\nlongest common subsequence of two sequences via dynamic\\nprogramming. Finally, Section 14.5 uses dynamic programming to\\nconstruct binary search trees that are optimal, given a known\\ndistribution of keys to be looked up.\\n14.1\\xa0\\xa0\\xa0\\xa0Rod cutting\\nOur ﬁrst example uses dynamic programming to solve a simple problem\\nin deciding where to cut steel rods. Serling Enterprises buys long steel\\nrods and cuts them into shorter rods, which it then sells. Each cut is free.\\nThe management of Serling Enterprises wants to know the best way to\\ncut up the rods.\\nSerling Enterprises has a table giving, for i = 1, 2, …, the price pi in\\ndollars that they charge for a rod of length i inches. The length of each\\nrod in inches is always an integer. Figure 14.1 gives a sample price table.\\nThe rod-cutting problem is the following. Given a rod of length n\\ninches and a table of prices pi for i = 1, 2, …, n, determine the maximum\\nrevenue rn obtainable by cutting up the rod and selling the pieces. If the', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 481}),\n",
              " Document(page_content='price pn for a rod of length n is large enough, an optimal solution might\\nrequire no cutting at all.\\nConsider the case when n = 4. Figure 14.2 shows all the ways to cut\\nup a rod of 4 inches in length, including the way with no cuts at all.\\nCutting a 4-inch rod into two 2-inch pieces produces revenue p2 + p2 =\\n5 + 5 = 10, which is optimal.\\nSerling Enterprises can cut up a rod of length n in 2n−1 different\\nways, since they have an independent option of cutting, or not cutting,\\nat distance i inches from the left end, for i = 1, 2, …, n − 1.1 We denote\\na decomposition into pieces using ordinary additive notation, so that 7\\n= 2 + 2 + 3 indicates that a rod of length 7 is cut into three pieces—two\\nof length 2 and one of length 3. If an optimal solution cuts the rod into\\nk pieces, for some 1 ≤ k ≤ n, then an optimal decomposition\\nn = i1 + i2 + ⋯ + ik\\nFigure 14.1 A sample price table for rods. Each rod of length i inches earns the company pi\\ndollars of revenue.\\nFigure 14.2 The 8 possible ways of cutting up a rod of length 4. Above each piece is the value of\\nthat piece, according to the sample price chart of Figure 14.1. The optimal strategy is part (c)—\\ncutting the rod into two pieces of length 2—which has total value 10.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 482}),\n",
              " Document(page_content='of the rod into pieces of lengths i1, i2, …, ik provides maximum\\ncorresponding revenue\\nFor the sample problem in Figure 14.1, you can determine the\\noptimal revenue ﬁgures ri, for i = 1, 2, …, 10, by inspection, with the\\ncorresponding optimal decompositions\\nr1 = 1 from solution 1 = 1 (no cuts),\\nr2 = 5 from solution 2 = 2 (no cuts),\\nr3 = 8 from solution 3 = 3 (no cuts),\\nr4 = 10from solution 4 = 2 + 2,\\nr5 = 13from solution 5 = 2 + 3,\\nr6 = 17from solution 6 = 6 (no cuts),\\nr7 = 18from solution 7 = 1 + 6 or 7 = 2 + 2 + 3,\\nr8 = 22from solution 8 = 2 + 6,\\nr9 = 25from solution 9 = 3 + 6,\\nr10 = 30from solution 10 = 10 (no cuts).\\nMore generally, we can express the values rn for n ≥ 1 in terms of\\noptimal revenues from shorter rods:\\nThe ﬁrst argument, pn, corresponds to making no cuts at all and selling\\nthe rod of length n as is. The other n − 1 arguments to max correspond\\nto the maximum revenue obtained by making an initial cut of the rod\\ninto two pieces of size i and n − i, for each i = 1, 2, …, n − 1, and then\\noptimally cutting up those pieces further, obtaining revenues ri and rn−i\\nfrom those two pieces. Since you don’t know ahead of time which value\\nof i optimizes revenue, you have to consider all possible values for i and\\npick the one that maximizes revenue. You also have the option of', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 483}),\n",
              " Document(page_content='picking no i at all if the greatest revenue comes from selling the rod\\nuncut.\\nTo solve the original problem of size n, you solve smaller problems of\\nthe same type. Once you make the ﬁrst cut, the two resulting pieces form\\nindependent instances of the rod-cutting problem. The overall optimal\\nsolution incorporates optimal solutions to the two resulting\\nsubproblems, maximizing revenue from each of those two pieces. We say\\nthat the rod-cutting problem exhibits optimal substructure: optimal\\nsolutions to a problem incorporate optimal solutions to related\\nsubproblems, which you may solve independently.\\nIn a related, but slightly simpler, way to arrange a recursive structure\\nfor the rod-cutting problem, let’s view a decomposition as consisting of\\na ﬁrst piece of length i cut off the left-hand end, and then a right-hand\\nremainder of length n − i. Only the remainder, and not the ﬁrst piece,\\nmay be further divided. Think of every decomposition of a length-n rod\\nin this way: as a ﬁrst piece followed by some decomposition of the\\nremainder. Then we can express the solution with no cuts at all by\\nsaying that the ﬁrst piece has size i = n and revenue pn and that the\\nremainder has size 0 with corresponding revenue r0 = 0. We thus obtain\\nthe following simpler version of equation (14.1):\\nIn this formulation, an optimal solution embodies the solution to only\\none related subproblem—the remainder—rather than two.\\nRecursive top-down implementation\\nThe CUT-ROD procedure on the following page implements the\\ncomputation implicit in equation (14.2) in a straightforward, top-down,\\nrecursive manner. It takes as input an array p[1 : n] of prices and an\\ninteger n, and it returns the maximum revenue possible for a rod of\\nlength n. For length n = 0, no revenue is possible, and so CUT-ROD\\nreturns 0 in line 2. Line 3 initializes the maximum revenue q to −∞, so\\nthat the for loop in lines 4–5 correctly computes q = max {pi + CUT-\\nROD(p, n − i) : 1 ≤ i ≤ n}. Line 6 then returns this value. A simple', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 484}),\n",
              " Document(page_content='induction on n proves that this answer is equal to the desired answer rn,\\nusing equation (14.2).\\nCUT-ROD(p, n)\\n1if\\xa0n == 0\\n2 return 0\\n3q = −∞\\n4for\\xa0i = 1 to\\xa0n\\n5 q = max {q, p[i] + CUT-ROD(p, n − i)}\\n6return\\xa0q\\nIf you code up CUT-ROD in your favorite programming language\\nand run it on your computer, you’ll ﬁnd that once the input size\\nbecomes moderately large, your program takes a long time to run. For n\\n= 40, your program may take several minutes and possibly more than an\\nhour. For large values of n, you’ll also discover that each time you\\nincrease n by 1, your program’s running time approximately doubles.\\nWhy is CUT-ROD so inefﬁcient? The problem is that CUT-ROD\\ncalls itself recursively over and over again with the same parameter\\nvalues, which means that it solves the same subproblems repeatedly.\\nFigure 14.3 shows a recursion tree demonstrating what happens for n =\\n4: CUT-ROD(p, n) calls CUT-ROD(p, n − i) for i = 1, 2, …, n.\\nEquivalently, CUT-ROD(p, n) calls CUT-ROD(p, j) for each j = 0, 1,\\n…, n − 1. When this process unfolds recursively, the amount of work\\ndone, as a function of n, grows explosively.\\nTo analyze the running time of CUT-ROD, let T(n) denote the total\\nnumber of calls made to CUT-ROD(p, n) for a particular value of n.\\nThis expression equals the number of nodes in a subtree whose root is\\nlabeled n in the recursion tree. The count includes the initial call at its\\nroot. Thus, T(0) = 1 and\\n', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 485}),\n",
              " Document(page_content='The initial 1 is for the call at the root, and the term T(j) counts the\\nnumber of calls (including recursive calls) due to the call CUT-ROD(p,\\nn − i), where j = n − i. As Exercise 14.1-1 asks you to show,\\nand so the running time of CUT-ROD is exponential in n.\\nIn retrospect, this exponential running time is not so surprising.\\nCUT-ROD explicitly considers all possible ways of cutting up a rod of\\nlength n. How many ways are there? A rod of length n has n − 1\\npotential locations to cut. Each possible way to cut up the rod makes a\\ncut at some subset of these n − 1 locations, including the empty set,\\nwhich makes for no cuts. Viewing each cut location as a distinct member\\nof a set of n − 1 elements, you can see that there are 2n−1 subsets. Each\\nleaf in the recursion tree of Figure 14.3 corresponds to one possible way\\nto cut up the rod. Hence, the recursion tree has 2n−1 leaves. The labels\\non the simple path from the root to a leaf give the sizes of each\\nremaining right-hand piece before making each cut. That is, the labels\\ngive the corresponding cut points, measured from the right-hand end of\\nthe rod.\\nFigure 14.3 The recursion tree showing recursive calls resulting from a call CUT-ROD(p, n) for\\nn = 4. Each node label gives the size n of the corresponding subproblem, so that an edge from a\\nparent with label s to a child with label t corresponds to cutting off an initial piece of size s − t\\nand leaving a remaining subproblem of size t. A path from the root to a leaf corresponds to one\\nof the 2n−1 ways of cutting up a rod of length n. In general, this recursion tree has 2n nodes and\\n2n−1 leaves.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 486}),\n",
              " Document(page_content='Using dynamic programming for optimal rod cutting\\nNow, let’s see how to use dynamic programming to convert CUT-ROD\\ninto an efﬁcient algorithm.\\nThe dynamic-programming method works as follows. Instead of\\nsolving the same subproblems repeatedly, as in the naive recursion\\nsolution, arrange for each subproblem to be solved only once. There’s\\nactually an obvious way to do so: the ﬁrst time you solve a subproblem,\\nsave its solution. If you need to refer to this subproblem’s solution again\\nlater, just look it up, rather than recomputing it.\\nSaving subproblem solutions comes with a cost: the additional\\nmemory needed to store solutions. Dynamic programming thus serves\\nas an example of a time-memory trade-off. The savings may be dramatic.\\nFor example, we’re about to use dynamic programming to go from the\\nexponential-time algorithm for rod cutting down to a Θ (n2)-time\\nalgorithm. A dynamic-programming approach runs in polynomial time\\nwhen the number of distinct subproblems involved is polynomial in the\\ninput size and you can solve each such subproblem in polynomial time.\\nThere are usually two equivalent ways to implement a dynamic-\\nprogramming approach. Solutions to the rod-cutting problem illustrate\\nboth of them.\\nThe ﬁrst approach is top-down with memoization.2 In this approach,\\nyou write the procedure recursively in a natural manner, but modiﬁed to\\nsave the result of each subproblem (usually in an array or hash table).\\nThe procedure now ﬁrst checks to see whether it has previously solved\\nthis subproblem. If so, it returns the saved value, saving further\\ncomputation at this level. If not, the procedure computes the value in\\nthe usual manner but also saves it. We say that the recursive procedure\\nhas been memoized: it “remembers” what results it has computed\\npreviously.\\nThe second approach is the bottom-up method. This approach\\ntypically depends on some natural notion of the “size” of a subproblem,\\nsuch that solving any particular subproblem depends only on solving\\n“smaller” subproblems. Solve the subproblems in size order, smallest\\nﬁrst, storing the solution to each subproblem when it is ﬁrst solved. In', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 487}),\n",
              " Document(page_content='this way, when solving a particular subproblem, there are already saved\\nsolutions for all of the smaller subproblems its solution depends upon.\\nYou need to solve each subproblem only once, and when you ﬁrst see it,\\nyou have already solved all of its prerequisite subproblems.\\nThese two approaches yield algorithms with the same asymptotic\\nrunning time, except in unusual circumstances where the top-down\\napproach does not actually recurse to examine all possible subproblems.\\nThe bottom-up approach often has much better constant factors, since\\nit has lower overhead for procedure calls.\\nThe procedures MEMOIZED-CUT-ROD and MEMOIZED-CUT-\\nROD-AUX on the facing page demonstrate how to memoize the top-\\ndown CUT-ROD procedure. The main procedure MEMOIZED-CUT-\\nROD initializes a new auxiliary array r[0 : n] with the value −∞ which,\\nsince known revenue values are always nonnegative, is a convenient\\nchoice for denoting “unknown.” MEMOIZED-CUT-ROD then calls\\nits helper procedure, MEMOIZED-CUT-ROD-AUX, which is just the\\nmemoized version of the exponential-time procedure, CUT-ROD. It\\nﬁrst checks in line 1 to see whether the desired value is already known\\nand, if it is, then line 2 returns it. Otherwise, lines 3–7 compute the\\ndesired value q in the usual manner, line 8 saves it in r[n], and line 9\\nreturns it.\\nThe bottom-up version, BOTTOM-UP-CUT-ROD on the next\\npage, is even simpler. Using the bottom-up dynamic-programming\\napproach, BOTTOM-UP-CUT-ROD takes advantage of the natural\\nordering of the subproblems: a subproblem of size i is “smaller” than a\\nsubproblem of size j if i < j. Thus, the procedure solves subproblems of\\nsizes j = 0, 1, …, n, in that order.\\nMEMOIZED-CUT-ROD(p, n)\\n1let r[0 : n] be a new array// will remember solution values in r\\n2for\\xa0i = 0 to\\xa0n\\n3 r[i] = −∞\\n4return MEMOIZED-CUT-ROD-AUX(p, n, r)\\nMEMOIZED-CUT-ROD-AUX(p, n, r)', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 488}),\n",
              " Document(page_content='1if\\xa0r[n] ≥ 0 // already have a solution for length n?\\n2 return\\xa0r[n]\\n3if\\xa0n == 0\\n4 q = 0\\n5else\\xa0q = −∞\\n6 for\\xa0i = 1 to\\xa0n//\\xa0i is the position of the ﬁrst cut\\n7 q = max {q, p[i] + MEMOIZED-CUT-ROD-AUX(p, n − i,\\nr)}\\n8r[n] = q // remember the solution value for length n\\n9return\\xa0q\\nBOTTOM-UP-CUT-ROD(p, n)\\n1let r[0 : n] be a new array// will remember solution values in r\\n2r[0] = 0\\n3for\\xa0j = 1 to\\xa0n // for increasing rod length j\\n4 q = −∞\\n5 for\\xa0i = 1 to\\xa0j //\\xa0i is the position of the ﬁrst cut\\n6 q = max {q, p[i] + r[j − i]}\\n7 r[j] = q // remember the solution value for length j\\n8return\\xa0r[n]\\nLine 1 of BOTTOM-UP-CUT-ROD creates a new array r[0 : n] in\\nwhich to save the results of the subproblems, and line 2 initializes r[0] to\\n0, since a rod of length 0 earns no revenue. Lines 3–6 solve each\\nsubproblem of size j, for j = 1, 2, …, n, in order of increasing size. The\\napproach used to solve a problem of a particular size j is the same as\\nthat used by CUT-ROD, except that line 6 now directly references array\\nentry r[j − i] instead of making a recursive call to solve the subproblem\\nof size j − i. Line 7 saves in r[j] the solution to the subproblem of size j.\\nFinally, line 8 returns r[n], which equals the optimal value rn.\\nThe bottom-up and top-down versions have the same asymptotic\\nrunning time. The running time of BOTTOM-UP-CUT-ROD is Θ (n2),\\ndue to its doubly nested loop structure. The number of iterations of its\\ninner for loop, in lines 5–6, forms an arithmetic series. The running time\\nof its top-down counterpart, MEMOIZEDCUT-ROD, is also Θ (n2),', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 489}),\n",
              " Document(page_content='although this running time may be a little harder to see. Because a\\nrecursive call to solve a previously solved subproblem returns\\nimmediately, MEMOIZED-CUT-ROD solves each subproblem just\\nonce. It solves subproblems for sizes 0, 1, …, n. To solve a subproblem\\nof size n, the for loop of lines 6–7 iterates n times. Thus, the total\\nnumber of iterations of this for loop, over all recursive calls of\\nMEMOIZED-CUT-ROD, forms an arithmetic series, giving a total of\\nΘ(n2) iterations, just like the inner for loop of BOTTOM-UP-CUT-\\nROD. (We actually are using a form of aggregate analysis here. We’ll see\\naggregate analysis in detail in Section 16.1.)\\nFigure 14.4 The subproblem graph for the rod-cutting problem with n = 4. The vertex labels give\\nthe sizes of the corresponding subproblems. A directed edge (x, y) indicates that solving\\nsubproblem x requires a solution to subproblem y. This graph is a reduced version of the\\nrecursion tree of Figure 14.3, in which all nodes with the same label are collapsed into a single\\nvertex and all edges go from parent to child.\\nSubproblem graphs\\nWhen you think about a dynamic-programming problem, you need to\\nunderstand the set of subproblems involved and how subproblems\\ndepend on one another.\\nThe subproblem graph for the problem embodies exactly this\\ninformation. Figure 14.4 shows the subproblem graph for the rod-\\ncutting problem with n = 4. It is a directed graph, containing one vertex\\nfor each distinct subproblem. The subproblem graph has a directed edge\\nfrom the vertex for subproblem x to the vertex for subproblem y if', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 490}),\n",
              " Document(page_content='determining an optimal solution for subproblem x involves directly\\nconsidering an optimal solution for subproblem y. For example, the\\nsubproblem graph contains an edge from x to y if a top-down recursive\\nprocedure for solving x directly calls itself to solve y. You can think of\\nthe subproblem graph as a “reduced” or “collapsed” version of the\\nrecursion tree for the top-down recursive method, with all nodes for the\\nsame subproblem coalesced into a single vertex and all edges directed\\nfrom parent to child.\\nThe bottom-up method for dynamic programming considers the\\nvertices of the subproblem graph in such an order that you solve the\\nsubproblems y adjacent to a given subproblem x before you solve\\nsubproblem x. (As Section B.4 notes, the adjacency relation in a\\ndirected graph is not necessarily symmetric.) Using terminology that\\nwe’ll see in Section 20.4, in a bottom-up dynamic-programming\\nalgorithm, you consider the vertices of the subproblem graph in an\\norder that is a “reverse topological sort,” or a “topological sort of the\\ntranspose” of the subproblem graph. In other words, no subproblem is\\nconsidered until all of the subproblems it depends upon have been\\nsolved. Similarly, using notions that we’ll visit in Section 20.3, you can\\nview the top-down method (with memoization) for dynamic\\nprogramming as a “depth-ﬁrst search” of the subproblem graph.\\nThe size of the subproblem graph G = (V, E) can help you determine\\nthe running time of the dynamic-programming algorithm. Since you\\nsolve each subproblem just once, the running time is the sum of the\\ntimes needed to solve each subproblem. Typically, the time to compute\\nthe solution to a subproblem is proportional to the degree (number of\\noutgoing edges) of the corresponding vertex in the subproblem graph,\\nand the number of subproblems is equal to the number of vertices in the\\nsubproblem graph. In this common case, the running time of dynamic\\nprogramming is linear in the number of vertices and edges.\\nReconstructing a solution\\nThe procedures MEMOIZED-CUT-ROD and BOTTOM-UP-CUT-\\nROD return the value of an optimal solution to the rod-cutting\\nproblem, but they do not return the solution itself: a list of piece sizes.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 491}),\n",
              " Document(page_content='Let’s see how to extend the dynamic-programming approach to\\nrecord not only the optimal value computed for each subproblem, but\\nalso a choice that led to the optimal value. With this information, you\\ncan readily print an optimal solution. The procedure EXTENDED-\\nBOTTOM-UP-CUT-ROD on the next page computes, for each rod size\\nj, not only the maximum revenue rj, but also sj, the optimal size of the\\nﬁrst piece to cut off. It’s similar to BOTTOM-UP-CUT-ROD, except\\nthat it creates the array s in line 1, and it updates s[j] in line 8 to hold the\\noptimal size i of the ﬁrst piece to cut off when solving a subproblem of\\nsize j.\\nThe procedure PRINT-CUT-ROD-SOLUTION on the following\\npage takes as input an array p[1 : n] of prices and a rod size n. It calls\\nEXTENDED-BOTTOM-UP-CUT-ROD to compute the array s[1 : n]\\nof optimal ﬁrst-piece sizes. Then it prints out the complete list of piece\\nsizes in an optimal decomposition of a rod of length n. For the sample\\nprice chart appearing in Figure 14.1, the call EXTENDED-BOTTOM-\\nUP-CUT-ROD(p, 10) returns the following arrays:\\ni012345678910\\nr[i]015810131718222530\\ns[i] 12322612310\\nA call to PRINT-CUT-ROD-SOLUTION(p, 10) prints just 10, but a\\ncall with n = 7 prints the cuts 1 and 6, which correspond to the ﬁrst\\noptimal decomposition for r7 given earlier.\\nEXTENDED-BOTTOM-UP-CUT-ROD(p, n)\\n\\xa0\\xa01let r[0 : n] and s[1 : n] be new arrays\\n\\xa0\\xa02r[0] = 0\\n\\xa0\\xa03for\\xa0j = 1 to\\xa0n // for increasing rod length j\\n\\xa0\\xa04q = −∞\\n\\xa0\\xa05for\\xa0i = 1 to\\xa0j //\\xa0i is the position of the ﬁrst cut\\n\\xa0\\xa06 if\\xa0q < p[i] + r[j − i]\\n\\xa0\\xa07 q = p[i] + r[j − i]\\n\\xa0\\xa08 s[j] = i // best cut location so far for length j\\n\\xa0\\xa09r[j] = q // remember the solution value for length j', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 492}),\n",
              " Document(page_content='10return\\xa0r and s\\nPRINT-CUT-ROD-SOLUTION(p, n)\\n1(r, s) = EXTENDED-BOTTOM-UP-CUT-ROD(p, n)\\n2while\\xa0n > 0\\n3 print s[n] // cut location for length n\\n4 n = n − s[n] // length of the remainder of the rod\\nExercises\\n14.1-1\\nShow that equation (14.4) follows from equation (14.3) and the initial\\ncondition T(0) = 1.\\n14.1-2\\nShow, by means of a counterexample, that the following “greedy”\\nstrategy does not always determine an optimal way to cut rods. Deﬁne\\nthe density of a rod of length i to be pi/i, that is, its value per inch. The\\ngreedy strategy for a rod of length n cuts off a ﬁrst piece of length i,\\nwhere 1 ≤ i ≤ n, having maximum density. It then continues by applying\\nthe greedy strategy to the remaining piece of length n − i.\\n14.1-3\\nConsider a modiﬁcation of the rod-cutting problem in which, in\\naddition to a price pi for each rod, each cut incurs a ﬁxed cost of c. The\\nrevenue associated with a solution is now the sum of the prices of the\\npieces minus the costs of making the cuts. Give a dynamic-programming\\nalgorithm to solve this modiﬁed problem.\\n14.1-4\\nModify CUT-ROD and MEMOIZED-CUT-ROD-AUX so that their\\nfor loops go up to only ⌊n/2 ⌋, rather than up to n. What other changes\\nto the procedures do you need to make? How are their running times\\naffected?\\n14.1-5', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 493}),\n",
              " Document(page_content='Modify MEMOIZED-CUT-ROD to return not only the value but the\\nactual solution.\\n14.1-6\\nThe Fibonacci numbers are deﬁned by recurrence (3.31) on page 69.\\nGive an O(n)-time dynamic-programming algorithm to compute the nth\\nFibonacci number. Draw the subproblem graph. How many vertices\\nand edges does the graph contain?\\n14.2\\xa0\\xa0\\xa0\\xa0Matrix-chain multiplication\\nOur next example of dynamic programming is an algorithm that solves\\nthe problem of matrix-chain multiplication. Given a sequence (chain)\\n〈A1, A2, …, An〉 of n matrices to be multiplied, where the matrices aren’t\\nnecessarily square, the goal is to compute the product\\nusing the standard algorithm3 for multiplying rectangular matrices,\\nwhich we’ll see in a moment, while minimizing the number of scalar\\nmultiplications.\\nYou can evaluate the expression (14.5) using the algorithm for\\nmultiplying pairs of rectangular matrices as a subroutine once you have\\nparenthesized it to resolve all ambiguities in how the matrices are\\nmultiplied together. Matrix multiplication is associative, and so all\\nparenthesizations yield the same product. A product of matrices is fully\\nparenthesized if it is either a single matrix or the product of two fully\\nparenthesized matrix products, surrounded by parentheses. For\\nexample, if the chain of matrices is 〈A1, A2, A3, A4〉, then you can fully\\nparenthesize the product A1A2A3A4 in ﬁve distinct ways:\\n(A1(A2(A3A4))),\\n(A1((A2A3)A4)),\\n((A1A2)(A3A4)),\\n((A1(A2A3))A4),', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 494}),\n",
              " Document(page_content='(((A1A2)A3)A4).\\nHow you parenthesize a chain of matrices can have a dramatic\\nimpact on the cost of evaluating the product. Consider ﬁrst the cost of\\nmultiplying two rectangular matrices. The standard algorithm is given\\nby the procedure RECTANGULAR-MATRIX-MULTIPLY, which\\ngeneralizes the square-matrix multiplication procedure MATRIX-\\nMULTIPLY on page 81. The RECTANGULAR-MATRIX-\\nMULTIPLY procedure computes C = C + A ·B for three matrices A =\\n(aij), B = (bij), and C = (cij), where A is p × q, B is q × r, and C is p × r.\\nRECTANGULAR-MATRIX-MULTIPLY(A, B, C, p, q, r)\\n1for\\xa0i = 1 to\\xa0p\\n2 for\\xa0j = 1 to\\xa0r\\n3 for\\xa0k = 1 to\\xa0q\\n4 cij = cij + aik · bkj\\nThe running time of RECTANGULAR-MATRIX-MULTIPLY is\\ndominated by the number of scalar multiplications in line 4, which is\\npqr. Therefore, we’ll consider the cost of multiplying matrices to be the\\nnumber of scalar multiplications. (The number of scalar multiplications\\ndominates even if we consider initializing C = 0 to perform just C = A\\n·B.)\\nTo illustrate the different costs incurred by different\\nparenthesizations of a matrix product, consider the problem of a chain\\n〈A1, A2, A3〉 of three matrices. Suppose that the dimensions of the\\nmatrices are 10 × 100, 100 × 5, and 5 × 50, respectively. Multiplying\\naccording to the parenthesization ((A1A2)A3) performs 10 · 100 · 5 =\\n5000 scalar multiplications to compute the 10 × 5 matrix product A1A2,\\nplus another 10 · 5 · 50 = 2500 scalar multiplications to multiply this\\nmatrix by A3, for a total of 7500 scalar multiplications. Multiplying\\naccording to the alternative parenthesization (A1(A2A3)) performs 100 ·\\n5 · 50 = 25,000 scalar multiplications to compute the 100 × 50 matrix', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 495}),\n",
              " Document(page_content='product A2A3, plus another 10 · 100 · 50 = 50,000 scalar multiplications\\nto multiply A1 by this matrix, for a total of 75,000 scalar\\nmultiplications. Thus, computing the product according to the ﬁrst\\nparenthesization is 10 times faster.\\nWe state the matrix-chain multiplication problem as follows: given a\\nchain 〈A1, A2, …, An〉 of n matrices, where for i = 1, 2, …, n, matrix Ai\\nhas dimension pi−1 × pi, fully parenthesize the product A1A2 ⋯ An in\\na way that minimizes the number of scalar multiplications. The input is\\nthe sequence of dimensions 〈p0, p1, p2, …, pn〉.\\nThe matrix-chain multiplication problem does not entail actually\\nmultiplying matrices. The goal is only to determine an order for\\nmultiplying matrices that has the lowest cost. Typically, the time\\ninvested in determining this optimal order is more than paid for by the\\ntime saved later on when actually performing the matrix multiplications\\n(such as performing only 7500 scalar multiplications instead of 75,000).\\nCounting the number of parenthesizations\\nBefore solving the matrix-chain multiplication problem by dynamic\\nprogramming, let us convince ourselves that exhaustively checking all\\npossible parenthesizations is not an efﬁcient algorithm. Denote the\\nnumber of alternative parenthesizations of a sequence of n matrices by\\nP(n). When n = 1, the sequence consists of just one matrix, and\\ntherefore there is only one way to fully parenthesize the matrix product.\\nWhen n ≥ 2, a fully parenthesized matrix product is the product of two\\nfully parenthesized matrix subproducts, and the split between the two\\nsubproducts may occur between the kth and (k + 1)st matrices for any k\\n= 1, 2, …, n − 1. Thus, we obtain the recurrence\\nProblem 12-4 on page 329 asked you to show that the solution to a\\nsimilar recurrence is the sequence of Catalan numbers, which grows as', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 496}),\n",
              " Document(page_content='Ω(4n/n3/2). A simpler exercise (see Exercise 14.2-3) is to show that the\\nsolution to the recurrence (14.6) is Ω(2n). The number of solutions is\\nthus exponential in n, and the brute-force method of exhaustive search\\nmakes for a poor strategy when determining how to optimally\\nparenthesize a matrix chain.\\nApplying dynamic programming\\nLet’s use the dynamic-programming method to determine how to\\noptimally parenthesize a matrix chain, by following the four-step\\nsequence that we stated at the beginning of this chapter:\\n1. Characterize the structure of an optimal solution.\\n2. Recursively deﬁne the value of an optimal solution.\\n3. Compute the value of an optimal solution.\\n4. Construct an optimal solution from computed information.\\nWe’ll go through these steps in order, demonstrating how to apply each\\nstep to the problem.\\nStep 1: The structure of an optimal parenthesization\\nIn the ﬁrst step of the dynamic-programming method, you ﬁnd the\\noptimal substructure and then use it to construct an optimal solution to\\nthe problem from optimal solutions to subproblems. To perform this\\nstep for the matrix-chain multiplication problem, it’s convenient to ﬁrst\\nintroduce some notation. Let Ai:j, where i ≤ j, denote the matrix that\\nresults from evaluating the product AiAi+1 ⋯ Aj. If the problem is\\nnontrivial, that is, i < j, then to parenthesize the product AiAi+1 ⋯ Aj,\\nthe product must split between Ak and Ak+1 for some integer k in the\\nrange i ≤ k < j. That is, for some value of k, ﬁrst compute the matrices\\nAi:k and Ak+1:j, and then multiply them together to produce the ﬁnal\\nproduct Ai:j. The cost of parenthesizing this way is the cost of', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 497}),\n",
              " Document(page_content='computing the matrix Ai:k, plus the cost of computing Ak+1:j, plus the\\ncost of multiplying them together.\\nThe optimal substructure of this problem is as follows. Suppose that\\nto optimally parenthesize AiAi+1 ⋯ Aj, you split the product between\\nAk and Ak+1. Then the way you parenthesize the “preﬁx” subchain\\nAiAi+1 ⋯ Ak within this optimal parenthesization of AiAi+1 ⋯ Aj\\nmust be an optimal parenthesization of AiAi+1 ⋯ Ak. Why? If there\\nwere a less costly way to parenthesize AiAi+1 ⋯ Ak, then you could\\nsubstitute that parenthesization in the optimal parenthesization of\\nAiAi+1 ⋯ Aj to produce another way to parenthesize AiAi+1 ⋯ Aj\\nwhose cost is lower than the optimum: a contradiction. A similar\\nobservation holds for how to parenthesize the subchain Ak+1Ak+2 ⋯\\nAj in the optimal parenthesization of AiAi+1 ⋯ Aj: it must be an\\noptimal parenthesization of Ak+1Ak+2 ⋯ Aj.\\nNow let’s use the optimal substructure to show how to construct an\\noptimal solution to the problem from optimal solutions to subproblems.\\nAny solution to a nontrivial instance of the matrix-chain multiplication\\nproblem requires splitting the product, and any optimal solution\\ncontains within it optimal solutions to subproblem instances. Thus, to\\nbuild an optimal solution to an instance of the matrix-chain\\nmultiplication problem, split the problem into two subproblems\\n(optimally parenthesizing AiAi+1 ⋯ Ak and Ak+1Ak+2 ⋯ Aj), ﬁnd\\noptimal solutions to the two subproblem instances, and then combine\\nthese optimal subproblem solutions. To ensure that you’ve examined the\\noptimal split, you must consider all possible splits.\\nStep 2: A recursive solution\\nThe next step is to deﬁne the cost of an optimal solution recursively in\\nterms of the optimal solutions to subproblems. For the matrix-chain\\nmultiplication problem, a subproblem is to determine the minimum cost\\nof parenthesizing AiAi+1 ⋯ Aj for 1 ≤ i ≤ j ≤ n. Given the input', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 498}),\n",
              " Document(page_content='dimensions 〈p0, p1, p2, …, pn〉, an index pair i, j speciﬁes a subproblem.\\nLet m[i, j] be the minimum number of scalar multiplications needed to\\ncompute the matrix Ai:j. For the full problem, the lowest-cost way to\\ncompute A1:n is thus m[1, n].\\nWe can deﬁne m[i, j] recursively as follows. If i = j, the problem is\\ntrivial: the chain consists of just one matrix Ai:i = Ai, so that no scalar\\nmultiplications are necessary to compute the product. Thus, m[i, i] = 0\\nfor i = 1, 2, …, n. To compute m[i, j] when i < j, we take advantage of\\nthe structure of an optimal solution from step 1. Suppose that an\\noptimal parenthesization splits the product AiAi+1 ⋯ Aj between Ak\\nand Ak+1, where i ≤ k < j. Then, m[i, j] equals the minimum cost m[i, k]\\nfor computing the subproduct Ai:k, plus the minimum cost m[k+1, j] for\\ncomputing the subproduct, Ak+1:j, plus the cost of multiplying these\\ntwo matrices together. Because each matrix Ai is pi−1 × pi, computing\\nthe matrix product Ai:kAk+1:j takes pi−1\\xa0pk pj scalar multiplications.\\nThus, we obtain\\nm[i, j] = m[i, k] + m[k + 1, j] + pi−1\\xa0pk pj.\\nThis recursive equation assumes that you know the value of k. But\\nyou don’t, at least not yet. You have to try all possible values of k. How\\nmany are there? Just j − i, namely k = i, i + 1, …, j − 1. Since the\\noptimal parenthesization must use one of these values for k, you need\\nonly check them all to ﬁnd the best. Thus, the recursive deﬁnition for\\nthe minimum cost of parenthesizing the product AiAi+1 ⋯ Aj becomes\\nThe m[i, j] values give the costs of optimal solutions to subproblems,\\nbut they do not provide all the information you need to construct an\\noptimal solution. To help you do so, let’s deﬁne s[i, j] to be a value of k\\nat which you split the product AiAi+1 ⋯ Aj in an optimal', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 499}),\n",
              " Document(page_content='parenthesization. That is, s[i, j] equals a value k such that m[i, j] = m[i, k]\\n+ m[k + 1, j] + pi−1\\xa0pk pj.\\nStep 3: Computing the optimal costs\\nAt this point, you could write a recursive algorithm based on recurrence\\n(14.7) to compute the minimum cost m[1, n] for multiplying A1A2 ⋯\\nAn. But as we saw for the rod-cutting problem, and as we shall see in\\nSection 14.3, this recursive algorithm takes exponential time. That’s no\\nbetter than the brute-force method of checking each way of\\nparenthesizing the product.\\nFortunately, there aren’t all that many distinct subproblems: just one\\nsubproblem for each choice of i and j satisfying 1 ≤ i ≤ j ≤ n, or \\n in all.4 A recursive algorithm may encounter each\\nsubproblem many times in different branches of its recursion tree. This\\nproperty of overlapping subproblems is the second hallmark of when\\ndynamic programming applies (the ﬁrst hallmark being optimal\\nsubstructure).\\nInstead of computing the solution to recurrence (14.7) recursively,\\nlet’s compute the optimal cost by using a tabular, bottom-up approach,\\nas in the procedure MATRIX-CHAIN-ORDER. (The corresponding\\ntop-down approach using memoization appears in Section 14.3.) The\\ninput is a sequence p = 〈p0, p1, …, pn〉 of matrix dimensions, along with\\nn, so that for i = 1, 2, …, n, matrix Ai has dimensions pi−1 × pi. The\\nprocedure uses an auxiliary table m[1 : n, 1 : n] to store the m[i, j] costs\\nand another auxiliary table s[1 : n − 1, 2 : n] that records which index k\\nachieved the optimal cost in computing m[i, j]. The table s will help in\\nconstructing an optimal solution.\\nMATRIX-CHAIN-ORDER(p, n)\\n\\xa0\\xa01let m[1 : n, 1 : n] and s[1 : n − 1, 2 : n] be new tables\\n\\xa0\\xa02for\\xa0i = 1 to\\xa0n // chain length 1\\n\\xa0\\xa03m[i, i] = 0\\n\\xa0\\xa04for\\xa0l = 2 to\\xa0n //\\xa0l is the chain length', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 500}),\n",
              " Document(page_content='\\xa0\\xa05for\\xa0i = 1 to\\xa0n − l + 1 // chain begins at Ai\\n\\xa0\\xa06 j = i + l − 1 // chain ends at Aj\\n\\xa0\\xa07 m[i, j] = ∞\\n\\xa0\\xa08 for\\xa0k = i\\xa0to\\xa0j − 1 // try Ai:kAk+1:j\\n\\xa0\\xa09 q = m[i, k] + m[k + 1, j] + pi−1pk pj\\n10 if\\xa0q < m[i, j]\\n11 m[i, j] = q // remember this cost\\n12 s[i, j] = k // remember this index\\n13return\\xa0m and s\\nIn what order should the algorithm ﬁll in the table entries? To answer\\nthis question, let’s see which entries of the table need to be accessed\\nwhen computing the cost m[i, j]. Equation (14.7) tells us that to compute\\nthe cost of matrix product Ai:j, ﬁrst the costs of the products Ai:k and\\nAk+1:j need to have been computed for all k = i, i + 1, …, j − 1. The\\nchain AiAi+1 ⋯ Aj consists of j − i + 1 matrices, and the chains AiAi+1\\n… Ak and Ak+1\\xa0Ak+2 … Aj consist of k − i + 1 and j − k matrices,\\nrespectively. Since k < j, a chain of k − i + 1 matrices consists of fewer\\nthan j − i + 1 matrices. Likewise, since k ≥ i, a chain of j − k matrices\\nconsists of fewer than j − i + 1 matrices. Thus, the algorithm should ﬁll\\nin the table m from shorter matrix chains to longer matrix chains. That\\nis, for the subproblem of optimally parenthesizing the chain AiAi+1 ⋯\\nAj, it makes sense to consider the subproblem size as the length j − i + 1\\nof the chain.\\nNow, let’s see how the MATRIX-CHAIN-ORDER procedure ﬁlls in\\nthe m[i, j] entries in order of increasing chain length. Lines 2–3 initialize\\nm[i, i] = 0 for i = 1, 2, …, n, since any matrix chain with just one matrix\\nrequires no scalar multiplications. In the for loop of lines 4–12, the loop\\nvariable l denotes the length of matrix chains whose minimum costs are\\nbeing computed. Each iteration of this loop uses recurrence (14.7) to\\ncompute m[i, i + l − 1] for i = 1, 2, …, n − l + 1. In the ﬁrst iteration, l =\\n2, and so the loop computes m[i, i + 1] for i = 1, 2, …, n − 1: the\\nminimum costs for chains of length l = 2. The second time through the', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 501}),\n",
              " Document(page_content='loop, it computes m[i, i + 2] for i = 1, 2, …, n − 2: the minimum costs\\nfor chains of length l = 3. And so on, ending with a single matrix chain\\nof length l = n and computing m[1, n]. When lines 7–12 compute an m[i,\\nj] cost, this cost depends only on table entries m[i, k] and m[k + 1, j],\\nwhich have already been computed.\\nFigure 14.5 illustrates the m and s tables, as ﬁlled in by the\\nMATRIX-CHAIN-ORDER procedure on a chain of n = 6 matrices.\\nSince m[i, j] is deﬁned only for i ≤ j, only the portion of the table m on or\\nabove the main diagonal is used. The ﬁgure shows the table rotated to\\nmake the main diagonal run horizontally. The matrix chain is listed\\nalong the bottom. Using this layout, the minimum cost m[i, j] for\\nmultiplying a subchain AiAi+1 ⋯ Aj of matrices appears at the\\nintersection of lines running northeast from Ai and northwest from Aj.\\nReading across, each diagonal in the table contains the entries for\\nmatrix chains of the same length. MATRIX-CHAIN-ORDER\\ncomputes the rows from bottom to top and from left to right within\\neach row. It computes each entry m[i, j] using the products pi−1\\xa0pk pj for\\nk = i, i + 1, …, j − 1 and all entries southwest and southeast from m[i, j].\\nA simple inspection of the nested loop structure of MATRIX-\\nCHAIN-ORDER yields a running time of O(n3) for the algorithm. The\\nloops are nested three deep, and each loop index (l, i, and k) takes on at\\nmost n − 1 values. Exercise 14.2-5 asks you to show that the running\\ntime of this algorithm is in fact also Ω(n3). The algorithm requires Θ (n2)\\nspace to store the m and s tables. Thus, MATRIX-CHAIN-ORDER is\\nmuch more efﬁcient than the exponential-time method of enumerating\\nall possible parenthesizations and checking each one.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 502}),\n",
              " Document(page_content='Figure 14.5 The m and s tables computed by MATRIX-CHAIN-ORDER for n = 6 and the\\nfollowing matrix dimensions:\\nmatrixA1 A2 A3 A4 A5 A6\\ndimension30 × 3535 × 1515 × 55 × 1010 × 2020 × 25\\nThe tables are rotated so that the main diagonal runs horizontally. The m table uses only the\\nmain diagonal and upper triangle, and the s table uses only the upper triangle. The minimum\\nnumber of scalar multiplications to multiply the 6 matrices is m[1, 6] = 15,125. Of the entries\\nthat are not tan, the pairs that have the same color are taken together in line 9 when computing\\nStep 4: Constructing an optimal solution\\nAlthough MATRIX-CHAIN-ORDER determines the optimal number\\nof scalar multiplications needed to compute a matrix-chain product, it\\ndoes not directly show how to multiply the matrices. The table s[1 : n −\\n1, 2 : n] provides the information needed to do so. Each entry s[i, j]\\nrecords a value of k such that an optimal parenthesization of AiAi+1 ⋯\\nAj splits the product between Ak and Ak+1. The ﬁnal matrix\\nmultiplication in computing A1:n optimally is A1:s[1,n]As[1,n]+1:n. The\\ns table contains the information needed to determine the earlier matrix', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 503}),\n",
              " Document(page_content='multiplications as well, using recursion: s[1, s[1, n]] determines the last\\nmatrix multiplication when computing A1:s[1,n] and s[s[1,n] + 1, n]\\ndetermines the last matrix multiplication when computing As[1,n]+1:n.\\nThe recursive procedure PRINT-OPTIMAL-PARENS on the facing\\npage prints an optimal parenthesization of the matrix chain product\\nAiAi+1 ⋯ Aj, given the s table computed by MATRIX-CHAIN-\\nORDER and the indices i and j. The initial call PRINT-OPTIMAL-\\nPARENS(s, 1, n) prints an optimal parenthesization of the full matrix\\nchain product A1A2 ⋯ An. In the example of Figure 14.5, the call\\nPRINT-OPTIMAL-PARENS(s, 1, 6) prints the optimal\\nparenthesization ((A1(A2A3))((A4A5)A6)).\\nPRINT-OPTIMAL-PARENS(s, i, j)\\n1if\\xa0i == j\\n2 print “A”i\\n3else print “(”\\n4 PRINT-OPTIMAL-PARENS(s, i, s[i, j])\\n5 PRINT-OPTIMAL-PARENS(s, s[i, j] + 1, j)\\n6 print “)”\\nExercises\\n14.2-1\\nFind an optimal parenthesization of a matrix-chain product whose\\nsequence of dimensions is 〈5, 10, 3, 12, 5, 50, 6〉.\\n14.2-2\\nGive a recursive algorithm MATRIX-CHAIN-MULTIPLY(A, s, i, j)\\nthat actually performs the optimal matrix-chain multiplication, given\\nthe sequence of matrices 〈A1, A2, …, An〉, the s table computed by\\nMATRIX-CHAIN-ORDER, and the indices i and j. (The initial call is\\nMATRIX-CHAIN-MULTIPLY(A, s, 1, n).) Assume that the call\\nRECTANGULAR-MATRIX-MULTIPLY(A, B) returns the product\\nof matrices A and B.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 504}),\n",
              " Document(page_content='14.2-3\\nUse the substitution method to show that the solution to the recurrence\\n(14.6) is Ω(2n).\\n14.2-4\\nDescribe the subproblem graph for matrix-chain multiplication with an\\ninput chain of length n. How many vertices does it have? How many\\nedges does it have, and which edges are they?\\n14.2-5\\nLet R(i, j) be the number of times that table entry m[i, j] is referenced\\nwhile computing other table entries in a call of MATRIX-CHAIN-\\nORDER. Show that the total number of references for the entire table is\\n(Hint: You may ﬁnd equation (A.4) on page 1141 u seful.)\\n14.2-6\\nShow that a full parenthesization of an n-element expression has exactly\\nn − 1 pairs of parentheses.\\n14.3\\xa0\\xa0\\xa0\\xa0Elements of dynamic programming\\nAlthough you have just seen two complete examples of the dynamic-\\nprogramming method, you might still be wondering just when the\\nmethod applies. From an engineering perspective, when should you look\\nfor a dynamic-programming solution to a problem? In this section, we’ll\\nexamine the two key ingredients that an optimization problem must\\nhave in order for dynamic programming to apply: optimal substructure\\nand overlapping subproblems. We’ll also revisit and discuss more fully\\nhow memoization might help you take advantage of the overlapping-\\nsubproblems property in a top-down recursive approach.\\nOptimal substructure', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 505}),\n",
              " Document(page_content='The ﬁrst step in solving an optimization problem by dynamic\\nprogramming is to characterize the structure of an optimal solution.\\nRecall that a problem exhibits optimal substructure if an optimal\\nsolution to the problem contains within it optimal solutions to\\nsubproblems. When a problem exhibits optimal substructure, that gives\\nyou a good clue that dynamic programming might apply. (As Chapter\\n15 discusses, it also might mean that a greedy strategy applies, however.)\\nDynamic programming builds an optimal solution to the problem from\\noptimal solutions to subproblems. Consequently, you must take care to\\nensure that the range of subproblems you consider includes those used\\nin an optimal solution.\\nOptimal substructure was key to solving both of the previous\\nproblems in this chapter. In Section 14.1, we observed that the optimal\\nway of cutting up a rod of length n (if Serling Enterprises makes any\\ncuts at all) involves optimally cutting up the two pieces resulting from\\nthe ﬁrst cut. In Section 14.2, we noted that an optimal parenthesization\\nof the matrix chain product AiAi+1 ⋯ Aj that splits the product\\nbetween Ak and Ak+1 contains within it optimal solutions to the\\nproblems of parenthesizing AiAi+1 ⋯ Ak and Ak+1Ak+2 ⋯ Aj.\\nYou will ﬁnd yourself following a common pattern in discovering\\noptimal substructure:\\n1. You show that a solution to the problem consists of making a\\nchoice, such as choosing an initial cut in a rod or choosing an\\nindex at which to split the matrix chain. Making this choice\\nleaves one or more subproblems to be solved.\\n2. You suppose that for a given problem, you are given the choice\\nthat leads to an optimal solution. You do not concern yourself\\nyet with how to determine this choice. You just assume that it\\nhas been given to you.\\n3. Given this choice, you determine which subproblems ensue and\\nhow to best characterize the resulting space of subproblems.\\n4. You show that the solutions to the subproblems used within an\\noptimal solution to the problem must themselves be optimal by', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 506}),\n",
              " Document(page_content='using a “cut-and-paste” technique. You do so by supposing that\\neach of the subproblem solutions is not optimal and then\\nderiving a contradiction. In particular, by “cutting out” the\\nnonoptimal solution to each subproblem and “pasting in” the\\noptimal one, you show that you can get a better solution to the\\noriginal problem, thus contradicting your supposition that you\\nalready had an optimal solution. If an optimal solution gives rise\\nto more than one subproblem, they are typically so similar that\\nyou can modify the cut-and-paste argument for one to apply to\\nthe others with little effort.\\nTo characterize the space of subproblems, a good rule of thumb says\\nto try to keep the space as simple as possible and then expand it as\\nnecessary. For example, the space of subproblems for the rod-cutting\\nproblem contained the problems of optimally cutting up a rod of length\\ni for each size i. This subproblem space worked well, and it was not\\nnecessary to try a more general space of subproblems.\\nConversely, suppose that you tried to constrain the subproblem\\nspace for matrix-chain multiplication to matrix products of the form\\nA1A2 ⋯ Aj. As before, an optimal parenthesization must split this\\nproduct between Ak and Ak+1 for some 1 ≤ k < j. Unless you can\\nguarantee that k always equals j − 1, you will ﬁnd that you have\\nsubproblems of the form A1A2 ⋯ Ak and Ak+1Ak+2 ⋯ Aj. Moreover,\\nthe latter subproblem does not have the form A1A2 ⋯ Aj. To solve this\\nproblem by dynamic programming, you need to allow the subproblems\\nto vary at “both ends.” That is, both i and j need to vary in the\\nsubproblem of parenthesizing the product AiAi+1 ⋯ Aj.\\nOptimal substructure varies across problem domains in two ways:\\n1. how many subproblems an optimal solution to the original\\nproblem uses, and\\n2. how many choices you have in determining which subproblem(s)\\nto use in an optimal solution.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 507}),\n",
              " Document(page_content='In the rod-cutting problem, an optimal solution for cutting up a rod of\\nsize n uses just one subproblem (of size n − i), but we have to consider n\\nchoices for i in order to determine which one yields an optimal solution.\\nMatrix-chain multiplication for the subchain AiAi+1 ⋯ Aj serves an\\nexample with two subproblems and j − i choices. For a given matrix Ak\\nwhere the product splits, two subproblems arise—parenthesizing\\nAiAi+1 ⋯ Ak and parenthesizing Ak+1Ak+2 ⋯ Aj—and we have to\\nsolve both of them optimally. Once we determine the optimal solutions\\nto subproblems, we choose from among j − i candidates for the index k.\\nInformally, the running time of a dynamic-programming algorithm\\ndepends on the product of two factors: the number of subproblems\\noverall and how many choices you look at for each subproblem. In rod\\ncutting, we had Θ (n) subproblems overall, and at most n choices to\\nexamine for each, yielding an O(n2) running time. Matrix-chain\\nmultiplication had Θ (n2) subproblems overall, and each had at most n −\\n1 choices, giving an O(n3) running time (actually, a Θ (n3) running time,\\nby Exercise 14.2-5).\\nUsually, the subproblem graph gives an alternative way to perform\\nthe same analysis. Each vertex corresponds to a subproblem, and the\\nchoices for a subproblem are the edges incident from that subproblem.\\nRecall that in rod cutting, the subproblem graph has n vertices and at\\nmost n edges per vertex, yielding an O(n2) running time. For matrix-\\nchain multiplication, if you were to draw the subproblem graph, it\\nwould have Θ (n2) vertices and each vertex would have degree at most n\\n− 1, giving a total of O(n3) vertices and edges.\\nDynamic programming often uses optimal substructure in a bottom-\\nup fashion. That is, you ﬁrst ﬁnd optimal solutions to subproblems and,\\nhaving solved the subproblems, you ﬁnd an optimal solution to the\\nproblem. Finding an optimal solution to the problem entails making a\\nchoice among subproblems as to which you will use in solving the\\nproblem. The cost of the problem solution is usually the subproblem\\ncosts plus a cost that is directly attributable to the choice itself. In rod\\ncutting, for example, ﬁrst we solved the subproblems of determining', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 508}),\n",
              " Document(page_content='optimal ways to cut up rods of length i for i = 0, 1, …, n − 1, and then\\nwe determined which of these subproblems yielded an optimal solution\\nfor a rod of length n, using equation (14.2). The cost attributable to the\\nchoice itself is the term pi in equation (14.2). In matrix-chain\\nmultiplication, we determined optimal parenthesizations of subchains\\nof AiAi+1 ⋯ Aj, and then we chose the matrix Ak at which to split the\\nproduct. The cost attributable to the choice itself is the term pi−1\\xa0pk pj.\\nChapter 15 explores “greedy algorithms,” which have many\\nsimilarities to dynamic programming. In particular, problems to which\\ngreedy algorithms apply have optimal substructure. One major\\ndifference between greedy algorithms and dynamic programming is that\\ninstead of ﬁrst ﬁnding optimal solutions to subproblems and then\\nmaking an informed choice, greedy algorithms ﬁrst make a “greedy”\\nchoice—the choice that looks best at the time—an d then solve a\\nresulting subproblem, without bothering to solve all possible related\\nsmaller subproblems. Surprisingly, in some cases this strategy w orks!\\nSubtleties\\nYou should be careful not to assume that optimal substructure applies\\nwhen it does not. Consider the following two problems whose input\\nconsists of a directed graph G = (V, E) and vertices u, v ∈ V.\\nUnweighted shortest path:5 Find a path from u to v consisting of the\\nfewest edges. Such a path must be simple, since removing a cycle from\\na path produces a path with fewer edges.\\nUnweighted longest simple path: Find a simple path from u to v\\nconsisting of the most edges. (Without the requirement that the path\\nmust be simple, the problem is undeﬁned, since repeatedly traversing a\\ncycle creates paths with an arbitrarily large number of edges.)\\nThe unweighted shortest-path problem exhibits optimal\\nsubstructure. Here’s how. Suppose that u ≠ v, so that the problem is\\nnontrivial. Then, any path p from u to v must contain an intermediate\\nvertex, say w. (Note that w may be u or v.) Then, we can decompose the', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 509}),\n",
              " Document(page_content='path \\n  into subpaths \\n . The number of edges in p equals\\nthe number of edges in p1 plus the number of edges in p2. We claim that\\nif p is an optimal (i.e., shortest) path from u to v, then p1 must be a\\nshortest path from u to w. Why? As suggested earlier, use a “cut-and-\\npaste” argument: if there were another path, say \\n, from u to w with\\nfewer edges than p1, then we could cut out p1 and paste in \\n to produce\\na path \\n  with fewer edges than p, thus contradicting p’s\\noptimality. Likewise, p2 must be a shortest path from w to v. Thus, to\\nﬁnd a shortest path from u to v, consider all intermediate vertices w,\\nﬁnd a shortest path from u to w and a shortest path from w to v, and\\nchoose an intermediate vertex w that yields the overall shortest path.\\nSection 23.2 uses a variant of this observation of optimal substructure\\nto ﬁnd a shortest path between every pair of vertices on a weighted,\\ndirected graph.\\nYou might be tempted to assume that the problem of ﬁnding an\\nunweighted longest simple path exhibits optimal substructure as well.\\nAfter all, if we decompose a longest simple path \\n  into subpaths \\n, then mustn’t p1 be a longest simple path from u to w, and\\nmustn’t p2 be a longest simple path from w to v? The answer is no!\\nFigure 14.6 supplies an example. Consider the path q → r → t, which is\\na longest simple path from q to t. Is q → r a longest simple path from q\\nto r? No, for the path q → s → t → r is a simple path that is longer. Is r\\n→ t a longest simple path from r to t? No again, for the path r → q → s\\n→ t is a simple path that is longer.\\nFigure 14.6 A directed graph showing that the problem of ﬁnding a longest simple path in an\\nunweighted directed graph does not have optimal substructure. The path q → r → t is a longest\\nsimple path from q to t, but the subpath q → r is not a longest simple path from q to r, nor is the\\nsubpath r → t a longest simple path from r to t.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 510}),\n",
              " Document(page_content='This example shows that for longest simple paths, not only does the\\nproblem lack optimal substructure, but you cannot necessarily assemble\\na “legal” solution to the problem from solutions to subproblems. If you\\ncombine the longest simple paths q → s → t → r and r → q → s → t,\\nyou get the path q → s → t → r → q → s → t, which is not simple.\\nIndeed, the problem of ﬁnding an unweighted longest simple path does\\nnot appear to have any sort of optimal substructure. No efﬁcient\\ndynamic-programming algorithm for this problem has ever been found.\\nIn fact, this problem is NP-complete, which—as  we shall see in Chapter\\n34—means that we are unlikely to ﬁnd a way to solve it in polynomial\\ntime.\\nWhy is the substructure of a longest simple path so different from\\nthat of a shortest path? Although a solution to a problem for both\\nlongest and shortest paths uses two subproblems, the subproblems in\\nﬁnding the longest simple path are not independent, whereas for shortest\\npaths they are. What do we mean by subproblems being independent?\\nWe mean that the solution to one subproblem does not affect the\\nsolution to another subproblem of the same problem. For the example\\nof Figure 14.6, we have the problem of ﬁnding a longest simple path\\nfrom q to t with two subproblems: ﬁnding longest simple paths from q\\nto r and from r to t. For the ﬁrst of these subproblems, we chose the\\npath q → s → t → r, which used the vertices s and t. These vertices\\ncannot appear in a solution to the second subproblem, since the\\ncombination of the two solutions to subproblems yields a path that is\\nnot simple. If vertex t cannot be in the solution to the second problem,\\nthen there is no way to solve it, since t is required to be on the path that\\nforms the solution, and it is not the vertex where the subproblem\\nsolutions are “spliced” together (that vertex being r). Because vertices s\\nand t appear in one subproblem solution, they cannot appear in the\\nother subproblem solution. One of them must be in the solution to the\\nother subproblem, however, and an optimal solution requires both.\\nThus, we say that these subproblems are not independent. Looked at\\nanother way, using resources in solving one subproblem (those resources\\nbeing vertices) renders them unavailable for the other subproblem.\\nWhy, then, are the subproblems independent for ﬁnding a shortest\\npath? The answer is that by nature, the subproblems do not share', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 511}),\n",
              " Document(page_content='resources. We claim that if a vertex w is on a shortest path p from u to v,\\nthen we can splice together any shortest path \\n  and any shortest\\npath \\n  to produce a shortest path from u to v. We are assured that,\\nother than w, no vertex can appear in both paths p1 and p2. Why?\\nSuppose that some vertex x ≠ w appears in both p1 and p2, so that we\\ncan decompose p1 as \\n  and p2 as \\n . By the optimal\\nsubstructure of this problem, path p has as many edges as p1 and p2\\ntogether. Let’s say that p has e edges. Now let us construct a path \\n from u to v. Because we have excised the paths from x to\\nw and from w to x, each of which contains at least one edge, path p′\\ncontains at most e − 2 edges, which contradicts the assumption that p is\\na shortest path. Thus, we are assured that the subproblems for the\\nshortest-path problem are independent.\\nThe two problems examined in Sections 14.1 and 14.2 have\\nindependent subproblems. In matrix-chain multiplication, the\\nsubproblems are multiplying subchains AiAi+1 ⋯ Ak and Ak+1Ak+2\\n⋯ Aj. These subchains are disjoint, so that no matrix could possibly be\\nincluded in both of them. In rod cutting, to determine the best way to\\ncut up a rod of length n, we looked at the best ways of cutting up rods of\\nlength i for i = 0, 1, …, n − 1. Because an optimal solution to the length-\\nn problem includes just one of these subproblem solutions (after cutting\\noff the ﬁrst piece), independence of subproblems is not an issue.\\nOverlapping subproblems\\nThe second ingredient that an optimization problem must have for\\ndynamic programming to apply is that the space of subproblems must\\nbe “small” in the sense that a recursive algorithm for the problem solves\\nthe same subproblems over and over, rather than always generating new\\nsubproblems. Typically, the total number of distinct subproblems is a\\npolynomial in the input size. When a recursive algorithm revisits the\\nsame problem repeatedly, we say that the optimization problem has\\noverlapping subproblems.6 In contrast, a problem for which a divide-\\nand-conquer approach is suitable usually generates brand-new problems', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 512}),\n",
              " Document(page_content='at each step of the recursion. Dynamic-programming algorithms\\ntypically take advantage of overlapping subproblems by solving each\\nsubproblem once and then storing the solution in a table where it can be\\nlooked up when needed, using constant time per lookup.\\nFigure 14.7 The recursion tree for the computation of RECURSIVE-MATRIX-CHAIN(p, 1,\\n4). Each node contains the parameters i and j. The computations performed in a subtree shaded\\nblue are replaced by a single table lookup in MEMOIZED-MATRIX-CHAIN.\\nIn Section 14.1, we brieﬂy examined how a recursive solution to rod\\ncutting makes exponentially many calls to ﬁnd solutions of smaller\\nsubproblems. The dynamic-programming solution reduces the running\\ntime from the exponential time of the recursive algorithm down to\\nquadratic time.\\nTo illustrate the overlapping-subproblems property in greater detail,\\nlet’s revisit the matrix-chain multiplication problem. Referring back to\\nFigure 14.5, observe that MATRIX-CHAIN-ORDER repeatedly looks\\nup the solution to subproblems in lower rows when solving subproblems\\nin higher rows. For example, it references entry m[3, 4] four times:\\nduring the computations of m[2, 4], m[1, 4], m[3, 5], and m[3, 6]. If the\\nalgorithm were to recompute m[3, 4] each time, rather than just looking\\nit up, the running time would increase dramatically. To see how,\\nconsider the inefﬁcient recursive procedure RECURSIVE-MATRIX-\\nCHAIN on the facing page, which determines m[i, j], the minimum\\nnumber of scalar multiplications needed to compute the matrix-chain\\nproduct Ai:j = AiAi+1 ⋯ Aj. The procedure is based directly on the', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 513}),\n",
              " Document(page_content='recurrence (14.7). Figure 14.7 shows the recursion tree produced by the\\ncall RECURSIVE-MATRIX-CHAIN(p, 1, 4). Each node is labeled by\\nthe values of the parameters i and j. Observe that some pairs of values\\noccur many times.\\nIn fact, the time to compute m[1, n] by this recursive procedure is at\\nleast exponential in n. To see why, let T(n) denote the time taken by\\nRECURSIVE-MATRIX-CHAIN\\xa0to compute an optimal\\nparenthesization of a chain of n matrices. Because the execution of lines\\n1–2 and of lines 6–7 each take at least unit time, as does the\\nmultiplication in line 5, inspection of the procedure yields the recurrence\\nRECURSIVE-MATRIX-CHAIN(p, i, j)\\n1if\\xa0i == j\\n2 return 0\\n3m[i, j] = ∞\\n4for\\xa0k = i\\xa0to\\xa0j − 1\\n5 q = RECURSIVE-MATRIX-CHAIN(p, i, k)\\n+ RECURSIVE-MATRIX-CHAIN(p, k + 1, j)\\n+ pi−1\\xa0pk pj\\n6 if\\xa0q < m[i, j]\\n7 m[i, j] = q\\n8return\\xa0m[i, j]\\nNoting that for i = 1, 2, …, n − 1, each term T(i) appears once as T(k)\\nand once as T(n − k), and collecting the n − 1 1s in the summation\\ntogether with the 1 out front, we can rewrite the recurrence as\\n', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 514}),\n",
              " Document(page_content='Let’s prove that T(n) = Ω(2n) using the substitution method.\\nSpeciﬁcally, we’ll show that T(n) ≥ 2n−1 for all n ≥ 1. For the base case n\\n= 1, the summation is empty, and we get T(1) ≥ 1 = 20. Inductively, for n\\n≥ 2 we have\\nwhich completes the proof. Thus, the total amount of work performed\\nby the call RECURSIVE-MATRIX-CHAIN(p, 1, n) is at least\\nexponential in n.\\nCompare this top-down, recursive algorithm (without memoization)\\nwith the bottom-up dynamic-programming algorithm. The latter is\\nmore efﬁcient because it takes advantage of the overlapping-\\nsubproblems property. Matrix-chain multiplication has only Θ (n2)\\ndistinct subproblems, and the dynamic-programming algorithm solves\\neach exactly once. The recursive algorithm, on the other hand, must\\nsolve each subproblem every time it reappears in the recursion tree.\\nWhenever a recursion tree for the natural recursive solution to a\\nproblem contains the same subproblem repeatedly, and the total\\nnumber of distinct subproblems is small, dynamic programming can\\nimprove efﬁciency, sometimes dramatically.\\nReconstructing an optimal solution\\nAs a practical matter, you’ll often want to store in a separate table\\nwhich choice you made in each subproblem so that you do not have to\\nreconstruct this information from the table of costs.\\nFor matrix-chain multiplication, the table s[i, j] saves a signiﬁcant\\namount of work when we need to reconstruct an optimal solution.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 515}),\n",
              " Document(page_content='Suppose that the MATRIX-CHAIN-ORDER procedure on page 378\\ndid not maintain the s[i, j] table, so that it ﬁlled in only the table m[i, j]\\ncontaining optimal subproblem costs. The procedure chooses from\\namong j − i possibilities when determining which subproblems to use in\\nan optimal solution to parenthesizing AiAi+1 ⋯ Aj, and j − i is not a\\nconstant. Therefore, it would take Θ (j −i) = ω(1) time to reconstruct\\nwhich subproblems it chose for a solution to a given problem. Because\\nMATRIX-CHAIN-ORDER stores in s[i, j] the index of the matrix at\\nwhich it split the product AiAi+1 ⋯ Aj, the PRINT-OPTIMAL-\\nPARENS procedure on page 381 can look up each choice in O(1) time.\\nMemoization\\nAs we saw for the rod-cutting problem, there is an alternative approach\\nto dynamic programming that often offers the efﬁciency of the bottom-\\nup dynamic-programming approach while maintaining a top-down\\nstrategy. The idea is to memoize the natural, but inefﬁcient, recursive\\nalgorithm. As in the bottom-up approach, you maintain a table with\\nsubproblem solutions, but the control structure for ﬁlling in the table is\\nmore like the recursive algorithm.\\nA memoized recursive algorithm maintains an entry in a table for the\\nsolution to each subproblem. Each table entry initially contains a\\nspecial value to indicate that the entry has yet to be ﬁlled in. When the\\nsubproblem is ﬁrst encountered as the recursive algorithm unfolds, its\\nsolution is computed and then stored in the table. Each subsequent\\nencounter of this subproblem simply looks up the value stored in the\\ntable and returns it.7\\nThe procedure MEMOIZED-MATRIX-CHAIN is a memoized\\nversion of the procedure RECURSIVE-MATRIX-CHAIN on page\\n389. Note where it resembles the memoized top-down method on page\\n369 for the rod-cutting problem.\\nMEMOIZED-MATRIX-CHAIN(p, n)\\n1let m[1 : n, 1 : n] be a new table\\n2for\\xa0i = 1 to\\xa0n', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 516}),\n",
              " Document(page_content='3 for\\xa0j = i\\xa0to\\xa0n\\n4 m[i, j] = ∞\\n5return LOOKUP-CHAIN(m, p, 1, n)\\nLOOKUP-CHAIN(m, p, i, j)\\n1if\\xa0m[i, j] < ∞\\n2 return\\xa0m[i, j]\\n3if\\xa0i == j\\n4 m[i, j] = 0\\n5else for\\xa0k = i\\xa0to\\xa0j − 1\\n6 q = LOOKUP-CHAIN(m, p, i, k)\\n+ LOOKUP-CHAIN(m, p, k + 1, j) + pi−1\\xa0pk pj\\n7 if\\xa0q < m[i, j]\\n8 m[i, j] = q\\n9return\\xa0m[i, j]\\nThe MEMOIZED-MATRIX-CHAIN procedure, like the bottom-\\nup MATRIX-CHAIN-ORDER procedure on page 378, maintains a\\ntable m[1 : n, 1 : n] of computed values of m[i, j], the minimum number\\nof scalar multiplications needed to compute the matrix Ai:j. Each table\\nentry initially contains the value ∞ to indicate that the entry has yet to\\nbe ﬁlled in. Upon calling LOOKUP-CHAIN(m, p, i, j), if line 1 ﬁnds\\nthat m[i, j] < ∞, then the procedure simply returns the previously\\ncomputed cost m[i, j] in line 2. Otherwise, the cost is computed as in\\nRECURSIVE-MATRIX-CHAIN, stored in m[i, j], and returned. Thus,\\nLOOKUP-CHAIN(m, p, i, j) always returns the value of m[i, j], but it\\ncomputes it only upon the ﬁrst call of LOOKUP-CHAIN with these\\nspeciﬁc values of i and j. Figure 14.7 illustrates how MEMOIZED-\\nMATRIX-CHAIN saves time compared with RECURSIVE-MATRIX-\\nCHAIN. Subtrees shaded blue represent values that are looked up\\nrather than recomputed.\\nLike the bottom-up procedure MATRIX-CHAIN-ORDER, the\\nmemoized procedure MEMOIZED-MATRIX-CHAIN runs in O(n3)\\ntime. To begin with, line 4 of MEMOIZED-MATRIX-CHAIN executes\\nΘ(n2) times, which dominates the running time outside of the call to', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 517}),\n",
              " Document(page_content='LOOKUP-CHAIN in line 5. We can categorize the calls of LOOKUP-\\nCHAIN into two types:\\n1. calls in which m[i, j] = ∞, so that lines 3–9 execute, and\\n2. calls in which m[i, j] < ∞, so that LOOKUP-CHAIN simply\\nreturns in line 2.\\nThere are Θ (n2) calls of the ﬁrst type, one per table entry. All calls of the\\nsecond type are made as recursive calls by calls of the ﬁrst type.\\nWhenever a given call of LOOKUP-CHAIN makes recursive calls, it\\nmakes O(n) of them. Therefore, there are O(n3) calls of the second type\\nin all. Each call of the second type takes O(1) time, and each call of the\\nﬁrst type takes O(n) time plus the time spent in its recursive calls. The\\ntotal time, therefore, is O(n3). Memoization thus turns an Ω(2n)-time\\nalgorithm into an O(n3)-time algorithm.\\nWe have seen how to solve the matrix-chain multiplication problem\\nby either a top-down, memoized dynamic-programming algorithm or a\\nbottom-up dynamic-programming algorithm in O(n3) time. Both the\\nbottom-up and memoized methods take advantage of the overlapping-\\nsubproblems property. There are only Θ (n2) distinct subproblems in\\ntotal, and either of these methods computes the solution to each\\nsubproblem only once. Without memoization, the natural recursive\\nalgorithm runs in exponential time, since solved subproblems are\\nrepeatedly solved.\\nIn general practice, if all subproblems must be solved at least once, a\\nbottom-up dynamic-programming algorithm usually outperforms the\\ncorresponding top-down memoized algorithm by a constant factor,\\nbecause the bottom-up algorithm has no overhead for recursion and\\nless overhead for maintaining the table. Moreover, for some problems\\nyou can exploit the regular pattern of table accesses in the dynamic-\\nprogramming algorithm to reduce time or space requirements even\\nfurther. On the other hand, in certain situations, some of the\\nsubproblems in the subproblem space might not need to be solved at all.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 518}),\n",
              " Document(page_content='In that case, the memoized solution has the advantage of solving only\\nthose subproblems that are deﬁnitely required.\\nExercises\\n14.3-1\\nWhich is a more efﬁcient way to determine the optimal number of\\nmultiplications in a matrix-chain multiplication problem: enumerating\\nall the ways of parenthesizing the product and computing the number of\\nmultiplications for each, or running RECURSIVE-MATRIX-CHAIN?\\nJustify your answer.\\n14.3-2\\nDraw the recursion tree for the MERGE-SORT procedure from Section\\n2.3.1 on an array of 16 elements. Explain why memoization fails to\\nspeed up a good divide-and-conquer algorithm such as MERGE-\\nSORT.\\n14.3-3\\nConsider the antithetical variant of the matrix-chain multiplication\\nproblem where the goal is to parenthesize the sequence of matrices so as\\nto maximize, rather than minimize, the number of scalar multiplications.\\nDoes this problem exhibit optimal substructure?\\n14.3-4\\nAs stated, in dynamic programming, you ﬁrst solve the subproblems\\nand then choose which of them to use in an optimal solution to the\\nproblem. Professor Capulet claims that she does not always need to\\nsolve all the subproblems in order to ﬁnd an optimal solution. She\\nsuggests that she can ﬁnd an optimal solution to the matrix-chain\\nmultiplication problem by always choosing the matrix Ak at which to\\nsplit the subproduct AiAi+1 ⋯ Aj (by selecting k to minimize the\\nquantity pi−1\\xa0pk pj) before solving the subproblems. Find an instance of\\nthe matrix-chain multiplication problem for which this greedy approach\\nyields a suboptimal solution.\\n14.3-5', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 519}),\n",
              " Document(page_content='Suppose that the rod-cutting problem of Section 14.1 also had a limit li\\non the number of pieces of length i allowed to be produced, for i = 1, 2,\\n…, n. Show that the optimal-substructure property described in Section\\n14.1 no longer holds.\\n14.4\\xa0\\xa0\\xa0\\xa0Longest common subsequence\\nBiological applications often need to compare the DNA of two (or\\nmore) different organisms. A strand of DNA consists of a string of\\nmolecules called bases, where the possible bases are adenine, cytosine,\\nguanine, and thymine. Representing each of these bases by its initial\\nletter, we can express a strand of DNA as a string over the 4-element set\\n{A, C, G, T}. (See Section C.1 for the deﬁnition of a string.) For\\nexample, the DNA of one organism may be S1 =\\nACCGGTCGAGTGCGCGGAAGCCGGCCGAA , and the DNA of another\\norganism may be S2 = GTCGTTCGGAATGCCGTTGCTCTGTAAA . One\\nreason to compare two strands of DNA is to determine how “similar”\\nthe two strands are, as some measure of how closely related the two\\norganisms are. We can, and do, deﬁne similarity in many different ways.\\nFor example, we can say that two DNA strands are similar if one is a\\nsubstring of the other. (Chapter 32 explores algorithms to solve this\\nproblem.) In our example, neither S1 nor S2 is a substring of the other.\\nAlternatively, we could say that two strands are similar if the number of\\nchanges needed to turn one into the other is small. (Problem 14-5 looks\\nat this notion.) Yet another way to measure the similarity of strands S1\\nand S2 is by ﬁnding a third strand S3 in which the bases in S3 appear in\\neach of S1 and S2. These bases must appear in the same order, but not\\nnecessarily consecutively. The longer the strand S3 we can ﬁnd, the\\nmore similar S1 and S2 are. In our example, the longest strand S3 is\\nGTCGTCGGAAGCCGGCCGAA .\\nWe formalize this last notion of similarity as the longest-common-\\nsubsequence problem. A subsequence of a given sequence is just the\\ngiven sequence with 0 or more elements left out. Formally, given a', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 520}),\n",
              " Document(page_content='sequence X = 〈x1, x2, …, xm〉, another sequence Z = 〈z1, z2, …, zk〉 is a\\nsubsequence of X if there exists a strictly increasing sequence 〈i1, i2, …,\\nik〉 of indices of X such that for all j = 1, 2, …, k, we have \\n . For\\nexample, Z = 〈B, C, D, B〉 is a subsequence of X = 〈A, B, C, B, D, A, B〉\\nwith corresponding index sequence 〈2, 3, 5, 7〉.\\nGiven two sequences X and Y, we say that a sequence Z is a common\\nsubsequence of X and Y if Z is a subsequence of both X and Y. For\\nexample, if X = 〈A, B, C, B, D, A, B〉 and Y = 〈B, D, C, A, B, A〉, the\\nsequence 〈B, C, A〉 is a common subsequence of both X and Y. The\\nsequence 〈B, C, A〉 is not a longest common subsequence (LCS) of X\\nand Y, however, since it has length 3 and the sequence 〈B, C, B, A〉,\\nwhich is also common to both sequences X and Y, has length 4. The\\nsequence 〈B, C, B, A〉 is an LCS of X and Y, as is the sequence 〈B, D, A,\\nB〉, since X and Y have no common subsequence of length 5 or greater.\\nIn the longest-common-subsequence problem, the input is two\\nsequences X = 〈x1, x2, …, xm〉 and Y = 〈y1, y2, …, yn〉, and the goal is\\nto ﬁnd a maximum-length common subsequence of X and Y. This\\nsection shows how to efﬁciently solve the LCS problem using dynamic\\nprogramming.\\nStep 1: Characterizing a longest common subsequence\\nYou can solve the LCS problem with a brute-force approach: enumerate\\nall subsequences of X and check each subsequence to see whether it is\\nalso a subsequence of Y, keeping track of the longest subsequence you\\nﬁnd. Each subsequence of X corresponds to a subset of the indices {1,\\n2, …, m} of X. Because X has 2m subsequences, this approach requires\\nexponential time, making it impractical for long sequences.\\nThe LCS problem has an optimal-substructure property, however, as\\nthe following theorem shows. As we’ll see, the natural classes of\\nsubproblems correspond to pairs of “preﬁxes” of the two input\\nsequences. To be precise, given a sequence X = 〈x1, x2, …, xm〉, we\\ndeﬁne the ith preﬁx of X, for i = 0, 1, …, m, as Xi = 〈x1, x2, …, xi〉. For', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 521}),\n",
              " Document(page_content='example, if X = 〈A, B, C, B, D, A, B〉, then X4 = 〈A, B, C, B〉 and X0 is\\nthe empty sequence.\\nTheorem 14.1 (Optimal substructure of an L CS)\\nLet X = 〈x1, x2, …, xm〉 and Y = 〈y1, y2, …, yn〉 be sequences, and let Z\\n= 〈z1, z2, …, zk〉 be any LCS of X and Y.\\n1. If xm = yn, then zk = xm = yn and Zk−1 is an LCS of Xm−1\\nand Yn−1.\\n2. If xm ≠ yn and zk ≠ xm, then Z is an LCS of Xm−1 and Y.\\n3. If xm ≠ yn and zk ≠ yn, then Z is an LCS of X and Yn−1.\\nProof\\xa0\\xa0\\xa0(1) If zk ≠ xm, then we could append xm = yn to Z to obtain a\\ncommon subsequence of X and Y of length k + 1, contradicting the\\nsupposition that Z is a longest common subsequence of X and Y. Thus,\\nwe must have zk = xm = yn. Now, the preﬁx Zk−1 is a length-(k − 1)\\ncommon subsequence of Xm−1 and Yn−1. We wish to show that it is an\\nLCS. Suppose for the purpose of contradiction that there exists a\\ncommon subsequence W of Xm−1 and Yn−1 with length greater than k\\n− 1. Then, appending xm = yn to W produces a common subsequence\\nof X and Y whose length is greater than k, which is a contradiction.\\n(2) If zk ≠ xm, then Z is a common subsequence of Xm−1 and Y. If\\nthere were a common subsequence W of Xm−1 and Y with length\\ngreater than k, then W would also be a common subsequence of Xm\\nand Y, contradicting the assumption that Z is an LCS of X and Y.\\n(3) The proof is symmetric to (2).\\n▪\\nThe way that Theorem 14.1 characterizes longest common\\nsubsequences says that an LCS of two sequences contains within it an\\nLCS of preﬁxes of the two sequences. Thus, the LCS problem has an\\noptimal-substructure property. A recursive solution also has the\\noverlapping-subproblems property, as we’ll see in a moment.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 522}),\n",
              " Document(page_content='Step 2: A recursive solution\\nTheorem 14.1 implies that you should examine either one or two\\nsubproblems when ﬁnding an LCS of X = 〈x1, x2, …, xm〉 and Y = 〈y1,\\ny2, …, yn〉. If xm = yn, you need to ﬁnd an LCS of Xm−1 and Yn−1.\\nAppending xm = yn to this LCS yields an LCS of X and Y. If xm ≠ yn,\\nthen you have to solve two subproblems: ﬁnding an LCS of Xm−1 and\\nY and ﬁnding an LCS of X and Yn−1. Whichever of these two LCSs is\\nlonger is an LCS of X and Y. Because these cases exhaust all\\npossibilities, one of the optimal subproblem solutions must appear\\nwithin an LCS of X and Y.\\nThe LCS problem has the overlapping-subproblems property. Here’s\\nhow. To ﬁnd an LCS of X and Y, you might need to ﬁnd the LCSs of X\\nand Yn−1 and of Xm−1 and Y. But each of these subproblems has the\\nsubsubproblem of ﬁnding an LCS of Xm−1 and Yn−1. Many other\\nsubproblems share subsubproblems.\\nAs in the matrix-chain multiplication problem, solving the LCS\\nproblem recursively involves establishing a recurrence for the value of an\\noptimal solution. Let’s deﬁne c[i, j] to be the length of an LCS of the\\nsequences Xi and Yj. If either i = 0 or j = 0, one of the sequences has\\nlength 0, and so the LCS has length 0. The optimal substructure of the\\nLCS problem gives the recursive formula\\nIn this recursive formulation, a condition in the problem restricts\\nwhich subproblems to consider. When xi = yj, you can and should\\nconsider the subproblem of ﬁnding an LCS of Xi−1 and Yj−1.\\nOtherwise, you instead consider the two subproblems of ﬁnding an LCS\\nof Xi and Yj−1 and of Xi−1 and Yj. In the previous dynamic-\\nprogramming algorithms we have examined—for rod cutting and\\nmatrix-chain multiplication—we didn’t rule out any subproblems due to', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 523}),\n",
              " Document(page_content='conditions in the problem. Finding an LCS is not the only dynamic-\\nprogramming algorithm that rules out subproblems based on conditions\\nin the problem. For example, the edit-distance problem (see Problem 14-\\n5) has this characteristic.\\nStep 3: Computing the length of an L CS\\nBased on equation (14.9), you could write an exponential-time recursive\\nalgorithm to compute the length of an LCS of two sequences. Since the\\nLCS problem has only Θ (mn) distinct subproblems (computing c[i, j] for\\n0 ≤ i ≤ m and 0 ≤ j ≤ n), dynamic programming can compute the\\nsolutions bottom up.\\nThe procedure LCS-LENGTH on the next page takes two sequences\\nX = 〈x1, x2, …, xm〉 and Y = 〈y1, y2, …, yn〉 as inputs, along with their\\nlengths. It stores the c[i, j] values in a table c[0 : m, 0 : n], and it\\ncomputes the entries in row-major order. That is, the procedure ﬁlls in\\nthe ﬁrst row of c from left to right, then the second row, and so on. The\\nprocedure also maintains the table b[1 : m, 1 : n] to help in constructing\\nan optimal solution. Intuitively, b[i, j] points to the table entry\\ncorresponding to the optimal subproblem solution chosen when\\ncomputing c[i, j]. The procedure returns the b and c tables, where c[m, n]\\ncontains the length of an LCS of X and Y. Figure 14.8 shows the tables\\nproduced by LCS-LENGTH on the sequences X = 〈A, B, C, B, D, A, B〉\\nand Y = 〈B, D, C, A, B, A〉. The running time of the procedure is Θ (mn),\\nsince each table entry takes Θ (1) time to compute.\\nLCS-LENGTH(X, Y, m, n)\\n\\xa0\\xa01let b[1 : m, 1 : n] and c[0 : m, 0 : n] be new tables\\n\\xa0\\xa02for\\xa0i = 1 to\\xa0m\\n\\xa0\\xa03c[i, 0] = 0\\n\\xa0\\xa04for\\xa0j = 0 to\\xa0n\\n\\xa0\\xa05c[0, j] = 0\\n\\xa0\\xa06for\\xa0i = 1 to\\xa0m // compute table entries in row-major order\\n\\xa0\\xa07for\\xa0j = 1 to\\xa0n\\n\\xa0\\xa08 if\\xa0xi == yj', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 524}),\n",
              " Document(page_content='\\xa0\\xa09 c[i, j] = c[i − 1, j − 1] + 1\\n10 b[i, j] = “ ↖”\\n11 elseif\\xa0c[i − 1, j] ≥ c[i, j − 1]\\n12 c[i, j] = c[i − 1, j]\\n13 b[i, j] = “ ↑ ”\\n14 else\\xa0c[i, j] = c[i, j − 1]\\n15 b[i, j] = “ ← ”\\n16return\\xa0c and b\\nPRINT-LCS(b, X, i, j)\\n\\xa0\\xa01if\\xa0i == 0 or j == 0\\n\\xa0\\xa02return // the LCS has length 0\\n\\xa0\\xa03if\\xa0b[i, j] == “ ↖”\\n\\xa0\\xa04PRINT-LCS(b, X, i − 1, j − 1)\\n\\xa0\\xa05print xi // same as yj\\n\\xa0\\xa06elseif\\xa0b[i, j] == “ ↑ ”\\n\\xa0\\xa07PRINT-LCS(b, X, i − 1, j)\\n\\xa0\\xa08else PRINT-LCS(b, X, i, j − 1)\\nStep 4: Constructing an LCS\\nWith the b table returned by LCS-LENGTH, you can quickly construct\\nan LCS of X = 〈x1, x2, …, xm〉 and Y = 〈y1, y2, …, yn〉. Begin at b[m, n]\\nand trace through the table by following the arrows. Each “ ↖”\\nencountered in an entry b[i, j] implies that xi = yj is an element of the\\nLCS that LCS-LENGTH found. This method gives you the elements of\\nthis LCS in reverse order. The recursive procedure PRINT-LCS prints\\nout an LCS of X and Y in the proper, forward order.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 525}),\n",
              " Document(page_content='Figure 14.8 The c and b tables computed by LCS-LENGTH on the sequences X = 〈A, B, C, B,\\nD, A, B〉 and Y = 〈B, D, C, A, B, A〉. The square in row i and column j contains the value of c[i,\\nj] and the appropriate arrow for the value of b[i, j]. The entry 4 in c[7, 6]—the lower right-hand\\ncorner of the table—is the length of an LCS 〈B, C, B, A〉 of X and Y. For i, j > 0, entry c[i, j]\\ndepends only on whether xi = yj and the values in entries c[i − 1, j], c[i, j − 1], and c[i − 1, j − 1],\\nwhich are computed before c[i, j]. To reconstruct the elements of an LCS, follow the b[i, j] arrows\\nfrom the lower right-hand corner, as shown by the sequence shaded blue. Each “ ↖” on the\\nshaded-blue sequence corresponds to an entry (highlighted) for which xi = yj is a member of an\\nLCS.\\nThe initial call is PRINT-LCS(b, X, m, n). For the b table in Figure\\n14.8, this procedure prints BCBA. The procedure takes O(m + n) time,\\nsince it decrements at least one of i and j in each recursive call.\\nImproving the code\\nOnce you have developed an algorithm, you will often ﬁnd that you can\\nimprove on the time or space it uses. Some changes can simplify the\\ncode and improve constant factors but otherwise yield no asymptotic\\nimprovement in performance. Others can yield substantial asymptotic\\nsavings in time and space.\\nIn the LCS algorithm, for example, you can eliminate the b table\\naltogether. Each c[i, j] entry depends on only three other c table entries:\\nc[i − 1, j − 1], c[i − 1, j], and c[i, j − 1]. Given the value of c[i, j], you can', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 526}),\n",
              " Document(page_content='determine in O(1) time which of these three values was used to compute\\nc[i, j], without inspecting table b. Thus, you can reconstruct an LCS in\\nO(m+n) time using a procedure similar to PRINT-LCS. (Exercise 14.4-2\\nasks you to give the pseudocode.) Although this method saves Θ (mn)\\nspace, the auxiliary space requirement for computing an LCS does not\\nasymptotically decrease, since the c table takes Θ (mn) space anyway.\\nYou can, however, reduce the asymptotic space requirements for\\nLCS-LENGTH, since it needs only two rows of table c at a time: the\\nrow being computed and the previous row. (In fact, as Exercise 14.4-4\\nasks you to show, you can use only slightly more than the space for one\\nrow of c to compute the length of an LCS.) This improvement works if\\nyou need only the length of an LCS. If you need to reconstruct the\\nelements of an LCS, the smaller table does not keep enough information\\nto retrace the algorithm’s steps in O(m + n) time.\\nExercises\\n14.4-1\\nDetermine an LCS of 〈1, 0, 0, 1, 0, 1, 0, 1〉 and 〈0, 1, 0, 1, 1, 0, 1, 1, 0〉.\\n14.4-2\\nGive pseudocode to reconstruct an LCS from the completed c table and\\nthe original sequences X = 〈x1, x2, …, xm〉 and Y = 〈y1, y2, …, yn〉 in\\nO(m + n) time, without using the b table.\\n14.4-3\\nGive a memoized version of LCS-LENGTH that runs in O(mn) time.\\n14.4-4\\nShow how to compute the length of an LCS using only 2 · min {m, n}\\nentries in the c table plus O(1) additional space. Then show how to do\\nthe same thing, but using min {m, n} entries plus O(1) additional space.\\n14.4-5\\nGive an O(n2)-time algorithm to ﬁnd the longest monotonically\\nincreasing subsequence of a sequence of n numbers.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 527}),\n",
              " Document(page_content='★ 14.4-6\\nGive an O(n lg n)-time algorithm to ﬁnd the longest monotonically\\nincreasing subsequence of a sequence of n numbers. (Hint: The last\\nelement of a candidate subsequence of length i is at least as large as the\\nlast element of a candidate subsequence of length i −1. Maintain\\ncandidate subsequences by linking them through the input sequence.)\\n14.5\\xa0\\xa0\\xa0\\xa0Optimal binary search trees\\nSuppose that you are designing a program to translate text from English\\nto Latvian. For each occurrence of each English word in the text, you\\nneed to look up its Latvian equivalent. You can perform these lookup\\noperations by building a binary search tree with n English words as keys\\nand their Latvian equivalents as satellite data. Because you will search\\nthe tree for each individual word in the text, you want the total time\\nspent searching to be as low as possible. You can ensure an O(lg n)\\nsearch time per occurrence by using a red-black tree or any other\\nbalanced binary search tree. Words appear with different frequencies,\\nhowever, and a frequently used word such as the can end up appearing\\nfar from the root while a rarely used word such as naumachia appears\\nnear the root. Such an organization would slow down the translation,\\nsince the number of nodes visited when searching for a key in a binary\\nsearch tree equals 1 plus the depth of the node containing the key. You\\nwant words that occur frequently in the text to be placed nearer the\\nroot.8 Moreover, some words in the text might have no Latvian\\ntranslation,9 and such words would not appear in the binary search tree\\nat all. How can you organize a binary search tree so as to minimize the\\nnumber of nodes visited in all searches, given that you know how often\\neach word occurs?\\nWhat you need is an optimal binary search tree. Formally, given a\\nsequence K = 〈k1, k2, …, kn〉 of n distinct keys such that k1 < k2 < … <\\nkn, build a binary search tree containing them. For each key ki, you are\\ngiven the probability pi that any given search is for key ki. Since some\\nsearches may be for values not in K, you also have n + 1 “dummy” keys', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 528}),\n",
              " Document(page_content='d0, d1, d2, …, dn representing those values. In particular, d0 represents\\nall values less than k1, dn represents all values greater than kn, and for i\\n= 1, 2, …, n − 1, the dummy key di represents all values between ki and\\nki+1. For each dummy key di, you have the probability qi that a search\\ncorresponds to di. Figure 14.9 shows two binary search trees for a set of\\nn = 5 keys. Each key ki is an internal node, and each dummy key di is a\\nleaf. Since every search is either successful (ﬁnding some key ki) or\\nunsuccessful (ﬁnding some dummy key di), we have\\n', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 529}),\n",
              " Document(page_content='Figure 14.9 Two binary search trees for a set of n = 5 keys with the following probabilities:\\ni 0 1 2 3 4 5\\npi0.15 0.10 0.05 0.10 0.20\\nqi0.05 0.10 0.05 0.05 0.05 0.10\\n(a) A binary search tree with expected search cost 2.80. (b) A binary search tree with expected\\nsearch cost 2.75. This tree is optimal.\\nKnowing the probabilities of searches for each key and each dummy\\nkey allows us to determine the expected cost of a search in a given\\nbinary search tree T. Let us assume that the actual cost of a search\\nequals the number of nodes examined, which is the depth of the node\\nfound by the search in T, plus 1. Then the expected cost of a search in T\\nis', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 530}),\n",
              " Document(page_content='where depthT denotes a node’s depth in the tree T. The last equation\\nfollows from equation (14.10). Figure 14.9 shows how to calculate the\\nexpected search cost node by node.\\nFor a given set of probabilities, your goal is to construct a binary\\nsearch tree whose expected search cost is smallest. We call such a tree an\\noptimal binary search tree. Figure 14.9(a) shows one binary search tree,\\nwith expected cost 2.80, for the probabilities given in the ﬁgure caption.\\nPart (b) of the ﬁgure displays an optimal binary search tree, with\\nexpected cost 2.75. This example demonstrates that an optimal binary\\nsearch tree is not necessarily a tree whose overall height is smallest. Nor\\ndoes an optimal binary search tree always have the key with the greatest\\nprobability at the root. Here, key k5 has the greatest search probability\\nof any key, yet the root of the optimal binary search tree shown is k2.\\n(The lowest expected cost of any binary search tree with k5 at the root is\\n2.85.)\\nAs with matrix-chain multiplication, exhaustive checking of all\\npossibilities fails to yield an efﬁcient algorithm. You can label the nodes\\nof any n-node binary tree with the keys k1, k2, …, kn to construct a\\nbinary search tree, and then add in the dummy keys as leaves. In\\nProblem 12-4 on page 329, we saw that the number of binary trees with\\nn nodes is Ω(4n/n3/2). Thus you would need to examine an exponential\\nnumber of binary search trees to perform an exhaustive search. We’ll see\\nhow to solve this problem more efﬁciently with dynamic programming.\\nStep 1: The structure of an optimal binary search tree\\nTo characterize the optimal substructure of optimal binary search trees,\\nwe start with an observation about subtrees. Consider any subtree of a\\nbinary search tree. It must contain keys in a contiguous range ki, …, kj,', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 531}),\n",
              " Document(page_content='for some 1 ≤ i ≤ j ≤ n. In addition, a subtree that contains keys ki, …, kj\\nmust also have as its leaves the dummy keys di−1, …, dj.\\nNow we can state the optimal substructure: if an optimal binary\\nsearch tree T has a subtree T′ containing keys ki, …, kj, then this\\nsubtree T′ must be optimal as well for the subproblem with keys ki, …,\\nkj and dummy keys di−1, …, dj. The usual cut-and-paste argument\\napplies. If there were a subtree T″ whose expected cost is lower than that\\nof T′, then cutting T′ out of T and pasting in T″ would result in a binary\\nsearch tree of lower expected cost than T, thus contradicting the\\noptimality of T.\\nWith the optimal substructure in hand, here is how to construct an\\noptimal solution to the problem from optimal solutions to subproblems.\\nGiven keys ki, …, kj, one of these keys, say kr (i ≤ r ≤ j), is the root of an\\noptimal subtree containing these keys. The left subtree of the root kr\\ncontains the keys ki, …, kr−1 (and dummy keys di−1, …, dr−1), and the\\nright subtree contains the keys kr+1, …, kj (and dummy keys dr, …, dj).\\nAs long as you examine all candidate roots kr, where i ≤ r ≤ j, and you\\ndetermine all optimal binary search trees containing ki, …, kr−1 and\\nthose containing kr+1, …, kj, you are guaranteed to ﬁnd an optimal\\nbinary search tree.\\nThere is one technical detail worth understanding about “empty”\\nsubtrees. Suppose that in a subtree with keys ki, …, kj, you select ki as\\nthe root. By the above argument, ki’s left subtree contains the keys ki,\\n…, ki−1: no keys at all. Bear in mind, however, that subtrees also\\ncontain dummy keys. We adopt the convention that a subtree\\ncontaining keys ki, …, ki−1 has no actual keys but does contain the\\nsingle dummy key di−1. Symmetrically, if you select kj as the root, then\\nkj’s right subtree contains the keys kj+1, …, kj. This right subtree\\ncontains no actual keys, but it does contain the dummy key dj.\\nStep 2: A recursive solution', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 532}),\n",
              " Document(page_content='To deﬁne the value of an optimal solution recursively, the subproblem\\ndomain is ﬁnding an optimal binary search tree containing the keys ki,\\n…, kj, where i ≥ 1, j ≤ n, and j ≥ i − 1. (When j = i − 1, there is just the\\ndummy key di−1, but no actual keys.) Let e[i, j] denote the expected cost\\nof searching an optimal binary search tree containing the keys ki, …, kj.\\nYour goal is to compute e[1, n], the expected cost of searching an\\noptimal binary search tree for all the actual and dummy keys.\\nThe easy case occurs when j = i − 1. Then the subproblem consists of\\njust the dummy key di−1. The expected search cost is e[i, i − 1] = qi−1.\\nWhen j ≥ i, you need to select a root kr from among ki, …, kj and\\nthen make an optimal binary search tree with keys ki, …, kr−1 as its left\\nsubtree and an optimal binary search tree with keys kr+1, …, kj as its\\nright subtree. What happens to the expected search cost of a subtree\\nwhen it becomes a subtree of a node? The depth of each node in the\\nsubtree increases by 1. By equation (14.11), the expected search cost of\\nthis subtree increases by the sum of all the probabilities in the subtree.\\nFor a subtree with keys ki, …, kj, denote this sum of probabilities as\\nThus, if kr is the root of an optimal subtree containing keys ki, …, kj,\\nwe have\\ne[i, j] = pr + (e[i, r − 1] + w(i, r − 1)) + (e[r + 1, j] + w(r + 1, j)).\\nNoting that\\nw(i, j) = w(i, r − 1) + pr + w(r + 1, j),\\nwe rewrite e[i, j] as\\nThe recursive equation (14.13) assumes that you know which node kr\\nto use as the root. Of course, you choose the root that gives the lowest\\nexpected search cost, giving the ﬁnal recursive formulation:', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 533}),\n",
              " Document(page_content='The e[i, j] values give the expected search costs in optimal binary\\nsearch trees. To help keep track of the structure of optimal binary\\nsearch trees, deﬁne root[i, j], for 1 ≤ i ≤ j ≤ n, to be the index r for which\\nkr is the root of an optimal binary search tree containing keys ki, …, kj.\\nAlthough we’ll see how to compute the values of root[i, j], the\\nconstruction of an optimal binary search tree from these values is left as\\nExercise 14.5-1.\\nStep 3: Computing the expected search cost of an opt imal binary search\\ntree\\nAt this point, you may have noticed some similarities between our\\ncharacterizations of optimal binary search trees and matrix-chain\\nmultiplication. For both problem domains, the subproblems consist of\\ncontiguous index subranges. A direct, recursive implementation of\\nequation (14.14) would be just as inefﬁcient as a direct, recursive matrix-\\nchain multiplication algorithm. Instead, you can store the e[i, j] values\\nin a table e[1 : n + 1, 0 : n]. The ﬁrst index needs to run to n + 1 rather\\nthan n because in order to have a subtree containing only the dummy\\nkey dn, you need to compute and store e[n + 1, n]. The second index\\nneeds to start from 0 because in order to have a subtree containing only\\nthe dummy key d0, you need to compute and store e[1, 0]. Only the\\nentries e[i, j] for which j ≥ i − 1 are ﬁlled in. The table root[i, j] records\\nthe root of the subtree containing keys ki, …, kj and uses only the\\nentries for which 1 ≤ i ≤ j ≤ n.\\nOne other table makes the dynamic-programming algorithm a little\\nfaster. Instead of computing the value of w(i, j) from scratch every time\\nyou compute e[i, j], which would take Θ (j − i) additions, store these\\nvalues in a table w[1 : n + 1, 0 : n]. For the base case, compute w[i, i − 1]\\n= qi−1 for 1 ≤ i ≤ n + 1. For j ≥ i, compute', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 534}),\n",
              " Document(page_content='Thus, you can compute the Θ (n2) values of w[i, j] in Θ (1) time each.\\nThe OPTIMAL-BST procedure on the next page takes as inputs the\\nprobabilities p1, …, pn and q0, …, qn and the size n, and it returns the\\ntables e and root. From the description above and the similarity to the\\nMATRIX-CHAIN-ORDER procedure in Section 14.2, you should ﬁnd\\nthe operation of this procedure to be fairly straightforward. The for\\nloop of lines 2–4 initializes the values of e[i, i − 1]and w[i, i − 1]. Then\\nthe for loop of lines 5–14 uses the recurrences (14.14) and (14.15) to\\ncompute e[i, j] and w[i, j] for all 1 ≤ i ≤ j ≤ n. In the ﬁrst iteration, when l\\n= 1, the loop computes e[i, i] and w[i, i] for i = 1, 2, …, n. The second\\niteration, with l = 2, computes e[i, i + 1] and w[i, i + 1] for i = 1, 2, …, n\\n− 1, and so on. The innermost for loop, in lines 10–14, tries each\\ncandidate index r to determine which key kr to use as the root of an\\noptimal binary search tree containing keys ki, …, kj. This for loop saves\\nthe current value of the index r in root[i, j] whenever it ﬁnds a better key\\nto use as the root.\\nOPTIMAL-BST(p, q, n)\\n\\xa0\\xa01let e[1 : n + 1, 0 : n], w[1 : n + 1, 0 : n],\\nand root[1 : n, 1 : n] be new tables\\n\\xa0\\xa02for\\xa0i = 1 to\\xa0n + 1 // base cases\\n\\xa0\\xa03e[i, i − 1] = qi−1 // equation (14.14)\\n\\xa0\\xa04w[i, i − 1] = qi−1\\n\\xa0\\xa05for\\xa0l = 1 to\\xa0n\\n\\xa0\\xa06for\\xa0i = 1 to\\xa0n − l + 1\\n\\xa0\\xa07 j = i + l − 1\\n\\xa0\\xa08 e[i, j] = ∞\\n\\xa0\\xa09 w[i, j] = w[i, j − 1] + pj + qj// equation (14.15)\\n10 for\\xa0r = i\\xa0to\\xa0j // try all possible roots r\\n11 t = e[i, r − 1] + e[r + 1, j] + w[i, j] // equation (14.14)\\n12 if\\xa0t < e[i, j] // new minimum?\\n13 e[i, j] = t', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 535}),\n",
              " Document(page_content='14 root[i, j] = r\\n15return\\xa0e and root\\nFigure 14.10 shows the tables e[i, j], w[i, j], and root[i, j] computed by\\nthe procedure OPTIMAL-BST on the key distribution shown in Figure\\n14.9. As in the matrix-chain multiplication example of Figure 14.5, the\\ntables are rotated to make the diagonals run horizontally. OPTIMAL-\\nBST computes the rows from bottom to top and from left to right\\nwithin each row.\\nThe OPTIMAL-BST procedure takes Θ (n3) time, just like\\nMATRIX-CHAIN-ORDER. Its running time is O(n3), since its for\\nloops are nested three deep and each loop index takes on at most n\\nvalues. The loop indices in OPTIMAL-BST do not have exactly the\\nsame bounds as those in MATRIX-CHAIN-ORDER, but they are\\nwithin at most 1 in all directions. Thus, like MATRIX-CHAIN-\\nORDER, the OPTIMAL-BST procedure takes Ω(n3) time.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 536}),\n",
              " Document(page_content='Figure 14.10 The tables e[i, j], w[i, j], and root[i, j] computed by OPTIMAL-BST on the key\\ndistribution shown in Figure 14.9. The tables are rotated so that the diagonals run horizontally.\\nExercises\\n14.5-1\\nWrite pseudocode for the procedure CONSTRUCT-OPTIMAL-\\nBST(root, n) which, given the table root[1 : n, 1 : n], outputs the\\nstructure of an optimal binary search tree. For the example in Figure\\n14.10, your procedure should print out the structure\\nk2 is the root\\nk1 is the left child of k2\\nd0 is the left child of k1\\nd1 is the right child of k1\\nk5 is the right child of k2', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 537}),\n",
              " Document(page_content='k4 is the left child of k5\\nk3 is the left child of k4\\nd2 is the left child of k3\\nd3 is the right child of k3\\nd4 is the right child of k4\\nd5 is the right child of k5\\ncorresponding to the optimal binary search tree shown in Figure\\n14.9(b).\\n14.5-2\\nDetermine the cost and structure of an optimal binary search tree for a\\nset of n = 7 keys with the following probabilities:\\ni 0 1 2 3 4 5 6 7\\npi0.04 0.06 0.08 0.02 0.10 0.12 0.14\\nqi0.06 0.06 0.06 0.06 0.05 0.05 0.05 0.05\\n14.5-3\\nSuppose that instead of maintaining the table w[i, j], you computed the\\nvalue of w(i, j) directly from equation (14.12) in line 9 of OPTIMAL-\\nBST and used this computed value in line 11. How would this change\\naffect the asymptotic running time of OPTIMAL-BST?\\n★ 14.5-4\\nKnuth [264] has shown that there are always roots of optimal subtrees\\nsuch that root[i, j − 1] ≤ root[i, j] ≤ root[i + 1, j] for all 1 ≤ i < j ≤ n. Use\\nthis fact to modify the OPTIMAL-BST procedure to run in Θ (n2) time.\\nProblems\\n14-1\\xa0\\xa0\\xa0\\xa0\\xa0L ongest simple path in a directed acyclic graph\\nYou are given a directed acyclic graph G = (V, E) with real-valued edge\\nweights and two distinguished vertices s and t. The weight of a path is\\nthe sum of the weights of the edges in the path. Describe a dynamic-', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 538}),\n",
              " Document(page_content='programming approach for ﬁnding a longest weighted simple path from\\ns to t. What is the running time of your algorithm?\\n14-2\\xa0\\xa0\\xa0\\xa0\\xa0L ongest palindrome subsequence\\nA palindrome is a nonempty string over some alphabet that reads the\\nsame forward and backward. Examples of palindromes are all strings of\\nlength 1, civic , racecar , and aibohphobia  (fear of palindromes).\\nGive an efﬁcient algorithm to ﬁnd the longest palindrome that is a\\nsubsequence of a given input string. For example, given the input\\ncharacter , your algorithm should return carac . What is the running\\ntime of your algorithm?\\n14-3\\xa0\\xa0\\xa0\\xa0\\xa0B itonic euclidean traveling-salesperson pr oblem\\nIn the euclidean traveling-salesperson problem, you are given a set of n\\npoints in the plane, and your goal is to ﬁnd the shortest closed tour that\\nconnects all n points.\\nFigure 14.11 Seven points in the plane, shown on a unit grid. (a) The shortest closed tour, with\\nlength approximately 24.89. This tour is not bitonic. (b) The shortest bitonic tour for the same\\nset of points. Its length is approximately 25.58.\\nFigure 14.11(a) shows the solution to a 7-point problem. The general\\nproblem is NP-hard, and its solution is therefore believed to require\\nmore than polynomial time (see Chapter 34).\\nJ. L. Bentley has suggested simplifying the problem by considering\\nonly bitonic tours, that is, tours that start at the leftmost point, go\\nstrictly rightward to the rightmost point, and then go strictly leftward\\nback to the starting point. Figure 14.11(b) shows the shortest bitonic', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 539}),\n",
              " Document(page_content='tour of the same 7 points. In this case, a polynomial-time algorithm is\\npossible.\\nDescribe an O(n2)-time algorithm for determining an optimal\\nbitonic tour. You may assume that no two points have the same x-\\ncoordinate and that all operations on real numbers take unit time.\\n(Hint: Scan left to right, maintaining optimal possibilities for the two\\nparts of the tour.)\\n14-4\\xa0\\xa0\\xa0\\xa0\\xa0P rinting neatly\\nConsider the problem of neatly printing a paragraph with a\\nmonospaced font (all characters having the same width). The input text\\nis a sequence of n words of lengths l1, l2, …, ln, measured in characters,\\nwhich are to be printed neatly on a number of lines that hold a\\nmaximum of M characters each. No word exceeds the line length, so\\nthat li ≤ M for i = 1, 2, …, n. The criterion of “neatness” is as follows. If\\na given line contains words i through j, where i ≤ j, and exactly one\\nspace appears between words, then the number of extra space characters\\nat the end of the line is \\n , which must be nonnegative\\nso that the words ﬁt on the line. The goal is to minimize the sum, over\\nall lines except the last, of the cubes of the numbers of extra space\\ncharacters at the ends of lines. Give a dynamic-programming algorithm\\nto print a paragraph of n words neatly. Analyze the running time and\\nspace requirements of your algorithm.\\n14-5\\xa0\\xa0\\xa0\\xa0\\xa0E dit distance\\nIn order to transform a source string of text x[1 : m] to a target string\\ny[1 : n], you can perform various transformation operations. The goal is,\\ngiven x and y, to produce a series of transformations that changes x to\\ny. An array z—assumed to be large enough to hold all the characters it\\nneeds—holds the intermediate results. Initially, z is empty, and at\\ntermination, you should have z[j] = y[j] for j = 1, 2, …, n. The procedure\\nfor solving this problem maintains current indices i into x and j into z,\\nand the operations are allowed to alter z and these indices. Initially, i = j\\n= 1. Every character in x must be examined during the transformation,', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 540}),\n",
              " Document(page_content='which means that at the end of the sequence of transformation\\noperations, i = m + 1.\\nYou may choose from among six transformation operations, each of\\nwhich has a constant cost that depends on the operation:\\nCopy a character from x to z by setting z[j] = x[i] and then incrementing\\nboth i and j. This operation examines x[i] and has cost QC.\\nReplace a character from x by another character c, by setting z[j] = c,\\nand then incrementing both i and j. This operation examines x[i] and\\nhas cost QR.\\nDelete a character from x by incrementing i but leaving j alone. This\\noperation examines x[i] and has cost QD.\\nInsert the character c into z by setting z[j] = c and then incrementing j,\\nbut leaving i alone. This operation examines no characters of x and\\nhas cost QI.\\nTwiddle (i.e., exchange) the next two characters by copying them from x\\nto z but in the opposite order: setting z[j] = x[i + 1] and z[j + 1] = x[i],\\nand then setting i = i + 2 and j = j + 2. This operation examines x[i]\\nand x[i + 1] and has cost QT.\\nKill the remainder of x by setting i = m + 1. This operation examines all\\ncharacters in x that have not yet been examined. This operation, if\\nperformed, must be the ﬁnal operation. It has cost QK.\\nFigure 14.12 gives one way to transform the source string\\nalgorithm  to the target string altruistic . Several other sequences\\nof transformation operations can transform algorithm  to\\naltruistic .\\nAssume that QC < QD + QI and QR < QD + QI, since otherwise,\\nthe copy and replace operations would not be used. The cost of a given\\nsequence of transformation operations is the sum of the costs of the\\nindividual operations in the sequence. For the sequence above, the cost\\nof transforming algorithm  to altruistic  is 3QC + QR + QD +\\n4QI + QT + QK.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 541}),\n",
              " Document(page_content='a. Given two sequences x[1 : m] and y[1 : n] and the costs of the\\ntransformation operations, the edit distance from x to y is the cost of\\nthe least expensive operation sequence that transforms x to y.\\nDescribe a dynamic-programming algorithm that ﬁnds the edit\\ndistance from x[1 : m] to y[1 : n] and prints an optimal operation\\nsequence. Analyze the running time and space requirements of your\\nalgorithm.\\nFigure 14.12 A sequence of operations that transforms the source algorithm  to the target\\nstring altruistic . The underlined characters are x[i] and z[j] after the operation.\\nThe edit-distance problem generalizes the problem of aligning two\\nDNA sequences (see, for example, Setubal and Meidanis [405, Section\\n3.2]). There are several methods for measuring the similarity of two\\nDNA sequences by aligning them. One such method to align two\\nsequences x and y consists of inserting spaces at arbitrary locations in\\nthe two sequences (including at either end) so that the resulting\\nsequences x′ and y′ have the same length but do not have a space in the\\nsame position (i.e., for no position j are both x′[j] and y′[j] a space).\\nThen we assign a “score” to each position. Position j receives a score as\\nfollows:\\n+1 if x′[j] = y′[j] and neither is a space,\\n−1 if x′[j] ≠ y′[j] and neither is a space,\\n−2 if either x′[j] or y′[j] is a space.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 542}),\n",
              " Document(page_content='The score for the alignment is the sum of the scores of the individual\\npositions. For example, given the sequences x = GATCGGCAT  and y =\\nCAATGTGAATC , one alignment is\\nG ATCG GCAT\\nCAAT GTGAATC\\n-*++*+*+-++*\\nA + under a position indicates a score of +1 for that position, a -\\nindicates a score of −1, and a * indicates a score of −2, so that this\\nalignment has a total score of 6 · 1 − 2 · 1 − 4 · 2 = −4.\\nb. Explain how to cast the problem of ﬁnding an optimal alignment as\\nan edit-distance problem using a subset of the transformation\\noperations copy, replace, delete, insert, twiddle, and kill.\\n14-6\\xa0\\xa0\\xa0\\xa0\\xa0P lanning a company party\\nProfessor Blutarsky is consulting for the president of a corporation that\\nis planning a company party. The company has a hierarchical structure,\\nthat is, the supervisor relation forms a tree rooted at the president. The\\nhuman resources department has ranked each employee with a\\nconviviality rating, which is a real number. In order to make the party\\nfun for all attendees, the president does not want both an employee and\\nhis or her immediate supervisor to attend.\\nProfessor Blutarsky is given the tree that describes the structure of\\nthe corporation, using the left-child, right-sibling representation\\ndescribed in Section 10.3. Each node of the tree holds, in addition to the\\npointers, the name of an employee and that employee’s conviviality\\nranking. Describe an algorithm to make up a guest list that maximizes\\nthe sum of the conviviality ratings of the guests. Analyze the running\\ntime of your algorithm.\\n14-7\\xa0\\xa0\\xa0\\xa0\\xa0V iterbi algorithm\\nDynamic programming on a directed graph can play a part in speech\\nrecognition. A directed graph G = (V, E) with labeled edges forms a\\nformal model of a person speaking a restricted language. Each edge (u,\\nv) ∈ E is labeled with a sound σ(u, v) from a ﬁnite set Σ  of sounds. Each', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 543}),\n",
              " Document(page_content='directed path in the graph starting from a distinguished vertex v0 ∈ V\\ncorresponds to a possible sequence of sounds produced by the model,\\nwith the label of a path being the concatenation of the labels of the\\nedges on that path.\\na. Describe an efﬁcient algorithm that, given an edge-labeled directed\\ngraph G with distinguished vertex v0 and a sequence s = 〈 σ1, σ2, …,\\nσk〉 of sounds from Σ , returns a path in G that begins at v0 and has s\\nas its label, if any such path exists. Otherwise, the algorithm should\\nreturn NO-SUCH-PATH. Analyze the running time of your\\nalgorithm. (Hint: You may ﬁnd concepts from Chapter 20 useful.)\\nNow suppose that every edge (u, v) ∈ E has an associated nonnegative\\nprobability p(u, v) of being traversed, so that the corresponding sound is\\nproduced. The sum of the probabilities of the edges leaving any vertex\\nequals 1. The probability of a path is deﬁned to be the product of the\\nprobabilities of its edges. Think of the probability of a path beginning at\\nvertex v0 as the probability that a “random walk” beginning at v0\\nfollows the speciﬁed path, where the edge leaving a vertex u is taken\\nrandomly, according to the probabilities of the available edges leaving u.\\nb. Extend your answer to part (a) so that if a path is returned, it is a\\nmost probable path starting at vertex v0 and having label s. Analyze\\nthe running time of your algorithm.\\n14-8\\xa0\\xa0\\xa0\\xa0\\xa0Image compression by seam carving\\nSuppose that you are given a color picture consisting of an m×n array\\nA[1 : m, 1 : n] of pixels, where each pixel speciﬁes a triple of red, green,\\nand blue (RGB) intensities. You want to compress this picture slightly,\\nby removing one pixel from each of the m rows, so that the whole\\npicture becomes one pixel narrower. To avoid incongruous visual effects,\\nhowever, the pixels removed in two adjacent rows must lie in either the\\nsame column or adjacent columns. In this way, the pixels removed form\\na “seam” from the top row to the bottom row, where successive pixels in\\nthe seam are adjacent vertically or diagonally.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 544}),\n",
              " Document(page_content='a. Show that the number of such possible seams grows at least\\nexponentially in m, assuming that n > 1.\\nb. Suppose now that along with each pixel A[i, j], you are given a real-\\nvalued disruption measure d[i, j], indicating how disruptive it would\\nbe to remove pixel A[i, j]. Intuitively, the lower a pixel’s disruption\\nmeasure, the more similar the pixel is to its neighbors. Deﬁne the\\ndisruption measure of a seam as the sum of the disruption measures\\nof its pixels.\\nGive an algorithm to ﬁnd a seam with the lowest disruption measure.\\nHow efﬁcient is your algorithm?\\n14-9\\xa0\\xa0\\xa0\\xa0\\xa0B reaking a string\\nA certain string-processing programming language allows you to break\\na string into two pieces. Because this operation copies the string, it costs\\nn time units to break a string of n characters into two pieces. Suppose\\nthat you want to break a string into many pieces. The order in which the\\nbreaks occur can affect the total amount of time used. For example,\\nsuppose that you want to break a 20-character string after characters 2,\\n8, and 10 (numbering the characters in ascending order from the left-\\nhand end, starting from 1). If you program the breaks to occur in left-\\nto-right order, then the ﬁrst break costs 20 time units, the second break\\ncosts 18 time units (breaking the string from characters 3 to 20 at\\ncharacter 8), and the third break costs 12 time units, totaling 50 time\\nunits. If you program the breaks to occur in right-to-left order, however,\\nthen the ﬁrst break costs 20 time units, the second break costs 10 time\\nunits, and the third break costs 8 time units, totaling 38 time units. In\\nyet another order, you could break ﬁrst at 8 (costing 20), then break the\\nleft piece at 2 (costing another 8), and ﬁnally the right piece at 10\\n(costing 12), for a total cost of 40.\\nDesign an algorithm that, given the numbers of characters after\\nwhich to break, determines a least-cost way to sequence those breaks.\\nMore formally, given an array L[1 : m] containing the break points for a\\nstring of n characters, compute the lowest cost for a sequence of breaks,\\nalong with a sequence of breaks that achieves this cost.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 545}),\n",
              " Document(page_content='14-10\\xa0\\xa0\\xa0\\xa0\\xa0P lanning an investment strategy\\nYour knowledge of algorithms helps you obtain an exciting job with a\\nhot startup, along with a $10,000 signing bonus. You decide to invest\\nthis money with the goal of maximizing your return at the end of 10\\nyears. You decide to use your investment manager, G. I. Luvcache, to\\nmanage your signing bonus. The company that Luvcache works with\\nrequires you to observe the following rules. It offers n different\\ninvestments, numbered 1 through n. In each year j, investment i provides\\na return rate of rij. In other words, if you invest d dollars in investment i\\nin year j, then at the end of year j, you have drij dollars. The return rates\\nare guaranteed, that is, you are given all the return rates for the next 10\\nyears for each investment. You make investment decisions only once per\\nyear. At the end of each year, you can leave the money made in the\\nprevious year in the same investments, or you can shift money to other\\ninvestments, by either shifting money between existing investments or\\nmoving money to a new investment. If you do not move your money\\nbetween two consecutive years, you pay a fee of f1 dollars, whereas if\\nyou switch your money, you pay a fee of f2 dollars, where f2 > f1. You\\npay the fee once per year at the end of the year, and it is the same\\namount, f2, whether you move money in and out of only one\\ninvestment, or in and out of many investments.\\na. The problem, as stated, allows you to invest your money in multiple\\ninvestments in each year. Prove that there exists an optimal investment\\nstrategy that, in each year, puts all the money into a single investment.\\n(Recall that an optimal investment strategy m aximizes the amount of\\nmoney after 10 years and is not concerned with any other objectives,\\nsuch as minimizing risk.)\\nb. Prove that the problem of planning your optimal investment strategy\\nexhibits optimal substructure.\\nc. Design an algorithm that plans your optimal investment strategy.\\nWhat is the running time of your algorithm?', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 546}),\n",
              " Document(page_content='d. Suppose that Luvcache’s company imposes the additional restriction\\nthat, at any point, you can have no more than $15, 000 i n any one\\ninvestment. Show that the problem of maximizing your income at the\\nend of 10 years no longer exhibits optimal substructure.\\n14-11\\xa0\\xa0\\xa0\\xa0\\xa0Inventory planning\\nThe Rinky Dink Company makes machines that resurface ice rinks. The\\ndemand for such products varies from month to month, and so the\\ncompany needs to develop a strategy to plan its manufacturing given\\nthe ﬂuctuating, but predictable, demand. The company wishes to design\\na plan for the next n months. For each month i, the company knows the\\ndemand di, that is, the number of machines that it will sell. Let \\nbe the total demand over the next n months. The company keeps a full-\\ntime staff who provide labor to manufacture up to m machines per\\nmonth. If the company needs to make more than m machines in a given\\nmonth, it can hire additional, part-time labor, at a cost that works out\\nto c dollars per machine. Furthermore, if the company is holding any\\nunsold machines at the end of a month, it must pay inventory costs. The\\ncompany can hold up to D machines, with the cost for holding j\\nmachines given as a function h(j) for j = 1, 2, …, D that monotonically\\nincreases with j.\\nGive an algorithm that calculates a plan for the company that\\nminimizes its costs while fulﬁlling all the demand. The running time\\nshould be polynomial in n and D.\\n14-12\\xa0\\xa0\\xa0\\xa0\\xa0Signing free-agent baseball players\\nSuppose that you are the general manager for a major-league baseball\\nteam. During the off-season, you need to sign some free-agent players\\nfor your team. The team owner has given you a budget of $X to spend\\non free agents. You are allowed to spend less than $X, but the owner\\nwill ﬁre you if you spend any more than $X.\\nYou are considering N different positions, and for each position, P\\nfree-agent players who play that position are available.10 Because you\\ndo not want to overload your roster with too many players at any\\nposition, for each position you may sign at most one free agent who', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 547}),\n",
              " Document(page_content='plays that position. (If you do not sign any players at a particular\\nposition, then you plan to stick with the players you already have at that\\nposition.)\\nTo determine how valuable a player is going to be, you decide to use\\na sabermetric statistic11 known as “WAR,” or “wins above\\nreplacement.” A player with a higher WAR is more valuable than a\\nplayer with a lower WAR. It is not necessarily more expensive to sign a\\nplayer with a higher WAR than a player with a lower WAR, because\\nfactors other than a player’s value determine how much it costs to sign\\nthem.\\nFor each available free-agent player p, you have three pieces of\\ninformation:\\nthe player’s position,\\np.cost, the amount of money it costs to sign the player, and\\np.war, the player’s WAR.\\nDevise an algorithm that maximizes the total WAR of the players\\nyou sign while spending no more than $X. You may assume that each\\nplayer signs for a multiple of $100, 000. Your algorithm should output\\nthe total WAR of the players you sign, the total amount of money you\\nspend, and a list of which players you sign. Analyze the running time\\nand space requirement of your algorithm.\\nChapter notes\\nBellman [44] began the systematic study of dynamic programming in\\n1955, publishing a book about it in 1957.  The word “programming,”\\nboth here and in linear programming, refers to using a tabular solution\\nmethod. Although optimization techniques incorporating elements of\\ndynamic programming were known earlier, Bellman provided the area\\nwith a solid mathematical basis.\\nGalil and Park [172] classify dynamic-programming algorithms\\naccording to the size of the table and the number of other table entries\\neach entry depends on. They call a dynamic-programming algorithm', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 548}),\n",
              " Document(page_content='tD/eD if its table size is O(nt) and each entry depends on O(ne) other\\nentries. For example, the matrix-chain multiplication algorithm in\\nSection 14.2 is 2D/1D, and the longest-common-subsequence algorithm\\nin Section 14.4 is 2D/0D.\\nThe MATRIX-CHAIN-ORDER algorithm on page 378 is by\\nMuraoka and Kuck [339]. Hu and Shing [230, 231] give an O(n lg n)-\\ntime algorithm for the matrix-chain multiplication problem.\\nThe O(mn)-time algorithm for the longest-common-subsequence\\nproblem appears to be a folk algorithm. Knuth [95] posed the question\\nof whether subquadratic algorithms for the LCS problem exist. Masek\\nand Paterson [316] answered this question in the afﬁrmative by giving\\nan algorithm that runs in O(mn/lg n) time, where n ≤ m and the\\nsequences are drawn from a set of bounded size. For the special case in\\nwhich no element appears more than once in an input sequence,\\nSzymanski [425] shows how to solve the problem in O((n + m) lg(n +\\nm)) time. Many of these results extend to the problem of computing\\nstring edit distances (Problem 14-5).\\nAn early paper on variable-length binary encodings by Gilbert and\\nMoore [181], which had applications to constructing optimal binary\\nsearch trees for the case in which all probabilities pi are 0, contains an\\nO(n3)-time algorithm. Aho, Hopcroft, and Ullman [5] present the\\nalgorithm from Section 14.5. Splay trees [418], which modify the tree in\\nresponse to the search queries, come within a constant factor of the\\noptimal bounds without being initialized with the frequencies. Exercise\\n14.5-4 is due to Knuth [264]. Hu and Tucker [232] devised an algorithm\\nfor the case in which all probabilities pi are 0 that uses O(n2) time and\\nO(n) space. Subsequently, Knuth [261] reduced the time to O(n lg n).\\nProblem 14-8 is due to Avidan and Shamir [30], who have posted on\\nthe web a wonderful video illustrating this image-compression\\ntechnique.\\n1 If pieces are required to be cut in order of monotonically increasing size, there are fewer ways\\nto consider. For n = 4, only 5 such ways are possible: parts (a), (b), (c), (e), and (h) in Figure', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 549}),\n",
              " Document(page_content='14.2. The number of ways is called the partition function, which is approximately equal to \\n. This quantity is less than 2n−1, but still much greater than any polynomial in n.\\nWe won’t pursue this line of inquiry further, however.\\n2 The technical term “memoization” is not a misspelling of “memorization.” The word\\n“memoization” comes from “memo,” since the technique consists of recording a value to be\\nlooked up later.\\n3 None of the three methods from Sections 4.1 and Section 4.2 can be used directly, because\\nthey apply only to square matrices.\\n4 The \\n term counts all pairs in which i < j. Because i and j may be equal, we need to add in\\nthe n term.\\n5 We use the term “unweighted” to distinguish this problem from that of ﬁnding shortest paths\\nwith weighted edges, which we shall see in Chapters 22 and 23. You can use the breadth-ﬁrst\\nsearch technique of Chapter 20 to solve the unweighted problem.\\n6 It may seem strange that dynamic programming relies on subproblems being both independent\\nand overlapping. Although these requirements may sound contradictory, they describe two\\ndifferent notions, rather than two points on the same axis. Two subproblems of the same\\nproblem are independent if they do not share resources. Two subproblems are overlapping if\\nthey are really the same subproblem that occurs as a subproblem of different problems.\\n7 This approach presupposes that you know the set of all possible subproblem parameters and\\nthat you have established the relationship between table positions and subproblems. Another,\\nmore general, approach is to memoize by using hashing with the subproblem parameters as\\nkeys.\\n8 If the subject of the text is ancient Rome, you might want naumachia to appear near the root.\\n9 Yes, naumachia has a Latvian counterpart: noma č ija.\\n10 Although there are nine positions on a baseball team, N is not necessarily equal to 9 because\\nsome general managers have particular ways of thinking about positions. For example, a general\\nmanager might consider right-handed pitchers and left-handed pitchers to be separate\\n“positions,” as well as starting pitchers, long relief pitchers (relief pitchers who can pitch several\\ninnings), and short relief pitchers (relief pitchers who normally pitch at most only one inning).\\n11\\xa0Sabermetrics is the application of statistical analysis to baseball records. It provides several\\nways to compare the relative values of individual players.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 550}),\n",
              " Document(page_content='15\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0Greedy Algorithms\\nAlgorithms for optimization problems typically go through a sequence\\nof steps, with a set of choices at each step. For many optimization\\nproblems, using dynamic programming to determine the best choices is\\noverkill, and simpler, more efﬁcient algorithms will do. A greedy\\nalgorithm always makes the choice that looks best at the moment. That\\nis, it makes a locally optimal choice in the hope that this choice leads to\\na globally optimal solution. This chapter explores optimization\\nproblems for which greedy algorithms provide optimal solutions. Before\\nreading this chapter, you should read about dynamic programming in\\nChapter 14, particularly Section 14.3.\\nGreedy algorithms do not always yield optimal solutions, but for\\nmany problems they do. We ﬁrst examine, in Section 15.1, a simple but\\nnontrivial problem, the activity-selection problem, for which a greedy\\nalgorithm efﬁciently computes an optimal solution. We’ll arrive at the\\ngreedy algorithm by ﬁrst considering a d ynamic-programming approach\\nand then showing that an optimal solution can result from always\\nmaking greedy choices. Section 15.2 reviews the basic elements of the\\ngreedy approach, giving a direct approach for proving greedy\\nalgorithms correct. Section 15.3 presents an important application of\\ngreedy techniques: designing data-compression (Huffman) codes.\\nFinally, Section 15.4 shows that in order to decide which blocks to\\nreplace when a miss occurs in a cache, the “furthest-in-future” strategy\\nis optimal if the sequence of block accesses is known in advance.\\nThe greedy method is quite powerful and works well for a wide range\\nof problems. Later chapters will present many algorithms that you can', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 551}),\n",
              " Document(page_content='view as applications of the greedy method, including minimum-\\nspanning-tree algorithms (Chapter 21), Dijkstra’s algorithm for shortest\\npaths from a single source (Section 22.3), and a greedy set-covering\\nheuristic (Section 35.3). Minimum-spanning-tree algorithms furnish a\\nclassic example of the greedy method. Although you can read this\\nchapter and Chapter 21 independently of each other, you might ﬁnd it\\nuseful to read them together.\\n15.1\\xa0\\xa0\\xa0\\xa0An activity-selection problem\\nOur ﬁrst example is the problem of scheduling several competing\\nactivities that require exclusive use of a common resource, with a go al of\\nselecting a maximum-size set of mutually compatible activities. Imagine\\nthat you are in charge of scheduling a conference room. You are\\npresented with a set S = {a1, a2, … , an} of n proposed activities that\\nwish to reserve the conference room, and the room can serve only one\\nactivity at a time. Each activity ai has a start time si and a ﬁnish time fi,\\nwhere 0 ≤ si < fi < ∞. If selected, activity ai takes place during the half-\\nopen time interval [si, fi). Activities ai and aj are compatible if the\\nintervals [si, fi) and [sj, fj) do not overlap. That is, ai and aj are\\ncompatible if si ≥ fj or sj ≥ fi. (Assume that if your staff needs time to\\nchange over the room from one activity to the next, the changeover time\\nis built into the intervals.) In the activity-selection problem, your goal is\\nto select a maximum-size subset of mutually compatible activities.\\nAssume that the activities are sorted in monotonically increasing order\\nof ﬁnish time:\\n(We’ll see later the advantage that this assumption provides.) For\\nexample, consider the set of activities in Figure 15.1. The subset {a3, a9,\\na11} consists of mutually compatible activities. It is not a maximum\\nsubset, however, since the subset {a1, a4, a8, a11} is larger. In fact, {a1,', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 552}),\n",
              " Document(page_content='a4, a8, a11} is a largest subset of mutually compatible activities, and\\nanother largest subset is {a2, a4, a9, a11}.\\nWe’ll see how to solve this problem, proceeding in several steps. First\\nwe’ll explore a dynamic-programming solution, in which you consider\\nseveral choices when determining which subproblems to use in an\\noptimal solution. We’ll then observe that you need to consider only one\\nchoice—the greedy choice—and that when you make the greedy choice,\\nonly one subproblem remains. Based on these observations, we’ll\\ndevelop a recursive greedy algorithm to solve the activity-selection\\nproblem. Finally, we’ll complete the process of developing a greedy\\nsolution by converting the recursive algorithm to an iterative one.\\nAlthough the steps we go through in this section are slightly more\\ninvolved than is typical when developing a greedy algorithm, they\\nillustrate the relationship between greedy algorithms and dynamic\\nprogramming.\\nFigure 15.1 A set {a1, a2, … , a11} of activities. Activity ai has start time si and ﬁnish time fi.\\nThe optimal substructure of the activity-selection pr oblem\\nLet’s verify that the activity-selection problem exhibits optimal\\nsubstructure. Denote by Sij the set of activities that start after activity ai\\nﬁnishes and that ﬁnish before activity aj starts. Suppose that you want\\nto ﬁnd a maximum set of mutually compatible activities in Sij, and\\nsuppose further that such a maximum set is Aij, which includes some\\nactivity ak. By including ak in an optimal solution, you are left with two\\nsubproblems: ﬁnding mutually compatible activities in the set Sik\\n(activities that start after activity ai ﬁnishes and that ﬁnish before\\nactivity ak starts) and ﬁnding mutually compatible activities in the set\\nSkj (activities that start after activity ak ﬁnishes and that ﬁnish before', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 553}),\n",
              " Document(page_content='activity aj starts). Let Aik = Aij ∩ Sik and Akj = Aij ∩ Skj, so that Aik\\ncontains the activities in Aij that ﬁnish before ak starts and Akj contains\\nthe activities in Aij that start after ak ﬁnishes. Thus, we have Aij = Aik ∪\\n{ak} ∪ Akj, and so the maximum-size set Aij of mutually compatible\\nactivities in Sij consists of |Aij | = |Aik| + |Akj | + 1 activities.\\nThe usual cut-and-paste argument shows that an optimal solution\\nAij must also include optimal solutions to the two subproblems for Sik\\nand Skj. If you could ﬁnd a set \\n of mutually compatible activities in\\nSkj where \\n , then you could use \\n, rather than Akj, in a\\nsolution to the subproblem for Sij. You would have constructed a set of \\n mutually compatible activities,\\nwhich contradicts the assumption that Aij is an optimal solution. A\\nsymmetric argument applies to the activities in Sik.\\nThis way of characterizing optimal substructure suggests that you\\ncan solve the activity-selection problem by dynamic programming. Let’s\\ndenote the size of an optimal solution for the set Sij by c[i, j]. Then, the\\ndynamic-programming approach gives the recurrence\\nc[i, j] = c[i, k] + c[k, j] + 1.\\nOf course, if you do not know that an optimal solution for the set Sij\\nincludes activity ak, you must examine all activities in Sij to ﬁnd which\\none to choose, so that\\nYou can then develop a recursive algorithm and memoize it, or you can\\nwork bottom-up and ﬁll in table entries as you go along. But you would\\nbe overlooking another important characteristic of the activity-selection\\nproblem that you can use to great advantage.\\nMaking the greedy choice', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 554}),\n",
              " Document(page_content='What if you could choose an activity to add to an optimal solution\\nwithout having to ﬁrst solve all the subproblems? That could save you\\nfrom having to consider all the choices inherent in recurrence (15.2). In\\nfact, for the activity-selection problem, you need to consider only one\\nchoice: the greedy choice.\\nWhat is the greedy choice for the activity-selection problem?\\nIntuition suggests that you should choose an activity that leaves the\\nresource available for as many other activities as possible. Of the\\nactivities you end up choosing, one of them must be the ﬁrst one to\\nﬁnish. Intuition says, therefore, choose the activity in S with the earliest\\nﬁnish time, since that leaves the resource available for as many of the\\nactivities that follow it as possible. (If more than one activity in S has\\nthe earliest ﬁnish time, then choose any such activity.) In other words,\\nsince the activities are sorted in monotonically increasing order by ﬁnish\\ntime, the greedy choice is activity a1. Choosing the ﬁrst activity to ﬁnish\\nis not the only way to think of making a greedy choice for this problem.\\nExercise 15.1-3 asks you to explore other possibilities.\\nOnce you make the greedy choice, you have only one remaining\\nsubproblem to solve: ﬁnding activities that start after a1 ﬁnishes. Why\\ndon’t you have to consider activities that ﬁnish before a1 starts? Because\\ns1 < f1, and because f1 is the earliest ﬁnish time of any activity, no\\nactivity can have a ﬁnish time less than or equal to s1. Thus, all\\nactivities that are compatible with activity a1 must start after a1 ﬁnishes.\\nFurthermore, we have already established that the activity-selection\\nproblem exhibits optimal substructure. Let Sk = {ai ∈ S : si ≥ fk} be the\\nset of activities that start after activity ak ﬁnishes. If you make the\\ngreedy choice of activity a1, then S1 remains as the only subproblem to\\nsolve.1 Optimal substructure says that if a1 belongs to an optimal\\nsolution, then an optimal solution to the original problem consists of\\nactivity a1 and all the activities in an optimal solution to the\\nsubproblem S1.\\nOne big question remains: Is this intuition correct? Is the greedy\\nchoice—in which you choose the ﬁrst activity to ﬁnish—always part of', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 555}),\n",
              " Document(page_content='some optimal solution? The following theorem shows that it is.\\nTheorem 15.1\\nConsider any nonempty subproblem Sk, and let am be an activity in Sk\\nwith the earliest ﬁnish time. Then am is included in some maximum-size\\nsubset of mutually compatible activities of Sk.\\nProof\\xa0\\xa0\\xa0Let Ak be a maximum-size subset of mutually compatible\\nactivities in Sk, and let aj be the activity in Ak with the earliest ﬁnish\\ntime. If aj = am, we are done, since we have shown that am belongs to\\nsome maximum-size subset of mutually compatible activities of Sk. If aj\\n≠ am, let the set \\n  be Ak but substituting am for aj.\\nThe activities in \\n are compatible, which follows because the activities\\nin Ak are compatible, aj is the ﬁrst activity in Ak to ﬁnish, and fm ≤ fj.\\nSince \\n , we conclude that \\n is a maximum-size subset of\\nmutually compatible activities of Sk, and it includes am.\\n▪\\nAlthough you might be able to solve the activity-selection problem\\nwith dynamic programming, Theorem 15.1 says that you don’t need to.\\nInstead, you can repeatedly choose the activity that ﬁnishes ﬁrst, keep\\nonly the activities compatible with this activity, and repeat until no\\nactivities remain. Moreover, because you always choose the activity with\\nthe earliest ﬁnish time, the ﬁnish times of the activities that you choose\\nmust strictly increase. You can consider each activity just once overall,\\nin monotonically increasing order of ﬁnish times.\\nAn algorithm to solve the activity-selection problem does not need\\nto work bottom-up, like a table-based dynamic-programming\\nalgorithm. Instead, it can work top-down, choosing an activity to put\\ninto the optimal solution that it constructs and then solving the\\nsubproblem of choosing activities from those that are compatible with\\nthose already chosen. Greedy algorithms typically have this top-down\\ndesign: make a choice and then solve a subproblem, rather than the\\nbottom-up technique of solving subproblems before making a c hoice.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 556}),\n",
              " Document(page_content='A recursive greedy algorithm\\nNow that you know you can bypass the dynamic-programming\\napproach and instead use a top-down, greedy algorithm, let’s see a\\nstraightforward, recursive procedure to solve the activity-selection\\nproblem. The procedure RECURSIVE-ACTIVITY-SELECTOR on\\nthe following page takes the start and ﬁnish times of the activities,\\nrepresented as arrays s and f,2 the index k that deﬁnes the subproblem\\nSk it is to solve, and the size n of the original problem. It returns a\\nmaximum-size set of mutually compatible activities in Sk. The\\nprocedure assumes that the n input activities are already ordered by\\nmonotonically increasing ﬁnish time, according to equation (15.1). If\\nnot, you can ﬁrst sort them into this order in O(n lg n) time, breaking\\nties arbitrarily. In order to start, add the ﬁctitious activity a0 with f0 =\\n0, so that subproblem S0 is the entire set of activities S. The initial call,\\nwhich solves the entire problem, is RECURSIVE-ACTIVITY-\\nSELECTOR (s, f, 0, n).\\nRECURSIVE-ACTIVITY-SELECTOR (s, f, k, n)\\n1m = k + 1\\n2while m ≤ n and s[m] < f [k]// ﬁnd the ﬁrst activity in Sk to ﬁnish\\n3 m = m + 1\\n4if m ≤ n\\n5 return {am} ∪ RECURSIVE-ACTIVITY-SELECTOR (s, f, m,\\nn)\\n6else return ∅\\nFigure 15.2 shows how the algorithm operates on the activities in\\nFigure 15.1. In a given recursive call RECURSIVE-ACTIVITY-\\nSELECTOR (s, f, k, n), the while loop of lines 2–3 looks for the ﬁrst\\nactivity in Sk to ﬁnish. The loop examines ak+1, ak+2, … , an, until it\\nﬁnds the ﬁrst activity am that is compatible with ak, which means that\\nsm ≥ fk. If the loop terminates because it ﬁnds such an activity, line 5', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 557}),\n",
              " Document(page_content='returns the union of {am} and the maximum-size subset of Sm returned\\nby the recursive call RECURSIVE-ACTIVITY-SELECTOR (s, f, m,\\nn). Alternatively, the loop may terminate because m > n, in which case\\nthe procedure has examined all activities in Sk without ﬁnding one that\\nis compatible with ak. In this case, Sk = ∅ , and so line 6 returns ∅ .\\nAssuming that the activities have already been sorted by ﬁnish times,\\nthe running time of the call RECURSIVE-ACTIVITY-SELECTOR (s,\\nf, 0, n) is Θ (n). To see why, observe that over all recursive calls, each\\nactivity is examined exactly once in the while loop test of line 2. In\\nparticular, activity ai is examined in the last call made in which k < i.\\nAn iterative greedy algorithm\\nThe recursive procedure can be converted to an iterative one because the\\nprocedure RECURSIVE-ACTIVITY-SELECTOR is almost “tail\\nrecursive” (see Problem 7-5): it ends with a recursive call to itself\\nfollowed by a union operation. It is usually a straightforward task to\\ntransform a tail-recursive procedure to an iterative form. In fact, some\\ncompilers for certain programming languages perform this task\\nautomatically.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 558}),\n",
              " Document(page_content='Figure 15.2 The operation of RECURSIVE-ACTIVITY-SELECTOR on the 11 activities from\\nFigure 15.1. Activities considered in each recursive call appear between horizontal lines. The\\nﬁctitious activity a0 ﬁnishes at time 0, and the initial call RECURSIVE-ACTIVITY-\\nSELECTOR (s, f, 0, 11), selects activity a1. In each recursive call, the activities that have already\\nbeen selected are blue, and the activity shown in tan is being considered. If the starting time of\\nan activity occurs before the ﬁnish time of the most recently added activity (the arrow between\\nthem points left), it is rejected. Otherwise (the arrow points directly up or to the right), it is\\nselected. The last recursive call, RECURSIVE-ACTIVITY-SELECTOR (s, f, 11, 11), returns\\n∅. The resulting set of selected activities is {a1, a4, a8, a11}.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 559}),\n",
              " Document(page_content='The procedure GREEDY-ACTIVITY-SELECTOR is an iterative\\nversion of the procedure RECURSIVE-ACTIVITY-SELECTOR. It,\\ntoo, assumes that the input activities are ordered by monotonically\\nincreasing ﬁnish time. It collects selected activities into a set A and\\nreturns this set when it is done.\\nGREEDY-ACTIVITY-SELECTOR (s, f, n)\\n1A = {a1}\\n2k = 1\\n3for m = 2 to n\\n4 if s[m] ≥ f [k] // is am in Sk?\\n5 A = A ∪ {am} // yes, so choose it\\n6 k = m // and continue from there\\n7return A\\nThe procedure works as follows. The variable k indexes the most\\nrecent addition to A, corresponding to the activity ak in the recursive\\nversion. Since the procedure considers the activities in order of\\nmonotonically increasing ﬁnish time, fk is always the maximum ﬁnish\\ntime of any activity in A. That is,\\nLines 1–2 select activity a1, initialize A to contain just this activity, and\\ninitialize k to index this activity. The for loop of lines 3–6 ﬁnds the\\nearliest activity in Sk to ﬁnish. The loop considers each activity am in\\nturn and adds am to A if it is compatible with all previously selected\\nactivities. Such an activity is the earliest in Sk to ﬁnish. To see whether\\nactivity am is compatible with every activity currently in A, it sufﬁces by\\nequation (15.3) to check (in line 4) that its start time sm is not earlier\\nthan the ﬁnish time fk of the activity most recently added to A. If\\nactivity am is compatible, then lines 5–6 add activity am to A and set k\\nto m. The set A returned by the call GREEDY-ACTIVITY-', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 560}),\n",
              " Document(page_content='SELECTOR (s, f) is precisely the set returned by the initial call\\nRECURSIVE-ACTIVITY-SELECTOR (s, f, 0, n).\\nLike the recursive version, GREEDY-ACTIVITY-SELECTOR\\nschedules a set of n activities in Θ (n) time, assuming that the activities\\nwere already sorted initially by their ﬁnish times.\\nExercises\\n15.1-1\\nGive a dynamic-programming algorithm for the activity-selection\\nproblem, based on recurrence (15.2). Have your algorithm compute the\\nsizes c[i, j] as deﬁned above and also produce the maximum-size subset\\nof mutually compatible activities. Assume that the inputs have been\\nsorted as in equation (15.1). Compare the running time of your solution\\nto the running time of GREEDY-ACTIVITY-SELECTOR.\\n15.1-2\\nSuppose that instead of always selecting the ﬁrst activity to ﬁnish, you\\ninstead select the last activity to start that is compatible with all\\npreviously selected activities. Describe how this approach is a greedy\\nalgorithm, and prove that it yields an optimal solution.\\n15.1-3\\nNot just any greedy approach to the activity-selection problem produces\\na maximum-size set of mutually compatible activities. Give an example\\nto show that the approach of selecting the activity of least duration\\nfrom among those that are compatible with previously selected activities\\ndoes not work. Do the same for the approaches of always selecting the\\ncompatible activity that overlaps the fewest other remaining activities\\nand always selecting the compatible remaining activity with the earliest\\nstart time.\\n15.1-4\\nYou are given a set of activities to schedule among a large number of\\nlecture halls, where any activity can take place in any lecture hall. You\\nwish to schedule all the activities using as few lecture halls as possible.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 561}),\n",
              " Document(page_content='Give an efﬁcient greedy algorithm to determine which activity should\\nuse which lecture hall.\\n(This problem is also known as the interval-graph coloring problem.\\nIt is modeled by an interval graph whose vertices are the given activities\\nand whose edges connect incompatible activities. The smallest number\\nof colors required to color every vertex so that no two adjacent vertices\\nhave the same color corresponds to ﬁnding the fewest lecture halls\\nneeded to schedule all of the given activities.)\\n15.1-5\\nConsider a modiﬁcation to the activity-selection problem in which each\\nactivity ai has, in addition to a start and ﬁnish time, a value vi. The\\nobjective is no longer to maximize the number of activities scheduled,\\nbut instead to maximize the total value of the activities scheduled. That\\nis, the goal is to choose a set A of compatible activities such that \\n is maximized. Give a polynomial-time algorithm for this\\nproblem.\\n15.2\\xa0\\xa0\\xa0\\xa0Elements of the greedy strategy\\nA greedy algorithm obtains an optimal solution to a problem by\\nmaking a sequence of choices. At each decision point, the algorithm\\nmakes the choice that seems best at the moment. This heuristic strategy\\ndoes not always produce an optimal solution, but as in the activity-\\nselection problem, sometimes it does. This section discusses some of the\\ngeneral properties of greedy methods.\\nThe process that we followed in Section 15.1 to develop a greedy\\nalgorithm was a bit more involved than is typical. It consisted of the\\nfollowing steps:\\n1. Determine the optimal substructure of the problem.\\n2. Develop a recursive solution. (For the activity-selection problem,\\nwe formulated recurrence (15.2), but bypassed developing a\\nrecursive algorithm based solely on this recurrence.)', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 562}),\n",
              " Document(page_content='3. Show that if you make the greedy choice, then only one\\nsubproblem remains.\\n4. Prove that it is always safe to make the greedy choice. (Steps 3\\nand 4 can occur in either order.)\\n5. Develop a recursive algorithm that implements the greedy\\nstrategy.\\n6. Convert the recursive algorithm to an iterative algorithm.\\nThese steps highlighted in great detail the dynamic-programming\\nunderpinnings of a greedy algorithm. For example, the ﬁrst cut at the\\nactivity-selection problem deﬁned the subproblems Sij, where both i and\\nj varied. We then found that if you always make the greedy choice, you\\ncan restrict the subproblems to be of the form Sk.\\nAn alternative approach is to fashion optimal substructure with a\\ngreedy choice in mind, so that the choice leaves just one subproblem to\\nsolve. In the activity-selection problem, start by dropping the second\\nsubscript and deﬁning subproblems of the form Sk. Then prove that a\\ngreedy choice (the ﬁrst activity am to ﬁnish in Sk), combined with an\\noptimal solution to the remaining set Sm of compatible activities, yields\\nan optimal solution to Sk. More generally, you can design greedy\\nalgorithms according to the following sequence of steps:\\n1. Cast the optimization problem as one in which you make a\\nchoice and are left with one subproblem to solve.\\n2. Prove that there is always an optimal solution to the original\\nproblem that makes the greedy choice, so that the greedy choice\\nis always safe.\\n3. Demonstrate optimal substructure by showing that, having made\\nthe greedy choice, what remains is a subproblem with the\\nproperty that if you combine an optimal solution to the\\nsubproblem with the greedy choice you have made, you arrive at\\nan optimal solution to the original problem.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 563}),\n",
              " Document(page_content='Later sections of this chapter will use this more direct process.\\nNevertheless, beneath every greedy algorithm, there is almost always a\\nmore cumbersome dynamic-programming solution.\\nHow can you tell whether a greedy algorithm will solve a particular\\noptimization problem? No way works all the time, but the greedy-choice\\nproperty and optimal substructure are the two key ingredients. If you\\ncan demonstrate that the problem has these properties, then you are well\\non the way to developing a greedy algorithm for it.\\nGreedy-choice property\\nThe ﬁrst key ingredient is the greedy-choice property: you can assemble\\na globally optimal solution by making locally optimal (greedy) choices.\\nIn other words, when you are considering which choice to make, you\\nmake the choice that looks best in the current problem, without\\nconsidering results from subproblems.\\nHere is where greedy algorithms differ from dynamic programming.\\nIn dynamic programming, you make a choice at each step, but the\\nchoice usually depends on the solutions to subproblems. Consequently,\\nyou typically solve dynamic-programming problems in a bottom-up\\nmanner, progressing from smaller subproblems to larger subproblems.\\n(Alternatively, you can solve them top down, but memoizing. Of course,\\neven though the code works top down, you still must solve the\\nsubproblems before making a choice.) In a greedy algorithm, you make\\nwhatever choice seems best at the moment and then solve the\\nsubproblem that remains. The choice made by a greedy algorithm may\\ndepend on choices so far, but it cannot depend on any future choices or\\non the solutions to subproblems. Thus, unlike dynamic programming,\\nwhich solves the subproblems before making the ﬁrst choice, a greedy\\nalgorithm makes its ﬁrst choice before solving any subproblems. A\\ndynamic-programming algorithm proceeds bottom up, whereas a greedy\\nstrategy usually progresses top down, making one greedy choice after\\nanother, reducing each given problem instance to a smaller one.\\nOf course, you need to prove that a greedy choice at each step yields\\na globally optimal solution. Typically, as in the case of Theorem 15.1,\\nthe proof examines a globally optimal solution to some subproblem. It', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 564}),\n",
              " Document(page_content='then shows how to modify the solution to substitute the greedy choice\\nfor some other choice, resulting in one similar, but smaller, subproblem.\\nYou can usually make the greedy choice more efﬁciently than when\\nyou have to consider a wider set of choices. For example, in the activity-\\nselection problem, assuming that the activities were already sorted in\\nmonotonically increasing order by ﬁnish times, each activity needed to\\nbe examined just once. By preprocessing the input or by using an\\nappropriate data structure (often a priority queue), you often can make\\ngreedy choices quickly, thus yielding an efﬁcient algorithm.\\nOptimal substructure\\nAs we saw in Chapter 14, a problem exhibits optimal substructure if an\\noptimal solution to the problem contains within it optimal solutions to\\nsubproblems. This property is a key ingredient of assessing whether\\ndynamic programming applies, and it’s also essential for greedy\\nalgorithms. As an example of optimal substructure, recall how Section\\n15.1 demonstrated that if an optimal solution to subproblem Sij\\nincludes an activity ak, then it must also contain optimal solutions to\\nthe subproblems Sik and Skj. Given this optimal substructure, we\\nargued that if you know which activity to use as ak, you can construct\\nan optimal solution to Sij by selecting ak along with all activities in\\noptimal solutions to the subproblems Sik and Skj. This observation of\\noptimal substructure gave rise to the recurrence (15.2) that describes the\\nvalue of an optimal solution.\\nYou will usually use a more direct approach regarding optimal\\nsubstructure when applying it to greedy algorithms. As mentioned\\nabove, you have the luxury of assuming that you arrived at a\\nsubproblem by having made the greedy choice in the original problem.\\nAll you really need to do is argue that an optimal solution to the\\nsubproblem, combined with the greedy choice already made, yields an\\noptimal solution to the original problem. This scheme implicitly uses\\ninduction on the subproblems to prove that making the greedy choice at\\nevery step produces an optimal solution.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 565}),\n",
              " Document(page_content='Greedy versus dynamic programming\\nBecause both the greedy and dynamic-programming strategies exploit\\noptimal substructure, you might be tempted to generate a dynamic-\\nprogramming solution to a problem when a greedy solution sufﬁces or,\\nconversely, you might mistakenly think that a greedy solution works\\nwhen in fact a dynamic-programming solution is required. To illustrate\\nthe subtle differences between the two techniques, let’s investigate two\\nvariants of a classical optimization problem.\\nThe 0-1 knapsack problem is the following. A thief robbing a store\\nwants to take the most valuable load that can be carried in a knapsack\\ncapable of carrying at most W pounds of loot. The thief can choose to\\ntake any subset of n items in the store. The ith item is worth vi dollars\\nand weighs wi pounds, where vi and wi are integers. Which items should\\nthe thief take? (We call this the 0-1 knapsack problem because for each\\nitem, the thief must either take it or leave it behind. The thief cannot\\ntake a fractional amount of an item or take an item more than once.)\\nIn the fractional knapsack problem, the setup is the same, but the\\nthief can take fractions of items, rather than having to make a binary (0-\\n1) choice for each item. You can think of an item in the 0-1 knapsack\\nproblem as being like a gold ingot and an item in the fractional\\nknapsack problem as more like gold dust.\\nBoth knapsack problems exhibit the optimal-substructure property.\\nFor the 0-1 problem, if the most valuable load weighing at most W\\npounds includes item j, then the remaining load must be the most\\nvaluable load weighing at most W − wj pounds that the thief can take\\nfrom the n − 1 original items excluding item j. For the comparable\\nfractional problem, if if the most valuable load weighing at most W\\npounds includes weight w of item j, then the remaining load must be the\\nmost valuable load weighing at most W − w pounds that the thief can\\ntake from the n − 1 original items plus wj − w pounds of item j.\\nAlthough the problems are similar, a greedy strategy works to solve\\nthe fractional knapsack problem, but not the 0-1 problem. To solve the\\nfractional problem, ﬁrst compute the value per pound vi/wi for each\\nitem. Obeying a greedy strategy, the thief begins by taking as much as', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 566}),\n",
              " Document(page_content='possible of the item with the greatest value per pound. If the supply of\\nthat item is exhausted and the thief can still carry more, then the thief\\ntakes as much as possible of the item with the next greatest value per\\npound, and so forth, until reaching the weight limit W. Thus, by sorting\\nthe items by value per pound, the greedy algorithm runs in O(n lg n)\\ntime. You are asked to prove that the fractional knapsack problem has\\nthe greedy-choice property in Exercise 15.2-1.\\nTo see that this greedy strategy does not work for the 0-1 knapsack\\nproblem, consider the problem instance illustrated in Figure 15.3(a).\\nThis example has three items and a knapsack that can hold 50 pounds.\\nItem 1 weighs 10 pounds and is worth $60. Item 2 weighs 20 pounds\\nand is worth $100. Item 3 weighs 30 pounds and is worth $120. Thus,\\nthe value per pound of item 1 is $6 per pound, which is greater than the\\nvalue per pound of either item 2 ($5 per pound) or item 3 ($4 per\\npound). The greedy strategy, therefore, would take item 1 ﬁrst. As you\\ncan see from the case analysis in Figure 15.3(b), however, the optimal\\nsolution takes items 2 and 3, leaving item 1 behind. The two possible\\nsolutions that take item 1 are both suboptimal.\\nFor the comparable fractional problem, however, the greedy strategy,\\nwhich takes item 1 ﬁrst, does yield an optimal solution, as shown in\\nFigure 15.3(c). Taking item 1 doesn’t work in the 0-1 problem, because\\nthe thief is unable to ﬁll the knapsack to capacity, and the empty space\\nlowers the effective value per pound of the load. In the 0-1 problem,\\nwhen you consider whether to include an item in the knapsack, you\\nmust compare the solution to the subproblem that includes the item\\nwith the solution to the subproblem that excludes the item before you\\ncan make the choice. The problem formulated in this way gives rise to\\nmany overlapping subproblems—a hallmark of dynamic programming,\\nand indeed, as Exercise 15.2-2 asks you to show, you can use dynamic\\nprogramming to solve the 0-1 problem.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 567}),\n",
              " Document(page_content='Figure 15.3 An example showing that the greedy strategy does not work for the 0-1 knapsack\\nproblem. (a) The thief must select a subset of the three items shown whose weight must not\\nexceed 50 pounds. (b) The optimal subset includes items 2 and 3. Any solution with item 1 is\\nsuboptimal, even though item 1 has the greatest value per pound. (c) For the fractional\\nknapsack problem, taking the items in order of greatest value per pound yields an optimal\\nsolution.\\nExercises\\n15.2-1\\nProve that the fractional knapsack problem has the greedy-choice\\nproperty.\\n15.2-2\\nGive a dynamic-programming solution to the 0-1 knapsack problem\\nthat runs in O(n W) time, where n is the number of items and W is the\\nmaximum weight of items that the thief can put in the knapsack.\\n15.2-3\\nSuppose that in a 0-1 knapsack problem, the order of the items when\\nsorted by increasing weight is the same as their order when sorted by\\ndecreasing value. Give an efﬁcient algorithm to ﬁnd an optimal solution\\nto this variant of the knapsack problem, and argue that your algorithm\\nis correct.\\n15.2-4\\nProfessor Gekko has always dreamed of inline skating across North\\nDakota. The professor plans to cross the state on highway U.S. 2, which', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 568}),\n",
              " Document(page_content='runs from Grand Forks, on the eastern border with Minnesota, to\\nWilliston, near the western border with Montana. The professor can\\ncarry two liters of water and can skate m miles before running out of\\nwater. (Because North Dakota is relatively ﬂat, the professor does not\\nhave to worry about drinking water at a greater rate on uphill sections\\nthan on ﬂat or downhill sections.) The professor will start in Grand\\nForks with two full liters of water. The professor has an ofﬁcial North\\nDakota state map, which shows all the places along U.S. 2 to reﬁll water\\nand the distances between these locations.\\nThe professor’s goal is to minimize the number of water stops along\\nthe route across the state. Give an efﬁcient method by which the\\nprofessor can determine which water stops to make. Prove that your\\nstrategy yields an optimal solution, and give its running time.\\n15.2-5\\nDescribe an efﬁcient algorithm that, given a set {x1, x2, … , xn} of\\npoints on the real line, determines the smallest set of unit-length closed\\nintervals that contains all of the given points. Argue that your algorithm\\nis correct.\\n★ 15.2-6\\nShow how to solve the fractional knapsack problem in O(n) time.\\n15.2-7\\nYou are given two sets A and B, each containing n positive integers. You\\ncan choose to reorder each set however you like. After reordering, let ai\\nbe the ith element of set A, and let bi be the ith element of set B. You\\nthen receive a payoff of \\n . Give an algorithm that maximizes your\\npayoff. Prove that your algorithm maximizes the payoff, and state its\\nrunning time, omitting the time for reordering the sets.\\n15.3\\xa0\\xa0\\xa0\\xa0Huffman codes\\nHuffman codes compress data well: savings of 20% to 90% are typical,\\ndepending on the characteristics of the data being compressed. The data', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 569}),\n",
              " Document(page_content='arrive as a sequence of characters. Huffman’s greedy algorithm uses a\\ntable giving how often each character occurs (its frequency) to build up\\nan optimal way of representing each character as a binary string.\\nSuppose that you have a 100,000-character data ﬁle that you wish to\\nstore compactly and you know that the 6 distinct characters in the ﬁle\\noccur with the frequencies given by Figure 15.4. The character a occurs\\n45,000 times, the character b occurs 13,000 times, and so on.\\nYou have many options for how to represent such a ﬁle of\\ninformation. Here, we consider the problem of designing a binary\\ncharacter code (or code for short) in which each character is represented\\nby a unique binary string, which we call a codeword. If you use a ﬁxed-\\nlength code, you need ⌈lg n ⌉ bits to represent n ≥ 2 characters. For 6\\ncharacters, therefore, you need 3 bits: a = 000, b = 001, c = 010, d =\\n011, e = 100, and f = 101. This method requires 300,000 bits to encode\\nthe entire ﬁle. Can you do better?\\nFigure 15.4 A character-coding problem. A data ﬁle of 100,000 characters contains only the\\ncharacters a–f, with the frequencies indicated. With each character represented by a 3-bit\\ncodeword, encoding the ﬁle requires 300,000 bits. With the variable-length code shown, the\\nencoding requires only 224,000 bits.\\nA variable-length code can do considerably better than a ﬁxed-length\\ncode. The idea is simple: give frequent characters short codewords and\\ninfrequent characters long codewords. Figure 15.4 shows such a code.\\nHere, the 1-bit string 0 represents a, and the 4-bit string 1100 represents\\nf. This code requires\\n(45 · 1 + 13 · 3 + 12 · 3 + 16 · 3 + 9 · 4 + 5 · 4) · 1,000 =  224, 000 b its\\nto represent the ﬁle, a savings of approximately 25%. In fact, this is an\\noptimal character code for this ﬁle, as we shall see.\\nPreﬁx-free codes', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 570}),\n",
              " Document(page_content='We consider here only codes in which no codeword is also a preﬁx of\\nsome other codeword. Such codes are called preﬁx-free codes. Although\\nwe won’t prove it here, a preﬁx-free code can always achieve the optimal\\ndata compression among any character code, and so we suffer no loss of\\ngenerality by restricting our attention to preﬁx-free codes.\\nEncoding is always simple for any binary character code: just\\nconcatenate the codewords representing each character of the ﬁle. For\\nexample, with the variable-length preﬁx-free code of Figure 15.4, the 4-\\ncharacter ﬁle face  has the encoding 1100 · 0 · 100 · 1101 =\\n110001001101, where “·” denotes concatenation.\\nPreﬁx-free codes are desirable because they simplify decoding. Since\\nno codeword is a preﬁx of any other, the codeword that begins an\\nencoded ﬁle is unambiguous. You can simply identify the initial\\ncodeword, translate it back to the original character, and repeat the\\ndecoding process on the remainder of the encoded ﬁle. In our example,\\nthe string 100011001101 parses uniquely as 100 · 0 · 1100 · 1101, which\\ndecodes to cafe .\\nFigure 15.5 Trees corresponding to the coding schemes in Figure 15.4. Each leaf is labeled with a\\ncharacter and its frequency of occurrence. Each internal node is labeled with the sum of the\\nfrequencies of the leaves in its subtree. All frequencies are in thousands. (a) The tree\\ncorresponding to the ﬁxed-length code a = 000, b = 001, c = 010, d = 011, e = 100, f = 101. (b)\\nThe tree corresponding to the optimal preﬁx-free code a = 0, b = 101, c = 100, d = 111, e =\\n1101, f = 1100.\\nThe decoding process needs a convenient representation for the\\npreﬁx-free code so that you can easily pick off the initial codeword. A', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 571}),\n",
              " Document(page_content='binary tree whose leaves are the given characters provides one such\\nrepresentation. Interpret the binary codeword for a character as the\\nsimple path from the root to that character, where 0 means “go to the\\nleft child” and 1 means “go to the right child.” Figure 15.5 shows the\\ntrees for the two codes of our example. Note that these are not binary\\nsearch trees, since the leaves need not appear in sorted order and\\ninternal nodes do not contain character keys.\\nAn optimal code for a ﬁle is always represented by a full binary tree,\\nin which every nonleaf node has two children (see Exercise 15.3-2). The\\nﬁxed-length code in our example is not optimal since its tree, shown in\\nFigure 15.5(a), is not a full binary tree: it contains codewords beginning\\nwith 10, but none beginning with 11. Since we can now restrict our\\nattention to full binary trees, we can say that if C is the alphabet from\\nwhich the characters are drawn and all character frequencies are\\npositive, then the tree for an optimal preﬁx-free code has exactly |C |\\nleaves, one for each letter of the alphabet, and exactly |C | − 1 internal\\nnodes (see Exercise B.5-3 on page 1175) .\\nGiven a tree T corresponding to a preﬁx-free code, we can compute\\nthe number of bits required to encode a ﬁle. For each character c in the\\nalphabet C, let the attribute c.freq denote the frequency of c in the ﬁle\\nand let dT(c) denote the depth of c’s leaf in the tree. Note that dT (c) is\\nalso the length of the codeword for character c. The number of bits\\nrequired to encode a ﬁle is thus\\nwhich we deﬁne as the cost of the tree T.\\nConstructing a Huffman code\\nHuffman invented a greedy algorithm that constructs an optimal preﬁx-\\nfree code, called a Huffman code in his honor. In line with our\\nobservations in Section 15.2, its proof of correctness relies on the\\ngreedy-choice property and optimal substructure. Rather than\\ndemonstrating that these properties hold and then developing', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 572}),\n",
              " Document(page_content='pseudocode, we present the pseudocode ﬁrst. Doing so will help clarify\\nhow the algorithm makes greedy choices.\\nThe procedure HUFFMAN assumes that C is a set of n characters\\nand that each character c ∈ C is an object with an attribute c.freq giving\\nits frequency. The algorithm builds the tree T corresponding to an\\noptimal code in a bottom-up manner. It begins with a set of |C | leaves\\nand performs a sequence of |C | − 1 “merging” operations to create the\\nﬁnal tree. The algorithm uses a min-priority queue Q, keyed on the freq\\nattribute, to identify the two least-frequent objects to merge together.\\nThe result of merging two objects is a new object whose frequency is the\\nsum of the frequencies of the two objects that were merged.\\nHUFFMAN(C)\\n\\xa0\\xa01n = |C | \\xa0\\n\\xa0\\xa02Q = C \\xa0\\n\\xa0\\xa03for i = 1 to n − 1 \\xa0\\n\\xa0\\xa04allocate a new node z \\xa0\\n\\xa0\\xa05x = EXTRACT-MIN(Q)\\xa0\\n\\xa0\\xa06y = EXTRACT-MIN(Q)\\xa0\\n\\xa0\\xa07z.left = x \\xa0\\n\\xa0\\xa08z.right = y \\xa0\\n\\xa0\\xa09z.freq = x.freq + y.freq \\xa0\\n10INSERT(Q, z) \\xa0\\n11return EXTRACT-MIN(Q)// the root of the tree is the only node\\nleft\\nFor our example, Huffman’s algorithm proceeds as shown in Figure\\n15.6. Since the alphabet contains 6 letters, the initial queue size is n = 6,\\nand 5 merge steps build the tree. The ﬁnal tree represents the optimal\\npreﬁx-free code. The codeword for a letter is the sequence of edge labels\\non the simple path from the root to the letter.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 573}),\n",
              " Document(page_content='Figure 15.6 The steps of Huffman’s algorithm for the frequencies given in Figure 15.4. Each part\\nshows the contents of the queue sorted into increasing order by frequency. Each step merges the\\ntwo trees with the lowest frequencies. Leaves are shown as rectangles containing a character and\\nits frequency. Internal nodes are shown as circles containing the sum of the frequencies of their\\nchildren. An edge connecting an internal node with its children is labeled 0 if it is an edge to a\\nleft child and 1 if it is an edge to a right child. The codeword for a letter is the sequence of labels\\non the edges connecting the root to the leaf for that letter. (a) The initial set of n = 6 nodes, one\\nfor each letter. (b)–(e) Intermediate stages. (f) The ﬁnal tree.\\nThe HUFFMAN procedure works as follows. Line 2 initializes the\\nmin-priority queue Q with the characters in C. The for loop in lines 3–\\n10 repeatedly extracts the two nodes x and y of lowest frequency from\\nthe queue and replaces them in the queue with a new node z\\nrepresenting their merger. The frequency of z is computed as the sum of\\nthe frequencies of x and y in line 9. The node z has x as its left child and\\ny as its right child. (This order is arbitrary. Switching the left and right\\nchild of any node yields a different code of the same cost.) After n − 1', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 574}),\n",
              " Document(page_content='mergers, line 11 returns the one node left in the queue, which is the root\\nof the code tree.\\nThe algorithm produces the same result without the variables x and\\ny, assigning the values returned by the EXTRACT-MIN calls directly\\nto z.left and z.right in lines 7 and 8, and changing line 9 to z.freq =\\nz.left.freq+z.right.freq. We’ll use the node names x and y in the proof of\\ncorrectness, however, so we leave them in.\\nThe running time of Huffman’s algorithm depends on how the min-\\npriority queue Q is implemented. Let’s assume that it’s implemented as\\na binary min-heap (see Chapter 6). For a set C of n characters, the\\nBUILD-MIN-HEAP procedure discussed in Section 6.3 can initialize Q\\nin line 2 in O(n) time. The for loop in lines 3–10 executes exactly n − 1\\ntimes, and since each heap operation runs in O(lg n) time, the loop\\ncontributes O(n lg n) to the running time. Thus, the total running time\\nof HUFFMAN on a set of n characters is O(n lg n).\\nCorrectness of Huffman’s algorithm\\nTo prove that the greedy algorithm HUFFMAN is correct, we’ll show\\nthat the problem of determining an optimal preﬁx-free code exhibits the\\ngreedy-choice and optimal-substructure properties. The next lemma\\nshows that the greedy-choice property holds.\\nLemma 15.2 (Optimal preﬁx-free codes have the greedy-choice property)\\nLet C be an alphabet in which each character c ∈ C has frequency\\nc.freq. Let x and y be two characters in C having the lowest frequencies.\\nThen there exists an optimal preﬁx-free code for C in which the\\ncodewords for x and y have the same length and differ only in the last\\nbit.\\nProof\\xa0\\xa0\\xa0The idea of the proof is to take the tree T representing an\\narbitrary optimal preﬁx-free code and modify it to make a tree\\nrepresenting another optimal preﬁx-free code such that the characters x\\nand y appear as sibling leaves of maximum depth in the new tree. In\\nsuch a tree, the codewords for x and y have the same length and differ\\nonly in the last bit.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 575}),\n",
              " Document(page_content='Let a and b be any two characters that are sibling leaves of maximum\\ndepth in T. Without loss of generality, assume that a.freq ≤ b.freq and\\nx.freq ≤ y.freq. Since x.freq and y.freq are the two lowest leaf\\nfrequencies, in order, and a.freq and b.freq are two arbitrary frequencies,\\nin order, we have x.freq ≤ a.freq and y.freq ≤ b.freq.\\nIn the remainder of the proof, it is possible that we could have x.freq\\n= a.freq or y.freq = b.freq, but x.freq = b.freq implies that a.freq = b.freq\\n= x.freq = y.freq (see Exercise 15.3-1), and the lemma would be trivially\\ntrue. Therefore, assume that x.freq ≠ b.freq, which means that x ≠ b.\\nFigure 15.7 An illustration of the key step in the proof of Lemma 15.2. In the optimal tree T,\\nleaves a and b are two siblings of maximum depth. Leaves x and y are the two characters with\\nthe lowest frequencies. They appear in arbitrary positions in T. Assuming that x ≠ b, swapping\\nleaves a and x produces tree T′, and then swapping leaves b and y produces tree T ″. Since each\\nswap does not increase the cost, the resulting tree T ″ is also an optimal tree.\\nAs Figure 15.7 shows, imagine exchanging the positions in T of a\\nand x to produce a tree T′, and then exchanging the positions in T′ of b\\nand y to produce a tree T″ in which x and y are sibling leaves of\\nmaximum depth. (Note that if x = b but y ≠ a, then tree T ″ does not\\nhave x and y as sibling leaves of maximum depth. Because we assume\\nthat x ≠ b, this situation cannot occur.) By equation (15.4), the\\ndifference in cost between T and T′ is\\n', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 576}),\n",
              " Document(page_content='because both a.freq − x.freq and dT (a) − dT (x) are nonnegative. More\\nspeciﬁcally, a.freq − x.freq is nonnegative because x is a minimum-\\nfrequency leaf, and dT (a) − dT (x) is nonnegative because a is a leaf of\\nmaximum depth in T. Similarly, exchanging y and b does not increase\\nthe cost, and so B(T′) − B(T ″) is nonnegative. Therefore, B(T ″) ≤ B(T′)\\n≤ B(T), and since T is optimal, we have B(T) ≤ B(T ″), which implies B(T\\n″) = B(T). Thus, T ″ is an optimal tree in which x and y appear as\\nsibling leaves of maximum depth, from which the lemma follows.\\n▪\\nLemma 15.2 implies that the process of building up an optimal tree\\nby mergers can, without loss of generality, begin with the greedy choice\\nof merging together those two characters of lowest frequency. Why is\\nthis a greedy choice? We can view the cost of a single merger as being\\nthe sum of the frequencies of the two items being merged. Exercise 15.3-\\n4 shows that the total cost of the tree constructed equals the sum of the\\ncosts of its mergers. Of all possible mergers at each step, HUFFMAN\\nchooses the one that incurs the least cost.\\nThe next lemma shows that the problem of constructing optimal\\npreﬁx-free codes has the optimal-substructure property.\\nLemma 15.3 (Optimal preﬁx-free codes have the optimal-substructure\\nproperty)\\nLet C be a given alphabet with frequency c.freq deﬁned for each\\ncharacter c ∈ C. Let x and y be two characters in C with minimum\\nfrequency. Let C′ be the alphabet C with the characters x and y removed\\nand a new character z added, so that C′ = (C − {x, y}) ∪  {z}. Deﬁne\\nfreq for all characters in C′ with the same values as in C, along with\\nz.freq = x.freq + y.freq. Let T′ be any tree representing an optimal\\npreﬁx-free code for alphabet C′. Then the tree T, obtained from T′ by\\nreplacing the leaf node for z with an internal node having x and y as\\nchildren, represents an optimal preﬁx-free code for the alphabet C.\\nProof\\xa0\\xa0\\xa0We ﬁrst show how to express the cost B(T) of tree T in terms of\\nthe cost B(T′) of tree T′, by considering the component costs in\\nequation (15.4). For each character c ∈ C − {x, y}, we have that dT (c)', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 577}),\n",
              " Document(page_content='= dT ′ (c), and hence c.freq · dT (c) = c.freq · dT ′ (c). Since dT (x) = dT\\n(y) = dT ′ (z) + 1, we have\\nx.freq · dT (x) + y.freq · dT (y)=(x.freq + y.freq)(dT ′ (z) + 1)\\n=z.freq · dT ′(z)+ (x.freq + y.freq),\\nfrom which we conclude that\\nB(T) = B(T′) + x.freq + y.freq\\nor, equivalently,\\nB(T′) = B(T) − x.freq − y.freq.\\nWe now prove the lemma by contradiction. Suppose that T does not\\nrepresent an optimal preﬁx-free code for C. Then there exists an optimal\\ntree T″ such that B(T″) < B(T). Without loss of generality (by Lemma\\n15.2), T″ has x and y as siblings. Let T″′ be the tree T″ with the common\\nparent of x and y replaced by a leaf z with frequency z.freq = x.freq +\\ny.freq. Then\\nB(T ‴)=B(T″) − x.freq − y.freq\\n<B(T) − x.freq − y.freq\\n=B(T′),\\nyielding a contradiction to the assumption that T′ represents an optimal\\npreﬁx-free code for C′. Thus, T must represent an optimal preﬁx-free\\ncode for the alphabet C.\\n▪\\nTheorem 15.4\\nProcedure HUFFMAN produces an optimal preﬁx-free code.\\nProof\\xa0\\xa0\\xa0Immediate from Lemmas 15.2 and 15.3.\\nExercises\\n15.3-1', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 578}),\n",
              " Document(page_content='Explain why, in the proof of Lemma 15.2, if x.freq = b.freq, then we\\nmust have a.freq = b.freq = x.freq = y.freq.\\n15.3-2\\nProve that a non-full binary tree cannot correspond to an optimal\\npreﬁx-free code.\\n15.3-3\\nWhat is an optimal Huffman code for the following set of frequencies,\\nbased on the ﬁrst 8 Fibonacci numbers?\\na:1 b:1 c:2 d:3 e:5 f:8 g:13 h:21\\nCan you generalize your answer to ﬁnd the optimal code when the\\nfrequencies are the ﬁrst n Fibonacci numbers?\\n15.3-4\\nProve that the total cost B(T) of a full binary tree T for a code equals\\nthe sum, over all internal nodes, of the combined frequencies of the two\\nchildren of the node.\\n15.3-5\\nGiven an optimal preﬁx-free code on a set C of n characters, you wish to\\ntransmit the code itself using as few bits as possible. Show how to\\nrepresent any optimal preﬁx-free code on C using only 2n − 1 + n ⌈lg n ⌉\\nbits. (Hint: Use 2n − 1 bits to specify the structure of the tree, as\\ndiscovered by a walk of the tree.)\\n15.3-6\\nGeneralize Huffman’s algorithm to ternary codewords (i.e., codewords\\nusing the symbols 0, 1, and 2), and prove that it yields optimal ternary\\ncodes.\\n15.3-7\\nA data ﬁle contains a sequence of 8-bit characters such that all 256\\ncharacters are about equally common: the maximum character\\nfrequency is less than twice the minimum character frequency. Prove', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 579}),\n",
              " Document(page_content='that Huffman coding in this case is no more efﬁcient than using an\\nordinary 8-bit ﬁxed-length code.\\n15.3-8\\nShow that no lossless (invertible) compression scheme can guarantee\\nthat for every input ﬁle, the corresponding output ﬁle is shorter. (Hint:\\nCompare the number of possible ﬁles with the number of possible\\nencoded ﬁles.)\\n15.4\\xa0\\xa0\\xa0\\xa0Ofﬂine caching\\nComputer systems can decrease the time to access data by storing a\\nsubset of the main memory in the cache: a small but faster memory. A\\ncache organizes data into cache blocks typically comprising 32, 64, or\\n128 bytes. You can also think of main memory as a cache for disk-\\nresident data in a virtual-memory system. Here, the blocks are called\\npages, and 4096 bytes is a typical size.\\nAs a computer program executes, it makes a sequence of memory\\nrequests. Say that there are n memory requests, to data in blocks b1, b2,\\n… , bn, in that order. The blocks in the access sequence might not be\\ndistinct, and indeed, any given block is usually accessed multiple times.\\nFor example, a program that accesses four distinct blocks p, q, r, s might\\nmake a sequence of requests to blocks s, q, s, q, q, s, p, p, r, s, s, q, p, r,\\nq. The cache can hold up to some ﬁxed number k of cache blocks. It\\nstarts out empty before the ﬁrst request. Each request causes at most\\none block to enter the cache and at most one block to be evicted from\\nthe cache. Upon a request for block bi, any one of three scenarios may\\noccur:\\n1. Block bi is already in the cache, due to a previous request for the\\nsame block. The cache remains unchanged. This situation is\\nknown as a cache hit.\\n2. Block bi is not in the cache at that time, but the cache contains\\nfewer than k blocks. In this case, block bi is placed into the', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 580}),\n",
              " Document(page_content='cache, so that the cache contains one more block than it did\\nbefore the request.\\n3. Block bi is not in the cache at that time and the cache is full: it\\ncontains k blocks. Block bi is placed into the cache, but before\\nthat happens, some other block in the cache must be evicted\\nfrom the cache in order to make room.\\nThe latter two situations, in which the requested block is not already\\nin the cache, are called cache misses. The goal is to minimize the number\\nof cache misses or, equivalently, to maximize the number of cache hits,\\nover the entire sequence of n requests. A cache miss that occurs while\\nthe cache holds fewer than k blocks—that is, as the cache is ﬁrst being\\nﬁlled up—is known as a compulsory miss, since no prior decision could\\nhave kept the requested block in the cache. When a cache miss occurs\\nand the cache is full, ideally the choice of which block to evict should\\nallow for the smallest possible number of cache misses over the entire\\nsequence of future requests.\\nTypically, caching is an online problem. That is, the computer has to\\ndecide which blocks to keep in the cache without knowing the future\\nrequests. Here, however, let’s consider the ofﬂine version of this\\nproblem, in which the computer knows in advance the entire sequence\\nof n requests and the cache size k, with a goal of minimizing the total\\nnumber of cache misses.\\nTo solve this ofﬂine problem, you can use a greedy strategy called\\nfurthest-in-future, which chooses to evict the block in the cache whose\\nnext access in the request sequence comes furthest in the future.\\nIntuitively, this strategy makes sense: if you’re not going to need\\nsomething for a while, why keep it around? We’ll show that the furthest-\\nin-future strategy is indeed optimal by showing that the ofﬂine caching\\nproblem exhibits optimal substructure and that furthest-in-future has\\nthe greedy-choice property.\\nNow, you might be thinking that since the computer usually doesn’t\\nknow the sequence of requests in advance, there is no point in studying\\nthe ofﬂine problem. Actually, there is. In some situations, you do know\\nthe sequence of requests in advance. For example, if you view the main', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 581}),\n",
              " Document(page_content='memory as the cache and the full set of data as residing on disk (or a\\nsolid-state drive), there are algorithms that plan out the entire set of\\nreads and writes in advance. Furthermore, we can use the number of\\ncache misses produced by an optimal algorithm as a baseline for\\ncomparing how well online algorithms perform. We’ll do just that in\\nSection 27.3.\\nOfﬂine caching can even model real-world problems. For example,\\nconsider a scenario where you know in advance a ﬁxed schedule of n\\nevents at known locations. Events may occur at a location multiple\\ntimes, not necessarily consecutively. You are managing a group of k\\nagents, you need to ensure that you have one agent at each location\\nwhen an event occurs, and you want to minimize the number of times\\nthat agents have to move. Here, the agents are like the blocks, the events\\nare like the requests, and moving an agent is akin to a cache miss.\\nOptimal substructure of ofﬂine caching\\nTo show that the ofﬂine problem exhibits optimal substructure, let’s\\ndeﬁne the subproblem (C, i) as processing requests for blocks bi, bi+1,\\n… , bn with cache conﬁguration C at the time that the request for block\\nbi occurs, that is, C is a subset of the set of blocks such that |C | ≤ k. A\\nsolution to subproblem (C, i) is a sequence of decisions that speciﬁes\\nwhich block to evict (if any) upon each request for blocks bi, bi+1, … ,\\nbn. An optimal solution to subproblem (C, i) minimizes the number of\\ncache misses.\\nConsider an optimal solution S to subproblem (C, i), and let C′ be\\nthe contents of the cache after processing the request for block bi in\\nsolution S. Let S′ be the subsolution of S for the resulting subproblem\\n(C′, i + 1). If the request for bi results in a cache hit, then the cache\\nremains unchanged, so that C′ = C. If the request for block bi results in\\na cache miss, then the contents of the cache change, so that C′ ≠ C. We\\nclaim that in either case, S′ is an optimal solution to subproblem (C′, i +\\n1). Why? If S′ is not an optimal solution to subproblem (C′, i + 1), then\\nthere exists another solution S″ to subproblem (C′, i + 1) that makes\\nfewer cache misses than S′. Combining S″ with the decision of S at the', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 582}),\n",
              " Document(page_content='request for block bi yields another solution that makes fewer cache\\nmisses than S, which contradicts the assumption that S is an optimal\\nsolution to subproblem (C, i).\\nTo quantify a recursive solution, we need a little more notation. Let\\nRC,i be the set of all cache conﬁgurations that can immediately follow\\nconﬁguration C after processing a request for block bi. If the request\\nresults in a cache hit, then the cache remains unchanged, so that RC,i =\\n{C }. If the request for bi results in a cache miss, then there are two\\npossibilities. If the cache is not full (|C | < k), then the cache is ﬁlling up\\nand the only choice is to insert bi into the cache, so that RC,i= {C ∪\\n{bi}}. If the cache is full (|C | = k) upon a cache miss, then RC,i\\ncontains k potential conﬁgurations: one for each candidate block in C\\nthat could be evicted and replaced by block bi. In this case, RC,i = {(C\\n− {x}) ∪ {bi} : x ∈ C }. For example, if C = {p, q, r}, k = 3, and block s\\nis requested, then RC,i = {{p, q, s},{p, r, s},{q, r, s}}.\\nLet miss(C, i) denote the minimum number of cache misses in a\\nsolution for subproblem (C, i). Here is a recurrence for miss(C, i):\\nGreedy-choice property\\nTo prove that the furthest-in-future strategy yields an optimal solution,\\nwe need to show that optimal ofﬂine caching exhibits the greedy-choice\\nproperty. Combined with the optimal-substructure property, the greedy-\\nchoice property will prove that furthest-in-future produces the\\nminimum possible number of cache misses.\\nTheorem 15.5 (Optimal ofﬂine caching has  the greedy-choice property)\\nConsider a subproblem (C, i) when the cache C contains k blocks, so\\nthat it is full, and a cache miss occurs. When block bi is requested, let z', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 583}),\n",
              " Document(page_content='= bm be the block in C whose next access is furthest in the future. (If\\nsome block in the cache will never again be referenced, then consider\\nany such block to be block z, and add a dummy request for block z =\\nbm = bn+1.) Then evicting block z upon a request for block bi is\\nincluded in some optimal solution for the subproblem (C, i).\\nProof\\xa0\\xa0\\xa0Let S be an optimal solution to subproblem (C, i). If S evicts\\nblock z upon the request for block bi, then we are done, since we have\\nshown that some optimal solution includes evicting z.\\nSo now suppose that optimal solution S evicts some other block x\\nwhen block bi is requested. We’ll construct another solution S′ to\\nsubproblem (C, i) which, upon the request for bi, evicts block z instead\\nof x and induces no more cache misses than S does, so that S′ is also\\noptimal. Because different solutions may yield different cache\\nconﬁgurations, denote by CS,j the conﬁguration of the cache under\\nsolution S just before the request for some block bj, and likewise for\\nsolution S′ and CS′,j. We’ll show how to construct S′ with the following\\nproperties:\\n1. For j = i + 1, … , m, let Dj = CS,j ∩ CS′,j. Then, |Dj | ≥ k − 1, so\\nthat the cache conﬁgurations CS,j and CS′,j differ by at most\\none block. If they differ, then CS,j = Dj ∪ {z} and CS′,j = Dj ∪\\n{y} for some block y ≠ z.\\n2. For each request of blocks bi, … , bm−1, if solution S has a\\ncache hit, then solution S′ also has a cache hit.\\n3. For all j > m, the cache conﬁgurations CS,j and CS′,j are\\nidentical.\\n4. Over the sequence of requests for blocks bi, … , bm, the number\\nof cache misses produced by solution S′ is at most the number of\\ncache misses produced by solution S.\\nWe’ll prove inductively that these properties hold for each request.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 584}),\n",
              " Document(page_content='1. We proceed by induction on j, for j = i +1, … , m. For the base\\ncase, the initial caches CS,i and CS′,i are identical. Upon the\\nrequest for block bi, solution S evicts x and solution S′ evicts z.\\nThus, cache conﬁgurations CS,i+1 and CS′,i+1 differ by just one\\nblock, CS,i+1 = Di+1 ∪ {z}, CS′,i+1 = Di+1 ∪ {x}, and x ≠ z.\\nThe inductive step deﬁnes how solution S′ behaves upon a\\nrequest for block bj for i + 1 ≤ j ≤ m − 1. The inductive\\nhypothesis is that property 1 holds when bj is requested. Because\\nz = bm is the block in CS,i whose next reference is furthest in the\\nfuture, we know that bj ≠ z. We consider several scenarios:\\nIf CS,j = CS′,j (so that |Dj | = k), then solution S′ makes\\nthe same decision upon the request for bj as S makes, so\\nthat CS,j+1 = CS′,j+1.\\nIf |Dj| = k − 1 and bj ∈ Dj, then both caches already\\ncontain block bj, and both solutions S and S′ have cache\\nhits. Therefore, CS,j+1 = CS,j and CS′,j+1 = CS′,j.\\nIf |Dj | = k − 1 and bj ∉ Dj, then because CS,j = Dj ∪ {z}\\nand bj ≠ z, solution S has a cache miss. It evicts either\\nblock z or some block w ∈ Dj.\\nIf solution S evicts block z, then CS,j+1 = Dj ∪ {bj}.\\nThere are two cases, depending on whether bj = y:\\nIf bj = y, then solution S′ has a cache hit, so\\nthat CS′,j+1 = CS′,j = Dj ∪ {bj}. Thus, CS,j+1\\n= CS′,j +1.\\nIf bj ≠ y, then solution S′ has a cache miss. It\\nevicts block y, so that CS′,j+1 = Dj ∪ {bj },\\nand again CS,j+1 = CS′,j+1.\\nIf solution S evicts some block w ∈ Dj, then CS,j+1\\n= (Dj − {w}) ∪  {bj, z}. Once again, there are two', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 585}),\n",
              " Document(page_content='cases, depending on whether bj = y:\\nIf bj = y, then solution S′ has a cache hit, so\\nthat CS′,j+1 = CS′,j = Dj ∪ {bj}. Since w ∈ Dj\\nand w was not evicted by solution S′, we have\\nw ∈ CS′,j +1. Therefore, w ∉ Dj+1 and bj ∈\\nDj+1, so that Dj+1 = (Dj − {w}) ∪ {bj }. Thus,\\nCS,j+1 = Dj+1 ∪ {z},CS′,j+1 = Dj +1 ∪ {w},\\nand because w ≠ z, property 1 holds when\\nblock bj+1 is requested. (In other words, block\\nw replaces block y in property 1.)\\nIf bj ≠ y, then solution S′ has a cache miss. It\\nevicts block w, so that CS′,j +1 = (Dj − {w}) ∪\\n{bj, y}. Therefore, we have that Dj+1 = (Dj −\\n{w}) ∪ {bj } and so CS,j+1 = Dj+1 ∪ {z} and\\nCS′,j+1 = Dj +1 ∪ {y}.\\n2. In the above discussion about maintaining property 1, solution S\\nmay have a cache hit in only the ﬁrst two cases, and solution S′\\nhas a cache hit in these cases if and only if S does.\\n3. If CS,m = CS′,m, then solution S′ makes the same decision upon\\nthe request for block z = bm as S makes, so that CS,m+1 =\\nCS′,m+1. If CS,m ≠ CS′,m, then by property 1, CS,m = Dm ∪{z}\\nand CS′,m = Dm ∪{y}, where y ≠ z. In this case, solution S has a\\ncache hit, so that CS,m+1 = CS,m = Dm ∪ {z}. Solution S′\\nevicts block y and brings in block z, so that CS′,m+1 = Dm ∪\\n{z} = CS,m+1. Thus, regardless of whether or not CS,m =\\nCS′,m, we have CS,m+1 = CS′,m+1, and starting with the\\nrequest for block bm+1, solution S′ simply makes the same\\ndecisions as S.\\n4. By property 2, upon the requests for blocks bi, … , bm−1,\\nwhenever solution S has a cache hit, so does S′. Only the request', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 586}),\n",
              " Document(page_content='for block bm = z remains to be considered. If S has a cache miss\\nupon the request for bm, then regardless of whether S′ has a\\ncache hit or a cache miss, we are done: S′ has at most the same\\nnumber of cache misses as S.\\nSo now suppose that S has a cache hit and S′ has a cache miss\\nupon the request for bm. We’ll show that there exists a request\\nfor at least one of blocks bi+1, … , bm−1 in which the request\\nresults in a cache miss for S and a cache hit for S′, thereby\\ncompensating for what happens upon the request for block bm.\\nThe proof is by contradiction. Assume that no request for blocks\\nbi+1, … , bm−1 results in a cache miss for S and a cache hit for\\nS′.\\nWe start by observing that once the caches CS,j and CS′j are\\nequal for some j > i, they remain equal thereafter. Observe also\\nthat if bm ∈ CS,m and bm ∉ CS′,m, then CS,m ≠ CS′,m.\\nTherefore, solution S cannot have evicted block z upon the\\nrequests for blocks bi, … , bm−1, for if it had, then these two\\ncache conﬁgurations would be equal. The remaining possibility\\nis that upon each of these requests, we had CS,j = Dj ∪ {z},\\nCS′,j = Dj ∪ {y} for some block y ≠ z, and solution S evicted\\nsome block w ∈ Dj. Moreover, since none of these requests\\nresulted in a cache miss for S and a cache hit for S′, the case of bj\\n= y never occurred. That is, for every request of blocks bi+1, … ,\\nbm−1, the requested block bj was never the block y ∈ CS′,j −\\nCS,j. In these cases, after processing the request, we had CS′,j +1\\n= Dj +1 ∪ {y}: the difference between the two caches did not\\nchange. Now, let’s go back to the request for block bi, where\\nafterward, we had CS′,i+1 = Di+1 ∪ {x}. Because every\\nsucceeding request until requesting block bm did not change the', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 587}),\n",
              " Document(page_content='difference between the caches, we had CS′,j = Dj ∪ {x} for j = i\\n+ 1, … , m.\\nBy deﬁnition, block z = bm is requested after block x. That\\nmeans at least one of blocks bi+1, … , bm−1 is block x. But for j\\n= i + 1, … , m, we have x ∈ CS′,j and x ∉ CS,j, so that at least\\none of these requests had a cache hit for S′ and a cache miss for\\nS, a contradiction. We conclude that if solution S has a cache hit\\nand solution S′ has a cache miss upon the request for block bm,\\nthen some earlier request had the opposite result, and so\\nsolution S′ produces no more cache misses than solution S.\\nSince S is assumed to be optimal, S′ is optimal as well.\\n▪\\nAlong with the optimal-substructure property, Theorem 15.5 tells us\\nthat the furthest-in-future strategy yields the minimum number of cache\\nmisses.\\nExercises\\n15.4-1\\nWrite pseudocode for a cache manager that uses the furthest-in-future\\nstrategy. It should take as input a set C of blocks in the cache, the\\nnumber of blocks k that the cache can hold, a sequence b1, b2, … , bn of\\nrequested blocks, and the index i into the sequence for the block bi\\nbeing requested. For each request, it should print out whether a cache\\nhit or cache miss occurs, and for each cache miss, it should also print\\nout which block, if any, is evicted.\\n15.4-2\\nReal cache managers do not know the future requests, and so they often\\nuse the past to decide which block to evict. The least-recently-used, or\\nLRU, strategy evicts the block that, of all blocks currently in the cache,\\nwas the least recently requested. (You can think of LRU as “furthest-in-\\npast.”) Give an example of a request sequence in which the LRU', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 588}),\n",
              " Document(page_content='strategy is not optimal, by showing that it induces more cache misses\\nthan the furthest-in-future strategy does on the same request sequence.\\n15.4-3\\nProfessor Croesus suggests that in the proof of Theorem 15.5, the last\\nclause in property 1 can change to CS′,j = Dj ∪ {x} or, equivalently,\\nrequire the block y given in property 1 to always be the block x evicted\\nby solution S upon the request for block bi. Show where the proof\\nbreaks down with this requirement.\\n15.4-4\\nThis section has assumed that at most one block is placed into the cache\\nwhenever a block is requested. You can imagine, however, a strategy in\\nwhich multiple blocks may enter the cache upon a single request. Show\\nthat for every solution that allows multiple blocks to enter the cache\\nupon each request, there is another solution that brings in only one\\nblock upon each request and is at least as good.\\nProblems\\n15-1\\xa0\\xa0\\xa0\\xa0\\xa0C oin changing\\nConsider the problem of making change for n cents using the smallest\\nnumber of coins. Assume that each coin’s value is an integer.\\na. Describe a greedy algorithm to make change consisting of quarters,\\ndimes, nickels, and pennies. Prove that your algorithm yields an\\noptimal solution.\\nb. Suppose that the available coins are in denominations that are powers\\nof c: the denominations are c0, c1, … , ck for some integers c > 1 and\\nk ≥ 1. Show that the greedy algorithm always yields an optimal\\nsolution.\\nc. Give a set of coin denominations for which the greedy algorithm does\\nnot yield an optimal solution. Your set should include a penny so that\\nthere is a solution for every value of n.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 589}),\n",
              " Document(page_content='d. Give an O(nk)-time algorithm that makes change for any set of k\\ndifferent coin denominations using the smallest number of coins,\\nassuming that one of the coins is a penny.\\n15-2\\xa0\\xa0\\xa0\\xa0\\xa0Scheduling to minimize average completion time\\nYou are given a set S = {a1, a2, … , an} of tasks, where task ai requires\\npi units of processing time to complete. Let Ci be the completion time of\\ntask ai, that is, the time at which task ai completes processing. Your\\ngoal is to minimize the average completion time, that is, to minimize \\n. For example, suppose that there are two tasks a1 and a2\\nwith p1 = 3 and p2 = 5, and consider the schedule in which a2 runs ﬁrst,\\nfollowed by a1. Then we have C2 = 5, C1 = 8, and the average\\ncompletion time is (5 + 8)/2 = 6.5. If task a1 runs ﬁrst, however, then we\\nhave C1 = 3, C2 = 8, and the average completion time is (3 + 8)/2 = 5.5.\\na. Give an algorithm that schedules the tasks so as to minimize the\\naverage completion time. Each task must run nonpreemptively, that is,\\nonce task ai starts, it must run continuously for pi units of time until it\\nis done. Prove that your algorithm minimizes the average completion\\ntime, and analyze the running time of your algorithm.\\nb. Suppose now that the tasks are not all available at once. That is, each\\ntask cannot start until its release time bi. Suppose also that tasks may\\nbe preempted, so that a task can be suspended and restarted at a later\\ntime. For example, a task ai with processing time pi = 6 and release\\ntime bi = 1 might start running at time 1 an d be preempted at time 4.\\nIt might then resume at time 10 but be preempted at time 11, and it\\nmight ﬁnally resume at time 13 and complete at time 15. Task ai has\\nrun for a total of 6 time units, but its running time has been divided\\ninto three pieces. Give an algorithm that schedules the tasks so as to\\nminimize the average completion time in this new scenario. Prove that\\nyour algorithm minimizes the average completion time, and analyze\\nthe running time of your algorithm.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 590}),\n",
              " Document(page_content='Chapter notes\\nMuch more material on greedy algorithms can be found in Lawler [276]\\nand Papadimitriou and Steiglitz [353]. The greedy algorithm ﬁrst\\nappeared in the combinatorial optimization literature in a 1971 article\\nby Edmonds [131].\\nThe proof of correctness of the greedy algorithm for the activity-\\nselection problem is based on that of Gavril [179].\\nHuffman codes were invented in 1952 [233]. Lelewer and Hirschberg\\n[294] surveys data-compression techniques known as of 1987.\\nThe furthest-in-future strategy was proposed by Belady [41], who\\nsuggested it for virtual-memory systems. Alternative proofs that\\nfurthest-in-future is optimal appear in articles by Lee et al. [284] and\\nVan Roy [443].\\n1 We sometimes refer to the sets Sk as subproblems rather than as just sets of activities. The\\ncontext will make it clear whether we are referring to Sk as a set of activities or as a subproblem\\nwhose input is that set.\\n2 Because the pseudocode takes s and f as arrays, it indexes into them with square brackets\\nrather than with subscripts.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 591}),\n",
              " Document(page_content='16\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0Amortized Analysis\\nImagine that you join Buff’s Gym. Buff charges a membership fee of\\n$60 per month, plus $3 for every time you use the gym. Because you are\\ndisciplined, you visit Buff’s Gym every day during the month of\\nNovember. On top of the $60 monthly charge for November, you pay\\nanother 3 × $30 = $90 that month. Although you can think of your fees\\nas a ﬂat fee of $60 and another $90 in daily fees, you can think about it\\nin another way. All together, you pay $150 o ver 30 d ays, or an average of\\n$5 per day. When you look at your fees in this way, you are amortizing\\nthe monthly fee over the 30 days of the month, spreading it out at $2 p er\\nday.\\nYou can do the same thing when you analyze running times. In an\\namortized analysis, you average the time required to perform a sequence\\nof data-structure operations over all the operations performed. With\\namortized analysis, you show that if you average over a sequence of\\noperations, then the average cost of an operation is small, even though a\\nsingle operation within the sequence might be expensive. Amortized\\nanalysis differs from average-case analysis in that probability is not\\ninvolved. An amortized analysis guarantees the average performance of\\neach operation in the worst case.\\nThe ﬁrst three sections of this chapter cover the three most common\\ntechniques used in amortized analysis. Section 16.1 starts with aggregate\\nanalysis, in which you determine an upper bound T (n) on the total cost\\nof a sequence of n operations. The average cost per operation is then T\\n(n)/n. You take the average cost as the amortized cost of each operation,\\nso that all operations have the same amortized cost.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 592}),\n",
              " Document(page_content='Section 16.2 covers the accounting method, in which you determine\\nan amortized cost of each operation. When there is more than one type\\nof operation, each type of operation may have a different amortized\\ncost. The accounting method overcharges some operations early in the\\nsequence, storing the overcharge as “prepaid credit” on speciﬁc objects\\nin the data structure. Later in the sequence, the credit pays for\\noperations that are charged less than they actually cost.\\nSection 16.3 discusses the potential method, which is like the\\naccounting method in that you determine the amortized cost of each\\noperation and may overcharge operations early on to compensate for\\nundercharges later. The potential method maintains the credit as the\\n“potential energy” of the data structure as a whole instead of\\nassociating the credit with individual objects within the data structure.\\nWe’ll use use two examples in this chapter to examine each of these\\nthree methods. One is a stack with the additional operation\\nMULTIPOP, which pops several objects at once. The other is a binary\\ncounter that counts up from 0 by means of the single operation\\nINCREMENT.\\nWhile reading this chapter, bear in mind that the charges assigned\\nduring an amortized analysis are for analysis purposes only. They need\\nnot—and should not—appear in the code. If, for example, you assign a\\ncredit to an object x when using the accounting method, you have no\\nneed to assign an appropriate amount to some attribute, such as\\nx.credit, in the code.\\nWhen you perform an amortized analysis, you often gain insight into\\na particular data structure, and this insight can help you optimize the\\ndesign. For example, Section 16.4 will use the potential method to\\nanalyze a dynamically expanding and contracting table.\\n16.1\\xa0\\xa0\\xa0\\xa0Aggregate analysis\\nIn aggregate analysis, you show that for all n, a sequence of n operations\\ntakes T (n) worst-case time in total. In the worst case, the average cost,\\nor amortized cost, per operation is therefore T (n)/n. This amortized cost\\napplies to each operation, even when there are several types of', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 593}),\n",
              " Document(page_content='operations in the sequence. The other two methods we shall study in\\nthis chapter, the accounting method and the potential method, may\\nassign different amortized costs to different types of operations.\\nStack operations\\nAs the ﬁrst example of aggregate analysis, let’s analyze stacks that have\\nbeen augmented with a new operation. Section 10.1.3 presented the two\\nfundamental stack operations, each of which takes O(1) time:\\nPUSH(S, x) pushes object x onto stack S.\\nPOP(S) pops the top of stack S and returns the popped object. Calling\\nPOP on an empty stack generates an error.\\nFigure 16.1 The action of MULTIPOP on a stack S, shown initially in (a). The top 4 objects are\\npopped by MULTIPOP(S, 4), whose result is shown in (b). The next operation is\\nMULTIPOP(S, 7), which empties the stack—shown in (c)—since fewer than 7 objects remained.\\nSince each of these operations runs in O(1) time, let us consider the cost\\nof each to be 1. The total cost of a sequence of n PUSH and POP\\noperations is therefore n, and the actual running time for n operations is\\ntherefore Θ (n).\\nNow let’s add the stack operation MULTIPOP(S, k), which removes\\nthe k top objects of stack S, popping the entire stack if the stack\\ncontains fewer than k objects. Of course, the procedure assumes that k is\\npositive, and otherwise, the MULTIPOP operation leaves the stack\\nunchanged. In the pseudocode for MULTIPOP, the operation STACK-\\nEMPTY returns TRUE if there are no objects currently on the stack,\\nand FALSE otherwise. Figure 16.1 shows an example of MULTIPOP.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 594}),\n",
              " Document(page_content='MULTIPOP(S, k)\\n1while not STACK-EMPTY(S) and k > 0\\n2 POP(S)\\n3 k = k − 1\\nWhat is the running time of MULTIPOP(S, k) on a stack of s\\nobjects? The actual running time is linear in the number of POP\\noperations actually executed, and thus we can analyze MULTIPOP in\\nterms of the abstract costs of 1 each for PUSH and POP. The number\\nof iterations of the while loop is the number min {s, k} of objects\\npopped off the stack. Each iteration of the loop makes one call to POP\\nin line 2. Thus, the total cost of MULTIPOP is min {s, k}, and the\\nactual running time is a linear function of this cost.\\nNow let’s analyze a sequence of n PUSH, POP, and MULTIPOP\\noperations on an initially empty stack. The worst-case cost of a\\nMULTIPOP operation in the sequence is O(n), since the stack size is at\\nmost n. The worst-case time of any stack operation is therefore O(n),\\nand hence a sequence of n operations costs O(n2), since the sequence\\ncontains at most n MULTIPOP operations costing O(n) each. Although\\nthis analysis is correct, the O(n2) result, which came from considering\\nthe worst-case cost of each operation individually, is not tight.\\nYes, a single MULTIPOP might be expensive, but an aggregate\\nanalysis shows that any sequence of n PUSH, POP, and MULTIPOP\\noperations on an initially empty stack has an upper bound on its cost of\\nO(n). Why? An object cannot be popped from the stack unless it was\\nﬁrst pushed. Therefore, the number of times that POP can be called on a\\nnonempty stack, including calls within MULTIPOP, is at most the\\nnumber of PUSH operations, which is at most n. For any value of n, any\\nsequence of n PUSH, POP, and MULTIPOP operations takes a total of\\nO(n) time. Averaging over the n operations gives an average cost per\\noperation of O(n)/n = O(1). Aggregate analysis assigns the amortized\\ncost of each operation to be the average cost. In this example, therefore,\\nall three stack operations have an amortized cost of O(1).', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 595}),\n",
              " Document(page_content='To recap: although the average cost, and hence the running time, of a\\nstack operation is O(1), the analysis did not rely on probabilistic\\nreasoning. Instead, the analysis yielded a worst-case bound of O(n) on a\\nsequence of n operations. Dividing this total cost by n yielded that the\\naverage cost per operation—that is, the amortized cost—is O(1).\\nIncrementing a binary counter\\nAs another example of aggregate analysis, consider the problem of\\nimplementing a k-bit binary counter that counts upward from 0. An\\narray A[0 : k − 1] of bits represents the counter. A binary number x that\\nis stored in the counter has its lowest-order bit in A[0] and its highest-\\norder bit in A[k − 1], so that \\n . Initially, x = 0, and thus\\nA[i] = 0 for i = 0, 1, … , k − 1. To add 1 (modulo 2k) to the value in the\\ncounter, call the INCREMENT procedure.\\nINCREMENT(A, k)\\n1i = 0\\n2while i < k and A[i] == 1\\n3 A[i] = 0\\n4 i = i + 1\\n5if i < k\\n6 A[i] = 1\\nFigure 16.2 shows what happens to a binary counter when\\nINCREMENT is called 16 times, starting with the initial value 0 and\\nending with the value 16. Each iteration of the while loop in lines 2–4\\nadds a 1 into position i. If A[i] = 1, then adding 1 ﬂips the bit to 0 in\\nposition i and yields a carry of 1, to be added into position i + 1 during\\nthe next iteration of the loop. Otherwise, the loop ends, and then, if i <\\nk, A[i] must be 0, so that line 6 adds a 1 into position i, ﬂipping the 0 to\\na 1. If the loop ends with i = k, then the call of INCREMENT ﬂipped\\nall k bits from 1 to 0. The cost of each INCREMENT operation is\\nlinear in the number of bits ﬂipped.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 596}),\n",
              " Document(page_content='Figure 16.2 An 8-bit binary counter as its value goes from 0 to 16 by a sequence of 16\\nINCREMENT operations. Bits that ﬂip to achieve the next value are shaded in blue. The\\nrunning cost for ﬂipping bits is shown at the right. The total cost is always less than twice the\\ntotal number of INCREMENT operations.\\nAs with the stack example, a cursory analysis yields a bound that is\\ncorrect but not tight. A single execution of INCREMENT takes Θ (k)\\ntime in the worst case, in which all the bits in array A are 1. Thus, a\\nsequence of n INCREMENT operations on an initially zero counter\\ntakes O(nk) time in the worst case.\\nAlthough a single call of INCREMENT might ﬂip all k bits, not all\\nbits ﬂip upon each call. (Note the similarity to MULTIPOP, where a\\nsingle call might pop many objects, but not every call pops many\\nobjects.) As Figure 16.2 shows, A[0] does ﬂip each time INCREMENT\\nis called. The next bit up, A[1], ﬂips only every other time: a sequence of\\nn INCREMENT operations on an initially zero counter causes A[1] to\\nﬂip ⌊n/2 ⌋ times. Similarly, bit A[2] ﬂips only every fourth time, or ⌊n/4 ⌋\\ntimes in a sequence of n INCREMENT operations. In general, for i = 0,\\n1, … , k − 1, bit A[i] ﬂips ⌊n/2i⌋ times in a sequence of n INCREMENT\\noperations on an initially zero counter. For i ≥ k, bit A[i] does not exist,\\nand so it cannot ﬂip. The total number of ﬂips in the sequence is thus', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 597}),\n",
              " Document(page_content='by equation (A.7) on page 1142. Thus, a sequence of n INCREMENT\\noperations on an initially zero counter takes O(n) time in the worst case.\\nThe average cost of each operation, and therefore the amortized cost\\nper operation, is O(n)/n = O(1).\\nExercises\\n16.1-1\\nIf the set of stack operations includes a MULTIPUSH operation, which\\npushes k items onto the stack, does the O(1) bound on the amortized\\ncost of stack operations continue to hold?\\n16.1-2\\nShow that if a DECREMENT operation is included in the k-bit counter\\nexample, n operations can cost as much as Θ (nk) time.\\n16.1-3\\nUse aggregate analysis to determine the amortized cost per operation\\nfor a sequence of n operations on a data structure in which the ith\\noperation costs i if i is an exact power of 2, and 1 otherwise.\\n16.2\\xa0\\xa0\\xa0\\xa0The accounting method\\nIn the accounting method of amortized analysis, you assign differing\\ncharges to different operations, with some operations charged more or\\nless than they actually cost. The amount that you charge an operation is\\nits amortized cost. When an operation’s amortized cost exceeds its actual\\ncost, you assign the difference to speciﬁc objects in the data structure as\\ncredit. Credit can help pay for later operations whose amortized cost is\\nless than their actual cost. Thus, you can view the amortized cost of an\\noperation as being split between its actual cost and credit that is either\\ndeposited or used up. Different operations may have different amortized', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 598}),\n",
              " Document(page_content='costs. This method differs from aggregate analysis, in which all\\noperations have the same amortized cost.\\nYou must choose the amortized costs of operations carefully. If you\\nwant to use amortized costs to show that in the worst case the average\\ncost per operation is small, you must ensure that the total amortized\\ncost of a sequence of operations provides an upper bound on the total\\nactual cost of the sequence. Moreover, as in aggregate analysis, the\\nupper bound must apply to all sequences of operations. Let’s denote the\\nactual cost of the ith operation by ci and the amortized cost of the ith\\noperation by ĉi. Then you need to have\\nfor all sequences of n operations. The total credit stored in the data\\nstructure is the difference between the total amortized cost and the total\\nactual cost, or \\n . By inequality (16.1), the total credit\\nassociated with the data structure must be nonnegative at all times. If\\nyou ever allowed the total credit to become negative (the result of\\nundercharging early operations with the promise of repaying the\\naccount later on), then the total amortized costs incurred at that time\\nwould be below the total actual costs incurred. In that case, for the\\nsequence of operations up to that time, the total amortized cost would\\nnot be an upper bound on the total actual cost. Thus, you must take\\ncare that the total credit in the data structure never becomes negative.\\nStack operations\\nTo illustrate the accounting method of amortized analysis, we return to\\nthe stack example. Recall that the actual costs of the operations were\\nPUSH 1,\\nPOP 1,\\nMULTIPOPmin {s,\\nk},', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 599}),\n",
              " Document(page_content='where k is the argument supplied to MULTIPOP and s is the stack size\\nwhen it is called. Let us assign the following am ortized costs:\\nPUSH 2,\\nPOP 0,\\nMULTIPOP0.\\nThe amortized cost of MULTIPOP is a constant (0), whereas the actual\\ncost is variable, and thus all three amortized costs are constant. In\\ngeneral, the amortized costs of the operations under consideration may\\ndiffer from each other, and they may even differ asymptotically.\\nNow let’s see how to pay for any sequence of stack operations by\\ncharging the amortized costs. Let $1 represent each unit of cost. At ﬁrst,\\nthe stack is empty. Recall the analogy of Section 10.1.3 between the\\nstack data structure and a stack of plates in a cafeteria. Upon pushing a\\nplate onto the stack, use $1 to pay the actual cost of the push, leaving a\\ncredit of $1 (out of the $2 charged). Place that $1 of credit on top of the\\nplate. At any point in time, every plate on the stack has $1 of credit on\\nit.\\nThe $1 stored on the plate serves to prepay the cost of popping the\\nplate from the stack. A POP operation incurs no charge: pay the actual\\ncost of popping a plate by taking the $1 of credit off the plate. Thus, by\\ncharging the PUSH operation a little bit more, we can view the POP\\noperation as free.\\nMoreover, the MULTIPOP operation also incurs no charge, since it’s\\njust repeated POP operations, each of which is free. If a MULTIPOP\\noperation pops k plates, then the actual cost is paid by the k dollars\\nstored on the k plates. Because each plate on the stack has $1 of credit\\non it, and the stack always has a nonnegative number of plates, the\\namount of credit is always nonnegative. Thus, for any sequence of n\\nPUSH, POP, and MULTIPOP operations, the total amortized cost is\\nan upper bound on the total actual cost. Since the total amortized cost\\nis O(n), so is the total actual cost.\\nIncrementing a binary counter', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 600}),\n",
              " Document(page_content='As another illustration of the accounting method, let’s analyze the\\nINCREMENT operation on a binary counter that starts at 0. Recall\\nthat the running time of this operation is proportional to the number of\\nbits ﬂipped, which serves as the cost for this example. Again, we’ll use\\n$1 to represent each unit of cost (the ﬂipping of a bit in this example).\\nFor the amortized analysis, the amortized cost to set a 0-bit to 1 is\\n$2. When a bit is set to 1, $1 of the $2 pays to actually set the bit. The\\nsecond $1 resides on the bit as credit to be used later if and when the bit\\nis reset to 0. At any point in time, every 1-bit in the counter has $1 of\\ncredit on it, and thus resetting a bit to 0 can be viewed as costing\\nnothing, and the $1 on the bit prepays for the reset.\\nHere is how to determine the amortized cost of INCREMENT. The\\ncost of resetting the bits to 0 within the while loop is paid for by the\\ndollars on the bits that are reset. The INCREMENT procedure sets at\\nmost one bit to 1, in line 6, and therefore the amortized cost of an\\nINCREMENT operation is at most $2. The number of 1-bits in the\\ncounter never becomes negative, and thus the amount of credit stays\\nnonnegative at all times. Thus, for n INCREMENT operations, the\\ntotal amortized cost is O(n), which bounds the total actual cost.\\nExercises\\n16.2-1\\nYou perform a sequence of PUSH and POP operations on a stack\\nwhose size never exceeds k. After every k operations, a copy of the entire\\nstack is made automatically, for backup purposes. Show that the cost of\\nn stack operations, including copying the stack, is O(n) by assigning\\nsuitable amortized costs to the various stack operations.\\n16.2-2\\nRedo Exercise 16.1-3 using an accounting method of analysis.\\n16.2-3\\nYou wish not only to increment a counter but also to reset it to 0 (i.e.,\\nmake all bits in it 0). Counting the time to examine or modify a bit as\\nΘ(1), show how to implement a counter as an array of bits so that any', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 601}),\n",
              " Document(page_content='sequence of n INCREMENT and RESET operations takes O(n) time\\non an initially zero counter. (Hint: Keep a pointer to the high-order 1.)\\n16.3\\xa0\\xa0\\xa0\\xa0The potential method\\nInstead of representing prepaid work as credit stored with speciﬁc\\nobjects in the data structure, the potential method of amortized analysis\\nrepresents the prepaid work as “potential energy,” or just “potential,”\\nwhich can be released to pay for future operations. The potential applies\\nto the data structure as a whole rather than to speciﬁc objects within the\\ndata structure.\\nThe potential method works as follows. Starting with an initial data\\nstructure D0, a sequence of n operations occurs. For each i = 1, 2, … , n,\\nlet ci be the actual cost of the ith operation and Di be the data structure\\nthat results after applying the ith operation to data structure Di−1. A\\npotential function Φ maps each data structure Di to a real number\\nΦ(Di), which is the potential associated with Di. The amortized cost ĉi of\\nthe ith operation with respect to potential function Φ  is deﬁned by\\nThe amortized cost of each operation is therefore its actual cost plus the\\nchange in potential due to the operation. By equation (16.2), the total\\namortized cost of the n operations is\\nThe second equation follows from equation (A.12) on page 1143\\nbecause the Φ (Di) terms telescope.\\nIf you can deﬁne a potential function Φ  so that Φ (Dn) ≥ Φ (D0), then\\nthe total amortized cost \\n  gives an upper bound on the total actual', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 602}),\n",
              " Document(page_content='cost \\n . In practice, you don’t always know how many operations\\nmight be performed. Therefore, if you require that Φ (Di) ≥ Φ (D0) for all\\ni, then you guarantee, as in the accounting method, that you’ve paid in\\nadvance. It’s usually simplest to just deﬁne Φ (D0) to be 0 and then show\\nthat Φ (Di) ≥ 0 for all i. (See Exercise 16.3-1 for an easy way to handle\\ncases in which Φ (D0) ≠ 0.)\\nIntuitively, if the potential difference Φ (Di) − Φ (Di−1) of the ith\\noperation is positive, then the amortized cost ĉi represents an\\novercharge to the ith operation, and the potential of the data structure\\nincreases. If the potential difference is negative, then the amortized cost\\nrepresents an undercharge to the ith operation, and the decrease in the\\npotential pays for the actual cost of the operation.\\nThe amortized costs deﬁned by equations (16.2) and (16.3) depend\\non the choice of the potential function Φ . Different potential functions\\nmay yield different amortized costs, yet still be upper bounds on the\\nactual costs. You will often ﬁnd trade-offs that you can make in\\nchoosing a potential function. The best potential function to use\\ndepends on the desired time bounds.\\nStack operations\\nTo illustrate the potential method, we return once again to the example\\nof the stack operations PUSH, POP, and MULTIPOP. We deﬁne the\\npotential function Φ  on a stack to be the number of objects in the stack.\\nThe potential of the empty initial stack D0 is Φ (D0) = 0. Since the\\nnumber of objects in the stack is never negative, the stack Di that results\\nafter the ith operation has nonnegative potential, and thus\\nΦ(Di)≥0\\n=Φ(D0).\\nThe total amortized cost of n operations with respect to Φ  therefore\\nrepresents an upper bound on the actual cost.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 603}),\n",
              " Document(page_content='Now let’s compute the amortized costs of the various stack\\noperations. If the ith operation on a stack containing s objects is a\\nPUSH operation, then the potential difference is\\nΦ(Di) − Φ (Di−1)=(s + 1) − s\\n=1.\\nBy equation (16.2), the amortized cost of this PUSH operation is\\nĉi=ci + Φ (Di) − Φ (Di−1)\\n=1 + 1\\n=2.\\nSuppose that the ith operation on the stack of s objects is\\nMULTIPOP(S, k), which causes k′ = min {s, k} objects to be popped\\noff the stack. The actual cost of the operation is k′, and the potential\\ndifference is\\nΦ(Di) − Φ (Di−1) = −k′.\\nThus, the amortized cost of the MULTIPOP operation is\\nĉi=ci + Φ (Di) − Φ (Di−1)\\n=k′ − k′\\n=0.\\nSimilarly, the amortized cost of an ordinary POP operation is 0.\\nThe amortized cost of each of the three operations is O(1), and thus\\nthe total amortized cost of a sequence of n operations is O(n). Since\\nΦ(Di) ≥ Φ (D0), the total amortized cost of n operations is an upper\\nbound on the total actual cost. The worst-case cost of n operations is\\ntherefore O(n).\\nIncrementing a binary counter\\nAs another example of the potential method, we revisit incrementing a\\nk-bit binary counter. This time, the potential of the counter after the ith', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 604}),\n",
              " Document(page_content='INCREMENT operation is deﬁned to be the number of 1-bits in the\\ncounter after the ith operation, which we’ll denote by bi.\\nHere is how to compute the amortized cost of an INCREMENT\\noperation. Suppose that the ith INCREMENT operation resets ti bits\\nto 0. The actual cost ci of the operation is therefore at most ti + 1, since\\nin addition to resetting ti bits, it sets at most one bit to 1. If bi = 0, then\\nthe ith operation had reset all k bits to 0, and so bi−1 = ti = k. If bi > 0,\\nthen bi = bi−1 −ti +1. In either case, bi ≤ bi−1 − ti + 1, and the potential\\ndifference is\\nΦ(Di) − Φ (Di−1)≤(bi−1 − ti + 1) − bi−1\\n=1 − ti.\\nThe amortized cost is therefore\\nĉi=ci + Φ (Di) − Φ (Di−1)\\n≤(ti + 1) + (1 − ti)\\n=2.\\nIf the counter starts at 0, then Φ (D0) = 0. Since Φ (Di) ≥ 0 for all i, the\\ntotal amortized cost of a sequence of n INCREMENT operations is an\\nupper bound on the total actual cost, and so the worst-case cost of n\\nINCREMENT operations is O(n).\\nThe potential method provides a simple and clever way to analyze\\nthe counter even when it does not start at 0. The counter starts with b0\\n1-bits, and after n INCREMENT operations it has bn 1-bits, where 0 ≤\\nb0, bn ≤ k. Rewrite equation (16.3) as\\nSince Φ (D0) = b0, Φ(Dn) = bn, and ĉi ≤ 2 for all 1 ≤ i ≤ n, the total\\nactual cost of n INCREMENT operations is', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 605}),\n",
              " Document(page_content='In particular, b0 ≤ k means that as long as k = O(n), the total actual cost\\nis O(n). In other words, if at least n = Ω(k) INCREMENT operations\\noccur, the total actual cost is O(n), no matter what initial value the\\ncounter contains.\\nExercises\\n16.3-1\\nSuppose you have a potential function Φ  such that Φ (Di) ≥ Φ (D0) for all\\ni, but Φ (D0) ≠ 0. Show that there exists a potential function Φ′  such that\\nΦ′(D0) = 0, Φ′ (Di) ≥ 0 for all i ≥ 1, and the amortized costs using Φ′  are\\nthe same as the amortized costs using Φ .\\n16.3-2\\nRedo Exercise 16.1-3 using a potential method of analysis.\\n16.3-3\\nConsider an ordinary binary min-heap data structure supporting the\\ninstructions INSERT and EXTRACT-MIN that, when there are n\\nitems in the heap, implements each operation in O(lg n) worst-case time.\\nGive a potential function Φ  such that the amortized cost of INSERT is\\nO(lg n) and the amortized cost of EXTRACT-MIN is O(1), and show\\nthat your potential function yields these amortized time bounds. Note\\nthat in the analysis, n is the number of items currently in the heap, and\\nyou do not know a bound on the maximum number of items that can\\never be stored in the heap.\\n16.3-4\\nWhat is the total cost of executing n of the stack operations PUSH,\\nPOP, and MULTIPOP, assuming that the stack begins with s0 objects\\nand ﬁnishes with sn objects?', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 606}),\n",
              " Document(page_content='16.3-5\\nShow how to implement a queue with two ordinary stacks (Exercise\\n10.1-7) so that the amortized cost of each ENQUEUE and each\\nDEQUEUE operation is O(1).\\n16.3-6\\nDesign a data structure to support the following two operations for a\\ndynamic multiset S of integers, which allows duplicate values:\\nINSERT(S, x) inserts x into S.\\nDELETE-LARGER-HALF(S) deletes the largest ⌈|S|/2 ⌉ elements from\\nS.\\nExplain how to implement this data structure so that any sequence of m\\nINSERT and DELETE-LARGER-HALF operations runs in O(m)\\ntime. Your implementation should also include a way to output the\\nelements of S in O(|S|) time.\\n16.4\\xa0\\xa0\\xa0\\xa0Dynamic tables\\nWhen you design an application that uses a table, you do not always\\nknow in advance how many items the table will hold. You might\\nallocate space for the table, only to ﬁnd out later that it is not enough.\\nThe program must then reallocate the table with a larger size and copy\\nall items stored in the original table over into the new, larger table.\\nSimilarly, if many items have been deleted from the table, it might be\\nworthwhile to reallocate the table with a smaller size. This section\\nstudies this problem of dynamically expanding and contracting a table.\\nAmortized analyses will show that the amortized cost of insertion and\\ndeletion is only O(1), even though the actual cost of an operation is\\nlarge when it triggers an expansion or a contraction. Moreover, you’ll\\nsee how to guarantee that the unused space in a dynamic table never\\nexceeds a constant fraction of the total space.\\nLet’s assume that the dynamic table supports the operations\\nTABLE-INSERT and TABLE-DELETE. TABLE-INSERT inserts\\ninto the table an item that occupies a single slot, that is, a space for one', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 607}),\n",
              " Document(page_content='item. Likewise, TABLE-DELETE removes an item from the table,\\nthereby freeing a slot. The details of the data-structuring method used\\nto organize the table are unimportant: it could be a stack (Section\\n10.1.3), a heap (Chapter 6), a hash table (Chapter 11), or something\\nelse.\\nIt is convenient to use a concept introduced in Section 11.2, where\\nwe analyzed hashing. The load factor α(T) of a nonempty table T is\\ndeﬁned as the number of items stored in the table divided by the size\\n(number of slots) of the table. An empty table (one with no slots) has\\nsize 0, and its load factor is deﬁned to be 1. If the load factor of a\\ndynamic table is bounded below by a constant, the unused space in the\\ntable is never more than a constant fraction of the total amount of\\nspace.\\nWe start by analyzing a dynamic table that allows only insertion and\\nthen move on to the more general case that supports both insertion and\\ndeletion.\\n16.4.1\\xa0\\xa0\\xa0\\xa0Table expansion\\nLet’s assume that storage for a table is allocated as an array of slots. A\\ntable ﬁlls up when all slots have been used or, equivalently, when its load\\nfactor is 1.1 In some software environments, upon an attempt to insert\\nan item into a full table, the only alternative is to abort with an error.\\nThe scenario in this section assumes, however, that the software\\nenvironment, like many modern ones, provides a memory-management\\nsystem that can allocate and free blocks of storage on request. Thus,\\nupon inserting an item into a full table, the system can expand the table\\nby allocating a new table with more slots than the old table had.\\nBecause the table must always reside in contiguous memory, the system\\nmust allocate a new array for the larger table and then copy items from\\nthe old table into the new table.\\nA common heuristic allocates a new table with twice as many slots as\\nthe old one. If the only table operations are insertions, then the load\\nfactor of the table is always at least 1/2, and thus the amount of wasted\\nspace never exceeds half the total space in the table.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 608}),\n",
              " Document(page_content='The TABLE-INSERT procedure on the following page assumes that\\nT is an object representing the table. The attribute T.table contains a\\npointer to the block of storage representing the table, T.num contains\\nthe number of items in the table, and T.size gives the total number of\\nslots in the table. Initially, the table is empty: T.num = T.size = 0.\\nThere are two types of insertion here: the TABLE-INSERT\\nprocedure itself and the elementary insertion into a table in lines 6 and\\n10. We can analyze the running time of TABLE-INSERT in terms of\\nthe number of elementary insertions by assigning a cost of 1 to each\\nelementary insertion. In most computing environments, the overhead\\nfor allocating an initial table in line 2 is constant and the overhead for\\nallocating and freeing storage in lines 5 and 7 is dominated by the cost\\nof transfer-ring items in line 6. Thus, the actual running time of\\nTABLE-INSERT is linear in the number of elementary insertions. An\\nexpansion occurs when lines 5–9 execute.\\nTABLE-INSERT(T, x)\\n\\xa0\\xa01if T.size == 0\\n\\xa0\\xa02allocate T.table with 1 slot\\n\\xa0\\xa03T.size = 1\\n\\xa0\\xa04if T.num == T.size\\n\\xa0\\xa05allocate new-table with 2 · T.size slots\\n\\xa0\\xa06insert all items in T.table into new-table\\n\\xa0\\xa07free T.table\\n\\xa0\\xa08T.table = new-table\\n\\xa0\\xa09T.size = 2 · T.size\\n10insert x into T.table\\n11T.num = T.num + 1\\nNow, we’ll use all three amortized analysis techniques to analyze a\\nsequence of n TABLE-INSERT operations on an initially empty table.\\nFirst, we need to determine the actual cost ci of the ith operation. If the\\ncurrent table has room for the new item (or if this is the ﬁrst operation),\\nthen ci = 1, since the only elementary insertion performed is the one in\\nline 10. If the current table is full, however, and an expansion occurs,', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 609}),\n",
              " Document(page_content='then ci = i: the cost is 1 for the elementary insertion in line 10 plus i − 1\\nfor the items copied from the old table to the new table in line 6. For n\\noperations, the worst-case cost of an operation is O(n), which leads to\\nan upper bound of O(n2) on the total running time for n operations.\\nThis bound is not tight, because the table rarely expands in the\\ncourse of n TABLE-INSERT operations. Speciﬁcally, the ith operation\\ncauses an expansion only when i − 1 is an exact power of 2. The\\namortized cost of an operation is in fact O(1), as an aggregate analysis\\nshows. The cost of the ith operation is\\nThe total cost of n TABLE-INSERT operations is therefore\\nbecause at most n operations cost 1 each and the costs of the remaining\\noperations form a geometric series. Since the total cost of n TABLE-\\nINSERT operations is bounded by 3n, the amortized cost of a single\\noperation is at most 3.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 610}),\n",
              " Document(page_content='Figure 16.3 Analysis of table expansion by the accounting method. Each call of TABLE-\\nINSERT charges $3 as follows: $1 to pay for the elementary insertion, $1 on the item inserted as\\nprepayment for it to be reinserted later, and $1 on an item that was already in the table, also as\\nprepayment for reinsertion. (a) The table immediately after an expansion, with 8 slots, 4 items\\n(tan slots), and no stored credit. (b)–(e) After each of 4 calls to TABLE-INSERT, the table has\\none more item, with $1 stored on the new item and $1 stored on one of the 4 items that were\\npresent immediately after the expansion. Slots with these new items are blue. (f) Upon the next\\ncall to TABLE-INSERT, the table is full, and so it expands again. Each item had $1 to pay for it\\nto be reinserted. Now the table looks as it did in part (a), with no stored credit but 16 slots and 8\\nitems.\\nThe accounting method can provide some intuition for why the\\namortized cost of a TABLE-INSERT operation should be 3. You can\\nthink of each item paying for three elementary insertions: inserting itself\\ninto the current table, moving itself the next time that the table expands,\\nand moving some other item that was already in the table the next time\\nthat the table expands. For example, suppose that the size of the table is\\nm immediately after an expansion, as shown in Figure 16.3 for m = 8.\\nThen the table holds m/2 items, and it contains no credit. Each call of\\nTABLE-INSERT charges $3. The elementary insertion that occurs\\nimmediately costs $1. Another $1 resides on the item inserted as credit.\\nThe third $1 resides as credit on one of the m/2 items already in the\\ntable. The table will not ﬁll again until another m/2 − 1 items have been\\ninserted, and thus, by the time the table contains m items and is full,\\neach item has $1 on it to pay for it to be reinserted it during the\\nexpansion.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 611}),\n",
              " Document(page_content='Now, let’s see how to use the potential method. We’ll use it again in\\nSection 16.4.2 to design a TABLE-DELETE operation that has an O(1)\\namortized cost as well. Just as the accounting method had no stored\\ncredit immediately after an expansion—that is, when T.num = T.size/2\\n—let’s deﬁne the potential to be 0 when T.num = T.size/2. As\\nelementary insertions occur, the potential needs to increase enough to\\npay for all the reinsertions that will happen when the table next expands.\\nThe table ﬁlls after another T.size/2 calls of TABLE-INSERT, when\\nT.num = T.size. The next call of TABLE-INSERT after these T.size/2\\ncalls triggers an expansion with a cost of T.size to reinsert all the items.\\nTherefore, over the course of T.size/2 calls of TABLE-INSERT, the\\npotential must increase from 0 to T.size. To achieve this increase, let’s\\ndesign the potential so that each call of TABLE-INSERT increases it by\\nuntil the table expands. You can see that the potential function\\nequals 0 immediately after the table expands, when T.num = T.size/2,\\nand it increases by 2 upon each insertion until the table ﬁlls. Once the\\ntable ﬁlls, that is, when T.num = T.size, the potential Φ (T) equals T.size.\\nThe initial value of the potential is 0, and since the table is always at\\nleast half full, T.num ≥ T.size/2, which implies that Φ (T) is always\\nnonnegative. Thus, the sum of the amortized costs of n TABLE-\\nINSERT operations gives an upper bound on the sum of the actual\\ncosts.\\nTo analyze the amortized costs of table operations, it is convenient to\\nthink in terms of the change in potential due to each operation. Letting\\nΦi denote the potential after the ith operation, we can rewrite equation\\n(16.2) as\\nĉi=ci + Φi − Φi−1\\n=ci + ΔΦi,', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 612}),\n",
              " Document(page_content='where ΔΦi is the change in potential due to the ith operation. First,\\nconsider the case when the ith insertion does not cause the table to\\nexpand. In this case, ΔΦi is 2. Since the actual cost ci is 1, the amortized\\ncost is\\nĉi=ci + ΔΦi\\n=1 + 2\\n=3.\\nNow, consider the change in potential when the table does expand\\nduring the ith insertion because it was full immediately before the\\ninsertion. Let numi denote the number of items stored in the table after\\nthe ith operation and sizei denote the total size of the table after the ith\\noperation, so that sizei−1 = numi−1 = i − 1 and therefore Φi−1 =\\n2(sizei−1 − sizei−1/2) = sizei−1 = i − 1. Immediately after the\\nexpansion, the potential goes down to 0, and then the new item is\\ninserted, causing the potential to increase to Φi = 2. Thus, when the ith\\ninsertion triggers an expansion, ΔΦi = 2 − (i − 1) = 3 − i. When the\\ntable expands in the ith TABLE-INSERT operation, the actual cost ci\\nequals i (to reinsert i − 1 items and insert the ith item), giving an\\namortized cost of\\nĉi=ci + ΔΦi\\n=i + (3 − i)\\n=3.\\nFigure 16.4 plots the values of numi, sizei, and Φi against i. Notice how\\nthe potential builds to pay for expanding the table.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 613}),\n",
              " Document(page_content='Figure 16.4 The effect of a sequence of n TABLE-INSERT operations on the number numi of\\nitems in the table (the brown line), the number sizei of slots in the table (the blue line), and the\\npotential Φi = 2(numi − sizei/2) (the red line), each being measured after the ith operation.\\nImmediately before an expansion, the potential has built up to the number of items in the table,\\nand therefore it can pay for moving all the items to the new table. Afterward, the potential drops\\nto 0, but it immediately increases by 2 upon insertion of the item that caused the expansion.\\n16.4.2\\xa0\\xa0\\xa0\\xa0Table expansion and contraction\\nTo implement a TABLE-DELETE operation, it is simple enough to\\nremove the speciﬁed item from the table. In order to limit the amount of\\nwasted space, however, you might want to contract the table when the\\nload factor becomes too small. Table contraction is analogous to table\\nexpansion: when the number of items in the table drops too low, allocate\\na new, smaller table and then copy the items from the old table into the\\nnew one. You can then free the storage for the old table by returning it\\nto the memory-management system. In order to not waste space, yet\\nkeep the amortized costs low, the insertion and deletion procedures\\nshould preserve two properties:\\nthe load factor of the dynamic table is bounded below by a\\npositive constant, as well as above by 1, and', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 614}),\n",
              " Document(page_content='the amortized cost of a table operation is bounded above by a\\nconstant.\\nThe actual cost of each operation equals the number of elementary\\ninsertions or deletions.\\nYou might think that if you double the table size upon inserting an\\nitem into a full table, then you should halve the size when deleting an\\nitem that would cause the table to become less than half full. This\\nstrategy does indeed guarantee that the load factor of the table never\\ndrops below 1/2. Unfortunately, it can also cause the amortized cost of\\nan operation to be quite large. Consider the following scenario. Perform\\nn operations on a table T of size n/2, where n is an exact power of 2. The\\nﬁrst n/2 operations are insertions, which by our previous analysis cost a\\ntotal of Θ (n). At the end of this sequence of insertions, T.num = T.size =\\nn/2. For the second n/2 operations, perform the following sequence:\\ninsert, delete, delete, insert, insert, delete, delete, insert, insert,\\n….\\nThe ﬁrst insertion causes the table to expand to size n. The two\\ndeletions that follow cause the table to contract back to size n/2. Two\\nfurther insertions cause another expansion, and so forth. The cost of\\neach expansion and contraction is Θ (n), and there are Θ (n) of them.\\nThus, the total cost of the n operations is Θ (n2), making the amortized\\ncost of an operation Θ (n).\\nThe problem with this strategy is that after the table expands, not\\nenough deletions occur to pay for a contraction. Likewise, after the\\ntable contracts, not enough insertions take place to pay for an\\nexpansion.\\nHow can we solve this problem? Allow the load factor of the table to\\ndrop below 1/2. Speciﬁcally, continue to double the table size upon\\ninserting an item into a full table, but halve the table size when deleting\\nan item causes the table to become less than 1/4 full, rather than 1/2 full\\nas before. The load factor of the table is therefore bounded below by the\\nconstant 1/4, and the load factor is 1/2 immediately after a contraction.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 615}),\n",
              " Document(page_content='An expansion or contraction should exhaust all the built-up\\npotential, so that immediately after expansion or contraction, when the\\nload factor is 1/2, the table’s potential is 0. Figure 16.5 shows the idea.\\nAs the load factor deviates from 1/2, the potential increases so that by\\nthe time an expansion or contraction occurs, the table has garnered\\nsufﬁcient potential to pay for copying all the items into the newly\\nallocated table. Thus, the potential function should grow to T.num by\\nthe time that the load factor has either increased to 1 or decreased to\\n1/4. Immediately after either expanding or contracting the table, the\\nload factor goes back to 1/2 and the table’s potential reduces back to 0.\\nFigure 16.5 How to think about the potential function Φ  for table insertion and deletion. When\\nthe load factor α is 1/2, the potential is 0. In order to accumulate sufﬁcient potential to pay for\\nreinserting all T.size items when the table ﬁlls, the potential needs to increase by 2 upon each\\ninsertion when α ≥ 1/2. Correspondingly, the potential decreases by 2 upon each deletion that\\nleaves α ≥ 1/2. In order to accrue enough potential to cover the cost of reinserting all T.size/4\\nitems when the table contracts, the potential needs to increase by 1 upon each deletion when α <\\n1/2, and correspondingly the potential decreases by 1 upon each insertion that leaves α < 1/2.\\nThe red area represents load factors less than 1/4, which are not allowed.\\nWe omit the code for TABLE-DELETE, since it is analogous to\\nTABLE-INSERT. We assume that if a contraction occurs during\\nTABLE-DELETE, it occurs after the item is deleted from the table. The\\nanalysis assumes that whenever the number of items in the table drops\\nto 0, the table occupies no storage. That is, if T.num = 0, then T.size = 0.\\nHow do we design a potential function that gives constant amortized\\ntime for both insertion and deletion? When the load factor is at least\\n1/2, the same potential function, Φ (T) = 2(T.num − T.size/2), that we', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 616}),\n",
              " Document(page_content='used for insertion still works. When the table is at least half full, each\\ninsertion increases the potential by 2 if the table does not expand, and\\neach deletion reduces the potential by 2 if it does not cause the load\\nfactor to drop below 1/2.\\nWhat about when the load factor is less than 1/2, that is, when 1/4 ≤\\nα(T) < 1/2? As before, when α(T) = 1/2, so that T.num = T.size/2, the\\npotential Φ (T) should be 0. To get the load factor from 1/2 down to 1/4,\\nT.size/4 deletions need to occur, at which time T.num = T.size/4. To pay\\nfor all the reinsertions, the potential must increase from 0 to T.size/4\\nover these T.size/4 deletions. Therefore, for each call of TABLE-\\nDELETE until the table contracts, the potential should increase by\\nLikewise, when α < 1/2, each call of TABLE-INSERT should decrease\\nthe potential by 1. When 1/4 ≤ α(T) < 1/2, the potential function\\nΦ(T) = T.size/2 − T.num\\nproduces this desired behavior.\\nPutting the two cases together, we get the potential function\\nThe potential of an empty table is 0 and the potential is never negative.\\nThus, the total amortized cost of a sequence of operations with respect\\nto Φ provides an upper bound on the actual cost of the sequence. Figure\\n16.6 illustrates how the potential function behaves over a sequence of\\ninsertions and deletions.\\nNow, let’s determine the amortized costs of each operation. As\\nbefore, let numi denote the number of items stored in the table after the\\nith operation, sizei denote the total size of the table after the ith\\noperation, αi = numi/sizei denote the load factor after the ith operation,\\nΦi denote the potential after the ith operation, and ΔΦi denote the', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 617}),\n",
              " Document(page_content='change in potential due to the ith operation. Initially, num0 = 0, size0 =\\n0, and Φ0 = 0.\\nThe cases in which the table does not expand or contract and the\\nload factor does not cross α = 1/2 are straightforward. As we have seen,\\nif αi−1 ≥ 1/2 and the ith operation is an insertion that does not cause the\\ntable to expand, then ΔΦi = 2. Likewise, if the ith operation is a deletion\\nand αi ≥ 1/2, then ΔΦi = −2. Furthermore, if αi−1 < 1/2 and the ith\\noperation is a deletion that does not trigger a contraction, then ΔΦi = 1,\\nand if the ith operation is an insertion and αi < 1/2, then ΔΦi = −1. In\\nother words, if no expansion or contraction occurs and the load factor\\ndoes not cross α = 1/2, then\\nif the load factor stays at or above 1/2, then the potential increases\\nby 2 for an insertion and decreases by 2 for a deletion, and\\nif the load factor stays below 1/2, then the potential increases by 1\\nfor a deletion and decreases by 1 for an insertion.\\nIn each of these cases, the actual cost ci of the ith operation is just 1,\\nand so', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 618}),\n",
              " Document(page_content='Figure 16.6 The effect of a sequence of n TABLE-INSERT and TABLE-DELETE operations\\non the number numi of items in the table (the brown line), the number sizei of slots in the table\\n(the blue line), and the potential (the red line)\\nwhere αi = numi/sizei, each measured after the ith operation. Immediately before an expansion\\nor contraction, the potential has built up to the number of items in the table, and therefore it\\ncan pay for moving all the items to the new table.\\nif the ith operation is an insertion, its amortized cost ĉi is ci +\\nΔΦi, which is 1 + 2 = 3 if the load factor stays at or above 1/2, and\\n1 + (−1) = 0 if the load factor stays below 1/2, and\\nif the ith operation is a deletion, its amortized cost ĉi is ci + ΔΦi,\\nwhich is 1 + (−2) = −1 if the load factor stays at or above 1/2, and\\n1 + 1 = 2 if the load factor stays below 1/2.\\nFour cases remain: an insertion that takes the load factor from below\\n1/2 to 1/2, a deletion that takes the load factor from 1/2 to below 1/2, a\\ndeletion that causes the table to contract, and an insertion that causes\\nthe table to expand. We analyzed that last case at the end of Section\\n16.4.1 to show that its amortized cost is 3.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 619}),\n",
              " Document(page_content='When the ith operation is a deletion that causes the table to contract,\\nwe have numi−1 = sizei−1/4 before the contraction, then the item is\\ndeleted, and ﬁnally numi = sizei/2 − 1 after the contraction. Thus, by\\nequation (16.5) we have\\nΦi−1=sizei−1/2 − numi−1\\n=sizei−1/2 − sizei−1/4\\n=sizei−1/4,\\nwhich also equals the actual cost ci of deleting one item and copying\\nsizei−1/4 − 1 items into the new, smaller table. Since numi = sizei/2 − 1\\nafter the operation has completed, αi < 1/2, and so\\nΦi=sizei/2 − numi\\n=1,\\ngiving ΔΦi = 1 − sizei−1/4. Therefore, when the ith operation is a\\ndeletion that triggers a contraction, its amortized cost is\\nĉi=ci + ΔΦi\\n=sizei−1/4 + (1 − sizei−1/4)\\n=1.\\nFinally, we handle the cases where the load factor ﬁts one case of\\nequation (16.5) before the operation and the other case afterward. We\\nstart with deletion, where we have numi−1 = sizei−1/2, so that αi−1 =\\n1/2, beforehand, and numi = sizei/2−1, so that αi < 1/2 afterward.\\nBecause αi−1 = 1/2, we have Φi−1 = 0, and because αi < 1/2, we have Φi\\n= sizei/2 − numi = 1. Thus we get that ΔΦi = 1 − 0 = 1. Since the ith\\noperation is a deletion that does not cause a contraction, the actual cost\\nci equals 1, and the amortized cost ĉi is ci + ΔΦi = 1 + 1 = 2.\\nConversely, if the ith operation is an insertion that takes the load\\nfactor from below 1/2 to equaling 1/2, the change in potential ΔΦi', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 620}),\n",
              " Document(page_content='equals −1. Again, the actual cost ci is 1, and now the amortized cost ĉi\\nis ci + ΔΦi = 1 + (−1) = 0.\\nIn summary, since the amortized cost of each operation is bounded\\nabove by a constant, the actual time for any sequence of n operations on\\na dynamic table is O(n).\\nExercises\\n16.4-1\\nUsing the potential method, analyze the amortized cost of the ﬁrst table\\ninsertion.\\n16.4-2\\nYou wish to implement a dynamic, open-address hash table. Why might\\nyou consider the table to be full when its load factor reaches some value\\nα that is strictly less than 1? Describe brieﬂy how to make insertion into\\na dynamic, open-address hash table run in such a way that the expected\\nvalue of the amortized cost per insertion is O(1). Why is the expected\\nvalue of the actual cost per insertion not necessarily O(1) for all\\ninsertions?\\n16.4-3\\nDiscuss how to use the accounting method to analyze both the insertion\\nand deletion operations, assuming that the table doubles in size when its\\nload factor exceeds 1 and the table halves in size when its load factor\\ngoes below 1/4.\\n16.4-4\\nSuppose that instead of contracting a table by halving its size when its\\nload factor drops below 1/4, you contract the table by multiplying its\\nsize by 2/3 when its load factor drops below 1/3. Using the potential\\nfunction\\nΦ(T) = |2(T.num − T.size/2)|,\\nshow that the amortized cost of a TABLE-DELETE that uses this\\nstrategy is bounded above by a constant.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 621}),\n",
              " Document(page_content='Problems\\n16-1\\xa0\\xa0\\xa0\\xa0\\xa0B inary reﬂected Gray code\\nA binary Gray code represents a sequence of nonnegative integers in\\nbinary such that to go from one integer to the next, exactly one bit ﬂips\\nevery time. The binary reﬂected Gray code represents a sequence of the\\nintegers 0 to 2k − 1 for some positive integer k according to the\\nfollowing recursive method:\\nFor k = 1, the binary reﬂected Gray code is 〈0, 1〉.\\nFor k ≥ 2, ﬁrst form the binary reﬂected Gray code for k − 1,\\ngiving the 2k−1 integers 0 to 2k−1 − 1. Then form the reﬂection\\nof this sequence, which is just the sequence in reverse. (That is, the\\nj th integer in the sequence becomes the (2k−1 − j − 1)st integer in\\nthe reﬂection). Next, add 2k−1 to each of the 2k−1 integers in the\\nreﬂected sequence. Finally, concatenate the two sequences.\\nFor example, for k = 2, ﬁrst form the binary reﬂected Gray code 〈0,\\n1〉 for k = 1. Its reﬂection is the sequence 〈1, 0〉. Adding 2k−1 = 2 to\\neach integer in the reﬂection gives the sequence 〈3, 2〉. Concatenating\\nthe two sequences gives 〈0, 1, 3, 2〉 or, in binary, 〈00, 01, 11, 10〉, so that\\neach integer differs from its predecessor by exactly one bit. For k = 3,\\nthe reﬂection of the binary reﬂected Gray code for k = 2 is 〈2, 3, 1, 0〉\\nand adding 2k−1 = 4 gives 〈6, 7, 5, 4〉. Concatenating produces the\\nsequence 〈0, 1, 3, 2, 6, 7, 5, 4〉, which in binary is 〈000, 001, 011, 010,\\n110, 111, 101,100〉. In the binary reﬂected Gray code, only one bit ﬂips\\neven when wrapping around from the last integer to the ﬁrst.\\na. Index the integers in a binary reﬂected Gray code from 0 to 2k − 1,\\nand consider the ith integer in the binary reﬂected Gray code. To go\\nfrom the (i −1)st integer to the ith integer in the binary reﬂected Gray\\ncode, exactly one bit ﬂips. Show how to determine which bit ﬂips,\\ngiven the index i.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 622}),\n",
              " Document(page_content='b. Assuming that given a bit number j, you can ﬂip bit j of an integer in\\nconstant time, show how to compute the entire binary reﬂected Gray\\ncode sequence of 2k numbers in Θ (2k) time.\\n16-2\\xa0\\xa0\\xa0\\xa0\\xa0M aking binary search dynam ic\\nBinary search of a sorted array takes logarithmic search time, but the\\ntime to insert a new element is linear in the size of the array. You can\\nimprove the time for insertion by keeping several sorted arrays.\\nSpeciﬁcally, suppose that you wish to support SEARCH and\\nINSERT on a set of n elements. Let k = ⌈lg(n + 1) ⌉, and let the binary\\nrepresentation of n be 〈nk−1, nk−2, … , n0〉. Maintain k sorted arrays\\nA0, A1, … , Ak−1, where for i = 0, 1, … , k − 1, the length of array Ai is\\n2i. Each array is either full or empty, depending on whether ni = 1 or ni\\n= 0, respectively. The total number of elements held in all k arrays is\\ntherefore \\n . Although each individual array is sorted,\\nelements in different arrays bear no particular relationship to each\\nother.\\na. Describe how to perform the SEARCH operation for this data\\nstructure. Analyze its worst-case running time.\\nb. Describe how to perform the INSERT operation. Analyze its worst-\\ncase and amortized running times, assuming that the only operations\\nare INSERT and SEARCH.\\nc. Describe how to implement DELETE. Analyze its worst-case and\\namortized running times, assuming that there can be DELETE,\\nINSERT, and SEARCH operations.\\n16-3\\xa0\\xa0\\xa0\\xa0\\xa0A mortized weight-balanced trees\\nConsider an ordinary binary search tree augmented by adding to each\\nnode x the attribute x.size, which gives the number of keys stored in the\\nsubtree rooted at x. Let α be a constant in the range 1/2 ≤ α < 1. We say\\nthat a given node x is α-balanced if x.left.size ≤ α · x.size and x.right.size\\n≤ α · x.size. The tree as a whole is α-balanced if every node in the tree is', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 623}),\n",
              " Document(page_content='α-balanced. The following amortized approach to maintaining weight-\\nbalanced trees was suggested by G. Varghese.\\na. A 1/2-balanced tree is, in a sense, as balanced as it can be. Given a\\nnode x in an arbitrary binary search tree, show how to rebuild the\\nsubtree rooted at x so that it becomes 1/2-balanced. Your algorithm\\nshould run in Θ (x.size) time, and it can use O(x.size) auxiliary storage.\\nb. Show that performing a search in an n-node α-balanced binary search\\ntree takes O(lg n) worst-case time.\\nFor the remainder of this problem, assume that the constant α is strictly\\ngreater than 1/2. Suppose that you implement INSERT and DELETE\\nas usual for an n-node binary search tree, except that after every such\\noperation, if any node in the tree is no longer α-balanced, then you\\n“rebuild” the subtree rooted at the highest such node in the tree so that\\nit becomes 1/2-balanced.\\nWe’ll analyze this rebuilding scheme using the potential method. For\\na node x in a binary search tree T, deﬁne\\nΔ(x) = |x.left.size − x.right.size|.\\nDeﬁne the potential of T as\\nwhere c is a sufﬁciently large constant that depends on α.\\nc. Argue that any binary search tree has nonnegative potential and also\\nthat a 1/2-balanced tree has potential 0.\\nd. Suppose that m units of potential can pay for rebuilding an m-node\\nsubtree. How large must c be in terms of α in order for it to take O(1)\\namortized time to rebuild a subtree that is not α-balanced?\\ne. Show that inserting a node into or deleting a n ode from an n-node α-\\nbalanced tree costs O(lg n) amortized time.\\n16-4\\xa0\\xa0\\xa0\\xa0\\xa0T he cost of restructuring red-black trees', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 624}),\n",
              " Document(page_content='There are four basic operations on red-black trees that perform\\nstructural modiﬁcations: node insertions, node deletions, rotations, and\\ncolor changes. We have seen that RB-INSERT and RB-DELETE use\\nonly O(1) rotations, node insertions, and node deletions to maintain the\\nred-black properties, but they may make many more color changes.\\na. Describe a legal red-black tree with n nodes such that calling RB-\\nINSERT to add the (n + 1)st node causes Ω(lg n) color changes. Then\\ndescribe a legal red-black tree with n nodes for which calling RB-\\nDELETE on a particular node causes Ω(lg n) color changes.\\nAlthough the worst-case number of color changes per operation can be\\nlogarithmic, you will prove that any sequence of m RB-INSERT and\\nRB-DELETE operations on an initially empty red-black tree causes\\nO(m) structural modiﬁcations in the worst case.\\nb. Some of the cases handled by the main loop of the code of both RB-\\nINSERT-FIXUP and RB-DELETE-FIXUP are terminating: once\\nencountered, they cause the loop to terminate after a constant number\\nof additional operations. For each of the cases of RB-INSERT-\\nFIXUP and RB-DELETE-FIXUP, specify which are terminating and\\nwhich are not. (Hint: Look at Figures 13.5, 13.6, and 13.7 in Sections\\n13.3 and 13.4.)\\nYou will ﬁrst analyze the structural modiﬁcations when only insertions\\nare performed. Let T be a red-black tree, and deﬁne Φ (T) to be the\\nnumber of red nodes in T. Assume that one unit of potential can pay for\\nthe structural modiﬁcations performed by any of the three cases of RB-\\nINSERT-FIXUP.\\nc. Let T′ be the result of applying Case 1 of RB-INSERT-FIXUP to T.\\nArgue that Φ (T′) = Φ (T) − 1.\\nd. We can break the operation of the RB-INSERT procedure into three\\nparts. List the structural modiﬁcations and potential changes resulting\\nfrom lines 1–16 of RB-INSERT, from nonterminating cases of RB-\\nINSERT-FIXUP, and from terminating cases of RB-INSERT-\\nFIXUP.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 625}),\n",
              " Document(page_content='e. Using part (d), argue that the amortized number of structural\\nmodiﬁcations performed by any call of RB-INSERT is O(1).\\nNext you will prove that there are O(m) structural modiﬁcations when\\nboth insertions and deletions occur. Deﬁne, for each node x,\\nNow redeﬁne the potential of a red-black tree T as\\nand let T′ be the tree that results from applying any nonterminating case\\nof RB-INSERT-FIXUP or RB-DELETE-FIXUP to T.\\nf. Show that Φ (T′) ≤ Φ (T) − 1 for all nonterminating cases of RB-\\nINSERT-FIXUP. Argue that the amortized number of structural\\nmodiﬁcations performed by any call of RB-INSERT-FIXUP is O(1).\\ng. Show that Φ (T′) ≤ Φ (T) − 1 for all nonterminating cases of RB-\\nDELETE-FIXUP. Argue that the amortized number of structural\\nmodiﬁcations performed by any call of RB-DELETE-FIXUP is O(1).\\nh. Complete the proof that in the worst case, any sequence of m RB-\\nINSERT and RB-DELETE operations performs O(m) structural\\nmodiﬁcations.\\nChapter notes\\nAho, Hopcroft, and Ullman [5] used aggregate analysis to determine the\\nrunning time of operations on a disjoint-set forest. We’ll analyze this\\ndata structure using the potential method in Chapter 19. Tarjan [430]\\nsurveys the accounting and potential methods of amortized analysis\\nand presents several applications. He attributes the accounting method\\nto several authors, including M. R. Brown, R. E. Tarjan, S. Huddleston,', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 626}),\n",
              " Document(page_content='and K. Mehlhorn. He attributes the potential method to D. D. Sleator.\\nThe term “amortized” is due to D. D. Sleator and R. E. Tarjan.\\nPotential functions are also useful for proving lower bounds for\\ncertain types of problems. For each conﬁguration of the problem, deﬁne\\na potential function that maps the conﬁguration to a real number. Then\\ndetermine the potential Φinit of the initial conﬁguration, the potential\\nΦﬁnal of the ﬁnal conﬁguration, and the maximum change in potential\\nΔΦmax due to any step. The number of steps must therefore be at least |\\nΦﬁnal − Φinit| / | ΔΦmax|. Examples of potential functions to prove\\nlower bounds in I/O complexity appear in works by Cormen, Sundquist,\\nand Wisniewski [105], Floyd [146], and Aggarwal and Vitter [3].\\nKrumme, Cybenko, and Venkataraman [271] applied potential\\nfunctions to prove lower bounds on gossiping: communicating a unique\\nitem from each vertex in a graph to every other vertex.\\n1 In some situations, such as an open-address hash table, it’s better to consider a table to be full\\nif its load factor equals some constant strictly less than 1. (See Exercise 16.4-2.)', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 627}),\n",
              " Document(page_content='Part V\\xa0\\xa0\\xa0\\xa0Advanced Data Structures', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 628}),\n",
              " Document(page_content='\\xa0\\n\\xa0\\nIntroduction\\nThis part returns to studying data structures that support operations on\\ndynamic sets, but at a more advanced level than Part III. One of the\\nchapters, for example, makes extensive use of the amortized analysis\\ntechniques from Chapter 16.\\nChapter 17 shows how to augment red-black trees—ad ding\\nadditional information in each node—to support dynamic-set\\noperations in addition to those covered in Chapters 12 and 13. The ﬁrst\\nexample augments red-black trees to dynamically maintain order\\nstatistics for a set of keys. Another example augments them in a\\ndifferent way to maintain intervals of real numbers. Chapter 17 includes\\na theorem giving sufﬁcient conditions for when a red-black tree can be\\naugmented while maintaining the O(lg n) running times for insertion\\nand deletion.\\nChapter 18 presents B-trees, which are balanced search trees\\nspeciﬁcally designed to be stored on disks. Since disks operate much\\nmore slowly than random-access memory, B-tree performance depends\\nnot only on how much computing time the dynamic-set operations\\nconsume but also on how many disk accesses they perform. For each B-\\ntree operation, the number of disk accesses increases with the height of\\nthe B-tree, but B-tree operations keep the height low.\\nChapter 19 examines data structures for disjoint sets. Starting with a\\nuniverse of n elements, each initially in its own singleton set, the\\noperation UNION unites two sets. At all times, the n elements are\\npartitioned into disjoint sets, even as calls to the UNION operation', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 629}),\n",
              " Document(page_content='change the members of a set dynamically. The query FIND-SET\\nidentiﬁes the unique set that contains a given element at the moment.\\nRepresenting each set as a simple rooted tree yields surprisingly fast\\noperations: a sequence of m operations runs in O(m α(n)) time, where\\nα(n) is an incredibly slowly growing function— α(n) is at most 4 in any\\nconceivable application. The amortized analysis that proves this time\\nbound is as complex as the data structure is simple.\\nThe topics covered in this part are by no means the only examples of\\n“advanced” data structures. Other advanced data structures include the\\nfollowing:\\nFibonacci heaps [156] implement mergeable heaps (see Problem\\n10-2 on page 268) with the operations INSERT, MINIMUM, and\\nUNION taking only O(1) actual and amortized time, and the\\noperations EXTRACT-MIN and DELETE taking O(lg n)\\namortized time. The most signiﬁcant advantage of these data\\nstructures, however, is that DECREASE-KEY takes only O(1)\\namortized time. Strict Fibonacci heaps [73], developed later, made\\nall of these time bounds actual. Because the DECREASE-KEY\\noperation takes constant amortized time, (strict) Fibonacci heaps\\nconstitute key components of some of the asymptotically fastest\\nalgorithms to date for graph problems.\\nDynamic trees [415, 429] maintain a forest of disjoint rooted trees.\\nEach edge in each tree has a real-valued cost. Dynamic trees\\nsupport queries to ﬁnd parents, roots, edge costs, and the\\nminimum edge cost on a simple path from a node up to a root.\\nTrees may be manipulated by cutting edges, updating all edge\\ncosts on a simple path from a node up to a root, linking a root\\ninto another tree, and making a node the root of the tree it\\nappears in. One implementation of dynamic trees gives an O(lg n)\\namortized time bound for each operation, while a more\\ncomplicated implementation yields O(lg n) worst-case time\\nbounds. Dynamic trees are used in some of the asymptotically\\nfastest network-ﬂow algorithms.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 630}),\n",
              " Document(page_content='Splay trees [418, 429] are a form of binary search tree on which\\nthe standard search-tree operations run in O(lg n) amortized time.\\nOne application of splay trees simpliﬁes dynamic trees.\\nPersistent data structures allow queries, and sometimes updates as\\nwell, on past versions of a data structure. For example, linked data\\nstructures can be made persistent with only a small time and\\nspace cost [126]. Problem 13-1 gives a simple example of a\\npersistent dynamic set.\\nSeveral data structures allow a faster implementation of\\ndictionary operations (INSERT, DELETE, and SEARCH) for a\\nrestricted universe of keys. By taking advantage of these\\nrestrictions, they are able to achieve better worst-case asymptotic\\nrunning times than comparison-based data structures. If the keys\\nare unique integers drawn from the set {0, 1, 2, … , u − 1}, where\\nu is an exact power of 2, then a recursive data structure known as\\na van Emde Boas tree [440, 441] supports each of the operations\\nSEARCH, INSERT, DELETE, MINIMUM, MAXIMUM,\\nSUCCESSOR, and PREDECESSOR in O(lg lg u) time. Fusion\\ntrees [157] were the ﬁrst data structure to allow faster dictionary\\noperations when the universe is restricted to integers,\\nimplementing these operations in O(lg n/lg lg n) time. Several\\nsubsequent data structures, including exponential search trees [17],\\nhave also given improved bounds on some or all of the dictionary\\noperations and are mentioned in the chapter notes throughout\\nthis book.\\nDynamic graph data structures support various queries while\\nallowing the structure of a graph to change through operations\\nthat insert or delete vertices or edges. Examples of the queries that\\nthey support include vertex connectivity [214], edge connectivity,\\nminimum spanning trees [213], biconnectivity, and transitive\\nclosure [212].\\nChapter notes throughout this book mention additional data structures.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 631}),\n",
              " Document(page_content='17\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0Augmenting Data Structures\\nSome solutions require no more than a “textbook” data structure—\\nsuch as a doubly linked list, a hash table, or a binary search tree—but\\nmany others require a dash of creativity. Rarely will you need to create\\nan entirely new type of data structure, though. More often, you can\\naugment a textbook data structure by storing additional information in\\nit. You can then program new operations for the data structure to\\nsupport your application. Augmenting a data structure is not always\\nstraightforward, however, since the added information must be updated\\nand maintained by the ordinary operations on the data structure.\\nThis chapter discusses two data structures based on red-black trees\\nthat are augmented with additional information. Section 17.1 describes\\na data structure that supports general order-statistic operations on a\\ndynamic set: quickly ﬁnding the ith smallest number or the rank of a\\ngiven element. Section 17.2 abstracts the process of augmenting a data\\nstructure and provides a theorem that you can use when augmenting\\nred-black trees. Section 17.3 uses this theorem to help design a data\\nstructure for maintaining a dynamic set of intervals, such as time\\nintervals. You can use this data structure to quickly ﬁnd an interval that\\noverlaps a given query interval.\\n17.1\\xa0\\xa0\\xa0\\xa0Dynamic order statistics\\nChapter 9 introduced the notion of an order statistic. Speciﬁcally, the\\nith order statistic of a set of n elements, where i ∈ {1, 2, … , n}, is', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 632}),\n",
              " Document(page_content='simply the element in the set with the ith smallest key. In Chapter 9, you\\nsaw how to determine any order statistic in O(n) time from an\\nunordered set. This section shows how to modify red-black trees so that\\nyou can determine any order statistic for a dynamic set in O(lg n) time\\nand also compute the rank of an element—its position in the linear\\norder of the set—in O(lg n) time.\\nFigure 17.1 An order-statistic tree, which is an augmented red-black tree. In addition to its usual\\nattributes, each node x has an attribute x.size, which is the number of nodes, other than the\\nsentinel, in the subtree rooted at x.\\nFigure 17.1 shows a data structure that can support fast order-\\nstatistic operations. An order-statistic tree T is simply a red-black tree\\nwith additional information stored in each node. Each node x contains\\nthe usual red-black tree attributes x.key, x.color, x.p, x.left, and x.right,\\nalong with a new attribute, x.size. This attribute contains the number of\\ninternal nodes in the subtree rooted at x (including x itself, but not\\nincluding any sentinels), that is, the size of the subtree. If we deﬁne the\\nsentinel’s size to be 0—that is, we set T.nil.size to be 0—then we have the\\nidentity\\nx.size = x.left.size + x.right.size + 1.\\nKeys need not be distinct in an order-statistic tree. For example, the\\ntree in Figure 17.1 has two keys with value 14 and two keys with value\\n21. When equal keys are present, the above notion of rank is not well\\ndeﬁned. We remove this ambiguity for an order-statistic tree by deﬁning\\nthe rank of an element as the position at which it would be printed in an\\ninorder walk of the tree. In Figure 17.1, for example, the key 14 stored', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 633}),\n",
              " Document(page_content='in a black node has rank 5, and the key 14 stored in a red node has rank\\n6.\\nRetrieving the element with a given rank\\nBefore we show how to maintain the size information during insertion\\nand deletion, let’s see how to implement two order-statistic queries that\\nuse this additional information. We begin with an operation that\\nretrieves the element with a given rank. The procedure OS-SELECT(x,\\ni) on the following page returns a pointer to the node containing the ith\\nsmallest key in the subtree rooted at x. To ﬁnd the node with the ith\\nsmallest key in an order-statistic tree T, call OS-SELECT(T.root, i).\\nHere is how OS-SELECT works. Line 1 computes r, the rank of\\nnode x within the subtree rooted at x. The value of x.left.size is the\\nnumber of nodes that come before x in an inorder tree walk of the\\nsubtree rooted at x. Thus, x.left.size + 1 is the rank of x within the\\nsubtree rooted at x. If i = r, then node x is the ith smallest element, and\\nso line 3 returns x. If i < r, then the ith smallest element resides in x’s\\nleft subtree, and therefore, line 5 recurses on x.left. If i > r, then the ith\\nsmallest element resides in x’s right subtree. Since the subtree rooted at\\nx contains r elements that come before x’s right subtree in an inorder\\ntree walk, the ith smallest element in the subtree rooted at x is the (i −\\nr)th smallest element in the subtree rooted at x.right. Line 6 determines\\nthis element recursively.\\nOS-SELECT(x, i)\\n1r = x.left.size + 1// rank of x within the subtree rooted at x\\n2if i == r\\n3 return x\\n4elseif i < r\\n5 return OS-SELECT(x.left, i)\\n6else return OS-SELECT(x.right, i − r)\\nAs an example of how OS-SELECT operates, consider a search for\\nthe 17th smallest element in the order-statistic tree of Figure 17.1. The\\nsearch starts with x as the root, whose key is 26, and with i = 17. Since', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 634}),\n",
              " Document(page_content='the size of 26’s left subtree is 12, its rank is 13. Thus, the node with rank\\n17 is the 17 − 13 = 4th smallest element in 26’s right subtree. In the\\nrecursive call, x is the node with key 41, and i = 4. Since the size of 41’s\\nleft subtree is 5, its rank within its subtree is 6. Therefore, the node with\\nrank 4 is the 4th smallest element in 41’s left subtree. In the recursive\\ncall, x is the node with key 30, and its rank within its subtree is 2. The\\nprocedure recurses once again to ﬁnd the 4 − 2 = 2nd smallest element\\nin the subtree rooted at the node with key 38. Its left subtree has size 1,\\nwhich means it is the second smallest element. Thus, the procedure\\nreturns a pointer to the node with key 38.\\nBecause each recursive call goes down one level in the order-statistic\\ntree, the total time for OS-SELECT is at worst proportional to the\\nheight of the tree. Since the tree is a red-black tree, its height is O(lg n),\\nwhere n is the number of nodes. Thus, the running time of OS-SELECT\\nis O(lg n) for a dynamic set of n elements.\\nDetermining the rank of an element\\nGiven a pointer to a node x in an order-statistic tree T, the procedure\\nOS-RANK on the facing page returns the position of x in the linear\\norder determined by an inorder tree walk of T.\\nOS-RANK(T, x)\\n1r = x.left.size + 1 // rank of x within the subtree\\nrooted at x\\n2y = x // root of subtree being examined\\n3while y ≠ T.root\\n4 if y == y.p.right \\xa0// if root of a right subtree …\\n5 r = r + y.p.left.size\\n+ 1\\xa0// … add in parent and its left\\nsubtree\\n6 y = y.p \\xa0// move y toward the root\\n7return r\\nThe OS-RANK procedure works as follows. You can think of node\\nx’s rank as the number of nodes preceding x in an inorder tree walk,\\nplus 1 for x itself. OS-RANK maintains the following loop invariant:', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 635}),\n",
              " Document(page_content='At the start of each iteration of the while loop of lines 3–6, r is\\nthe rank of x.key in the subtree rooted at node y.\\nWe use this loop invariant to show that OS-RANK works correctly as\\nfollows:\\nInitialization: Prior to the ﬁrst iteration, line 1 sets r to be the rank of\\nx.key within the subtree rooted at x. Setting y = x in line 2 makes the\\ninvariant true the ﬁrst time the test in line 3 executes.\\nMaintenance: At the end of each iteration of the while loop, line 6 sets y\\n= y.p. Thus, we must show that if r is the rank of x.key in the subtree\\nrooted at y at the start of the loop body, then r is the rank of x.key in\\nthe subtree rooted at y.p at the end of the loop body. In each iteration\\nof the while loop, consider the subtree rooted at y.p. The value of r\\nalready includes the number of nodes in the subtree rooted at node y\\nthat precede x in an inorder walk, and so the procedure must add the\\nnodes in the subtree rooted at y’s sibling that precede x in an inorder\\nwalk, plus 1 for y.p if it, too, precedes x. If y is a left child, then\\nneither y.p nor any node in y.p’s right subtree precedes x, and so OS-\\nRANK leaves r alone. Otherwise, y is a right child and all the nodes in\\ny.p’s left subtree precede x, as does y.p itself. In this case, line 5 adds\\ny.p.left.size + 1 to the current value of r.\\nTermination: Because each iteration of the loop moves y toward the root\\nand the loop terminates when y = T.root, the loop eventually\\nterminates. Moreover, the subtree rooted at y is the entire tree. Thus,\\nthe value of r is the rank of x.key in the entire tree.\\nAs an example, when OS-RANK runs on the order-statistic tree of\\nFigure 17.1 to ﬁnd the rank of the node with key 38, the following\\nsequence of values of y.key and r occurs at the top of the while loop:\\niterationy.keyr\\n1 38 2\\n2 30 4\\n3 41 4\\n4 2617', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 636}),\n",
              " Document(page_content='The procedure returns the rank 17.\\nSince each iteration of the while loop takes O(1) time, and y goes up\\none level in the tree with each iteration, the running time of OS-RANK\\nis at worst proportional to the height of the tree: O(lg n) on an n-node\\norder-statistic tree.\\nMaintaining subtree sizes\\nGiven the size attribute in each node, OS-SELECT and OS-RANK can\\nquickly compute order-statistic information. But if the basic modifying\\noperations on red-black trees cannot efﬁciently maintain the size\\nattribute, our work will have been for naught. Let’s see how to maintain\\nsubtree sizes for both insertion and deletion without affecting the\\nasymptotic running time of either operation.\\nRecall from Section 13.3 that insertion into a red-black tree consists\\nof two phases. The ﬁrst phase goes down the tree from the root,\\ninserting the new node as a child of an existing node. The second phase\\ngoes up the tree, changing colors and performing rotations to maintain\\nthe red-black properties.\\nTo maintain the subtree sizes in the ﬁrst phase, simply increment\\nx.size for each node x on the simple path traversed from the root down\\ntoward the leaves. The new node added gets a size of 1. Since there are\\nO(lg n) nodes on the traversed path, the additional cost of maintaining\\nthe size attributes is O(lg n).\\nIn the second phase, the only structural changes to the underlying\\nred-black tree are caused by rotations, of which there are at most two.\\nMoreover, a rotation is a local operation: only two nodes have their size\\nattributes invalidated. The link around which the rotation is performed\\nis incident on these two nodes. Referring to the code for LEFT-\\nROTATE(T, x) on page 336, add the following lines:\\n13y.size = x.size\\n14x.size = x.left.size + x.right.size + 1\\nFigure 17.2 illustrates how the attributes are updated. The change to\\nRIGHT-ROTATE is symmetric.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 637}),\n",
              " Document(page_content='Since inserting into a red-black tree requires at most two rotations,\\nupdating the size attributes in the second phase costs only O(1)\\nadditional time. Thus, the total time for insertion into an n-node order-\\nstatistic tree is O(lg n), which is asymptotically the same as for an\\nordinary red-black tree.\\nFigure 17.2 Updating subtree sizes during rotations. The updates are local, requiring only the\\nsize information stored in x, y, and the roots of the subtrees shown as triangles.\\nDeletion from a red-black tree also consists of two phases: the ﬁrst\\noperates on the underlying search tree, and the second causes at most\\nthree rotations and otherwise performs no structural changes. (See\\nSection 13.4.) The ﬁrst phase removes one node z from the tree and\\ncould move at most two other nodes within the tree (nodes y and x in\\nFigure 12.4 on page 323). To update the subtree sizes, simply traverse a\\nsimple path from the lowest node that moves (starting from its original\\nposition within the tree) up to the root, decrementing the size attribute\\nof each node on the path. Since this path has length O(lg n) in an n-\\nnode red-black tree, the additional time spent maintaining size\\nattributes in the ﬁrst phase is O(lg n). For the O(1) rotations in the\\nsecond phase of deletion, handle them in the same manner as for\\ninsertion. Thus, both insertion and deletion, including maintaining the\\nsize attributes, take O(lg n) time for an n-node order-statistic tree.\\nExercises\\n17.1-1\\nShow how OS-SELECT(T.root, 10) operates on the red-black tree T\\nshown in Figure 17.1.\\n17.1-2', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 638}),\n",
              " Document(page_content='Show how OS-RANK(T, x) operates on the red-black tree T shown in\\nFigure 17.1 and the node x with x.key = 35.\\n17.1-3\\nWrite a nonrecursive version of OS-SELECT.\\n17.1-4\\nWrite a procedure OS-KEY-RANK(T, k) that takes an order-statistic\\ntree T and a key k and returns the rank of k in the dynamic set\\nrepresented by T. Assume that the keys of T are distinct.\\n17.1-5\\nGiven an element x in an n-node order-statistic tree and a natural\\nnumber i, show how to determine the ith successor of x in the linear\\norder of the tree in O(lg n) time.\\n17.1-6\\nThe procedures OS-SELECT and OS-RANK use the size attribute of a\\nnode only to compute a rank. Suppose that you store in each node its\\nrank in the subtree of which it is the root instead of the size attribute.\\nShow how to maintain this information during insertion and deletion.\\n(Remember that these two operations can cause rotations.)\\n17.1-7\\nShow how to use an order-statistic tree to count the number of\\ninversions (see Problem 2-4 on page 47) in an array of n distinct\\nelements in O(n lg n) time.\\n★ 17.1-8\\nConsider n chords on a circle, each deﬁned by its endpoints. Describe an\\nO(n lg n)-time algorithm to determine the number of pairs of chords\\nthat intersect inside the circle. (For example, if the n chords are all\\ndiameters that meet at the center, then the answer is \\n.) Assume that\\nno two chords share an endpoint.\\n17.2\\xa0\\xa0\\xa0\\xa0How to augment a data structure', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 639}),\n",
              " Document(page_content='The process of augmenting a basic data structure to support additional\\nfunctionality occurs quite frequently in algorithm design. We’ll use it\\nagain in the next section to design a data structure that supports\\noperations on intervals. This section examines the steps involved in such\\naugmentation. It includes a useful theorem that allows you to augment\\nred-black trees easily in many cases.\\nYou can break the process of augmenting a data structure into four\\nsteps:\\n1. Choose an underlying data structure.\\n2. Determine additional information to maintain in the underlying\\ndata structure.\\n3. Verify that you can maintain the additional information for the\\nbasic modifying operations on the underlying data structure.\\n4. Develop new operations.\\nAs with any prescriptive design method, you’ll rarely be able to follow\\nthe steps precisely in the order given. Most design work contains an\\nelement of trial and error, and progress on all steps usually proceeds in\\nparallel. There is no point, for example, in determining additional\\ninformation and developing new operations (steps 2 and 4) if you\\ncannot maintain the additional information efﬁciently. Nevertheless,\\nthis four-step method provides a good focus for your efforts in\\naugmenting a data structure, and it is also a good framework for\\ndocumenting an augmented data structure.\\nWe followed these four steps in Section 17.1 to design order-statistic\\ntrees. For step 1, we chose red-black trees as the underlying data\\nstructure. Red-black trees seemed like a good starting point because\\nthey efﬁciently support other dynamic-set operations on a total order,\\nsuch as MINIMUM, MAXIMUM, SUCCESSOR, and\\nPREDECESSOR.\\nIn Step 2, we added the size attribute, so that each node x stores the\\nsize of the subtree rooted at x. Generally, the additional information\\nmakes operations more efﬁcient. For example, it is possible to\\nimplement OS-SELECT and OS-RANK using just the keys stored in', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 640}),\n",
              " Document(page_content='the tree, but then they would not run in O(lg n) time. Sometimes, the\\nadditional information is pointer information rather than data, as in\\nExercise 17.2-1.\\nFor step 3, we ensured that insertion and deletion can maintain the\\nsize attributes while still running in O(lg n) time. Ideally, you would like\\nto update only a few elements of the data structure in order to maintain\\nthe additional information. For example, if each node simply stores its\\nrank in the tree, the OS-SELECT and OS-RANK procedures run\\nquickly, but inserting a new minimum element might cause a change to\\nthis information in every node of the tree. Because we chose to store\\nsubtree sizes instead, inserting a new element causes information to\\nchange in only O(lg n) nodes.\\nIn Step 4, we developed the operations OS-SELECT and OS-\\nRANK. After all, the need for new operations is why anyone bothers to\\naugment a data structure in the ﬁrst place. Occasionally, rather than\\ndeveloping new operations, you can use the additional information to\\nexpedite existing ones, as in Exercise 17.2-1.\\nAugmenting red-black trees\\nWhen red-black trees underlie an augmented data structure, we can\\nprove that insertion and deletion can always efﬁciently maintain certain\\nkinds of additional information, thereby simplifying step 3. The proof\\nof the following theorem is similar to the argument from Section 17.1\\nthat we can maintain the size attribute for order-statistic trees.\\nTheorem 17.1 (Augmenting a red-black tree)\\nLet f be an attribute that augments a red-black tree T of n nodes, and\\nsuppose that the value of f for each node x depends only the\\ninformation in nodes x, x.left, and x.right (possibly including x.left.f\\nand x.right.f), and that the value of x.f can be computed from this\\ninformation in O(1) time. Then, the insertion and deletion operations\\ncan maintain the values of f in all nodes of T without asymptotically\\naffecting the O(lg n) running times of these operations.\\nProof\\xa0\\xa0\\xa0The main idea of the proof is that a change to an f attribute in a\\nnode x propagates only to ancestors of x in the tree. That is, changing', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 641}),\n",
              " Document(page_content='x.f may require x.p.f to be updated, but nothing else; updating x.p.f may\\nrequire x.p.p.f to be updated, but nothing else; and so on up the tree.\\nAfter updating T.root.f, no other node depends on the new value, and\\nso the process terminates. Since the height of a red-black tree is O(lg n),\\nchanging an f attribute in a node costs O(lg n) time in updating all\\nnodes that depend on the change.\\nAs we saw in Section 13.3, insertion of a node x into red-black tree T\\nconsists of two phases. If the tree T is empty, then the ﬁrst phase simply\\nmakes x be the root of T. If T is not empty, then the ﬁrst phase inserts x\\nas a child of an existing node. Because we assume that the value of x.f\\ndepends only on information in the other attributes of x itself and the\\ninformation in x’s children, and because x’s children are both the\\nsentinel T.nil, it takes only O(1) time to compute the value of x.f.\\nHaving computed x.f, the change propagates up the tree. Thus, the total\\ntime for the ﬁrst phase of insertion is O(lg n). During the second phase,\\nthe only structural changes to the tree come from rotations. Since only\\ntwo nodes change in a rotation, but a change to an attribute might need\\nto propagate up to the root, the total time for updating the f attributes is\\nO(lg n) per rotation. Since the number of rotations during insertion is at\\nmost two, the total time for insertion is O(lg n).\\nLike insertion, deletion has two phases, as Section 13.4 discusses. In\\nthe ﬁrst phase, changes to the tree occur when a node is deleted, and at\\nmost two other nodes could move within the tree. Propagating the\\nupdates to f caused by these changes costs at most O(lg n), since the\\nchanges modify the tree locally along a simple path from the lowest\\nchanged node to the root. Fixing up the red-black tree during the\\nsecond phase requires at most three rotations, and each rotation\\nrequires at most O(lg n) time to propagate the updates to f. Thus, like\\ninsertion, the total time for deletion is O(lg n).\\n▪\\nIn many cases, such as maintaining the size attributes in order-\\nstatistic trees, the cost of updating after a rotation is O(1), rather than\\nthe O(lg n) derived in the proof of Theorem 17.1. Exercise 17.2-3 gives\\nan example.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 642}),\n",
              " Document(page_content='On the other hand, when an update after a rotation requires a\\ntraversal all the way up to the root, it is important that insertion into\\nand deletion from a red-black tree require a constant number of\\nrotations. The chapter notes for Chapter 13 list other schemes for\\nbalancing search trees that do not bound the number of rotations per\\ninsertion or deletion by a constant. If each operation might require Θ (lg\\nn) rotations and each rotation traverses a path up to the root, then a\\nsingle operation could require Θ (lg2n) time, rather than the O(lg n) time\\nbound given by Theorem 17.1.\\nExercises\\n17.2-1\\nShow, by adding pointers to the nodes, how to support each of the\\ndynamic-set queries MINIMUM, MAXIMUM, SUCCESSOR, and\\nPREDECESSOR in O(1) worst-case time on an augmented order-\\nstatistic tree. The asymptotic performance of other operations on order-\\nstatistic trees should not be affected.\\n17.2-2\\nCan you maintain the black-heights of nodes in a red-black tree as\\nattributes in the nodes of the tree without affecting the asymptotic\\nperformance of any of the red-black tree operations? Show how, or\\nargue why not. How about maintaining the depths of nodes?\\n17.2-3\\nLet ⊗  be an associative binary operator, and let a be an attribute\\nmaintained in each node of a red-black tree. Suppose that you want to\\ninclude in each node x an additional attribute f such that x.f = x1.a ⊗\\nx2.a ⊗ … ⊗  xm.a, where x1, x2, … , xm is the inorder listing of nodes\\nin the subtree rooted at x. Show how to update the f attributes in O(1)\\ntime after a rotation. Modify your argument slightly to apply it to the\\nsize attributes in order-statistic trees.\\n17.3\\xa0\\xa0\\xa0\\xa0Interval trees', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 643}),\n",
              " Document(page_content='This section shows how to augment red-black trees to support\\noperations on dynamic sets of intervals. In this section, we’ll assume\\nthat intervals are closed. Extending the results to open and half-open\\nintervals is conceptually straightforward. (See page 1157 for deﬁnitions\\nof closed, open, and half-open intervals.)\\nIntervals are convenient for representing events that each occupy a\\ncontinuous period of time. For example, you could query a database of\\ntime intervals to ﬁnd out which events occurred during a given interval.\\nThe data structure in this section provides an efﬁcient means for\\nmaintaining such an interval database.\\nA simple way to represent an interval [t1, t2] is as an object i with\\nattributes i.low = t1 (the low endpoint) and i.high = t2 (the high\\nendpoint). We say that intervals i and i′ overlap if i ∩i′ ≠ ∅ , that is, if\\ni.low ≤ i′.high and i′.low ≤ i.high.\\nFigure 17.3 The interval trichotomy for two closed intervals i and i′. (a) If i and i′ overlap, there\\nare four situations, and in each, i.low ≤ i′.high and i′.low ≤ i.high. (b) The intervals do not\\noverlap, and i.high < i′.low. (c) The intervals do not overlap, and i′.high < i.low.\\nAs Figure 17.3 shows, any two intervals i and i′ satisfy the interval\\ntrichotomy, that is, exactly one of the following three properties holds:\\na. i and i′ overlap,\\nb. i is to the left of i′ (i.e., i.high < i′.low),\\nc. i is to the right of i′ (i.e., i′.high < i.low).\\nAn interval tree is a red-black tree that maintains a dynamic set of\\nelements, with each element x containing an interval x.int. Interval trees', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 644}),\n",
              " Document(page_content='support the following operations:\\nINTERVAL-INSERT(T, x) adds the element x, whose int attribute is\\nassumed to contain an interval, to the interval tree T.\\nINTERVAL-DELETE(T, x) removes the element x from the interval\\ntree T.\\nINTERVAL-SEARCH(T, i) returns a pointer to an element x in the\\ninterval tree T such that x.int overlaps interval i, or a pointer to the\\nsentinel T.nil if no such element belongs to the set.\\nFigure 17.4 shows how an interval tree represents a set of intervals. The\\nfour-step method from Section 17.2 will guide our design of an interval\\ntree and the operations that run on it.\\nStep 1: Underlying data structure\\nA red-black tree serves as the underlying data structure. Each node x\\ncontains an interval x.int. The key of x is the low endpoint, x.int.low, of\\nthe interval. Thus, an inorder tree walk of the data structure lists the\\nintervals in sorted order by low endpoint.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 645}),\n",
              " Document(page_content='Figure 17.4 An interval tree. (a) A set of 10 intervals, shown sorted bottom to top by left\\nendpoint. (b) The interval tree that represents them. Each node x contains an interval, shown\\nabove the dashed line, and the maximum value of any interval endpoint in the subtree rooted at\\nx, shown below the dashed line. An inorder tree walk of the tree lists the nodes in sorted order\\nby left endpoint.\\nStep 2: Additional information\\nIn addition to the intervals themselves, each node x contains a value\\nx.max, which is the maximum value of any interval endpoint stored in\\nthe subtree rooted at x.\\nStep 3: Maintaining the information\\nWe must verify that insertion and deletion take O(lg n) time on an\\ninterval tree of n nodes. It is simple enough to determine x.max in O(1)', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 646}),\n",
              " Document(page_content='time, given interval x.int and the max values of node x’s children:\\nx.max = max {x.int.high, x.left.max, x.right.max}.\\nThus, by Theorem 17.1, insertion and deletion run in O(lg n) time. In\\nfact, you can use either Exercise 17.2-3 or 17.3-1 to show how to update\\nall the max attributes that change after a rotation in just O(1) time.\\nStep 4: Developing new operations\\nThe only new operation is INTERVAL-SEARCH(T, i), which ﬁnds a\\nnode in tree T whose interval overlaps interval i. If there is no interval in\\nthe tree that overlaps i, the procedure returns a pointer to the sentinel\\nT.nil.\\nINTERVAL-SEARCH(T, i)\\n1x = T.root\\n2while x ≠ T.nil and i does not overlap x.int\\n3 if x.left ≠ T.nil and x.left.max ≥ i.low\\n4 x = x.left// overlap in left subtree or no overlap in right\\nsubtree\\n5 else x =\\nx.right// no overlap in left subtree\\n6return x\\nThe search for an interval that overlaps i starts at the root of the tree\\nand proceeds downward. It terminates when either it ﬁnds an\\noverlapping interval or it reaches the sentinel T.nil. Since each iteration\\nof the basic loop takes O(1) time, and since the height of an n-node red-\\nblack tree is O(lg n), the INTERVAL-SEARCH procedure takes O(lg n)\\ntime.\\nBefore we see why INTERVAL-SEARCH is correct, let’s examine\\nhow it works on the interval tree in Figure 17.4. Let’s look for an\\ninterval that overlaps the interval i = [22, 25]. Begin with x as the root,\\nwhich contains [16, 21] and does not overlap i. Since x.left.max = 23 is\\ngreater than i.low = 22, the loop continues with x as the left child of the\\nroot—the node containing [8, 9], which also does not overlap i. This', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 647}),\n",
              " Document(page_content='time, x.left.max = 10 is less than i.low = 22, and so the loop continues\\nwith the right child of x as the new x. Because the interval [15, 23]\\nstored in this node overlaps i, the procedure returns this node.\\nNow let’s try an unsuccessful search, for an interval that overlaps i =\\n[11, 14] in the interval tree of Figure 17.4. Again, begin with x as the\\nroot. Since the root’s interval [16, 21] does not overlap i, and since\\nx.left.max = 23 is greater than i.low = 11, go left to the node containing\\n[8, 9]. Interval [8, 9] does not overlap i, and x.left.max = 10 is less than\\ni.low = 11, and so the search goes right. (No interval in the left subtree\\noverlaps i.) Interval [15, 23] does not overlap i, and its left child is T.nil,\\nso again the search goes right, the loop terminates, and INTERVAL-\\nSEARCH returns the sentinel T.nil.\\nTo see why INTERVAL-SEARCH is correct, we must understand\\nwhy it sufﬁces to examine a single path from the root. The basic idea is\\nthat at any node x, if x.int does not overlap i, the search always proceeds\\nin a safe direction: the search will deﬁnitely ﬁnd an overlapping interval\\nif the tree contains one. The following theorem states this property more\\nprecisely.\\nTheorem 17.2\\nAny execution of INTERVAL-SEARCH(T, i) either returns a node\\nwhose interval overlaps i, or it returns T.nil and the tree T contains no\\nnode whose interval overlaps i.\\nProof\\xa0\\xa0\\xa0The while loop of lines 2–5 terminates when either x = T.nil or i\\noverlaps x.int. In the latter case, it is certainly correct to return x.\\nTherefore, we focus on the former case, in which the while loop\\nterminates because x = T.nil, which is the node that INTERVAL-\\nSEARCH returns.\\nWe’ll prove that if the procedure returns T.nil, then it did not miss\\nany intervals in T that overlap i. The idea is to show that whether the\\nsearch goes left in line 4 or right in line 5, it always heads toward a node\\ncontaining an interval overlapping i, if any such interval exists. In\\nparticular, we’ll prove that\\n1. If the search goes left in line 4, then the left subtree of node x\\ncontains an interval that overlaps i or the right subtree of x', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 648}),\n",
              " Document(page_content='contains no interval that overlaps i. Therefore, even if x’s left\\nsubtree contains no interval that overlaps i but the search goes\\nleft, it does not make a mistake, because x’s right subtree does\\nnot contain an interval overlapping i, either.\\n2. If the search goes right in line 5, then the left subtree of x\\ncontains no interval that overlaps i. Thus, if the search goes\\nright, it does not make a mistake.\\nFor both cases, we rely on the interval trichotomy. Let’s start with\\nthe case where the search goes right, whose proof is simpler. By the tests\\nin line 3, we know that x.left = T.nil or x.left.max < i.low. If x.left =\\nT.nil, then x’s left subtree contains no interval that overlaps i, since it\\ncontains no intervals at all. Now suppose that x.left ≠ T.nil, so that we\\nmust have x.left.max < i.low. Consider any interval i′ in x’s left subtree.\\nBecause x.left.max is the maximum endpoint in x’s left subtree, we have\\ni′.high ≤ x.left.max. Thus, as Figure 17.5(a) shows,\\ni′.high≤x.left.max\\n<i.low.\\nBy the interval trichotomy, therefore, intervals i and i′ do not overlap,\\nand so x’s left subtree contains no interval that overlaps i.\\nFigure 17.5 Intervals in the proof of Theorem 17.2. The value of x.left.max is shown in each case\\nas a dashed line. (a) The search goes right. No interval i′ in x’s left subtree can overlap i. (b) The\\nsearch goes left. The left subtree of x contains an interval that overlaps i (situation not shown),\\nor x’s left subtree contains an interval i′ such that i′.high = x.left.max. Since i does not overlap i′,\\nneither does it overlap any interval i″ in x’s right subtree, since i′.low ≤ i″.low.\\nNow we examine the case in which the search goes left. If the left\\nsubtree of node x contains an interval that overlaps i, we’re done, so let’s', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 649}),\n",
              " Document(page_content='assume that no node in x’s left subtree overlaps i. We need to show that\\nin this case, no node in x’s right subtree overlaps i, so that going left will\\nnot miss any overlaps in x’s right subtree. By the tests in line 3, the left\\nsubtree of x is not empty and x.left.max ≥ i.low. By the deﬁnition of the\\nmax attribute, x’s left subtree contains some interval i′ such that\\ni′.high=x.left.max\\n≥i.low,\\nas illustrated in Figure 17.5(b). Since i′ is in x’s left subtree, it does not\\noverlap i, and since i′.high ≥ i.low, the interval trichotomy tells us that\\ni.high < i′.low. Now we bring in the property that interval trees are\\nkeyed on the low endpoints of intervals. Because i′ is in x’s left subtree,\\nwe have i′.low ≤ x.int.low. Now consider any interval i″ in x’s right\\nsubtree, so that x.int.low ≤ i″.low. Putting inequalities together, we get\\ni.high<i′.low\\n≤x.int.low\\n≤i″.low.\\nBecause i.high < i″.low, the interval trichotomy tells us that i and i″ do\\nnot overlap. Since we chose i″ as any interval in x’s right subtree, no\\nnode in x’s right subtree overlaps i.\\n▪\\nThus, the INTERVAL-SEARCH procedure works correctly.\\nExercises\\n17.3-1\\nWrite pseudocode for LEFT-ROTATE that operates on nodes in an\\ninterval tree and updates all the max attributes that change in O(1) time.\\n17.3-2\\nDescribe an efﬁcient algorithm that, given an interval i, returns an\\ninterval overlapping i that has the minimum low endpoint, or T.nil if no\\nsuch interval exists.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 650}),\n",
              " Document(page_content='17.3-3\\nGiven an interval tree T and an interval i, describe how to list all\\nintervals in T that overlap i in O(min {n, k lg n}) time, where k is the\\nnumber of intervals in the output list. (Hint: One simple method makes\\nseveral queries, modifying the tree between queries. A slightly more\\ncomplicated method does not modify the tree.)\\n17.3-4\\nSuggest modiﬁcations to the interval-tree procedures to support the new\\noperation INTERVAL-SEARCH-EXACTLY(T, i), where T is an\\ninterval tree and i is an interval. The operation should return a pointer\\nto a node x in T such that x.int.low = i.low and x.int.high = i.high, or\\nT.nil if T contains no such node. All operations, including INTERVAL-\\nSEARCH-EXACTLY, should run in O(lg n) time on an n-node interval\\ntree.\\n17.3-5\\nShow how to maintain a dynamic set Q of numbers that supports the\\noperation MIN-GAP, which gives the absolute value of the difference of\\nthe two closest numbers in Q. For example, if we have Q = {1, 5, 9, 15,\\n18, 22}, then MIN-GAP(Q) returns 3, since 15 and 18 are the two\\nclosest numbers in Q. Make the operations INSERT, DELETE,\\nSEARCH, and MIN-GAP as efﬁcient as possible, and analyze their\\nrunning times.\\n★ 17.3-6\\nVLSI databases commonly represent an integrated circuit as a list of\\nrectangles. Assume that each rectangle is rectilinearly oriented (sides\\nparallel to the x- and y-axes), so that each rectangle is represented by\\nfour values: its minimum and maximum x- and y-coordinates. Give an\\nO(n lg n)-time algorithm to decide whether a set of n rectangles so\\nrepresented contains two rectangles that overlap. Your algorithm need\\nnot report all intersecting pairs, but it must report that an overlap exists\\nif one rectangle entirely covers another, even if the boundary lines do\\nnot intersect. (Hint: Move a “sweep” line across the set of rectangles.)', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 651}),\n",
              " Document(page_content='Problems\\n17-1\\xa0\\xa0\\xa0\\xa0\\xa0Point of maximum overlap\\nYou wish to keep track of a point of maximum overlap in a set of\\nintervals—a point with the largest number of intervals in the set that\\noverlap it.\\na. Show that there is always a point of maximum overlap that is an\\nendpoint of one of the intervals.\\nb. Design a data structure that efﬁciently supports the operations\\nINTERVAL-INSERT, INTERVAL-DELETE, and FIND-POM,\\nwhich returns a point of maximum overlap. (Hint: Keep a red-black\\ntree of all the endpoints. Associate a value of +1 with each left\\nendpoint, and associate a value of −1 with each right endpoint.\\nAugment each node of the tree with some extra information to\\nmaintain the point of maximum overlap.)\\n17-2\\xa0\\xa0\\xa0\\xa0\\xa0Josephus permutation\\nWe deﬁne the Josephus problem as follows. A group of n people form a\\ncircle, and we are given a positive integer m ≤ n. Beginning with a\\ndesignated ﬁrst person, proceed around the circle, removing every mth\\nperson. After each person is removed, counting continues around the\\ncircle that remains. This process continues until nobody remains in the\\ncircle. The order in which the people are removed from the circle deﬁnes\\nthe (n, m)-Josephus permutation of the integers 1, 2, … , n. For example,\\nthe (7, 3)-Josephus permutation is 〈3, 6, 2, 7, 5, 1, 4〉.\\na. Suppose that m is a constant. Describe an O(n)-time algorithm that,\\ngiven an integer n, outputs the (n, m)-Josephus permutation.\\nb. Suppose that m is not necessarily a constant. Describe an O(n lg n)-\\ntime algorithm that, given integers n and m, outputs the (n, m)-\\nJosephus permutation.\\nChapter notes', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 652}),\n",
              " Document(page_content='In their book, Preparata and Shamos [364] describe several of the\\ninterval trees that appear in the literature, citing work by H.\\nEdelsbrunner (1980) and E. M. McCreight (1981) . The book details an\\ninterval tree that, given a static database of n intervals, allows us to\\nenumerate all k intervals that overlap a given query interval in O(k + lg\\nn) time.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 653}),\n",
              " Document(page_content='18\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0B-Trees\\nB-trees are balanced search trees designed to work well on disk drives or\\nother direct-access secondary storage devices. B-trees are similar to red-\\nblack trees (Chapter 13), but they are better at minimizing the number\\nof operations that access disks. (We often say just “disk” instead of\\n“disk drive.”) Many database systems use B-trees, or variants of B-trees,\\nto store information.\\nB-trees differ from red-black trees in that B-tree nodes may have\\nmany children, from a few to thousands. That is, the “branching factor”\\nof a B-tree can be quite large, although it usually depends on\\ncharacteristics of the disk drive used. B-trees are similar to red-black\\ntrees in that every n-node B-tree has height O(lg n), so that B-trees can\\nimplement many dynamic-set operations in O(lg n) time. But a B-tree\\nhas a larger branching factor than a red-black tree, so the base of the\\nlogarithm that expresses its height is larger, and hence its height can be\\nconsiderably lower.\\nB-trees generalize binary search trees in a natural manner. Figure\\n18.1 shows a simple B-tree. If an internal B-tree node x contains x.n\\nkeys, then x has x.n + 1 children. The keys in node x serve as dividing\\npoints separating the range of keys handled by x into x.n + 1 subranges,\\neach handled by one child of x. A search for a key in a B-tree makes an\\n(x.n + 1)-way decision based on comparisons with the x.n keys stored at\\nnode x. An internal node contains pointers to its children, but a leaf\\nnode does not.\\nSection 18.1 gives a precise deﬁnition of B-trees and proves that the\\nheight of a B-tree grows only logarithmically with the number of nodes', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 654}),\n",
              " Document(page_content='it contains. Section 18.2 describes how to search for a key and insert a\\nkey into a B-tree, and Section 18.3 discusses deletion. Before\\nproceeding, however, we need to ask why we evaluate data structures\\ndesigned to work on a disk drive differently from data structures\\ndesigned to work in main random-access memory.\\nFigure 18.1 A B-tree whose keys are the consonants of English. An internal node x containing\\nx.n keys has x.n + 1 children. All leaves are at the same depth in the tree. The blue nodes are\\nexamined in a search for the letter R.\\nData structures on secondary storage\\nComputer systems take advantage of various technologies that provide\\nmemory capacity. The main memory of a computer system normally\\nconsists of silicon memory chips. This technology is typically more than\\nan order of magnitude more expensive per bit stored than magnetic\\nstorage technology, such as tapes or disk drives. Most computer systems\\nalso have secondary storage based on solid-state drives (SSDs) or\\nmagnetic disk drives. The amount of such secondary storage often\\nexceeds the amount of primary memory by one to two orders of\\nmagnitude. SSDs have faster access times than magnetic disk drives,\\nwhich are mechanical devices. In recent years, SSD capacities have\\nincreased while their prices have decreased. Magnetic disk drives\\ntypically have much higher capacities than SSDs, and they remain a\\nmore cost-effective means for storing massive amounts of information.\\nDisk drives that store several terabytes1 can be found for under $100.\\nFigure 18.2 shows a typical disk drive. The drive consists of one or\\nmore platters, which rotate at a constant speed around a common\\nspindle. A magnetizable material covers the surface of each platter. The', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 655}),\n",
              " Document(page_content='drive reads and writes each platter by a head at the end of an arm. The\\narms can move their heads toward or away from the spindle. The\\nsurface that passes underneath a given head when it is stationary is\\ncalled a track.\\nAlthough disk drives are cheaper and have higher capacity than\\nmain memory, they are much, much slower because they have moving\\nmechanical parts. The mechanical motion has two components: platter\\nrotation and arm movement. As of this writing, commodity disk drives\\nrotate at speeds of 5400–15,000 revolutions per minute (RPM). Typical\\nspeeds are 15,000 RPM in server-grade drives, 7200 RPM in drives for\\ndesktops, and 5400 RPM in drives for laptops. Although 7200 RPM\\nmay seem fast, one rotation takes 8.33 milliseconds, which is over 5\\norders of magnitude longer than the 50 nanosecond access times (more\\nor less) commonly found for main memory. In other words, if a\\ncomputer waits a full rotation for a particular item to come under the\\nread/write head, it could access main memory more than 100,000 times\\nduring that span. The average wait is only half a rotation, but still, the\\ndifference in access times for main memory compared with disk drives is\\nenormous. Moving the arms also takes some time. As of this writing,\\naverage access times for commodity disk drives are around 4\\nmilliseconds.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 656}),\n",
              " Document(page_content='Figure 18.2 A typical magnetic disk drive. It consists of one or more platters covered with a\\nmagnetizable material (two platters are shown here) that rotate around a spindle. Each platter is\\nread and written with a head, shown in red, at the end of an arm. Arms rotate around a\\ncommon pivot axis. A track, drawn in blue, is the surface that passes beneath the read/write\\nhead when the head is stationary.\\nIn order to amortize the time spent waiting for mechanical\\nmovements, also known as latency, disk drives access not just one item\\nbut several at a time. Information is divided into a number of equal-\\nsized blocks of bits that appear consecutively within tracks, and each\\ndisk read or write is of one or more entire blocks.2 Typical disk drives\\nhave block sizes running from 512 to 4096 bytes. Once the read/write\\nhead is positioned correctly and the platter has rotated to the beginning\\nof the desired block, reading or writing a magnetic disk drive is entirely\\nelectronic (aside from the rotation of the platter), and the disk drive can\\nquickly read or write large amounts of data.\\nOften, accessing a block of information and reading it from a disk\\ndrive takes longer than processing all the information read. For this\\nreason, in this chapter we’ll look separately at the two principal\\ncomponents of the running time:\\nthe number of disk accesses, and\\nthe CPU (computing) time.\\nWe measure the number of disk accesses in terms of the number of\\nblocks of information that need to be read from or written to the disk', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 657}),\n",
              " Document(page_content='drive. Although disk-access time is not constant—it depends on the\\ndistance between the current track and the desired track and also on the\\ninitial rotational position of the platters—the number of blocks read or\\nwritten provides a good ﬁrst-order approximation of the total time\\nspent accessing the disk drive.\\nIn a typical B-tree application, the amount of data handled is so\\nlarge that all the data do not ﬁt into main memory at once. The B-tree\\nalgorithms copy selected blocks from disk into main memory as needed\\nand write back onto disk the blocks that have changed. B-tree\\nalgorithms keep only a constant number of blocks in main memory at\\nany time, and thus the size of main memory does not limit the size of B-\\ntrees that can be handled.\\nB-tree procedures need to be able to read information from disk into\\nmain memory and write information from main memory to disk.\\nConsider some object x. If x is currently in the computer’s main\\nmemory, then the code can refer to the attributes of x as usual: x.key,\\nfor example. If x resides on disk, however, then the procedure must\\nperform the operation DISK-READ(x) to read the block containing\\nobject x into main memory before it can refer to x’s attributes. (Assume\\nthat if x is already in main memory, then DISK-READ(x) requires no\\ndisk accesses: it is a “no-op.”) Similarly, procedures call DISK-\\nWRITE(x) to save any changes that have been made to the attributes of\\nobject x by writing to disk the block containing x. Thus, the typical\\npattern for working with an object is as follows:\\nx = a pointer to some object\\nDISK-READ(x)\\noperations that access and/or modify the attributes of x\\nDISK-WRITE(x)// omitted if no attributes of x were changed\\nother operations that access but do not modify attributes of x\\nThe system can keep only a limited number of blocks in main memory\\nat any one time. Our B-tree algorithms assume that the system\\nautomatically ﬂushes from main memory blocks that are no longer in\\nuse.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 658}),\n",
              " Document(page_content='Since in most systems the running time of a B-tree algorithm\\ndepends primarily on the number of DISK-READ and DISK-WRITE\\noperations it performs, we typically want each of these operations to\\nread or write as much information as possible. Thus, a B-tree node is\\nusually as large as a whole disk block, and this size limits the number of\\nchildren a B-tree node can have.\\nFigure 18.3 A B-tree of height 2 containing over one billion keys. Shown inside each node x is\\nx.n, the number of keys in x. Each internal node and leaf contains 1000 keys. This B-tree has\\n1001 nodes at depth 1 and over one million leaves at depth 2.\\nLarge B-trees stored on disk drives often have branching factors\\nbetween 50 and 2000, depending on the size of a key relative to the size\\nof a block. A large branching factor dramatically reduces both the\\nheight of the tree and the number of disk accesses required to ﬁnd any\\nkey. Figure 18.3 shows a B-tree with a branching factor of 1001 and\\nheight 2 that can store over one billion keys. Nevertheless, if the root\\nnode is kept permanently in main memory, at most two disk accesses\\nsufﬁce to ﬁnd any key in this tree.\\n18.1\\xa0\\xa0\\xa0\\xa0Deﬁnition of B-trees\\nTo keep things simple, let’s assume, as we have for binary search trees\\nand red-black trees, that any satellite information associated with a key\\nresides in the same node as the key. In practice, you might actually store\\nwith each key just a pointer to another disk block containing the\\nsatellite information for that key. The pseudocode in this chapter\\nimplicitly assumes that the satellite information associated with a key, or', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 659}),\n",
              " Document(page_content='the pointer to such satellite information, travels with the key whenever\\nthe key is moved from node to node. A common variant on a B-tree,\\nknown as a B+-tree, stores all the satellite information in the leaves and\\nstores only keys and child pointers in the internal nodes, thus\\nmaximizing the branching factor of the internal nodes.\\nA B-tree T is a rooted tree with root T.root having the following\\nproperties:\\n1. Every node x has the following attributes:\\na. x.n, the number of keys currently stored in node x,\\nb. the x.n keys themselves, x.key1, x.key2, … , x.keyx.n, stored in\\nmonotonically increasing order, so that x.key1 ≤ x.key2 ≤ ⋯ ≤\\nx.keyx.n,\\nc. x.leaf, a boolean value that is TRUE if x is a leaf and FALSE\\nif x is an internal node.\\n2. Each internal node x also contains x.n + 1 pointers x.c1, x.c2, …\\n, x.cx.n+1 to its children. Leaf nodes have no children, and so\\ntheir ci attributes are undeﬁned.\\n3. The keys x.keyi separate the ranges of keys stored in each\\nsubtree: if ki is any key stored in the subtree with root x.ci, then\\nk1 ≤ x.key1 ≤ k2 ≤ x.key2 ≤ ⋯ ≤ x.keyx.n ≤ kx.n+1.\\n4. All leaves have the same depth, which is the tree’s height h.\\n5. Nodes have lower and upper bounds on the number of keys they\\ncan contain, expressed in terms of a ﬁxed integer t ≥ 2 called the\\nminimum degree of the B-tree:\\na. Every node other than the root must have at least t − 1 keys.\\nEvery internal node other than the root thus has at least t\\nchildren. If the tree is nonempty, the root must have at least\\none key.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 660}),\n",
              " Document(page_content='b. Every node may contain at most 2t − 1 keys. Therefore, an\\ninternal node may have at most 2t children. We say that a node\\nis full if it contains exactly 2t − 1 keys.3\\nThe simplest B-tree occurs when t = 2. Every internal node then has\\neither 2, 3, or 4 children, and it is a 2-3-4 tree. In practice, however,\\nmuch larger values of t yield B-trees with smaller height.\\nThe height of a B-tree\\nThe number of disk accesses required for most operations on a B-tree is\\nproportional to the height of the B-tree. The following theorem bounds\\nthe worst-case height of a B-tree.\\nFigure 18.4 A B-tree of height 3 containing a minimum possible number of keys. Shown inside\\neach node x is x.n.\\nTheorem 18.1\\nIf n ≥ 1, then for any n-key B-tree T of height h and minimum degree t ≥\\n2,\\nProof\\xa0\\xa0\\xa0By deﬁnition, the root of a nonempty B-tree T contains at least\\none key, and all other nodes contain at least t − 1 keys. Let h be the\\nheight of T. Then T contains at least 2 nodes at depth 1, at least 2t', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 661}),\n",
              " Document(page_content='nodes at depth 2, at least 2t2 nodes at depth 3, and so on, until at depth\\nh, it has at least 2th−1 nodes. Figure 18.4 illustrates such a tree for h =\\n3. The number n of keys therefore satisﬁes the inequality\\nso that th ≤ (n + 1)/2. Taking base-t logarithms of both sides proves the\\ntheorem.\\n▪\\nYou can see the power of B-trees as compared with red-black trees.\\nAlthough the height of the tree grows as O(log n) in both cases (recall\\nthat t is a constant), for B-trees the base of the logarithm can be many\\ntimes larger. Thus, B-trees save a factor of about lg t over red-black trees\\nin the number of nodes examined for most tree operations. Because\\nexamining an arbitrary node in a tree usually entails accessing the disk,\\nB-trees avoid a substantial number of disk accesses.\\nExercises\\n18.1-1\\nWhy isn’t a minimum degree of t = 1 allowed?\\n18.1-2\\nFor what values of t is the tree of Figure 18.1 a legal B-tree?\\n18.1-3\\nShow all legal B-trees of minimum degree 2 that store the keys 1, 2, 3, 4,\\n5.\\n18.1-4\\nAs a function of the minimum degree t, what is the maximum number\\nof keys that can be stored in a B-tree of height h?', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 662}),\n",
              " Document(page_content='18.1-5\\nDescribe the data structure that results if each black node in a red-black\\ntree absorbs its red children, incorporating their children with its own.\\n18.2\\xa0\\xa0\\xa0\\xa0Basic operations on B-trees\\nThis section presents the details of the operations B-TREE-SEARCH,\\nB-TREE-CREATE, and B-TREE-INSERT. These procedures observe\\ntwo conventions:\\nThe root of the B-tree is always in main memory, so that no\\nprocedure ever needs to perform a DISK-READ on the root. If\\nany changes to the root node occur, however, then DISK-WRITE\\nmust be called on the root.\\nAny nodes that are passed as parameters must already have had a\\nDISK-READ operation performed on them.\\nThe procedures are all “one-pass” algorithms that proceed downward\\nfrom the root of the tree, without having to back up.\\nSearching a B-tree\\nSearching a B-tree is much like searching a binary search tree, except\\nthat instead of making a binary, or “two-way,” branching decision at\\neach node, the search makes a multiway branching decision according\\nto the number of the node’s children. More precisely, at each internal\\nnode x, the search makes an (x.n + 1)-way branching decision.\\nThe procedure B-TREE-SEARCH generalizes the TREE-SEARCH\\nprocedure deﬁned for binary search trees on page 316. It takes as input\\na pointer to the root node x of a subtree and a key k to be searched for\\nin that subtree. The top-level call is thus of the form B-TREE-\\nSEARCH(T.root, k). If k is in the B-tree, then B-TREE-SEARCH\\nreturns the ordered pair (y, i) consisting of a node y and an index i such\\nthat y.keyi = k. Otherwise, the procedure returns NIL.\\nB-TREE-SEARCH(x, k)', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 663}),\n",
              " Document(page_content='1i = 1\\n2while i ≤ x.n and k > x.keyi\\n3 i = i + 1\\n4if i ≤ x.n and k == x.keyi\\n5 return (x, i)\\n6elseif x.leaf\\n7 returnNIL\\n8else DISK-READ(x.ci)\\n9 return B-TREE-SEARCH(x.ci, k)\\nUsing a linear-search procedure, lines 1–3 o f B-TREE-SEARCH ﬁnd\\nthe smallest index i such that k ≤ x.keyi, or else they set i to x.n + 1.\\nLines 4–5 check to see whether the search has discovered the key,\\nreturning if it has. Otherwise, if x is a leaf, then line 7 terminates the\\nsearch unsuccessfully, and if x is an internal node, lines 8–9 recurse to\\nsearch the appropriate subtree of x, after performing the necessary\\nDISK-READ on that child. Figure 18.1 illustrates the operation of B-\\nTREE-SEARCH. The blue nodes are those examined during a search\\nfor the key R.\\nAs in the TREE-SEARCH procedure for binary search trees, the\\nnodes encountered during the recursion form a simple path downward\\nfrom the root of the tree. The B-TREE-SEARCH procedure therefore\\naccesses O(h) = O(logt n) disk blocks, where h is the height of the B-tree\\nand n is the number of keys in the B-tree. Since x.n < 2t, the while loop\\nof lines 2–3 takes O(t) time within each node, and the total CPU time is\\nO(th) = O(t logtn).\\nCreating an empty B-tree\\nTo build a B-tree T, ﬁrst use the B-TREE-CREATE procedure on the\\nnext page to create an empty root node and then call the B-TREE-\\nINSERT procedure on page 508 to add new keys. Both of these\\nprocedures use an auxiliary procedure ALLOCATE-NODE, whose\\npseudocode we omit and which allocates one disk block to be used as a\\nnew node in O(1) time. A node created by ALLOCATE-NODE requires', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 664}),\n",
              " Document(page_content='no DISK-READ, since there is as yet no useful information stored on\\nthe disk for that node. B-TREE-CREATE requires O(1) disk operations\\nand O(1) CPU time.\\nB-TREE-CREATE(T)\\n1x = ALLOCATE-NODE()\\n2x.leaf = TRUE\\n3x.n = 0\\n4DISK-WRITE(x)\\n5T.root = x\\nInserting a key into a B-tree\\nInserting a key into a B-tree is signiﬁcantly more complicated than\\ninserting a key into a binary search tree. As with binary search trees,\\nyou search for the leaf position at which to insert the new key. With a B-\\ntree, however, you cannot simply create a new leaf node and insert it, as\\nthe resulting tree would fail to be a valid B-tree. Instead, you insert the\\nnew key into an existing leaf node. Since you cannot insert a key into a\\nleaf node that is full, you need an operation that splits a full node y\\n(having 2t − 1 keys) around its median key y.keyt into two nodes having\\nonly t − 1 keys each. The median key moves up into y’s parent to\\nidentify the dividing point between the two new trees. But if y’s parent is\\nalso full, you must split it before you can insert the new key, and thus\\nyou could end up splitting full nodes all the way up the tree.\\nTo avoid having to go back up the tree, just split every full node you\\nencounter as you go down the tree. In this way, whenever you need to\\nsplit a full node, you are assured that its parent is not full. Inserting a\\nkey into a B-tree then requires only a single pass down the tree from the\\nroot to a leaf.\\nSplitting a node in a B-tree\\nThe procedure B-TREE-SPLIT-CHILD on the facing page takes as\\ninput a nonfull internal node x (assumed to reside in main memory) and', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 665}),\n",
              " Document(page_content='an index i such that x.ci (also assumed to reside in main memory) is a\\nfull child of x. The procedure splits this child in two and adjusts x so\\nthat it has an additional child. To split a full root, you ﬁrst need to make\\nthe root a child of a new empty root node, so that you can use B-TREE-\\nSPLIT-CHILD. The tree thus grows in height by 1: splitting is the only\\nmeans by which the tree grows taller.\\nB-TREE-SPLIT-CHILD(x, i)\\n\\xa0\\xa01y = x.ci // full node to split\\n\\xa0\\xa02z = ALLOCATE-NODE()// z will take half of y\\n\\xa0\\xa03z.leaf = y.leaf\\n\\xa0\\xa04z.n = t − 1\\n\\xa0\\xa05for j = 1 to t − 1 // z gets y’s greatest keys …\\n\\xa0\\xa06z.keyj = y.keyj+t\\n\\xa0\\xa07if not y.leaf\\n\\xa0\\xa08for j = 1 to t // … and its corresponding children\\n\\xa0\\xa09 z.cj = y.cj+t\\n10y.n = t − 1 // y keeps t − 1 keys\\n11for j = x.n + 1 downto i + 1// shift x’s children to the right …\\n12x.cj+1 = x.cj\\n13x.ci+1 = z // … to make room for z as a child\\n14for j = x.ndownto i // shift the corresponding keys in x\\n15x.keyj+1 = x.keyj\\n16x.keyi = y.keyt // insert y’s median key\\n17x.n = x.n + 1 // x has gained a child\\n18DISK-WRITE(y)\\n19DISK-WRITE(z)\\n20DISK-WRITE(x)\\nFigure 18.5 illustrates how a node splits. B-TREE-SPLIT-CHILD\\nsplits the full node y = x.ci about its median key (S in the ﬁgure), which\\nmoves up into y’s parent node x. Those keys in y that are greater than', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 666}),\n",
              " Document(page_content='the median key move into a new node z, which becomes a new child of\\nx.\\nB-TREE-SPLIT-CHILD works by straightforward cutting and\\npasting. Node x is the parent of the node y being split, which is x’s ith\\nchild (set in line 1). Node y originally has 2t children and 2t − 1 keys,\\nbut splitting reduces y to t children and t − 1 keys. The t largest children\\nand t − 1 keys of node y move over to node z, which becomes a new\\nchild of x, positioned just after y in x’s table of children. The median\\nkey of y moves up to become the key in node x that separates the\\npointers to nodes y and z.\\nLines 2–9 create node z and give it the largest t − 1 keys and, if y and\\nz are internal nodes, the corresponding t children of y. Line 10 adjusts\\nthe key count for y. Then, lines 11–17 shift keys and child pointers in x\\nto the right in order to make room for x’s new child, insert z as a new\\nchild of x, move the median key from y up to x in order to separate y\\nfrom z, and adjust x’s key count. Lines 18–20 write out all modiﬁed disk\\nblocks. The CPU time used by B-TREE-SPLIT-CHILD is Θ (t), due to\\nthe for loops in lines 5–6 and 8–9. (The for loops in lines 11–12 and 14–\\n15 also run for O(t) iterations.) The procedure performs O(1) disk\\noperations.\\nFigure 18.5 Splitting a node with t = 4. Node y = x.ci splits into two nodes, y and z, and the\\nmedian key S of y moves up into y’s parent.\\nInserting a key into a B-tree in a single pass down the tree', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 667}),\n",
              " Document(page_content='Inserting a key k into a B-tree T of height h requires just a single pass\\ndown the tree and O(h) disk accesses. The CPU time required is O(th) =\\nO(t logt n). The B-TREE-INSERT procedure uses B-TREE-SPLIT-\\nCHILD to guarantee that the recursion never descends to a full node. If\\nthe root is full, B-TREE-INSERT splits it by calling the procedure B-\\nTREE-SPLIT-ROOT on the facing page.\\nB-TREE-INSERT(T, k)\\n1r = T.root\\n2if r.n == 2t − 1\\n3 s = B-TREE-SPLIT-ROOT(T)\\n4 B-TREE-INSERT-NONFULL(s, k)\\n5else B-TREE-INSERT-NONFULL(r, k)\\nB-TREE-INSERT works as follows. If the root is full, then line 3\\ncalls B-TREE-SPLIT-ROOT in line 3 to split it. A new node s (with\\ntwo children) becomes the root and is returned by B-TREE-SPLIT-\\nROOT. Splitting the root, illustrated in Figure 18.6, is the only way to\\nincrease the height of a B-tree. Unlike a binary search tree, a B-tree\\nincreases in height at the top instead of at the bottom. Regardless of\\nwhether the root split, B-TREE-INSERT ﬁnishes by calling B-TREE-\\nINSERT-NONFULL to insert key k into the tree rooted at the nonfull\\nroot node, which is either the new root (the call in line 4) or the original\\nroot (the call in line 5).', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 668}),\n",
              " Document(page_content='Figure 18.6 Splitting the root with t = 4. Root node r splits in two, and a new root node s is\\ncreated. The new root contains the median key of r and has the two halves of r as children. The\\nB-tree grows in height by one when the root is split. A B-tree’s height increases only when the\\nroot splits.\\nB-TREE-SPLIT-ROOT(T)\\n1s = ALLOCATE-NODE()\\n2s.leaf = FALSE\\n3s.n = 0\\n4s.c1 = T.root\\n5T.root = s\\n6B-TREE-SPLIT-CHILD(s, 1)\\n7return s\\nThe auxiliary procedure B-TREE-INSERT-NONFULL on page 511\\ninserts key k into node x, which is assumed to be nonfull when the\\nprocedure is called. B-TREEINSERT-NONFULL recurses as\\nnecessary down the tree, at all times guaranteeing that the node to\\nwhich it recurses is not full by calling B-TREE-SPLIT-CHILD as\\nnecessary. The operation of B-TREE-INSERT and the recursive\\noperation of B-TREE-INSERT-NONFULL guarantee that this\\nassumption is true.\\nFigure 18.7 illustrates the various cases of how B-TREE-INSERT-\\nNONFULL inserts a key into a B-tree. Lines 3–8 handle the case in\\nwhich x is a leaf node by inserting key k into x, shifting to the right all', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 669}),\n",
              " Document(page_content='keys in x that are greater than k. If x is not a leaf node, then k should go\\ninto the appropriate leaf node in the subtree rooted at internal node x.\\nLines 9–11 determine the child x.ci to which the recursion descends.\\nLine 13 detects whether the recursion would descend to a full child, in\\nwhich case line 14 calls B-TREE-SPLIT-CHILD to split that child into\\ntwo nonfull children, and lines 15–16 determine which of the two\\nchildren is the correct one to descend to. (Note that DISK-READ(x.ci)\\nis not needed after line 16 increments i, since the recursion descends in\\nthis case to a child that was just created by B-TREE-SPLIT-CHILD.)\\nThe net effect of lines 13–16 is thus to guarantee that the procedure\\nnever recurses to a full node. Line 17 then recurses to insert k into the\\nappropriate subtree.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 670}),\n",
              " Document(page_content='Figure 18.7 Inserting keys into a B-tree. The minimum degree t for this B-tree is 3, so that a node\\ncan hold at most 5 keys. Blue nodes are modiﬁed by the insertion process. (a) The initial tree for\\nthis example. (b) The result of inserting B into the initial tree. This case is a simple insertion into\\na leaf node. (c) The result of inserting Q into the previous tree. The node RST U V splits into\\ntwo nodes containing RS and U V, the key T moves up to the root, and Q is inserted in the\\nleftmost of the two halves (the RS node). (d) The result of inserting L into the previous tree. The\\nroot splits right away, since it is full, and the B-tree grows in height by one. Then L is inserted\\ninto the leaf containing JK. (e) The result of inserting F into the previous tree. The node\\nABCDE splits before F is inserted into the rightmost of the two halves (the DE node).\\nB-TREE-INSERT-NONFULL(x, k)', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 671}),\n",
              " Document(page_content='\\xa0\\xa01i = x.n\\n\\xa0\\xa02if x.leaf // inserting into a leaf?\\n\\xa0\\xa03while i ≥ 1 and k < x.keyi// shift keys in x to make room for k\\n\\xa0\\xa04 x.keyi+1 = x.keyi\\n\\xa0\\xa05 i = i − 1\\n\\xa0\\xa06x.keyi+1 = k // insert key k in x\\n\\xa0\\xa07x.n = x.n + 1 // now x has 1 more key\\n\\xa0\\xa08DISK-WRITE(x)\\n\\xa0\\xa09else while i ≥ 1 and k < x.keyi// ﬁnd the child where k belongs\\n10 i = i − 1\\n11i = i + 1\\n12DISK-READ(x.ci)\\n13if x.ci.n == 2t − 1 // split the child if it’s full\\n14 B-TREE-SPLIT-CHILD(x, i)\\n15 if k > x.keyi // does k go into x.ci or x.ci+1?\\n16 i = i + 1\\n17B-TREE-INSERT-NONFULL(x.ci, k)\\nFor a B-tree of height h, B-TREE-INSERT performs O(h) disk\\naccesses, since only O(1) DISK-READ and DISK-WRITE operations\\noccur at each level of the tree. The total CPU time used is O(t) in each\\nlevel of the tree, or O(th) = O(t logt n) overall. Since B-TREE-INSERT-\\nNONFULL is tail-recursive, you can instead implement it with a while\\nloop, thereby demonstrating that the number of blocks that need to be\\nin main memory at any time is O(1).\\nExercises\\n18.2-1\\nShow the results of inserting the keys\\nF, S, Q, K, C, L, H, T, V, W, M, R, N, P, A, B, X, Y, D, Z, E\\nin order into an empty B-tree with minimum degree 2. Draw only the\\nconﬁgurations of the tree just before some node must split, and also', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 672}),\n",
              " Document(page_content='draw the ﬁnal conﬁguration.\\n18.2-2\\nExplain under what circumstances, if any, redundant DISK-READ or\\nDISK-WRITE operations occur during the course of executing a call to\\nB-TREE-INSERT. (A redundant DISK-READ is a DISK-READ for a\\nblock that is already in memory. A redundant DISK-WRITE writes to\\ndisk a block of information that is identical to what is already stored\\nthere.)\\n18.2-3\\nProfessor Bunyan asserts that the B-TREE-INSERT procedure always\\nresults in a B-tree with the minimum possible height. Show that the\\nprofessor is mistaken by proving that with t = 2 and the set of keys {1,\\n2, … , 15}, there is no insertion sequence that results in a B-tree with the\\nminimum possible height.\\n★ 18.2-4\\nIf you insert the keys {1, 2, … , n} into an empty B-tree with minimum\\ndegree 2, how many nodes does the ﬁnal B-tree have?\\n18.2-5\\nSince leaf nodes require no pointers to children, they could conceivably\\nuse a different (larger) t value than internal nodes for the same disk\\nblock size. Show how to modify the procedures for creating and\\ninserting into a B-tree to handle this variation.\\n18.2-6\\nSuppose that you implement B-TREE-SEARCH to use binary search\\nrather than linear search within each node. Show that this change makes\\nthe required CPU time O(lg n), independent of how t might be chosen\\nas a function of n.\\n18.2-7\\nSuppose that disk hardware allows you to choose the size of a disk\\nblock arbitrarily, but that the time it takes to read the disk block is\\na+bt, where a and b are speciﬁed constants and t is the minimum degree', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 673}),\n",
              " Document(page_content='for a B-tree using blocks of the selected size. Describe how to choose t\\nso as to minimize (approximately) the B-tree search time. Suggest an\\noptimal value of t for the case in which a = 5 milliseconds and b = 10\\nmicroseconds.\\n18.3\\xa0\\xa0\\xa0\\xa0Deleting a key from a B-tree\\nDeletion from a B-tree is analogous to insertion but a little more\\ncomplicated, because you can delete a key from any node—not just a\\nleaf—and when you delete a key from an internal node, you must\\nrearrange the node’s children. As in insertion, you must guard against\\ndeletion producing a tree whose structure violates the B-tree properties.\\nJust as a node should not get too big due to insertion, a node must not\\nget too small during deletion (except that the root is allowed to have\\nfewer than the minimum number t − 1 of keys). And just as a simple\\ninsertion algorithm might have to back up if a node on the path to\\nwhere the key is to be inserted is full, a simple approach to deletion\\nmight have to back up if a node (other than the root) along the path to\\nwhere the key is to be deleted has the minimum number of keys.\\nThe procedure B-TREE-DELETE deletes the key k from the subtree\\nrooted at x. Unlike the procedures TREE-DELETE on page 325 and\\nRB-DELETE on page 348, which are given the node to delete—\\npresumably as the result of a prior search—B-TREE-DELETE\\ncombines the search for key k with the deletion process. Why do we\\ncombine search and deletion in B-TREE-DELETE? Just as B-TREE-\\nINSERT prevents any node from becoming overfull (having more than\\n2t − 1 keys) while making a single pass down the tree, B-TREE-\\nDELETE prevents any node from becoming underfull (having fewer\\nthan t − 1 keys) while also making a single pass down the tree, searching\\nfor and ultimately deleting the key.\\nTo prevent any node from becoming underfull, the design of B-\\nTREE-DELETE guarantees that whenever it calls itself recursively on a\\nnode x, the number of keys in x is at least the minimum degree t at the\\ntime of the call. (Although the root may have fewer than t keys and a\\nrecursive call may be made from the root, no recursive call is made on', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 674}),\n",
              " Document(page_content='the root.) This condition requires one more key than the minimum\\nrequired by the usual B-tree conditions, and so a key might have to be\\nmoved from x into one of its child nodes (still leaving x with at least the\\nminimum t − 1 keys) before a recursive call is made on that child, thus\\nallowing deletion to occur in one downward pass without having to\\ntraverse back up the tree.\\nWe describe how the procedure B-TREE-DELETE(T, k) deletes a\\nkey k from a B-tree T instead of presenting detailed pseudocode. We\\nexamine three cases, illustrated in Figure 18.8. The cases are for when\\nthe search arrives at a leaf, at an internal node containing key k, and at\\nan internal node not containing key k. As mentioned above, in all three\\ncases node x has at least t keys (with the possible exception of when x is\\nthe root). Cases 2 and 3—when x is an internal node—guarantee this\\nproperty as the recursion descends through the B-tree.\\n', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 675}),\n",
              " Document(page_content='Figure 18.8 Deleting keys from a B-tree. The minimum degree for this B-tree is t = 3, so that,\\nother than the root, every node must have at least 2 keys. Blue nodes are those that are modiﬁed\\nby the deletion process. (a) The B-tree of Figure 18.7(e). (b) Deletion of F, which is case 1:\\nsimple deletion from a leaf when all nodes visited during the search (other than the root) have at\\nleast t = 3 keys. (c) Deletion of M, which is case 2a: the predecessor L of M moves up to take\\nM’s position. (d) Deletion of G, which is case 2c: push G down to make node DEGJK and then\\ndelete G from this leaf (case 1). (e) Deletion of D, which is case 3b: since the recursion cannot\\ndescend to node CL because it has only 2 keys, push P down and merge it with CL and TX to\\nform CLP TX. Then delete D from a leaf (case 1). (e0) After (e), delete the empty root. The tree\\nshrinks in height by 1. (f) Deletion of B, which is case 3a: C moves to ﬁll B’s position and E\\nmoves to ﬁll C’s position.\\nCase 1: The search arrives at a leaf node x. If x contains key k, then\\ndelete k from x. If x does not contain key k, then k was not in the B-\\ntree and nothing else needs to be done.\\nCase 2: The search arrives at an internal node  x that contains key k. Let k\\n= x.keyi. One of the following three cases applies, depending on the\\nnumber of keys in x.ci (the child of x that precedes k) and x.ci+1 (the\\nchild of x that follows k).\\nCase 2a: x.ci has at least t keys. Find the predecessor k′ of k in the\\nsubtree rooted at x.ci. Recursively delete k′ from x.ci, and replace k by\\nk′ in x. (Key k′ can be found and deleted in a single downward pass.)', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 676}),\n",
              " Document(page_content='Case 2b: x.ci has t − 1 keys and x.ci+1has at least t keys. This case is\\nsymmetric to case 2a. Find the successor k′ of k in the subtree rooted\\nat x.ci+1. Recursively delete k′ from x.ci+1, and replace k by k′ in x.\\n(Again, ﬁnding and deleting k′ can be done in a single downward\\npass.)\\nCase 2c: Both x.ci and x.ci+1have t − 1 keys. Merge k and all of x.ci+1\\ninto x.ci, so that x loses both k and the pointer to x.ci+1, and x.ci now\\ncontains 2t − 1 keys. Then free x.ci+1 and recursively delete k from\\nx.ci.\\nCase 3: The search arrives at an internal node  x that does not contain key\\nk. Continue searching down the tree while ensuring that each node\\nvisited has at least t keys. To do so, determine the root x.ci of the\\nappropriate subtree that must contain k, if k is in the tree at all. If x.ci\\nhas only t − 1 keys, execute case 3a or 3b as necessary to guarantee\\ndescending to a node containing at least t keys. Then ﬁnish by\\nrecursing on the appropriate child of x.\\nCase 3a: x.ci has only t − 1 keys but has an immediate sibling with at\\nleast t keys. Give x.ci an extra key by moving a key from x down into\\nx.ci, moving a key from x.ci’s immediate left or right sibling up into x,\\nand moving the appropriate child pointer from the sibling into x.ci.\\nCase 3b: x.ci and each of x.ci’s immediate siblings have t − 1 keys. (It is\\npossible for x.ci to have either one or two siblings.) Merge x.ci with\\none sibling, which involves moving a key from x down into the new\\nmerged node to become the median key for that node.\\nIn cases 2c and 3b, if node x is the root, it could end up having no\\nkeys. When this situation occurs, then x is deleted, and x’s only child\\nx.c1 becomes the new root of the tree. This action decreases the height\\nof the tree by one and preserves the property that the root of the tree\\ncontains at least one key (unless the tree is empty).\\nSince most of the keys in a B-tree are in the leaves, deletion\\noperations often end up deleting keys from leaves. The B-TREE-', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 677}),\n",
              " Document(page_content='DELETE procedure then acts in one downward pass through the tree,\\nwithout having to back up. When deleting a key in an internal node x,\\nhowever, the procedure might make a downward pass through the tree\\nto ﬁnd the key’s predecessor or successor and then return to node x to\\nreplace the key with its predecessor or successor (cases 2a and 2b).\\nReturning to node x does not require a traversal through all the levels\\nbetween x and the node containing the predecessor or successor,\\nhowever, since the procedure can just keep a pointer to x and the key\\nposition within x and put the predecessor or successor key directly\\nthere.\\nAlthough this procedure seems complicated, it involves only O(h)\\ndisk operations for a B-tree of height h, since only O(1) calls to DISK-\\nREAD and DISK-WRITE are made between recursive invocations of\\nthe procedure. The CPU time required is O(th) = O(t logtn).\\nExercises\\n18.3-1\\nShow the results of deleting C, P, and V, in order, from the tree of\\nFigure 18.8(f).\\n18.3-2\\nWrite pseudocode for B-TREE-DELETE.\\nProblems\\n18-1\\xa0\\xa0\\xa0\\xa0\\xa0Stacks on secondary storage\\nConsider implementing a stack in a computer that has a relatively small\\namount of fast primary memory and a relatively large amount of slower\\ndisk storage. The operations PUSH and POP work on single-word\\nvalues. The stack can grow to be much larger than can ﬁt in memory,\\nand thus most of it must be stored on disk.\\nA simple, but inefﬁcient, stack implementation keeps the entire stack\\non disk. Maintain in memory a stack pointer, which is the disk address\\nof the top element on the stack. Indexing block numbers and word', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 678}),\n",
              " Document(page_content='offsets within blocks from 0, if the pointer has value p, the top element\\nis the (p mod m)th word on block ⌊p/m ⌋ of the disk, where m is the\\nnumber of words per block.\\nTo implement the PUSH operation, increment the stack pointer,\\nread the appropriate block into memory from disk, copy the element to\\nbe pushed to the appropriate word on the block, and write the block\\nback to disk. A POP operation is similar. Read in the appropriate block\\nfrom disk, save the top of the stack, decrement the stack pointer, and\\nreturn the saved value. You need not write back the block, since it was\\nnot modiﬁed, and the word in the block that contained the popped\\nvalue is ignored.\\nAs in the analyses of B-tree operations, two costs matter: the total\\nnumber of disk accesses and the total CPU time. A disk access also\\nincurs a cost in CPU time. In particular, any disk access to a block of m\\nwords incurs charges of one disk access and Θ (m) CPU time.\\na. Asymptotically, what is the worst-case number of disk accesses for n\\nstack operations using this simple implementation? What is the CPU\\ntime for n stack operations? Express your answer in terms of m and n\\nfor this and subsequent parts.\\nNow consider a stack implementation in which you keep one block of\\nthe stack in memory. (You also maintain a small amount of memory to\\nrecord which block is currently in memory.) You can perform a stack\\noperation only if the relevant disk block resides in memory. If necessary,\\nyou can write the block currently in memory to the disk and read the\\nnew block from the disk into memory. If the relevant disk block is\\nalready in memory, then no disk accesses are required.\\nb. What is the worst-case number of disk accesses required for n PUSH\\noperations? What is the CPU time?\\nc. What is the worst-case number of disk accesses required for n stack\\noperations? What is the CPU time?\\nSuppose that you now implement the stack by keeping two blocks in\\nmemory (in addition to a small number of words for bookkeeping).', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 679}),\n",
              " Document(page_content='d. Describe how to manage the stack blocks so that the amortized\\nnumber of disk accesses for any stack operation is O(1/m) and the\\namortized CPU time for any stack operation is O(1).\\n18-2\\xa0\\xa0\\xa0\\xa0\\xa0Joining and splitting 2-3-4 trees\\nThe join operation takes two dynamic sets S′ and S″ and an element x\\nsuch that x′.key < x.key < x″.key for any x′ ∈ S′ and x″ ∈ S″. It returns\\na set S = S′ ∪ {x} ∪ S″. The split operation is like an “inverse” join:\\ngiven a dynamic set S and an element x ∈ S, it creates a set S′ that\\nconsists of all elements in S − {x} whose keys are less than x.key and\\nanother set S″ that consists of all elements in S − {x} whose keys are\\ngreater than x.key. This problem investigates how to implement these\\noperations on 2-3-4 trees (B-trees with t = 2). Assume for convenience\\nthat elements consist only of keys and that all key values are distinct.\\na. Show how to maintain, for every node x of a 2-3-4 tree, the height of\\nthe subtree rooted at x as an attribute x.height. Make sure that your\\nimplementation does not affect the asymptotic running times of\\nsearching, insertion, and deletion.\\nb. Show how to implement the join operation. Given two 2-3-4 trees T′\\nand T″ and a key k, the join operation should run in O(1 + |h′ − h″|)\\ntime, where h′ and h″ are the heights of T′ and T″, respectively.\\nc. Consider the simple path p from the root of a 2-3-4 tree T to a given\\nkey k, the set S′ of keys in T that are less than k, and the set S″ of keys\\nin T that are greater than k. Show that p breaks S′ into a set of trees \\n and a set of keys \\n  such that \\n  for i\\n= 1, 2, … , m and any keys \\n  and \\n . What is the relationship\\nbetween the heights of \\n and \\n? Describe how p breaks S″ into sets\\nof trees and keys.\\nd. Show how to implement the split operation on T. Use the join\\noperation to assemble the keys in S′ into a single 2-3-4 tree T′ and the\\nkeys in S″ into a single 2-3-4 tree T″. The running time of the split\\noperation should be O(lg n), where n is the number of keys in T. (Hint:\\nThe costs for joining should telescope.)', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 680}),\n",
              " Document(page_content='Chapter notes\\nKnuth [261], Aho, Hopcroft, and Ullman [5], and Sedgewick and\\nWayne [402] give further discussions of balanced-tree schemes and B-\\ntrees. Comer [99] provides a comprehensive survey of B-trees. Guibas\\nand Sedgewick [202] discuss the relationships among various kinds of\\nbalanced-tree schemes, including red-black trees and 2-3-4 trees.\\nIn 1970, J. E. Hopcroft invented 2-3 trees, a precursor to B-trees and\\n2-3-4 trees, in which every internal node has either two or three children.\\nBayer and McCreight [39] introduced B-trees in 1972 with no\\nexplanation of their choice of name.\\nBender, Demaine, and Farach-Colton [47] studied how to make B-\\ntrees perform well in the presence of memory-hierarchy effects. Their\\ncache-oblivious algorithms work efﬁciently without explicitly knowing\\nthe data transfer sizes within the memory hierarchy.\\n1 When specifying disk capacities, one terabyte is one trillion bytes, rather than 240 bytes.\\n2 SSDs also exhibit greater latency than main memory and access data in blocks.\\n3 Another common variant on a B-tree, known as a B*-tree, requires each internal node to be at\\nleast 2/3 full, rather than at least half full, as a B-tree requires.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 681}),\n",
              " Document(page_content='19\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0Data Structures for Disjoint Sets\\nSome applications involve grouping n distinct elements into a collection\\nof disjoint sets—sets with no elements in common. These applications\\noften need to perform two operations in particular: ﬁnding the unique\\nset that contains a given element and uniting two sets. This chapter\\nexplores methods for maintaining a data structure that supports these\\noperations.\\nSection 19.1 describes the operations supported by a disjoint-set data\\nstructure and presents a simple application. Section 19.2 looks at a\\nsimple linked-list implementation for disjoint sets. Section 19.3 presents\\na more efﬁcient representation using rooted trees. The running time\\nusing the tree representation is theoretically superlinear, but for all\\npractical purposes it is linear. Section 19.4 deﬁnes and discusses a very\\nquickly growing function and its very slowly growing inverse, which\\nappears in the running time of operations on the tree-based\\nimplementation, and then, by a complex amortized analysis, proves an\\nupper bound on the running time that is just barely superlinear.\\n19.1\\xa0\\xa0\\xa0\\xa0Disjoint-set operations\\nA disjoint-set data structure maintains a collection S = {S1, S2, … , Sk}\\nof disjoint dynamic sets. To identify each set, choose a representative,\\nwhich is some member of the set. In some applications, it doesn’t matter\\nwhich member is used as the representative; it matters only that if you\\nask for the representative of a dynamic set twice without modifying the', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 682}),\n",
              " Document(page_content='set between the requests, you get the same answer both times. Other\\napplications may require a prespeciﬁed rule for choosing the\\nrepresentative, such as choosing the smallest member in the set (for a set\\nwhose elements can be ordered).\\nAs in the other dynamic-set implementations we have studied, each\\nelement of a set is represented by an object. Letting x denote an object,\\nwe’ll see how to support the following operations:\\nMAKE-SET(x), where x does not already belong to some other set,\\ncreates a new set whose only member (and thus representative) is x.\\nUNION(x, y) unites two disjoint, dynamic sets that contain x and y, say\\nSx and Sy, into a new set that is the union of these two sets. The\\nrepresentative of the resulting set is any member of Sx ∪ Sy, although\\nmany implementations of UNION speciﬁcally choose the\\nrepresentative of either Sx or Sy as the new representative. Since the\\nsets in the collection must at all times be disjoint, the UNION\\noperation destroys sets Sx and Sy, removing them from the collection\\nS. In practice, implementations often absorb the elements of one of\\nthe sets into the other set.\\nFIND-SET(x) returns a pointer to the representative of the unique set\\ncontaining x.\\nThroughout this chapter, we’ll analyze the running times of disjoint-\\nset data structures in terms of two parameters: n, the number of MAKE-\\nSET operations, and m, the total number of MAKE-SET, UNION, and\\nFIND-SET operations. Because the total number of operations m\\nincludes the n MAKE-SET operations, m ≥ n. The ﬁrst n operations are\\nalways MAKE-SET operations, so that after the ﬁrst n operations, the\\ncollection consists of n singleton sets. Since the sets are disjoint at all\\ntimes, each UNION operation reduces the number of sets by 1. After n −\\n1 UNION operations, therefore, only one set remains, and so at most n −\\n1 UNION operations can occur.\\nAn application of disjoint-set data structures', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 683}),\n",
              " Document(page_content='One of the many applications of disjoint-set data structures arises in\\ndetermining the connected components of an undirected graph (see\\nSection B.4). Figure 19.1(a), for example, shows a graph with four\\nconnected components.\\nThe procedure CONNECTED-COMPONENTS on the following\\npage uses the disjoint-set operations to compute the connected\\ncomponents of a graph. Once the CONNECTED-COMPONENTS\\nprocedure has preprocessed the graph, the procedure SAME-\\nCOMPONENT answers queries about whether two vertices belong to\\nthe same connected component. In pseudocode, we denote the set of\\nvertices of a graph G by G.V and the set of edges by G.E.\\nThe procedure CONNECTED-COMPONENTS initially places each\\nvertex v in its own set. Then, for each edge (u, v), it unites the sets\\ncontaining u and v. By Exercise 19.1-2, after all the edges are processed,\\ntwo vertices belong to the same connected component if and only if the\\nobjects corresponding to the vertices belong to the same set. Thus\\nCONNECTED-COMPONENTS computes sets in such a way that the\\nprocedure SAME-COMPONENT can determine whether two vertices\\nare in the same connected component. Figure 19.1(b) illustrates how\\nCONNECTED-COMPONENTS computes the disjoint sets.\\nFigure 19.1 (a) A graph with four connected components: {a, b, c, d}, {e, f, g}, {h, i}, and {j }. (b)\\nThe collection of disjoint sets after processing each edge.\\nCONNECTED-COMPONENTS(G)', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 684}),\n",
              " Document(page_content='1for each vertex v ∈ G.V\\n2 MAKE-SET(v)\\n3for each edge (u, v) ∈ G.E\\n4 if FIND-SET(u) ≠ FIND-SET(v)\\n5 UNION(u, v)\\nSAME-COMPONENT(u, v)\\n1if FIND-SET(u) == FIND-SET(v)\\n2 return TRUE\\n3else returnFALSE\\nIn an actual implementation of this connected-components\\nalgorithm, the representations of the graph and the disjoint-set data\\nstructure would need to reference each other. That is, an object\\nrepresenting a vertex would contain a pointer to the corresponding\\ndisjoint-set object, and vice versa. Since these programming details\\ndepend on the implementation language, we do not address them further\\nhere.\\nWhen the edges of the graph are static—not changing over time—\\ndepth-ﬁrst search can compute the connected components faster (see\\nExercise 20.3-12 on page 572). Sometimes, however, the edges are added\\ndynamically, with the connected components updated as each edge is\\nadded. In this case, the implementation given here can be more efﬁcient\\nthan running a new depth-ﬁrst search for each new edge.\\nExercises\\n19.1-1\\nThe CONNECTED-COMPONENTS procedure is run on the\\nundirected graph G = (V, E), where V = {a, b, c, d, e, f, g, h, i, j, k}, and\\nthe edges of E are processed in the order (d, i), (f, k), (g, i), (b, g), (a, h),\\n(i, j), (d, k), (b, j), (d, f), (g, j), (a, e). List the vertices in each connected\\ncomponent after each iteration of lines 3–5.\\n19.1-2\\nShow that after all edges are processed by CONNECTED-\\nCOMPONENTS, two vertices belong to the same connected component', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 685}),\n",
              " Document(page_content='if and only if they belong to the same set.\\n19.1-3\\nDuring the execution of CONNECTED-COMPONENTS on an\\nundirected graph G = (V, E) with k connected components, how many\\ntimes is FIND-SET called? How many times is UNION called? Express\\nyour answers in terms of |V |, |E|, and k.\\n19.2\\xa0\\xa0\\xa0\\xa0Linked-list representation of disjoint sets\\nFigure 19.2(a) shows a simple way to implement a disjoint-set data\\nstructure: each set is represented by its own linked list. The object for\\neach set has attributes head, pointing to the ﬁrst object in the list, and\\ntail, pointing to the last object. Each object in the list contains a set\\nmember, a pointer to the next object in the list, and a pointer back to the\\nset object. Within each linked list, the objects may appear in any order.\\nThe representative is the set member in the ﬁrst object in the list.\\nWith this linked-list representation, both MAKE-SET and FIND-\\nSET require only O(1) time. To carry out MAKE-SET(x), create a new\\nlinked list whose only object is x. For FIND-SET(x), just follow the\\npointer from x back to its set object and then return the member in the\\nobject that head points to. For example, in Figure 19.2(a), the call FIND-\\nSET(g) returns f.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 686}),\n",
              " Document(page_content='Figure 19.2 (a) Linked-list representations of two sets. Set S1 contains members d, f, and g, with\\nrepresentative f, and set S2 contains members b, c, e, and h, with representative c. Each object in\\nthe list contains a set member, a pointer to the next object in the list, and a pointer back to the set\\nobject. Each set object has pointers head and tail to the ﬁrst and last objects, respectively. (b) The\\nresult of UNION(g, e), which appends the linked list containing e to the linked list containing g.\\nThe representative of the resulting set is f. The set object for e’s list, S2, is destroyed.\\nA simple implementation of union\\nThe simplest implementation of the UNION operation using the linked-\\nlist set representation takes signiﬁcantly more time than MAKE-SET or\\nFIND-SET. As Figure 19.2(b) shows, the operation UNION(x, y)\\nappends y’s list onto the end of x’s list. The representative of x’s list\\nbecomes the representative of the resulting set. To quickly ﬁnd where to\\nappend y’s list, use the tail pointer for x’s list. Because all members of y’s\\nlist join x’s list, the UNION operation destroys the set object for y’s list.\\nThe UNION operation is where this implementation pays the price for\\nFIND-SET taking constant time: UNION must also update the pointer\\nto the set object for each object originally on y’s list, which takes time\\nlinear in the length of y’s list. In Figure 19.2, for example, the operation\\nUNION(g, e) causes pointers to be updated in the objects for b, c, e, and\\nh.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 687}),\n",
              " Document(page_content='In fact, we can construct a sequence of m operations on n objects that\\nrequires Θ (n2) time. Starting with objects x1, x2, … , xn, execute the\\nsequence of n MAKE-SET operations followed by n − 1 UNION\\noperations shown in Figure 19.3, so that m = 2n−1. The n MAKE-SET\\noperations take Θ (n) time. Because the ith UNION operation updates i\\nobjects, the total number of objects updated by all n−1 UNION\\noperations forms an arithmetic series:\\nFigure 19.3 A sequence of 2n − 1 operations on n objects that takes Θ (n2) time, or Θ (n) time per\\noperation on average, using the linked-list set representation and the simple implementation of\\nUNION.\\nThe total number of operations is 2n−1, and so each operation on\\naverage requires Θ (n) time. That is, the amortized time of an operation is\\nΘ(n).\\nA weighted-union heuristic\\nIn the worst case, the above implementation of UNION requires an\\naverage of Θ (n) time per call, because it might be appending a longer list\\nonto a shorter list, and the procedure must update the pointer to the set\\nobject for each member of the longer list. Suppose instead that each list\\nalso includes the length of the list (which can be maintained\\nstraightforwardly with constant overhead) and that the UNION', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 688}),\n",
              " Document(page_content='procedure always appends the shorter list onto the longer, breaking ties\\narbitrarily. With this simple weighted-union heuristic, a single UNION\\noperation can still take Ω(n) time if both sets have Ω(n) members. As the\\nfollowing theorem shows, however, a sequence of m MAKE-SET,\\nUNION, and FIND-SET operations, n of which are MAKE-SET\\noperations, takes O(m + n lg n) time.\\nTheorem 19.1\\nUsing the linked-list representation of disjoint sets and the weighted-\\nunion heuristic, a sequence of m MAKE-SET, UNION, and FIND-SET\\noperations, n of which are MAKE-SET operations, takes O(m + n lg n)\\ntime.\\nProof\\xa0\\xa0\\xa0Because each UNION operation unites two disjoint sets, at most\\nn − 1 UNION operations occur over all. We now bound the total time\\ntaken by these UNION operations. We start by determining, for each\\nobject, an upper bound on the number of times the object’s pointer back\\nto its set object is updated. Consider a particular object x. Each time x’s\\npointer is updated, x must have started in the smaller set. The ﬁrst time\\nx’s pointer is updated, therefore, the resulting set must have at least 2\\nmembers. Similarly, the next time x’s pointer is updated, the resulting set\\nmust have had at least 4 members. Continuing on, for any k ≤ n, after x’s\\npointer has been updated ⌈lg k ⌉ times, the resulting set must have at least\\nk members. Since the largest set has at most n members, each object’s\\npointer is updated at most ⌈lg n ⌉ times over all the UNION operations.\\nThus the total time spent updating object pointers over all UNION\\noperations is O(n lg n). We must also account for updating the tail\\npointers and the list lengths, which take only Θ (1) time per UNION\\noperation. The total time spent in all UNION operations is thus O(n lg\\nn).\\nThe time for the entire sequence of m operations follows. Each\\nMAKE-SET and FIND-SET operation takes O(1) time, and there are\\nO(m) of them. The total time for the entire sequence is thus O(m + n lg\\nn).\\n▪', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 689}),\n",
              " Document(page_content='Exercises\\n19.2-1\\nWrite pseudocode for MAKE-SET, FIND-SET, and UNION using the\\nlinked-list representation and the weighted-union heuristic. Make sure to\\nspecify the attributes that you assume for set objects and list objects.\\n19.2-2\\nShow the data structure that results and the answers returned by the\\nFIND-SET operations in the following program. Use the linked-list\\nrepresentation with the weighted-union heuristic. Assume that if the sets\\ncontaining xi and xj have the same size, then the operation UNION(xi,\\nxj) appends xj’s list onto xi’s list.\\n\\xa0\\xa01for i = 1 to 16\\n\\xa0\\xa02MAKE-SET(xi)\\n\\xa0\\xa03for i = 1 to 15 by 2\\n\\xa0\\xa04UNION(xi, xi+1)\\n\\xa0\\xa05for i = 1 to 13 by 4\\n\\xa0\\xa06UNION(xi, xi+2)\\n\\xa0\\xa07UNION(x1, x5)\\n\\xa0\\xa08UNION(x11, x13)\\n\\xa0\\xa09UNION(x1, x10)\\n10FIND-SET(x2)\\n11FIND-SET(x9)\\n19.2-3\\nAdapt the aggregate proof of Theorem 19.1 to obtain amortized time\\nbounds of O(1) for MAKE-SET and FIND-SET and O(lg n) for\\nUNION using the linked-list representation and the weighted-union\\nheuristic.\\n19.2-4\\nGive a tight asymptotic bound on the running time of the sequence of\\noperations in Figure 19.3 assuming the linked-list representation and the\\nweighted-union heuristic.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 690}),\n",
              " Document(page_content='19.2-5\\nProfessor Gompers suspects that it might be possible to keep just one\\npointer in each set object, rather than two (head and tail), while keeping\\nthe number of pointers in each list element at two. Show that the\\nprofessor’s suspicion is well founded by describing how to represent each\\nset by a linked list such that each operation has the same running time as\\nthe operations described in this section. Describe also how the\\noperations work. Your scheme should allow for the weighted-union\\nheuristic, with the same effect as described in this section. (Hint: Use the\\ntail of a linked list as its set’s representative.)\\n19.2-6\\nSuggest a simple change to the UNION procedure for the linked-list\\nrepresentation that removes the need to keep the tail pointer to the last\\nobject in each list. Regardless of whether the weighted-union heuristic is\\nused, your change should not change the asymptotic running time of the\\nUNION procedure. (Hint: Rather than appending one list to another,\\nsplice them together.)\\n19.3\\xa0\\xa0\\xa0\\xa0Disjoint-set forests\\nA faster implementation of disjoint sets represents sets by rooted trees,\\nwith each node containing one member and each tree representing one\\nset. In a disjoint-set forest, illustrated in Figure 19.4(a), each member\\npoints only to its parent. The root of each tree contains the\\nrepresentative and is its own parent. As we’ll see, although the\\nstraightforward algorithms that use this representation are no faster than\\nones that use the linked-list representation, two heuristics—“u nion by\\nrank” and “path compression”—yi eld an asymptotically optimal\\ndisjoint-set data structure.\\nThe three disjoint-set operations have simple implementations. A\\nMAKE-SET operation simply creates a tree with just one node. A\\nFIND-SET operation follows parent pointers until it reaches the root of\\nthe tree. The nodes visited on this simple path toward the root constitute\\nthe ﬁnd path. A UNION operation, shown in Figure 19.4(b), simply\\ncauses the root of one tree to point to the root of the other.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 691}),\n",
              " Document(page_content='Figure 19.4 A disjoint-set forest. (a) Trees representing the two sets of Figure 19.2. The tree on\\nthe left represents the set {b, c, e, h}, with c as the representative, and the tree on the right\\nrepresents the set {d, f, g}, with f as the representative. (b) The result of UNION (e, g).\\nHeuristics to improve the running time\\nSo far, disjoint-set forests have not improved on the linked-list\\nimplementation. A sequence of n − 1 UNION operations could create a\\ntree that is just a linear chain of n nodes. By using two heuristics,\\nhowever, we can achieve a running time that is almost linear in the total\\nnumber m of operations.\\nThe ﬁrst heuristic, union by rank, is similar to the weighted-union\\nheuristic we used with the linked-list representation. The common-sense\\napproach is to make the root of the tree with fewer nodes point to the\\nroot of the tree with more nodes. Rather than explicitly keeping track of\\nthe size of the subtree rooted at each node, however, we’ll adopt an\\napproach that eases the analysis. For each node, maintain a rank, which\\nis an upper bound on the height of the node. Union by rank makes the\\nroot with smaller rank point to the root with larger rank during a\\nUNION operation.\\nThe second heuristic, path compression, is also quite simple and\\nhighly effective. As shown in Figure 19.5, FIND-SET operations use it to\\nmake each node on the ﬁnd path point directly to the root. Path\\ncompression does not change any ranks.\\nPseudocode for disjoint-set forests', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 692}),\n",
              " Document(page_content='The union-by-rank heuristic requires its implementation to keep track of\\nranks. With each node x, maintain the integer value x.rank, which is an\\nupper bound on the height of x (the number of edges in the longest\\nsimple path from a descendant leaf to x). When MAKE-SET creates a\\nsingleton set, the single node in the corresponding tree has an initial rank\\nof 0. Each FIND-SET operation leaves all ranks unchanged. The\\nUNION operation has two cases, depending on whether the roots of the\\ntrees have equal rank. If the roots have unequal ranks, make the root\\nwith higher rank the parent of the root with lower rank, but don’t\\nchange the ranks themselves. If the roots have equal ranks, arbitrarily\\nchoose one of the roots as the parent and increment its rank.\\nFigure 19.5 Path compression during the operation FIND-SET. Arrows and self-loops at roots\\nare omitted. (a) A tree representing a set prior to executing FIND-SET(a). Triangles represent\\nsubtrees whose roots are the nodes shown. Each node has a pointer to its parent. (b) The same set\\nafter executing FIND-SET(a). Each node on the ﬁnd path now points directly to the root.\\nLet’s put this method into pseudocode, appearing on the next page.\\nThe parent of node x is denoted by x.p. The LINK procedure, a\\nsubroutine called by UNION, takes pointers to two roots as inputs. The\\nFIND-SET procedure with path compression, implemented recursively,\\nturns out to be quite simple.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 693}),\n",
              " Document(page_content='The FIND-SET procedure is a two-pass method: as it recurses, it\\nmakes one pass up the ﬁnd path to ﬁnd the root, and as the recursion\\nunwinds, it makes a second pass back down the ﬁnd path to update each\\nnode to point directly to the root. Each call of FIND-SET(x) returns x.p\\nin line 3. If x is the root, then FIND-SET skips line 2 and just returns\\nx.p, which is x. In this case the recursion bottoms out. Otherwise, line 2\\nexecutes, and the recursive call with parameter x.p returns a pointer to\\nthe root. Line 2 updates node x to point directly to the root, and line 3\\nreturns this pointer.\\nMAKE-SET(x)\\n1x.p = x\\n2x.rank = 0\\nUNION(x, y)\\n1LINK(FIND-SET(x), FIND-SET(y))\\nLINK(x, y)\\n1if x.rank > y.rank\\n2 y.p = x\\n3else x.p = y\\n4 if x.rank == y.rank\\n5 y.rank = y.rank + 1\\nFIND-SET(x)\\n1if x ≠ x.p // not the root?\\n2 x.p = FIND-SET(x.p) // the root becomes the parent\\n3return x.p // return the root\\nEffect of the heuristics on the running time\\nSeparately, either union by rank or path compression improves the\\nrunning time of the operations on disjoint-set forests, and combining the\\ntwo heuristics yields an even greater improvement. Alone, union by rank\\nyields a running time of O(m lg n) for a sequence of m operations, n of\\nwhich are MAKE-SET (see Exercise 19.4-4), and this bound is tight (see', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 694}),\n",
              " Document(page_content='Exercise 19.3-3). Although we won’t prove it here, for a sequence of n\\nMAKE-SET operations (and hence at most n − 1 UNION operations)\\nand f FIND-SET operations, the worst-case running time using only the\\npath-compression heuristic is Θ (n + f · (1 + log2+f/nn)).\\nCombining union by rank and path compression gives a worst-case\\nrunning time of O(m α(n)), where α(n) is a very slowly growing function,\\ndeﬁned in Section 19.4. In any conceivable application of a disjoint-set\\ndata structure, α(n) ≤ 4, and thus, its running time is as good as linear in\\nm for all practical purposes. Mathematically speaking, however, it is\\nsuperlinear. Section 19.4 proves this O(m α(n)) upper bound.\\nExercises\\n19.3-1\\nRedo Exercise 19.2-2 using a disjoint-set forest with union by rank and\\npath compression. Show the resulting forest with each node including its\\nxi and rank.\\n19.3-2\\nWrite a nonrecursive version of FIND-SET with path compression.\\n19.3-3\\nGive a sequence of m MAKE-SET, UNION, and FIND-SET\\noperations, n of which are MAKE-SET operations, that takes Ω(m lg n)\\ntime when using only union by rank and not path compression.\\n19.3-4\\nConsider the operation PRINT-SET(x), which is given a node x and\\nprints all the members of x’s set, in any order. Show how to add just a\\nsingle attribute to each node in a disjoint-set forest so that PRINT-\\nSET(x) takes time linear in the number of members of x’s set and the\\nasymptotic running times of the other operations are unchanged.\\nAssume that you can print each member of the set in O(1) time.\\n★ 19.3-5\\nShow that any sequence of m MAKE-SET, FIND-SET, and LINK\\noperations, where all the LINK operations appear before any of the', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 695}),\n",
              " Document(page_content='FIND-SET operations, takes only O(m) time when using both path\\ncompression and union by rank. You may assume that the arguments to\\nLINK are roots within the disjoint-set forest. What happens in the same\\nsituation when using only path compression and not union by rank?\\n★ 19.4 Analysis of union by rank with pat h compression\\nAs noted in Section 19.3, the combined union-by-rank and path-\\ncompression heuristic runs in O(m α(n)) time for m disjoint-set\\noperations on n elements. In this section, we’ll explore the function α to\\nsee just how slowly it grows. Then we’ll analyze the running time using\\nthe potential method of amortized analysis.\\nA very quickly growing function and i ts very slowly growing inverse\\nFor integers j, k ≥ 0, we deﬁne the function Ak(j) as\\nwhere the expression \\n  uses the functional-iteration notation\\ndeﬁned in equation (3.30) on page 68. Speciﬁcally, equation (3.30) gives \\n and \\n  for i ≥ 1. We call the parameter\\nk the level of the function A.\\nThe function Ak(j) strictly increases with both j and k. To see just\\nhow quickly this function grows, we ﬁrst obtain closed-form expressions\\nfor A1(j) and A2(j).\\nLemma 19.2\\nFor any integer j ≥ 1, we have A1(j) = 2j + 1.\\nProof\\xa0\\xa0\\xa0We ﬁrst use induction on i to show that \\n . For the\\nbase case, \\n . For the inductive step, assume that ', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 696}),\n",
              " Document(page_content='. Then \\n .\\nFinally, we note that \\n .\\n▪\\nLemma 19.3\\nFor any integer j ≥ 1, we have A2 (j) = 2j+1(j + 1) − 1.\\nProof\\xa0\\xa0\\xa0We ﬁrst use induction on i to show that \\n .\\nFor the base case, we have \\n . For the inductive\\nstep, assume that \\n . Then \\n. Finally, we note that \\n .\\nNow we can see how quickly Ak(j) grows by simply examining Ak(1)\\nfor levels k = 0, 1, 2, 3, 4. From the deﬁnition of A0(j) and the above\\nlemmas, we have A0(1) = 1 + 1 = 2, A1(1) = 2 · 1 + 1 = 3, and A2(1) =\\n21+1 · (1 + 1) − 1 = 7. We also have\\nA3(1)=\\n=A2(A2(1))\\n=A2(7)\\n=28 · 8 − 1\\n=211 − 1\\n=2047\\nand\\nA4(1)=\\n=A3(A3(1))\\n=A3(2047)\\n=\\n', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 697}),\n",
              " Document(page_content='≫A2(2047)\\n=22048 · 2048 − 1\\n=22059 − 1\\n>22056\\n=(24)514\\n=16514\\n≫1080,\\nwhich is the estimated number of atoms in the observable universe. (The\\nsymbol “ ≫” denotes the “much-greater-than” relation.)\\nWe deﬁne the inverse of the function Ak(n), for integer n ≥ 0, by\\nIn words, α(n) is the lowest level k for which Ak(1) is at least n. From the\\nabove values of Ak(1), we see that\\nIt is only for values of n so large that the term “astronomical”\\nunderstates them (greater than A4(1), a huge number) that α(n) > 4, and\\nso α(n) ≤ 4 for all practical purposes.\\nProperties of ranks\\nIn the remainder of this section, we prove an O(m α(n)) bound on the\\nrunning time of the disjoint-set operations with union by rank and path\\ncompression. In order to prove this bound, we ﬁrst prove some simple\\nproperties of ranks.\\nLemma 19.4', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 698}),\n",
              " Document(page_content='For all nodes x, we have x.rank ≤ x.p.rank, with strict inequality if x ≠ x.p\\n(x is not a root). The value of x.rank is initially 0, increases through time\\nuntil x ≠ x.p, and from then on, x.rank does not change. The value of\\nx.p.rank monotonically increases over time.\\nProof\\xa0\\xa0\\xa0The proof is a straightforward induction on the number of\\noperations, using the implementations of MAKE-SET, UNION, and\\nFIND-SET that appear on page 530, and is left as Exercise 19.4-1.\\n▪\\nCorollary 19.5\\nOn the simple path from any node going up toward a root, node ranks\\nstrictly increase.\\n▪\\nLemma 19.6\\nEvery node has rank at most n − 1.\\nProof\\xa0\\xa0\\xa0Each node’s rank starts at 0, and it increases only upon LINK\\noperations. Because there are at most n − 1 UNION operations, there\\nare also at most n − 1 LINK operations. Because each LINK operation\\neither leaves all ranks alone or increases some node’s rank by 1, all ranks\\nare at most n − 1.\\n▪\\nLemma 19.6 provides a weak bound on ranks. In fact, every node has\\nrank at most ⌊lg n ⌋ (see Exercise 19.4-2). The looser bound of Lemma\\n19.6 sufﬁces for our purposes, however.\\nProving the time bound\\nIn order to prove the O(m α(n)) time bound, we’ll use the potential\\nmethod of amortized analysis from Section 16.3. In performing the\\namortized analysis, it will be convenient to assume that we invoke the\\nLINK operation rather than the UNION operation. That is, since the\\nparameters of the LINK procedure are pointers to two roots, we act as\\nthough we perform the appropriate FIND-SET operations separately.\\nThe following lemma shows that even if we count the extra FIND-SET', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 699}),\n",
              " Document(page_content='operations induced by UNION calls, the asymptotic running time\\nremains unchanged.\\nLemma 19.7\\nSuppose that we convert a sequence S′ of m′ MAKE-SET, UNION, and\\nFIND-SET operations into a sequence S of m MAKE-SET, LINK, and\\nFIND-SET operations by turning each UNION into two FIND-SET\\noperations followed by one LINK. Then, if sequence S runs in O(m α(n))\\ntime, sequence S′ runs in O(m′ α(n)) time.\\nProof\\xa0\\xa0\\xa0Since each UNION operation in sequence S′ is converted into\\nthree operations in S, we have m′ ≤ m ≤ 3m′, so that m = Θ (m′), Thus, an\\nO(m α(n)) time bound for the converted sequence S implies an O(m′ α(n))\\ntime bound for the original sequence S′.\\n▪\\nFrom now on, we assume that the initial sequence of m′ MAKE-SET,\\nUNION, and FIND-SET operations has been converted to a sequence\\nof m MAKE-SET, LINK, and FIND-SET operations. We now prove an\\nO(m α(n)) time bound for the converted sequence and appeal to Lemma\\n19.7 to prove the O(m′ α(n)) running time of the original sequence of m′\\noperations.\\nPotential function\\nThe potential function we use assigns a potential ϕq(x) to each node x in\\nthe disjoint-set forest after q operations. For the potential Φq of the\\nentire forest after q operations, sum the individual node potentials: \\n. Because the forest is empty before the ﬁrst operation, the\\nsum is taken over an empty set, and so Φ0 = 0. No potential Φq is ever\\nnegative.\\nThe value of ϕq(x) depends on whether x is a tree root after the qth\\noperation. If it is, or if x.rank = 0, then ϕq(x) = α(n) · x.rank.\\nNow suppose that after the qth operation, x is not a root and that\\nx.rank ≥ 1. We need to deﬁne two auxiliary functions on x before we can', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 700}),\n",
              " Document(page_content='deﬁne ϕq(x). First we deﬁne\\nThat is, level(x) is the greatest level k for which Ak, applied to x’s rank, is\\nno greater than x’s parent’s rank.\\nWe claim that\\nwhich we see as follows. We have\\nx.p.rank≥x.rank + 1(by Lemma 19.4 because x is not a root)\\n=A0(x.rank)(by the deﬁnition (19.1) of A0(j)),\\nwhich implies that level(x) ≥ 0, and\\nA α(n)(x.rank)≥A α(n)(1)(because Ak(j) is strictly increasing)\\n≥n (by the deﬁnition (19.2) of α(n))\\n>x.p.rank(by Lemma 19.6),\\nwhich implies that level(x) < α(n).\\nFor a given nonroot node x, the value of level(x) monotonically\\nincreases over time. Why? Because x is not a root, its rank does not\\nchange. The rank of x.p monotonically increases over time, since if x.p is\\nnot a root then its rank does not change, and if x.p is a root then its rank\\ncan never decrease. Thus, the difference between x.rank and x.p.rank\\nmonotonically increases over time. Therefore, the value of k needed for\\nAk(x.rank) to overtake x.p.rank monotonically increases over time as\\nwell.\\nThe second auxiliary function applies when x.rank ≥ 1: iter(x) = max\\nThat is, iter(x) is the largest number of times we can iteratively apply\\nAlevel(x), applied initially to x’s rank, before exceeding x’s parent’s rank.\\nWe claim that when x.rank ≥ 1, we have\\n', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 701}),\n",
              " Document(page_content='which we see as follows. We have\\nx.p.rank≥Alevel(x)(x.rank)(by the deﬁnition (19.3) of level(x))\\n=\\n (by the deﬁnition (3.30) of functional\\niteration),\\nwhich implies that iter(x) ≥ 1. We also have\\n=Alevel(x)+1(x.rank)(by the deﬁnition (19.1) of Ak(j))\\n>x.p.rank (by the deﬁnition (19.3) of\\nlevel(x)),\\nwhich implies that iter(x) ≤ x.rank. Note that because x.p.rank\\nmonotonically increases over time, in order for iter(x) to decrease,\\nlevel(x) must increase. As long as level(x) remains unchanged, iter(x)\\nmust either increase or remain unchanged.\\nWith these auxiliary functions in place, we are ready to deﬁne the\\npotential of node x after q operations:\\nWe next investigate some useful properties of node potentials.\\nLemma 19.8\\nFor every node x, and for all operation counts q, we have\\n0 ≤ ϕq(x) ≤ α(n) · x.rank.\\nProof\\xa0\\xa0\\xa0If x is a root or x.rank = 0, then ϕq(x) = α(n) · x.rank by\\ndeﬁnition. Now suppose that x is not a root and that x.rank ≥ 1. We can\\nobtain a lower bound on ϕq(x) by maximizing level(x) and iter(x). The\\nbounds (19.4) and (19.6) give α(n) − level(x) ≥ 1 and iter(x) ≤ x.rank.\\nThus, we have\\nϕq(x)=( α(n) − level(x)) · x.rank − iter(x)', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 702}),\n",
              " Document(page_content='≥x.rank − x.rank\\n=0.\\nSimilarly, minimizing level(x) and iter(x) provides an upper bound on\\nϕq(x). By the bound (19.4), level(x) ≥ 0, and by the bound (19.6), iter(x)\\n≥ 1. Thus, we have\\nϕq(x)≤( α(n) − 0) · x.rank − 1\\n= α(n) · x.rank − 1\\n< α(n) · x.rank.\\n▪\\nCorollary 19.9\\nIf node x is not a root and x.rank > 0, then ϕq(x) < α(n) · x.rank.\\nPotential changes and amortized costs of operations\\nWe are now ready to examine how the disjoint-set operations affect node\\npotentials. Once we understand how each operation can change the\\npotential, we can determine the amortized costs.\\nLemma 19.10\\nLet x be a node that is not a root, and suppose that the qth operation is\\neither a LINK or a FIND-SET. Then after the qth operation, ϕq(x) ≤\\nϕq−1(x). Moreover, if x.rank ≥ 1 and either level(x) or iter(x) changes\\ndue to the qth operation, then ϕq(x) ≤ ϕq−1(x) − 1. That is, x’s potential\\ncannot increase, and if it has positive rank and either level(x) or iter(x)\\nchanges, then x’s potential drops by at least 1.\\nProof\\xa0\\xa0\\xa0Because x is not a root, the qth operation does not change x.rank,\\nand because n does not change after the initial n MAKE-SET\\noperations, α(n) remains unchanged as well. Hence, these components of\\nthe formula for x’s potential remain the same after the qth operation. If\\nx.rank = 0, then ϕq(x) = ϕq−1(x) = 0.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 703}),\n",
              " Document(page_content='Now assume that x.rank ≥ 1. Recall that level(x) monotonically\\nincreases over time. If the qth operation leaves level(x) unchanged, then\\niter(x) either increases or remains unchanged. If both level(x) and iter(x)\\nare unchanged, then ϕq(x) = ϕq−1(x). If level(x) is unchanged and\\niter(x) increases, then it increases by at least 1, and so ϕq(x) ≤ ϕq−1(x) −\\n1.\\nFinally, if the qth operation increases level(x), it increases by at least\\n1, so that the value of the term ( α(n) − level(x)) · x.rank drops by at least\\nx.rank. Because level(x) increased, the value of iter(x) might drop, but\\naccording to the bound (19.6), the drop is by at most x.rank − 1. Thus,\\nthe increase in potential due to the change in iter(x) is less than the\\ndecrease in potential due to the change in level(x), yielding ϕq(x) ≤\\nϕq−1(x) − 1.\\n▪\\nOur ﬁnal three lemmas show that the amortized cost of each MAKE-\\nSET, LINK, and FIND-SET operation is O( α(n)). Recall from equation\\n(16.2) on page 456 that the amortized cost of each operation is its actual\\ncost plus the change in potential due to the operation.\\nLemma 19.11\\nThe amortized cost of each MAKE-SET operation is O(1).\\nProof\\xa0\\xa0\\xa0Suppose that the qth operation is MAKE-SET(x). This operation\\ncreates node x with rank 0, so that ϕq(x) = 0. No other ranks or\\npotentials change, and so Φq = Φq−1. Noting that the actual cost of the\\nMAKE-SET operation is O(1) completes the proof.\\n▪\\nLemma 19.12\\nThe amortized cost of each LINK operation is O( α(n)).\\nProof\\xa0\\xa0\\xa0Suppose that the qth operation is LINK(x, y). The actual cost of\\nthe LINK operation is O(1). Without loss of generality, suppose that the\\nLINK makes y the parent of x.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 704}),\n",
              " Document(page_content='To determine the change in potential due to the LINK, note that the\\nonly nodes whose potentials may change are x, y, and the children of y\\njust prior to the operation. We’ll show that the only node whose\\npotential can increase due to the LINK is y, and that its increase is at\\nmost α(n):\\nBy Lemma 19.10, any node that is y’s child just before the LINK\\ncannot have its potential increase due to the LINK.\\nFrom the deﬁnition (19.7) of ϕq(x), note that, since x was a root\\njust before the qth operation, ϕq−1(x) = α(n) · x.rank at that time.\\nIf x.rank = 0, then ϕq(x) = ϕq−1(x) = 0. Otherwise,\\nϕq(x)< α(n) · x.rank(by Corollary 19.9)\\n= ϕq−1(x),\\nand so x’s potential decreases.\\nBecause y is a root prior to the LINK, ϕq−1(y) = α(n) · y.rank.\\nAfter the LINK operation, y remains a root, so that y’s potential\\nstill equals α(n) times its rank after the operation. The LINK\\noperation either leaves y’s rank alone or increases y’s rank by 1.\\nTherefore, either ϕq(y) = ϕq−1(y) or ϕq(y) = ϕq−1(y) + α(n).\\nThe increase in potential due to the LINK operation, therefore, is at\\nmost α(n). The amortized cost of the LINK operation is O(1) + α(n) =\\nO( α(n)).\\n▪\\nLemma 19.13\\nThe amortized cost of each FIND-SET operation is O( α(n)).\\nProof\\xa0\\xa0\\xa0Suppose that the qth operation is a FIND-SET and that the ﬁnd\\npath contains s nodes. The actual cost of the FIND-SET operation is\\nO(s). We will show that no node’s potential increases due to the FIND-\\nSET and that at least max {0, s − ( α(n) + 2)} nodes on the ﬁnd path have\\ntheir potential decrease by at least 1.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 705}),\n",
              " Document(page_content='We ﬁrst show that no node’s potential increases. Lemma 19.10 takes\\ncare of all nodes other than the root. If x is the root, then its potential is\\nα(n) · x.rank, which does not change due to the FIND-SET operation.\\nNow we show that at least max {0, s − ( α(n) + 2)} nodes have their\\npotential decrease by at least 1. Let x be a node on the ﬁnd path such\\nthat x.rank > 0 and x is followed somewhere on the ﬁnd path by another\\nnode y that is not a root, where level(y) = level(x) just before the FIND-\\nSET operation. (Node y need not immediately follow x on the ﬁnd path.)\\nAll but at most α(n) + 2 nodes on the ﬁnd path satisfy these constraints\\non x. Those that do not satisfy them are the ﬁrst node on the ﬁnd path\\n(if it has rank 0), the last node on the path (i.e., the root), and the last\\nnode w on the path for which level(w) = k, for each k = 0, 1, 2, … , α(n)\\n−1.\\nConsider such a node x. It has positive rank and is followed\\nsomewhere on the ﬁnd path by nonroot node y such that level(y) =\\nlevel(x) before the path compression occurs. We claim that the path\\ncompression decreases x’s potential by at least 1. To prove this claim, let\\nk = level(x) = level(y) and i = iter(x) before the path compression occurs.\\nJust prior to the path compression caused by the FIND-SET, we have\\nx.p.rank≥\\n (by the deﬁnition (19.5) of iter(x)),\\ny.p.rank≥Ak(y.rank)(by the deﬁnition (19.3) of level(y)),\\ny.rank≥x.p.rank (by Corollary 19.5 and because y follows x on the\\nﬁnd path).\\nPutting these inequalities together gives\\ny.p.rank≥Ak(y.rank)\\n≥Ak(x.p.rank)(because Ak(j) is strictly increasing)\\n≥\\n=\\n (by the deﬁnition (3.30) of functional iteration).\\nBecause path compression makes x and y have the same parent, after\\npath compression we have x.p.rank = y.p.rank. The parent of y might\\nchange due to the path compression, but if it does, the rank of y’s new', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 706}),\n",
              " Document(page_content='parent compared with the rank of y’s parent before path compression is\\neither the same or greater. Since x.rank does not change, \\n after path compression. By the\\ndeﬁnition (19.5) of the iter function, the value of iter(x) increases from i\\nto at least i + 1. By Lemma 19.10, ϕq(x) ≤ ϕq−1(x) − 1, so that x’s\\npotential decreases by at least 1.\\nThe amortized cost of the FIND-SET operation is the actual cost\\nplus the change in potential. The actual cost is O(s), and we have shown\\nthat the total potential decreases by at least max {0, s − ( α(n) + 2)}. The\\namortized cost, therefore, is at most O(s) − (s − ( α(n) + 2)) = O(s) − s +\\nO( α(n)) = O( α(n)), since we can scale up the units of potential to\\ndominate the constant hidden in O(s). (See Exercise 19.4-6.)\\n▪\\nPutting the preceding lemmas together yields the following theorem.\\nTheorem 19.14\\nA sequence of m MAKE-SET, UNION, and FIND-SET operations, n\\nof which are MAKE-SET operations, can be performed on a disjoint-set\\nforest with union by rank and path compression in O(m α(n)) time.\\nProof\\xa0\\xa0\\xa0Immediate from Lemmas 19.7, 19.11, 19.12, and 19.13.\\n▪\\nExercises\\n19.4-1\\nProve Lemma 19.4.\\n19.4-2\\nProve that every node has rank at most ⌊lg n ⌋.\\n19.4-3\\nIn light of Exercise 19.4-2, how many bits are necessary to store x.rank\\nfor each node x?\\n19.4-4', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 707}),\n",
              " Document(page_content='Using Exercise 19.4-2, give a simple proof that operations on a disjoint-\\nset forest with union by rank but without path compression run in O(m\\nlg n) time.\\n19.4-5\\nProfessor Dante reasons that because node ranks increase strictly along\\na simple path to the root, node levels must monotonically increase along\\nthe path. In other words, if x.rank > 0 and x.p is not a root, then level(x)\\n≤ level(x.p). Is the professor correct?\\n19.4-6\\nThe proof of Lemma 19.13 ends with scaling the units of potential to\\ndominate the constant hidden in the O(s) term. To be more precise in the\\nproof, you need to change the deﬁnition (19.7) of the potential function\\nto multiply each of the two cases by a constant, say c, that dominates the\\nconstant in the O(s) term. How must the rest of the analysis change to\\naccommodate this updated potential function?\\n★ 19.4-7\\nConsider the function α′(n) = min {k : Ak(1) ≥ lg(n + 1)}. Show that α′(n)\\n≤ 3 for all practical values of n and, using Exercise 19.4-2, show how to\\nmodify the potential-function argument to prove that performing a\\nsequence of m MAKE-SET, UNION, and FIND-SET operations, n of\\nwhich are MAKE-SET operations, on a disjoint-set forest with union by\\nrank and path compression takes O(m α′(n)) time.\\nProblems\\n19-1\\xa0\\xa0\\xa0\\xa0\\xa0O fﬂine minimum\\nIn the ofﬂine minimum problem, you maintain a dynamic set T of\\nelements from the domain {1, 2, … , n} under the operations INSERT\\nand EXTRACT-MIN. The input is a sequence S of n INSERT and m\\nEXTRACT-MIN calls, where each key in {1, 2, … , n} is inserted exactly\\nonce. Your goal is to determine which key is returned by each\\nEXTRACT-MIN call. Speciﬁcally, you must ﬁll in an array extracted[1:\\nm], where for i = 1, 2, … , m, extracted[i] is the key returned by the ith', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 708}),\n",
              " Document(page_content='EXTRACT-MIN call. The problem is “ofﬂine” in the sense that you are\\nallowed to process the entire sequence S before determining any of the\\nreturned keys.\\na. Consider the following instance of the ofﬂine minimum problem, in\\nwhich each operation INSERT(i) is represented by the value of i and\\neach EXTRACT-MIN is represented by the letter E:\\n4, 8, E, 3, E, 9, 2, 6, E, E, E, 1, 7, E, 5.\\nFill in the correct values in the extracted array.\\nTo develop an algorithm for this problem, break the sequence S into\\nhomogeneous subsequences. That is, represent S by\\nI1, E, I2, E, I3, … , Im, E, Im+1,\\nwhere each E represents a single EXTRACT-MIN call and each Ij\\nrepresents a (possibly empty) sequence of INSERT calls. For each\\nsubsequence Ij, initially place the keys inserted by these operations into a\\nset Kj, which is empty if Ij is empty. Then execute the OFFLINE-\\nMINIMUM procedure.\\nOFFLINE-MINIMUM(m, n)\\n1for i = 1 to n\\n2 determine j such that i ∈ Kj\\n3 if j ≠ m + 1\\n4 extracted[j] = i\\n5 let l be the smallest value greater than j for which set Kl exists\\n6 Kl = Kj ∪ Kl, destroying Kj\\n7return extracted\\nb. Argue that the array extracted returned by OFFLINE-MINIMUM is\\ncorrect.\\nc. Describe how to implement OFFLINE-MINIMUM efﬁciently with a\\ndisjoint-set data structure. Give as tight a bound as you can on the\\nworst-case running time of your implementation.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 709}),\n",
              " Document(page_content='19-2\\xa0\\xa0\\xa0\\xa0\\xa0D epth determination\\nIn the depth-determination problem, you maintain a forest ℱ = {Ti} of\\nrooted trees under three operations:\\nMAKE-TREE(v) creates a tree whose only node is v.\\nFIND-DEPTH(v) returns the depth of node v within its tree.\\nGRAFT(r, v) makes node r, which is assumed to be the root of a tree,\\nbecome the child of node v, which is assumed to be in a different tree\\nfrom r but may or may not itself be a root.\\na. Suppose that you use a tree representation similar to a disjoint-set\\nforest: v.p is the parent of node v, except that v.p = v if v is a root.\\nSuppose further that you implement GRAFT(r, v) by setting r.p = v\\nand FIND-DEPTH(v) by following the ﬁnd path from v up to the root,\\nreturning a count of all nodes other than v encountered. Show that the\\nworst-case running time of a sequence of m MAKE-TREE, FIND-\\nDEPTH, and GRAFT operations is Θ (m2).\\nBy using the union-by-rank and path-compression heuristics, you can\\nreduce the worst-case running time. Use the disjoint-set forest S = {Si},\\nwhere each set Si (which is itself a tree) corresponds to a tree Ti in the\\nforest ℱ. The tree structure within a set Si, however, does not necessarily\\ncorrespond to that of Ti. In fact, the implementation of Si does not\\nrecord the exact parent-child relationships but nevertheless allows you to\\ndetermine any node’s depth in Ti.\\nThe key idea is to maintain in each node v a “pseudodistance” v.d,\\nwhich is deﬁned so that the sum of the pseudodistances along the simple\\npath from v to the root of its set Si equals the depth of v in Ti. That is, if\\nthe simple path from v to its root in Si is v0, v1, … , vk, where v0 = v and\\nvk is Si’s root, then the depth of v in Ti is \\n .\\nb. Give an implementation of MAKE-TREE.\\nc. Show how to modify FIND-SET to implement FIND-DEPTH. Your\\nimplementation should perform path compression, and its running', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 710}),\n",
              " Document(page_content='time should be linear in the length of the ﬁnd path. Make sure that\\nyour implementation updates pseudodistances correctly.\\nd. Show how to implement GRAFT(r, v), which combines the sets\\ncontaining r and v, by modifying the UNION and LINK procedures.\\nMake sure that your implementation updates pseudodistances\\ncorrectly. Note that the root of a set Si is not necessarily the root of the\\ncorresponding tree Ti.\\ne. Give a tight bound on the worst-case running time of a sequence of m\\nMAKE-TREE, FIND-DEPTH, and GRAFT operations, n of which\\nare MAKE-TREE operations.\\n19-3\\xa0\\xa0\\xa0\\xa0\\xa0Tarjan’s ofﬂine lowest-common-ancestors algorithm\\nThe lowest common ancestor of two nodes u and v in a rooted tree T is\\nthe node w that is an ancestor of both u and v and that has the greatest\\ndepth in T. In the ofﬂine lowest-common-ancestors problem, you are given\\na rooted tree T and an arbitrary set P = {{u, v}} of unordered pairs of\\nnodes in T, and you wish to determine the lowest common ancestor of\\neach pair in P.\\nTo solve the ofﬂine lowest-common-ancestors problem, the LCA\\nprocedure on the following page performs a tree walk of T with the\\ninitial call LCA(T.root). Assume that each node is colored WHITE prior\\nto the walk.\\na. Argue that line 10 executes exactly once for each pair {u, v} ∈ P.\\nb. Argue that at the time of the call LCA(u), the number of sets in the\\ndisjoint-set data structure equals the depth of u in T.\\nLCA(u)\\n\\xa0\\xa01MAKE-SET(u)\\n\\xa0\\xa02FIND-SET(u).ancestor = u\\n\\xa0\\xa03for each child v of u in T\\n\\xa0\\xa04LCA(v)\\n\\xa0\\xa05UNION(u, v)\\n\\xa0\\xa06FIND-SET(u).ancestor = u', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 711}),\n",
              " Document(page_content='\\xa0\\xa07u.color = BLACK\\n\\xa0\\xa08for each node v such that {u, v} ∈ P\\n\\xa0\\xa09if v.color == BLACK\\n10 print “The lowest common ancestor of”\\nu “and” v “is” FIND-SET(v).ancestor\\nc. Prove that LCA correctly prints the lowest common ancestor of u and\\nv for each pair {u, v} ∈ P.\\nd. Analyze the running time of LCA, assuming that you use the\\nimplementation of the disjoint-set data structure in Section 19.3.\\nChapter notes\\nMany of the important results for disjoint-set data structures are due at\\nleast in part to R. E. Tarjan. Using aggregate analysis, Tarjan [427, 429]\\ngave the ﬁrst tight upper bound in terms of the very slowly growing\\ninverse \\n  of Ackermann’s function. (The function Ak(j) given in\\nSection 19.4 is similar to Ackermann’s function, and the function α(n) is\\nsimilar to \\n . Both α(n) and \\n  are at most 4 for all conceivable\\nvalues of m and n.) An upper bound of O(m lg* n) was proven earlier by\\nHopcroft and Ullman [5, 227]. The treatment in Section 19.4 is adapted\\nfrom a later analysis by Tarjan [431], which is based on an analysis by\\nKozen [270]. Harfst and Reingold [209] give a potential-based version of\\nTarjan’s earlier bound.\\nTarjan and van Leeuwen [432] discuss variants on the path-\\ncompression heuristic, including “one-pass methods,” which sometimes\\noffer better constant factors in their performance than do two-pass\\nmethods. As with Tarjan’s earlier analyses of the basic path-compression\\nheuristic, the analyses by Tarjan and van Leeuwen are aggregate. Harfst\\nand Reingold [209] later showed how to make a small change to the\\npotential function to adapt their path-compression analysis to these one-\\npass variants. Goel et al. [182] prove that linking disjoint-set trees\\nrandomly yields the same asymptotic running time as union by rank.\\nGabow and Tarjan [166] show that in certain applications, the disjoint-\\nset operations can be made to run in O(m) time.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 712}),\n",
              " Document(page_content='Tarjan [428] showed that a lower bound of \\n  time is\\nrequired for operations on any disjoint-set data structure satisfying\\ncertain technical conditions. This lower bound was later generalized by\\nFredman and Saks [155], who showed that in the worst case, \\n (lg n)-bit words of memory must be accessed.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 713}),\n",
              " Document(page_content='Part VI\\xa0\\xa0\\xa0\\xa0Graph Algorithms', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 714}),\n",
              " Document(page_content='\\xa0\\n\\xa0\\nIntroduction\\nGraph problems pervade computer science, and algorithms for working\\nwith them are fundamental to the ﬁeld. Hundreds of interesting\\ncomputational problems are couched in terms of graphs. This part\\ntouches on a few of the more signiﬁcant ones.\\nChapter 20 shows how to represent a graph in a computer and then\\ndiscusses algorithms based on searching a graph using either breadth-\\nﬁrst search or depth-ﬁrst search. The chapter gives two applications of\\ndepth-ﬁrst search: topologically sorting a directed acyclic graph and\\ndecomposing a directed graph into its strongly connected components.\\nChapter 21 describes how to compute a minimum-weight spanning\\ntree of a graph: the least-weight way of connecting all of the vertices\\ntogether when each edge has an associated weight. The algorithms for\\ncomputing minimum spanning trees serve as good examples of greedy\\nalgorithms (see Chapter 15).\\nChapters 22 and 23 consider how to compute shortest paths between\\nvertices when each edge has an associated length or “weight.” Chapter\\n22 shows how to ﬁnd shortest paths from a given source vertex to all\\nother vertices, and Chapter 23 examines methods to compute shortest\\npaths between every pair of vertices.\\nChapter 24 shows how to compute a maximum ﬂow of material in a\\nﬂow network, which is a directed graph having a speciﬁed source vertex\\nof material, a speciﬁed sink vertex, and speciﬁed capacities for the\\namount of material that can traverse each directed edge. This general', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 715}),\n",
              " Document(page_content='problem arises in many forms, and a good algorithm for computing\\nmaximum ﬂows can help solve a variety of related problems efﬁciently.\\nFinally, Chapter 25 explores matchings in bipartite graphs: methods\\nfor pairing up vertices that are partitioned into two sets by selecting\\nedges that go between the sets. Bipartite-matching problems model\\nseveral situations that arise in the real world. The chapter examines how\\nto ﬁnd a matching of maximum cardinality; the “stable-marriage\\nproblem,” which has the highly practical application of matching\\nmedical residents to hospitals; and assignment problems, which\\nmaximize the total weight of a bipartite matching.\\nWhen we characterize the running time of a graph algorithm on a\\ngiven graph G = (V, E), we usually measure the size of the input in\\nterms of the number of vertices |V| and the number of edges |E| of the\\ngraph. That is, we denote the size of the input with two parameters, not\\njust one. We adopt a common notational convention for these\\nparameters. Inside asymptotic notation (such as O-notation or Θ-\\nnotation), and only inside such notation, the symbol V denotes |V | and\\nthe symbol E denotes |E|. For example, we might say, “the algorithm\\nruns in O(VE) time,” meaning that the algorithm runs in O(|V| |E|) time.\\nThis convention makes the running-time formulas easier to read,\\nwithout risk of ambiguity.\\nAnother convention we adopt appears in pseudocode. We denote the\\nvertex set of a graph G by G.V and its edge set by G.E. That is, the\\npseudocode views vertex and edge sets as attributes of a gr aph.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 716}),\n",
              " Document(page_content='20\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0Elementary Graph Algorithms\\nThis chapter presents methods for representing a graph and for\\nsearching a graph. Searching a graph means systematically following the\\nedges of the graph so as to visit the vertices of the graph. A graph-\\nsearching algorithm can discover much about the structure of a graph.\\nMany algorithms begin by searching their input graph to obtain this\\nstructural information. Several other graph algorithms elaborate on\\nbasic graph searching. Techniques for searching a graph lie at the heart\\nof the ﬁeld of graph algorithms.\\nSection 20.1 discusses the two most common computational\\nrepresentations of graphs: as adjacency lists and as adjacency matrices.\\nSection 20.2 presents a simple graph-searching algorithm called\\nbreadth-ﬁrst search and shows how to create a breadth-ﬁrst tree. Section\\n20.3 presents depth-ﬁrst search and proves some standard results about\\nthe order in which depth-ﬁrst search visits vertices. Section 20.4\\nprovides our ﬁrst real application of depth-ﬁrst search: topologically\\nsorting a directed acyclic graph. A second application of depth-ﬁrst\\nsearch, ﬁnding the strongly connected components of a directed graph,\\nis the topic of Section 20.5.\\n20.1\\xa0\\xa0\\xa0\\xa0Representations of graphs\\nYou can choose between two standard ways to represent a gr aph G = (V,\\nE): as a collection of adjacency lists or as an adjacency matrix. Either\\nway applies to both directed and undirected graphs. Because the', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 717}),\n",
              " Document(page_content='adjacency-list representation provides a compact way to represent\\nsparse graphs—those for which |E| is much less than |V|2—it is usually\\nthe method of choice. Most of the graph algorithms presented in this\\nbook assume that an input graph is represented in adjacency-list form.\\nYou might prefer an adjacency-matrix representation, however, when\\nthe graph is dense—|E| is close to |V|2—or when you need to be able to\\ntell quickly whether there is an edge connecting two given vertices. For\\nexample, two of the all-pairs shortest-paths algorithms presented in\\nChapter 23 assume that their input graphs are represented by adjacency\\nmatrices.\\nFigure 20.1 Two representations of an undirected graph. (a) An undirected graph G with 5\\nvertices and 7 edges. (b) An adjacency-list representation of G. (c) The adjacency-matrix\\nrepresentation of G.\\nFigure 20.2 Two representations of a directed graph. (a) A directed graph G with 6 vertices and 8\\nedges. (b) An adjacency-list representation of G. (c) The adjacency-matrix representation of G.\\nThe adjacency-list representation of a graph G = (V, E) consists of an\\narray Adj of |V| lists, one for each vertex in V. For each u ∈ V, the\\nadjacency list Adj[u] contains all the vertices v such that there is an edge\\n(u, v) ∈ E. That is, Adj[u] consists of all the vertices adjacent to u in G.\\n(Alternatively, it can contain pointers to these vertices.) Since the', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 718}),\n",
              " Document(page_content='adjacency lists represent the edges of a graph, our pseudocode treats the\\narray Adj as an attribute of the graph, just like the edge set E. In\\npseudocode, therefore, you will see notation such as G.Adj[u]. Figure\\n20.1(b) is an adjacency-list representation of the undirected graph in\\nFigure 20.1(a). Similarly, Figure 20.2(b) is an adjacency-list\\nrepresentation of the directed graph in Figure 20.2(a).\\nIf G is a directed graph, the sum of the lengths of all the adjacency\\nlists is |E|, since an edge of the form (u, v) is represented by having v\\nappear in Adj[u]. If G is an undirected graph, the sum of the lengths of\\nall the adjacency lists is 2 |E|, since if (u, v) is an undirected edge, then u\\nappears in v’s adjacency list and vice versa. For both directed and\\nundirected graphs, the adjacency-list representation has the desirable\\nproperty that the amount of memory it requires is Θ (V + E). Finding\\neach edge in the graph also takes Θ (V + E) time, rather than just Θ (E),\\nsince each of the |V| adjacency lists must be examined. Of course, if |E| =\\nΩ(V)—such as in a connected, undirected graph or a strongly\\nconnected, directed graph—we can say that ﬁnding each edge takes\\nΘ(E) time.\\nAdjacency lists can also represent weighted graphs, that is, graphs for\\nwhich each edge has an associated weight given by a weight function w :\\nE → ℝ. For example, let G = (V, E) be a weighted graph with weight\\nfunction w. Then you can simply store the weight w(u, v) of the edge (u,\\nv) ∈  E with vertex v in u’s adjacency list. The adjacency-list\\nrepresentation is quite robust in that you can modify it to support many\\nother graph variants.\\nA potential disadvantage of the adjacency-list representation is that\\nit provides no quicker way to determine whether a given edge (u, v) is\\npresent in the graph than to search for v in the adjacency list Adj[u]. An\\nadjacency-matrix representation of the graph remedies this\\ndisadvantage, but at the cost of using asymptotically more memory. (See\\nExercise 20.1-8 for suggestions of variations on adjacency lists that\\npermit faster edge lookup.)\\nThe adjacency-matrix representation of a graph G = (V, E) assumes\\nthat the vertices are numbered 1, 2, … , |V| in some arbitrary manner.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 719}),\n",
              " Document(page_content='Then the adjacency-matrix representation of a graph G consists of a |V|\\n× |V| matrix A = (aij) such that\\nFigures 20.1(c) and 20.2(c) are the adjacency matrices of the undirected\\nand directed graphs in Figures 20.1(a) and 20.2(a), respectively. The\\nadjacency matrix of a graph requires Θ (V2) memory, independent of the\\nnumber of edges in the graph. Because ﬁnding each edge in the graph\\nrequires examining the entire adjacency matrix, doing so takes Θ (V2)\\ntime.\\nObserve the symmetry along the main diagonal of the adjacency\\nmatrix in Figure 20.1(c). Since in an undirected graph, (u, v) and (v, u)\\nrepresent the same edge, the adjacency matrix A of an undirected graph\\nis its own transpose: A = AT. In some applications, it pays to store only\\nthe entries on and above the diagonal of the adjacency matrix, thereby\\ncutting the memory needed to store the graph almost in half.\\nLike the adjacency-list representation of a graph, an adjacency\\nmatrix can represent a weighted graph. For example, if G = (V, E) is a\\nweighted graph with edge-weight function w, you can store the weight\\nw(u, v) of the edge (u, v) ∈ E as the entry in row u and column v of the\\nadjacency matrix. If an edge does not exist, you can store a NIL value\\nas its corresponding matrix entry, though for many problems it is\\nconvenient to use a value such as 0 or ∞.\\nAlthough the adjacency-list representation is asymptotically at least\\nas space-efﬁcient as the adjacency-matrix representation, adjacency\\nmatrices are simpler, and so you might prefer them when graphs are\\nreasonably small. Moreover, adjacency matrices carry a further\\nadvantage for unweighted graphs: they require only one bit per entry.\\nRepresenting attributes\\nMost algorithms that operate on graphs need to maintain attributes for\\nvertices and/or edges. We indicate these attributes using our usual\\nnotation, such as v.d for an attribute d of a vertex v. When we indicate', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 720}),\n",
              " Document(page_content='edges as pairs of vertices, we use the same style of notation. For\\nexample, if edges have an attribute f, then we denote this attribute for\\nedge (u, v) by (u, v).f. For the purpose of presenting and understanding\\nalgorithms, our attribute notation sufﬁces.\\nImplementing vertex and edge attributes in real programs can be\\nanother story entirely. There is no one best way to store and access\\nvertex and edge attributes. For a given situation, your decision will likely\\ndepend on the programming language you are using, the algorithm you\\nare implementing, and how the rest of your program uses the graph. If\\nyou represent a graph using adjacency lists, one design choice is to\\nrepresent vertex attributes in additional arrays, such as an array d[1 : |V|]\\nthat parallels the Adj array. If the vertices adjacent to u belong to Adj[u],\\nthen the attribute u.d can actually be stored in the array entry d[u].\\nMany other ways of implementing attributes are possible. For example,\\nin an object-oriented programming language, vertex attributes might be\\nrepresented as instance variables within a subclass of a Vertex class.\\nExercises\\n20.1-1\\nGiven an adjacency-list representation of a directed graph, how long\\ndoes it take to compute the out-degree of every vertex? How long does it\\ntake to compute the in-degrees?\\n20.1-2\\nGive an adjacency-list representation for a complete binary tree on 7\\nvertices. Give an equivalent adjacency-matrix representation. Assume\\nthat the edges are undirected and that the vertices are numbered from 1\\nto 7 as in a binary heap.\\n20.1-3\\nThe transpose of a directed graph G = (V, E) is the graph GT = (V, ET),\\nwhere ET = {(v, u) ∈ V × V : (u, v) ∈ E}. That is, GT is G with all its\\nedges reversed. Describe efﬁcient algorithms for computing GT from G,\\nfor both the adjacency-list and adjacency-matrix representations of G.\\nAnalyze the running times of your algorithms.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 721}),\n",
              " Document(page_content='20.1-4\\nGiven an adjacency-list representation of a multigraph G = (V, E),\\ndescribe an O(V + E)-time algorithm to compute the adjacency-list\\nrepresentation of the “equivalent” undirected graph G′ = (V, E′), where\\nE′ consists of the edges in E with all multiple edges between two vertices\\nreplaced by a single edge and with all self-loops removed.\\n20.1-5\\nThe square of a directed graph G = (V, E) is the graph G2 = (V, E2)\\nsuch that (u, v) ∈ E2 if and only if G contains a path with at most two\\nedges between u and v. Describe efﬁcient algorithms for computing G2\\nfrom G for both the adjacency-list and adjacency-matrix representations\\nof G. Analyze the running times of your algorithms.\\n20.1-6\\nMost graph algorithms that take an adjacency-matrix representation as\\ninput require Ω(V2) time, but there are some exceptions. Show how to\\ndetermine whether a directed graph G contains a universal sink—a\\nvertex with in-degree |V| – 1 and out-degree 0—in O(V) time, given an\\nadjacency matrix for G.\\n20.1-7\\nThe incidence matrix of a directed graph G = (V, E) with no self-loops is\\na |V| × |E| matrix B = (bij) such that\\nDescribe what the entries of the matrix product BBT represent, where\\nBT is the transpose of B.\\n20.1-8\\nSuppose that instead of a linked list, each array entry Adj[u] is a hash\\ntable containing the vertices v for which (u, v) ∈ E, with collisions\\nresolved by chaining. Under the assumption of uniform independent', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 722}),\n",
              " Document(page_content='hashing, if all edge lookups are equally likely, what is the expected time\\nto determine whether an edge is in the graph? What disadvantages does\\nthis scheme have? Suggest an alternate data structure for each edge list\\nthat solves these problems. Does your alternative have disadvantages\\ncompared with the hash table?\\n20.2\\xa0\\xa0\\xa0\\xa0Breadth-ﬁrst search\\nBreadth-ﬁrst search is one of the simplest algorithms for searching a\\ngraph and the archetype for many important graph algorithms. Prim’s\\nminimum-spanning-tree algorithm (Section 21.2) and Dijkstra’s single-\\nsource shortest-paths algorithm (Section 22.3) use ideas similar to those\\nin breadth-ﬁrst search.\\nGiven a graph G = (V, E) and a distinguished source vertex s,\\nbreadth-ﬁrst search systematically explores the edges of G to “discover”\\nevery vertex that is reachable from s. It computes the distance from s to\\neach reachable vertex, where the distance to a vertex v equals the\\nsmallest number of edges needed to go from s to v. Breadth-ﬁrst search\\nalso produces a “breadth-ﬁrst tree” with root s that contains all\\nreachable vertices. For any vertex v reachable from s, the simple path in\\nthe breadth-ﬁrst tree from s to v corresponds to a shortest path from s\\nto v in G, that is, a path containing the smallest number of edges. The\\nalgorithm works on both directed and undirected graphs.\\nBreadth-ﬁrst search is so named because it expands the frontier\\nbetween discovered and undiscovered vertices uniformly across the\\nbreadth of the frontier. You can think of it as discovering vertices in\\nwaves emanating from the source vertex. That is, starting from s, the\\nalgorithm ﬁrst discovers all neighbors of s, which have distance 1. Then\\nit discovers all vertices with distance 2, then all vertices with distance 3,\\nand so on, until it has discovered every vertex reachable from s.\\nIn order to keep track of the waves of vertices, breadth-ﬁrst search\\ncould maintain separate arrays or lists of the vertices at each distance\\nfrom the source vertex. Instead, it uses a single ﬁrst-in, ﬁrst-out queue\\n(see Section 10.1.3) containing some vertices at a distance k, possibly', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 723}),\n",
              " Document(page_content='followed by some vertices at distance k + 1. The queue, therefore,\\ncontains portions of two consecutive waves at any time.\\nTo keep track of progress, breadth-ﬁrst search colors each vertex\\nwhite, gray, or black. All vertices start out white, and vertices not\\nreachable from the source vertex s stay white the entire time. A vertex\\nthat is reachable from s is discovered the ﬁrst time it is encountered\\nduring the search, at which time it becomes gray, indicating that is now\\non the frontier of the search: the boundary between discovered and\\nundiscovered vertices. The queue contains all the gray vertices.\\nEventually, all the edges of a gray vertex will be explored, so that all of\\nits neighbors will be discovered. Once all of a vertex’s edges have been\\nexplored, the vertex is behind the frontier of the search, and it goes from\\ngray to black.1\\nBreadth-ﬁrst search constructs a breadth-ﬁrst tree, initially\\ncontaining only its root, which is the source vertex s. Whenever the\\nsearch discovers a white vertex v in the course of scanning the adjacency\\nlist of a gray vertex u, the vertex v and the edge (u, v) are added to the\\ntree. We say that u is the predecessor or parent of v in the breadth-ﬁrst\\ntree. Since every vertex reachable from s is discovered at most once, each\\nvertex reachable from s has exactly one parent. (There is one exception:\\nbecause s is the root of the breadth-ﬁrst tree, it has no parent.) Ancestor\\nand descendant relationships in the breadth-ﬁrst tree are deﬁned relative\\nto the root s as usual: if u is on the simple path in the tree from the root\\ns to vertex v, then u is an ancestor of v and v is a descendant of u.\\nThe breadth-ﬁrst-search procedure BFS on the following page\\nassumes that the graph G = (V, E) is represented using adjacency lists. It\\ndenotes the queue by Q, and it attaches three additional attributes to\\neach vertex v in the graph:\\nv.color is the color of v: WHITE, GRAY, or BLACK.\\nv.d holds the distance from the source vertex s to v, as computed\\nby the algorithm.\\nv.π is v’s predecessor in the breadth-ﬁrst tree. If v has no\\npredecessor because it is the source vertex or is undiscovered, then\\nv.π NIL.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 724}),\n",
              " Document(page_content='Figure 20.3 illustrates the progress of BFS on an undirected graph.\\nThe procedure BFS works as follows. With the exception of the\\nsource vertex s, lines 1–4 paint every vertex white, set u.d = ∞ for each\\nvertex u, and set the parent of every vertex to be NIL. Because the\\nsource vertex s is always the ﬁrst vertex discovered, lines 5–7 paint s\\ngray, set s.d to 0, and set the predecessor of s to NIL. Lines 8–9 create\\nthe queue Q, initially containing just the source vertex.\\nThe while loop of lines 10–18 iterates as long as there remain gray\\nvertices, which are on the frontier: discovered vertices that have not yet\\nhad their adjacency lists fully examined. This while loop maintains the\\nfollowing invariant:\\nAt the test in line 10, the queue Q consists of the set of gray\\nvertices.\\nAlthough we won’t use this loop invariant to prove correctness, it is easy\\nto see that it holds prior to the ﬁrst iteration and that each iteration of\\nthe loop maintains the invariant. Prior to the ﬁrst iteration, the only\\ngray vertex, and the only vertex in Q, is the source vertex s. Line 11\\ndetermines the gray vertex u at the head of the queue Q and removes it\\nfrom Q. The for loop of lines 12–17 considers each vertex v in the\\nadjacency list of u. If v is white, then it has not yet been discovered, and\\nthe procedure discovers it by executing lines 14–17.  These lines paint\\nvertex v gray, set v’s distance v.d to u.d + 1, record u as v’s parent v.π,\\nand place v at the tail of the queue Q. Once the procedure has examined\\nall the vertices on u’s adjacency list, it blackens u in line 18, indicating\\nthat u is now behind the frontier. The loop invariant is maintained\\nbecause whenever a vertex is painted gray (in line 14) it is also enqueued\\n(in line 17), and whenever a vertex is dequeued (in line 11) it is also\\npainted black (in line 18).\\nBFS(G, s)\\n\\xa0\\xa01for each vertex u ∈ G.V – {s}\\n\\xa0\\xa02u.color = WHITE\\n\\xa0\\xa03u.d = ∞\\n\\xa0\\xa04u.π NIL', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 725}),\n",
              " Document(page_content='\\xa0\\xa05s.color = GRAY\\n\\xa0\\xa06s.d = 0\\n\\xa0\\xa07s.π NIL\\n\\xa0\\xa08Q = Ø\\n\\xa0\\xa09ENQUEUE(Q, s)\\n10while Q ≠ Ø\\n11u = DEQUEUE(Q)\\n12for each vertex v in G.Adj[u] // search the neighbors of u\\n13 if v.color == WHITE // is v being discovered now?\\n14 v.color = GRAY\\n15 v.d = u.d + 1\\n16 v.π = u\\n17 ENQUEUEd(Q, v) // v is now on the frontier\\n18u.color = BLACK // u is now behind the frontier\\nThe results of breadth-ﬁrst search may depend upon the order in\\nwhich the neighbors of a given vertex are visited in line 12: the breadth-\\nﬁrst tree may vary, but the distances d computed by the algorithm do\\nnot. (See Exercise 20.2-5.)\\nA simple change allows the BFS procedure to terminate in many\\ncases before the queue Q becomes empty. Because each vertex is\\ndiscovered at most once and receives a ﬁnite d value only when it is\\ndiscovered, the algorithm can terminate once every vertex has a ﬁnite d\\nvalue. If BFS keeps count of how many vertices have been discovered, it\\ncan terminate once either the queue Q is empty or all |V| vertices are\\ndiscovered.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 726}),\n",
              " Document(page_content='Figure 20.3 The operation of BFS on an undirected graph. Each part shows the graph and the\\nqueue Q at the beginning of each iteration of the while loop of lines 10–18. Vertex distances\\nappear within each vertex and below vertices in the queue. The tan region surrounds the frontier\\nof the search, consisting of the vertices in the queue. The light blue region surrounds the vertices\\nbehind the frontier, which have been dequeued. Each part highlights in orange the vertex\\ndequeued and the breadth-ﬁrst tree edges added, if any, in the previous iteration. Blue edges\\nbelong to the breadth-ﬁrst tree constructed so far.\\nAnalysis\\nBefore proving the various properties of breadth-ﬁrst search, let’s take\\non the easier job of analyzing its running time on an input graph G =\\n(V, E). We use aggregate analysis, as we saw in Section 16.1. After\\ninitialization, breadth-ﬁrst search never whitens a vertex, and thus the', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 727}),\n",
              " Document(page_content='test in line 13 ensures that each vertex is enqueued at most once, and\\nhence dequeued at most once. The operations of enqueuing and\\ndequeuing take O(1) time, and so the total time devoted to queue\\noperations is O(V). Because the procedure scans the adjacency list of\\neach vertex only when the vertex is dequeued, it scans each adjacency\\nlist at most once. Since the sum of the lengths of all |V| adjacency lists is\\nΘ(E), the total time spent in scanning adjacency lists is O(V + E). The\\noverhead for initialization is O(V), and thus the total running time of\\nthe BFS procedure is O(V + E). Thus, breadth-ﬁrst search runs in time\\nlinear in the size of the adjacency-list representation of G.\\nShortest paths\\nNow, let’s see why breadth-ﬁrst search ﬁnds the shortest distance from a\\ngiven source vertex s to each vertex in a graph. Deﬁne the shortest-path\\ndistance δ(s, v) from s to v as the minimum number of edges in any path\\nfrom vertex s to vertex v. If there is no path from s to v, then δ (s, v) = ∞.\\nWe call a path of length δ (s, v) from s to v a shortest path2 from s to v.\\nBefore showing that breadth-ﬁrst search correctly computes shortest-\\npath distances, we investigate an important property of shortest-path\\ndistances.\\nLemma 20.1\\nLet G = (V, E) be a directed or undirected graph, and let s ∈ V be an\\narbitrary vertex. Then, for any edge (u, v) ∈ E,\\nδ(s, v) ≤ δ(s, u) + 1.\\nProof\\xa0\\xa0\\xa0If u is reachable from s, then so is v. In this case, the shortest\\npath from s to v cannot be longer than the shortest path from s to u\\nfollowed by the edge (u, v), and thus the inequality holds. If u is not\\nreachable from s, then δ (s, u) = ∞, and again, the inequality holds.\\n▪\\nOur goal is to show that the BFS procedure properly computes v.d =\\nδ(s, v) for each vertex v ∈ V. We ﬁrst show that v.d bounds δ (s, v) from\\nabove.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 728}),\n",
              " Document(page_content='Lemma 20.2\\nLet G = (V, E) be a directed or undirected graph, and suppose that BFS\\nis run on G from a given source vertex s ∈ V. Then, for each vertex v ∈\\nV, the value v.d computed by BFS satisﬁes v.d ≥ δ(s, v) at all times,\\nincluding at termination.\\nProof\\xa0\\xa0\\xa0The lemma is true intuitively, because any ﬁnite value assigned\\nto v.d equals the number of edges on some path from s to v. The formal\\nproof is by induction on the number of ENQUEUE operations. The\\ninductive hypothesis is that v.d ≥ δ(s, v) for all v ∈ V.\\nThe base case of the induction is the situation immediately after\\nenqueuing s in line 9 of BFS. The inductive hypothesis holds here,\\nbecause s.d = 0 = δ (s, s) and v.d = 1 ∞ δ (s, v) for all v ∈ V – {s}.\\nFor the inductive step, consider a white vertex v that is discovered\\nduring the search from a vertex u. The inductive hypothesis implies that\\nu.d ≥ δ(s, u). The assignment performed by line 15 an d Lemma 20. 1 give\\nv.d=u.d + 1\\n≥δ(s, u) + 1\\n≥δ(s, v).\\nVertex v is then enqueued, and it is never enqueued again because it is\\nalso grayed and lines 14–17 execute only for white vertices. Thus, the\\nvalue of v.d never changes again, and the inductive hypothesis is\\nmaintained.\\n▪\\nTo prove that v.d = δ (s, v), we ﬁrst show more precisely how the\\nqueue Q operates during the course of BFS. The next lemma shows that\\nat all times, the d values of vertices in the queue either are all the same\\nor form a sequence 〈k, k, … , k, k + 1, k + 1, … , k + 1〉 for some integer\\nk ≥ 0.\\nLemma 20.3\\nSuppose that during the execution of BFS on a graph G = (V, E), the\\nqueue Q contains the vertices 〈v1, v2, … , vr〉, where v1 is the head of Q', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 729}),\n",
              " Document(page_content='and vr is the tail. Then, vr.d ≤ v1.d + 1 and vi.d ≤ vi+1.d for i = 1, 2, … ,\\nr – 1.\\nProof\\xa0\\xa0\\xa0The proof is by induction on the number of queue operations.\\nInitially, when the queue contains only s, the lemma trivially holds.\\nFor the inductive step, we must prove that the lemma holds after\\nboth dequeuing and enqueuing a vertex. First, we examine dequeuing.\\nWhen the head v1 of the queue is dequeued, v2 becomes the new head.\\n(If the queue becomes empty, then the lemma holds vacuously.) By the\\ninductive hypothesis, v1.d ≤ v2.d. But then we have vr.d ≤ v1.d+1 ≤ v2.d\\n+ 1, and the remaining inequalities are unaffected. Thus, the lemma\\nfollows with v2 as the new head.\\nNow, we examine enqueuing. When line 17 of BFS enqueues a vertex\\nv onto a queue containing vertices 〈v1, v2, … , vr〉, the enqueued vertex\\nbecomes vr+1. If the queue was empty before v was enqueued, then\\nafter enqueuing v, we have r = 1 and the lemma trivially holds. Now\\nsuppose that the queue was nonempty when v was enqueued. At that\\ntime, the procedure has most recently removed vertex u, whose\\nadjacency list is currently being scanned, from the queue Q. Just before\\nu was removed, we had u = v1 and the inductive hypothesis held, so that\\nu.d ≤ v2.d and vr.d ≤ u.d + 1. After u is removed from the queue, the\\nvertex that had been v2 becomes the new head v1 of the queue, so that\\nnow u.d ≤ v1.d. Thus, vr+1.d = v.d = u.d + 1 ≤ v1.d + 1. Since vr.d ≤ u.d +\\n1, we have vr.d ≤ u.d + 1 = v.d = vr+1.d, and the remaining inequalities\\nare unaffected. Thus, the lemma follows when v is enqueued.\\n▪\\nThe following corollary shows that the d values at the time that\\nvertices are enqueued monotonically increase over time.\\nCorollary 20.4\\nSuppose that vertices vi and vj are enqueued during the execution of\\nBFS, and that vi is enqueued before vj. Then vi.d ≤ vj.d at the time that vj\\nis enqueued.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 730}),\n",
              " Document(page_content='Proof\\xa0\\xa0\\xa0Immediate from Lemma 20.3 and the property that each vertex\\nreceives a ﬁnite d value at most once during the course of BFS.\\n▪\\nWe can now prove that breadth-ﬁrst search correctly ﬁnds shortest-\\npath distances.\\nTheorem 20.5 (Correctness of breadth-ﬁrst search)\\nLet G = (V, E) be a directed or undirected graph, and suppose that BFS\\nis run on G from a given source vertex s ∈ V. Then, during its\\nexecution, BFS discovers every vertex v ∈ V that is reachable from the\\nsource s, and upon termination, v.d = δ(s, v) for all v ∈ V. Moreover, for\\nany vertex v ≠ s that is reachable from s, one of the shortest paths from s\\nto v is a shortest path from s to v.π followed by the edge (v.π, v).\\nProof\\xa0\\xa0\\xa0Assume for the purpose of contradiction that some vertex\\nreceives a d value not equal to its shortest-path distance. Of all such\\nvertices, let v be a vertex that has the minimum δ (s, v). By Lemma 20.2,\\nwe have v.d ≥ δ(s, v), and thus v.d > δ (s, v). We cannot have v = s,\\nbecause s.d = 0 and δ (s, s) = 0. Vertex v must be reachable from s, for\\notherwise we would have δ (s, v) = ∞ ≥ v.d. Let u be the vertex\\nimmediately preceding v on some shortest path from s to v (since v ≠ s,\\nvertex u must exist), so that δ (s, v) = δ (s, u)+1. Because δ (s, u) < δ (s, v),\\nand because of how we chose v, we have u.d = δ (s, u). Putting these\\nproperties together gives\\nNow consider the time when BFS chooses to dequeue vertex u from\\nQ in line 11. At this time, vertex v is either white, gray, or black. We\\nshall show that each of these cases leads to a contradiction of inequality\\n(20.1). If v is white, then line 15 sets v.d = u.d + 1, contradicting\\ninequality (20.1). If v is black, then it was already removed from the\\nqueue and, by Corollary 20.4, we have v.d ≤ u.d, again contradicting\\ninequality (20.1). If v is gray, then it was painted gray upon dequeuing\\nsome vertex w, which was removed from Q earlier than u and for which\\nv.d = w.d + 1. By Corollary 20.4, however, w.d ≤ u.d, and so v.d = w.d + 1\\n≤ u.d + 1, once again contradicting inequality (20.1).', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 731}),\n",
              " Document(page_content='Thus we conclude that v.d = δ (s, v) for all v ∈ V. All vertices v\\nreachable from s must be discovered, for otherwise they would have ∞ =\\nv.d > δ(s, v). To conclude the proof of the theorem, observe from lines\\n15–16 that if v.π = u, then v.d = u.d + 1. Thus, to form a shortest path\\nfrom s to v, take a shortest path from s to v.π and then traverse the edge\\n(v.π v).\\n▪\\nBreadth-ﬁrst trees\\nThe blue edges in Figure 20.3 show the breadth-ﬁrst tree built by the\\nBFS procedure as it searches the graph. The tree corresponds to the π\\nattributes. More formally, for a graph G = (V, E) with source s, we\\ndeﬁne the predecessor subgraph of G as Gπ = (Vπ, Eπ), where\\nand\\nThe predecessor subgraph Gπ is a breadth-ﬁrst tree if Vπ consists of the\\nvertices reachable from s and, for all v ∈ Vπ, the subgraph Gπ contains\\na unique simple path from s to v that is also a shortest path from s to v\\nin G. A breadth-ﬁrst tree is in fact a tree, since it is connected and |Eπ| =\\n|Vπ| − 1 (see Theorem B.2 on page 1169). We call the edges in Eπtree\\nedges.\\nThe following lemma shows that the predecessor subgraph produced\\nby the BFS procedure is a breadth-ﬁrst tree.\\nLemma 20.6\\nWhen applied to a directed or undirected graph G = (V, E), procedure\\nBFS constructs π so that the predecessor subgraph Gπ = (Vπ, Eπ) is a\\nbreadth-ﬁrst tree.\\nProof\\xa0\\xa0\\xa0Line 16 of BFS sets v.π = u if and only if (u, v) = E and δ (s, v) <\\n∞—that is, if v is reachable from s—and thus Vπ consists of the vertices', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 732}),\n",
              " Document(page_content='in V reachable from s. Since the predecessor subgraph Gπ forms a tree,\\nby Theorem B.2, it contains a unique simple path from s to each vertex\\nin Vπ. Applying Theorem 20.5 inductively yields that every such path is\\na shortest path in G.\\nThe PRINT-PATH procedure prints out the vertices on a shortest\\npath from s to v, assuming that BFS has already computed a breadth-\\nﬁrst tree. This procedure runs in time linear in the number of vertices in\\nthe path printed, since each recursive call is for a path one vertex\\nshorter.\\nPRINT-PATH(G, s, v)\\n\\xa0\\xa01if v == s\\n\\xa0\\xa02print s\\n\\xa0\\xa03elseif v.π == NIL\\n\\xa0\\xa04print “no path from” s “to” v “exists”\\n\\xa0\\xa05else PRINT-PATH(G, s, v.π)\\n\\xa0\\xa06print v\\nExercises\\n20.2-1\\nShow the d and π values that result from running breadth-ﬁrst search on\\nthe directed graph of Figure 20.2(a), using vertex 3 as the source.\\n20.2-2\\nShow the d and π values that result from running breadth-ﬁrst search on\\nthe undirected graph of Figure 20.3, using vertex u as the source.\\nAssume that neighbors of a vertex are visited in alphabetical order.\\n20.2-3\\nShow that using a single bit to store each vertex color sufﬁces by\\narguing that the BFS procedure produces the same result if line 18 is\\nremoved. Then show how to obviate the need for vertex colors\\naltogether.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 733}),\n",
              " Document(page_content='20.2-4\\nWhat is the running time of BFS if we represent its input graph by an\\nadjacency matrix and modify the algorithm to handle this form of\\ninput?\\n20.2-5\\nArgue that in a breadth-ﬁrst search, the value u.d assigned to a vertex u\\nis independent of the order in which the vertices appear in each\\nadjacency list. Using Figure 20.3 as an example, show that the breadth-\\nﬁrst tree computed by BFS can depend on the ordering within\\nadjacency lists.\\n20.2-6\\nGive an example of a directed graph G = (V, E), a source vertex s ∈ V,\\nand a set of tree edges Eπ ⊆ E such that for each vertex v ∈ V, the\\nunique simple path in the graph (V, Eπ) from s to v is a shortest path in\\nG, yet the set of edges Eπ cannot be produced by running BFS on G, no\\nmatter how the vertices are ordered in each adjacency list.\\n20.2-7\\nThere are two types of professional wrestlers: “faces” (short for\\n“babyfaces,” i.e., “good guys”) and “heels” (“bad guys”). Between any\\npair of professional wrestlers, there may or may not be a rivalry. You are\\ngiven the names of n professional wrestlers and a list of r pairs of\\nwrestlers for which there are rivalries. Give an O(n + r)-time algorithm\\nthat determines whether it is possible to designate some of the wrestlers\\nas faces and the remainder as heels such that each rivalry is between a\\nface and a heel. If it is possible to perform such a designation, your\\nalgorithm should produce it.\\n★ 20.2-8\\nThe diameter of a tree T = (V, E) is deﬁned as max { δ (u, v) : u, v ∈ V},\\nthat is, the largest of all shortest-path distances in the tree. Give an\\nefﬁcient algorithm to compute the diameter of a tree, and analyze the\\nrunning time of your algorithm.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 734}),\n",
              " Document(page_content='20.3\\xa0\\xa0\\xa0\\xa0Depth-ﬁrst search\\nAs its name implies, depth-ﬁrst search searches “deeper” in the graph\\nwhenever possible. Depth-ﬁrst search explores edges out of the most\\nrecently discovered vertex v that still has unexplored edges leaving it.\\nOnce all of v’s edges have been explored, the search “backtracks” to\\nexplore edges leaving the vertex from which v was discovered. This\\nprocess continues until all vertices that are reachable from the original\\nsource vertex have been discovered. If any undiscovered vertices remain,\\nthen depth-ﬁrst search selects one of them as a new source, repeating the\\nsearch from that source. The algorithm repeats this entire process until\\nit has discovered every vertex.3\\nAs in breadth-ﬁrst search, whenever depth-ﬁrst search discovers a\\nvertex v during a scan of the adjacency list of an already discovered\\nvertex u, it records this event by setting v’s predecessor attribute v.π to u.\\nUnlike breadth-ﬁrst search, whose predecessor subgraph forms a tree,\\ndepth-ﬁrst search produces a predecessor subgraph that might contain\\nseveral trees, because the search may repeat from multiple sources.\\nTherefore, we deﬁne the predecessor subgraph of a depth-ﬁrst search\\nslightly differently from that of a breadth-ﬁrst search: it always includes\\nall vertices, and it accounts for multiple sources. Speciﬁcally, for a\\ndepth-ﬁrst search the predecessor subgraph is Gπ = (V, Eπ), where\\nEπ = {(v.π, v) : v ∈ V and v.π ≠ NIL}.\\nThe predecessor subgraph of a depth-ﬁrst search forms a depth-ﬁrst\\nforest comprising several depth-ﬁrst trees. The edges in Eπ are tree\\nedges.\\nLike breadth-ﬁrst search, depth-ﬁrst search colors vertices during the\\nsearch to indicate their state. Each vertex is initially white, is grayed\\nwhen it is discovered in the search, and is blackened when it is ﬁnished,\\nthat is, when its adjacency list has been examined completely. This\\ntechnique guarantees that each vertex ends up in exactly one depth-ﬁrst\\ntree, so that these trees are disjoint.\\nBesides creating a depth-ﬁrst forest, depth-ﬁrst search also\\ntimestamps each vertex. Each vertex v has two timestamps: the ﬁrst', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 735}),\n",
              " Document(page_content='timestamp v.d records when v is ﬁrst discovered (and grayed), and the\\nsecond timestamp v.f records when the search ﬁnishes examining v’s\\nadjacency list (and blackens v). These timestamps provide important\\ninformation about the structure of the graph and are generally helpful\\nin reasoning about the behavior of depth-ﬁrst search.\\nThe procedure DFS on the facing page records when it discovers\\nvertex u in the attribute u.d and when it ﬁnishes vertex u in the attribute\\nu.f. These timestamps are integers between 1 and 2 |V|, since there is one\\ndiscovery event and one ﬁnishing event for each of the |V| vertices. For\\nevery vertex u,\\nVertex u is WHITE before time u.d, GRAY between time u.d and time\\nu.f, and BLACK thereafter. In the DFS procedure, the input graph G\\nmay be undirected or directed. The variable time is a global variable\\nused for timestamping. Figure 20.4 illustrates the progress of DFS on\\nthe graph shown in Figure 20.2 (but with vertices labeled by letters\\nrather than numbers).\\nDFS(G)\\n\\xa0\\xa01for each vertex u ∈ G.V\\n\\xa0\\xa02u.color = WHITE\\n\\xa0\\xa03u.π = NIL\\n\\xa0\\xa04time = 0\\n\\xa0\\xa05for each vertex u ∈ G.V\\n\\xa0\\xa06if u.color == WHITE\\n\\xa0\\xa07 DFS-VISIT(G, u)\\nDFS-VISIT(G, u)\\n\\xa0\\xa01time = time + 1 // white vertex u has just been discovered\\n\\xa0\\xa02u.d = time\\n\\xa0\\xa03u.color = GRAY\\n\\xa0\\xa04for each vertex v in G.Adj[u]// explore each edge (u, v)\\n\\xa0\\xa05if v.color == WHITE\\n\\xa0\\xa06 v.π = u', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 736}),\n",
              " Document(page_content='\\xa0\\xa07 DFS-VISIT(G, v)\\n\\xa0\\xa08time = time + 1\\n\\xa0\\xa09u.f = time\\n10u.color = BLACK // blacken u; it is ﬁnished\\nThe DFS procedure works as follows. Lines 1–3 paint all vertices\\nwhite and initialize their π attributes to NIL. Line 4 resets the global\\ntime counter. Lines 5–7 check each vertex in V in turn and, when a\\nwhite vertex is found, visit it by calling DFS-VISIT. Upon every call of\\nDFS-VISIT(G, u) in line 7, vertex u becomes the root of a new tree in\\nthe depth-ﬁrst forest. When DFS returns, every vertex u has been\\nassigned a discovery time u.d and a ﬁnish time u.f.\\nIn each call DFS-VISIT(G, u), vertex u is initially white. Lines 1–3\\nincrement the global variable time, record the new value of time as the\\ndiscovery time u.d, and paint u gray. Lines 4–7 examine each vertex v\\nadjacent to u and recursively visit v if it is white. As line 4 considers each\\nvertex v ∈ Adj[u], the depth-ﬁrst search explores edge (u, v). Finally,\\nafter every edge leaving u has been explored, lines 8–10 increment time,\\nrecord the ﬁnish time in u.f, and paint u black.\\nThe results of depth-ﬁrst search may depend upon the order in which\\nline 5 of DFS examines the vertices and upon the order in which line 4\\nof DFS-VISIT visits the neighbors of a vertex. These different visitation\\norders tend not to cause problems in practice, because many\\napplications of depth-ﬁrst search can use the result from any depth-ﬁrst\\nsearch.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 737}),\n",
              " Document(page_content='Figure 20.4 The progress of the depth-ﬁrst-search algorithm DFS on a directed graph. Edges are\\nclassiﬁed as they are explored: tree edges are labeled T, back edges B, forward edges F, and cross\\nedges C. Timestamps within vertices indicate discovery time/ﬁnish times. Tree edges are\\nhighlighted in blue. Orange highlights indicate vertices whose discovery or ﬁnish times change\\nand edges that are explored in each step.\\nWhat is the running time of DFS? The loops on lines 1–3 and lines\\n5–7 of DFS take Θ (V) time, exclusive of the time to execute the calls to\\nDFS-VISIT. As we did for breadth-ﬁrst search, we use aggregate\\nanalysis. The procedure DFS-VISIT is called exactly once for each\\nvertex v ∈ V, since the vertex u on which DFS-VISIT is invoked must\\nbe white and the ﬁrst thing DFS-VISIT does is paint vertex u gray.\\nDuring an execution of DFS-VISIT(G, v), the loop in lines 4–7 executes\\n|Adj[v]| times. Since Σv ∈V |Adj[v]| = Θ (E) and DFS-VISIT is called once\\nper vertex, the total cost of executing lines 4–7 of DFS-VISIT is Θ (V +\\nE). The running time of DFS is therefore Θ (V + E).\\nProperties of depth-ﬁrst search', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 738}),\n",
              " Document(page_content='Depth-ﬁrst search yields valuable information about the structure of a\\ngraph. Perhaps the most basic property of depth-ﬁrst search is that the\\npredecessor subgraph Gπ does indeed form a forest of trees, since the\\nstructure of the depth-ﬁrst trees exactly mirrors the structure of\\nrecursive calls of DFS-VISIT. That is, u = v.π if and only if DFS-\\nVISIT(G, v) was called during a search of u’s adjacency list.\\nAdditionally, vertex v is a descendant of vertex u in the depth-ﬁrst forest\\nif and only if v is discovered during the time in which u is gray.\\nAnother important property of depth-ﬁrst search is that discovery\\nand ﬁnish times have parenthesis structure. If the DFS-VISIT procedure\\nwere to print a left parenthesis “(u” when it discovers vertex u and to\\nprint a right parenthesis “u)” when it ﬁnishes u, then the printed\\nexpression would be well formed in the sense that the parentheses are\\nproperly nested. For example, the depth-ﬁrst search of Figure 20.5(a)\\ncorresponds to the parenthesization shown in Figure 20.5(b). The\\nfollowing theorem provides another way to characterize the parenthesis\\nstructure.\\nTheorem 20.7 (Parenthesis theorem)\\nIn any depth-ﬁrst search of a (directed or undirected) graph G = (V, E),\\nfor any two vertices u and v, exactly one of the following three\\nconditions holds:\\nthe intervals [u.d, u.f] and [v.d, v.f] are entirely disjoint, and neither\\nu nor v is a descendant of the other in the depth-ﬁrst forest,\\nthe interval [u.d, u.f] is contained entirely within the interval [v.d,\\nv.f], and u is a descendant of v in a depth-ﬁrst tree, or\\nthe interval [v.d, v.f] is contained entirely within the interval [u.d,\\nu.f], and v is a descendant of u in a depth-ﬁrst tree.\\nProof\\xa0\\xa0\\xa0We begin with the case in which u.d < v.d. We consider two\\nsubcases, according to whether v.d < u.f. The ﬁrst subcase occurs when\\nv.d < u.f, so that v was discovered while u was still gray, which implies\\nthat v is a descendant of u. Moreover, since v was discovered after u, all\\nof its outgoing edges are explored, and v is ﬁnished, before the search\\nreturns to and ﬁnishes u. In this case, therefore, the interval [v.d, v.f] is', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 739}),\n",
              " Document(page_content='entirely contained within the interval [u.d, u.f]. In the other subcase, u.f\\n< v.d, and by inequality (20.4), u.d < u.f < v.d < v.f, and thus the\\nintervals [u.d, u.f] and [v.d, v.f] are disjoint. Because the intervals are\\ndisjoint, neither vertex was discovered while the other was gray, and so\\nneither vertex is a descendant of the other.\\nFigure 20.5 Properties of depth-ﬁrst search. (a) The result of a depth-ﬁrst search of a directed\\ngraph. Vertices are timestamped and edge types are indicated as in Figure 20.4. (b) Intervals for\\nthe discovery time and ﬁnish time of each vertex correspond to the parenthesization shown.\\nEach rectangle spans the interval given by the discovery and ﬁnish times of the corresponding\\nvertex. Only tree edges are shown. If two intervals overlap, then one is nested within the other,\\nand the vertex corresponding to the smaller interval is a descendant of the vertex corresponding\\nto the larger. (c) The graph of part (a) redrawn with all tree and forward edges going down\\nwithin a depth-ﬁrst tree and all back edges going up from a descendant to an ancestor.\\nThe case in which v.d < u.d is similar, with the roles of u and v\\nreversed in the above argument.\\n▪\\nCorollary 20.8 (Nesting of descendant s’ intervals)\\nVertex v is a proper descendant of vertex u in the depth-ﬁrst forest for a\\n(directed or undirected) graph G if and only if u.d < v.d < v.f < u.f.\\nProof\\xa0\\xa0\\xa0Immediate from Theorem 20.7.\\n▪', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 740}),\n",
              " Document(page_content='The next theorem gives another important characterization of when\\none vertex is a descendant of another in the depth-ﬁrst forest.\\nTheorem 20.9 (White-path theorem)\\nIn a depth-ﬁrst forest of a (directed or undirected) graph G = (V, E),\\nvertex v is a descendant of vertex u if and only if at the time u.d that the\\nsearch discovers u, there is a path from u to v consisting entirely of white\\nvertices.\\nProof\\xa0\\xa0\\xa0 ⇒ : If v = u, then the path from u to v contains just vertex u,\\nwhich is still white when u.d receives a value. Now, suppose that v is a\\nproper descendant of u in the depth-ﬁrst forest. By Corollary 20.8, u.d <\\nv.d, and so v is white at time u.d. Since v can be any descendant of u, all\\nvertices on the unique simple path from u to v in the depth-ﬁrst forest\\nare white at time u.d.\\n⇐: Suppose that there is a path of white vertices from u to v at time\\nu.d, but v does not become a descendant of u in the depth-ﬁrst tree.\\nWithout loss of generality, assume that every vertex other than v along\\nthe path becomes a descendant of u. (Otherwise, let v be the closest\\nvertex to u along the path that doesn’t become a descendant of u.) Let w\\nbe the predecessor of v in the path, so that w is a descendant of u (w and\\nu may in fact be the same vertex). By Corollary 20.8, w.f ≤ u.f. Because v\\nmust be discovered after u is discovered, but before w is ﬁnished, u.d <\\nv.d < w.f ≤ u.f. Theorem 20.7 then implies that the interval [v.d, v.f] is\\ncontained entirely within the interval [u.d, u.f]. By Corollary 20.8, v\\nmust after all be a descendant of u.\\n▪\\nClassiﬁcation of edges\\nYou can obtain important information about a graph by classifying its\\nedges during a depth-ﬁrst search. For example, Section 20.4 will show\\nthat a directed graph is acyclic if and only if a depth-ﬁrst search yields\\nno “back” edges (Lemma 20.11).\\nThe depth-ﬁrst forest Gπ produced by a depth-ﬁrst search on graph\\nG can contain four types of edges:', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 741}),\n",
              " Document(page_content='1. Tree edges are edges in the depth-ﬁrst forest Gπ. Edge (u, v) is a\\ntree edge if v was ﬁrst discovered by exploring edge (u, v).\\n2. Back edges are those edges (u, v) connecting a vertex u to an\\nancestor v in a depth-ﬁrst tree. We consider self-loops, which\\nmay occur in directed graphs, to be back edges.\\n3. Forward edges are those nontree edges (u, v) connecting a vertex\\nu to a proper descendant v in a depth-ﬁrst tree.\\n4. Cross edges are all other edges. They can go between vertices in\\nthe same depth-ﬁrst tree, as long as one vertex is not an ancestor\\nof the other, or they can go between vertices in different depth-\\nﬁrst trees.\\nIn Figures 20.4 and 20.5, edge labels indicate edge types. Figure 20.5(c)\\nalso shows how to redraw the graph of Figure 20.5(a) so that all tree\\nand forward edges head downward in a depth-ﬁrst tree and all back\\nedges go up. You can redraw any graph in this fashion.\\nThe DFS algorithm has enough information to classify some edges\\nas it encounters them. The key idea is that when an edge (u, v) is ﬁrst\\nexplored, the color of vertex v says something about the edge:\\n1. WHITE indicates a tree edge,\\n2. GRAY indicates a back edge, and\\n3. BLACK indicates a forward or cross edge.\\nThe ﬁrst case is immediate from the speciﬁcation of the algorithm. For\\nthe second case, observe that the gray vertices always form a linear\\nchain of descendants corresponding to the stack of active DFS-VISIT\\ninvocations. The number of gray vertices is 1 more than the depth in the\\ndepth-ﬁrst forest of the vertex most recently discovered. Depth-ﬁrst\\nsearch always explores from the deepest gray vertex, so that an edge that\\nreaches another gray vertex has reached an ancestor. The third case\\nhandles the remaining possibility. Exercise 20.3-5 asks you to show that\\nsuch an edge (u, v) is a forward edge if u.d < v.d and a cross edge if u.d >\\nv.d.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 742}),\n",
              " Document(page_content='According to the following theorem, forward and cross edges never\\noccur in a depth-ﬁrst search of an undirected graph.\\nTheorem 20.10\\nIn a depth-ﬁrst search of an undirected graph G, every edge of G is\\neither a tree edge or a back edge.\\nProof\\xa0\\xa0\\xa0Let (u, v) be an arbitrary edge of G, and suppose without loss of\\ngenerality that u.d < v.d. Then, while u is gray, the search must discover\\nand ﬁnish v before it ﬁnishes u, since v is on u’s adjacency list. If the ﬁrst\\ntime that the search explores edge (u, v), it is in the direction from u to v,\\nthen v is undiscovered (white) until that time, for otherwise the search\\nwould have explored this edge already in the direction from v to u. Thus,\\n(u, v) becomes a tree edge. If the search explores (u, v) ﬁrst in the\\ndirection from v to u, then (u, v) is a back edge, since there must be a\\npath of tree edges from u to v.\\n▪\\nSince (u, v) and (v, u) are really the same edge in an undirected\\ngraph, the proof of Theorem 20.10 says how to classify the edge. When\\nsearching from a vertex, which must be gray, if the adjacent vertex is\\nwhite, then the edge is a tree edge. Otherwise, the edge is a back edge.\\nThe next two sections apply the above theorems about depth-ﬁrst\\nsearch.\\nFigure 20.6 A directed graph for use in Exercises 20.3-2 and 20.5-2.\\nExercises\\n20.3-1', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 743}),\n",
              " Document(page_content='Make a 3-by-3 chart with row and column labels WHITE, GRAY, and\\nBLACK. In each cell (i, j), indicate whether, at any point during a\\ndepth-ﬁrst search of a directed graph, there can be an edge from a\\nvertex of color i to a vertex of color j. For each possible edge, indicate\\nwhat edge types it can be. Make a second such chart for depth-ﬁrst\\nsearch of an undirected graph.\\n20.3-2\\nShow how depth-ﬁrst search works on the graph of Figure 20.6. Assume\\nthat the for loop of lines 5–7 of the DFS procedure considers the\\nvertices in alphabetical order, and assume that each adjacency list is\\nordered alphabetically. Show the discovery and ﬁnish times for each\\nvertex, and show the classiﬁcation of each edge.\\n20.3-3\\nShow the parenthesis structure of the depth-ﬁrst search of Figure 20.4.\\n20.3-4\\nShow that using a single bit to store each vertex color sufﬁces by\\narguing that the DFS procedure produces the same result if line 10 of\\nDFS-VISIT is removed.\\n20.3-5\\nShow that in a directed graph, edge (u, v) is\\na. a tree edge or forward edge if and only if u.d < v.d < v.f < u.f,\\nb. a back edge if and only if v.d ≤ u.d < u.f ≤ v.f, and\\nc. a cross edge if and only if v.d < v.f < u.d < u.f.\\n20.3-6\\nRewrite the procedure DFS, using a s tack to eliminate recursion.\\n20.3-7\\nGive a counterexample to the conjecture that if a directed graph G\\ncontains a path from u to v, and if u.d < v.d in a depth-ﬁrst search of G,\\nthen v is a descendant of u in the depth-ﬁrst forest produced.\\n20.3-8', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 744}),\n",
              " Document(page_content='Give a counterexample to the conjecture that if a directed graph G\\ncontains a path from u to v, then any depth-ﬁrst search must result in v.d\\n≤ u.f.\\n20.3-9\\nModify the pseudocode for depth-ﬁrst search so that it prints out every\\nedge in the directed graph G, together with its type. Show what\\nmodiﬁcations, if any, you need to make if G is undirected.\\n20.3-10\\nExplain how a vertex u of a directed graph can end up in a depth-ﬁrst\\ntree containing only u, even though u has both incoming and outgoing\\nedges in G.\\n20.3-11\\nLet G = (V, E) be a connected, undirected graph. Give an O(V + E)-\\ntime algorithm to compute a path in G that traverses each edge in E\\nexactly once in each direction. Describe how you can ﬁnd your way out\\nof a maze if you are given a large supply of pennies.\\n20.3-12\\nShow how to use a depth-ﬁrst search of an undirected graph G to\\nidentify the connected components of G, so that the depth-ﬁrst forest\\ncontains as many trees as G has connected components. More precisely,\\nshow how to modify depth-ﬁrst search so that it assigns to each vertex v\\nan integer label v.cc between 1 and k, where k is the number of\\nconnected components of G, such that u.cc = v.cc if and only if u and v\\nbelong to the same connected component.\\n★ 20.3-13\\nA directed graph G = (V, E) is singly connected if u ⇝ v implies that G\\ncontains at most one simple path from u to v for all vertices u, v ∈ V.\\nGive an efﬁcient algorithm to determine whether a directed graph is\\nsingly connected.\\n20.4\\xa0\\xa0\\xa0\\xa0Topological sort', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 745}),\n",
              " Document(page_content='This section shows how to use depth-ﬁrst search to perform a\\ntopological sort of a directed acyclic graph, or a “dag” as it is\\nsometimes called. A topological sort of a dag G = (V, E) is a linear\\nordering of all its vertices such that if G contains an edge (u, v), then u\\nappears before v in the ordering. Topological sorting is deﬁned only on\\ndirected graphs that are acyclic; no linear ordering is possible when a\\ndirected graph contains a cycle. Think of a topological sort of a graph\\nas an ordering of its vertices along a horizontal line so that all directed\\nedges go from left to right. Topological sorting is thus different from the\\nusual kind of “sorting” studied in Part II.\\nMany applications use directed acyclic graphs to indicate\\nprecedences among events. Figure 20.7 gives an example that arises\\nwhen Professor Bumstead gets dressed in the morning. The professor\\nmust don certain garments before others (e.g., socks before shoes).\\nOther items may be put on in any order (e.g., socks and pants). A\\ndirected edge (u, v) in the dag of Figure 20.7(a) indicates that garment u\\nmust be donned before garment v. A topological sort of this dag\\ntherefore gives a possible order for getting dressed. Figure 20.7(b) shows\\nthe topologically sorted dag as an ordering of vertices along a\\nhorizontal line such that all directed edges go from left to right.\\nThe procedure TOPOLOGICAL-SORT topologically sorts a dag.\\nFigure 20.7(b) shows how the topologically sorted vertices appear in\\nreverse order of their ﬁnish times.\\nTOPOLOGICAL-SORT(G)\\n1call DFS(G) to compute ﬁnish times v.f for each vertex v\\n2as each vertex is ﬁnished, insert it onto the front of a linked list\\n3return the linked list of vertices\\nThe TOPOLOGICAL-SORT procedure runs in Θ (V + E) time, since\\ndepth-ﬁrst search takes Θ (V + E) time and it takes O(1) time to insert\\neach of the |V| vertices onto the front of the linked list.\\nTo prove the correctness of this remarkably simple and efﬁcient\\nalgorithm, we start with the following key lemma characterizing\\ndirected acyclic graphs.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 746}),\n",
              " Document(page_content='Lemma 20.11\\nA directed graph G is acyclic if and only if a depth-ﬁrst search of G\\nyields no back edges.\\nFigure 20.7 (a) Professor Bumstead topologically sorts his clothing when getting dressed. Each\\ndirected edge (u, v) means that garment u must be put on before garment v. The discovery and\\nﬁnish times from a depth-ﬁrst search are shown next to each vertex. (b) The same graph shown\\ntopologically sorted, with its vertices arranged from left to right in order of decreasing ﬁnish\\ntime. All directed edges go from left to right.\\nProof\\xa0\\xa0\\xa0 ⇒: Suppose that a depth-ﬁrst search produces a back edge (u, v).\\nThen vertex v is an ancestor of vertex u in the depth-ﬁrst forest. Thus, G\\ncontains a path from v to u, and the back edge (u, v) completes a cycle.\\n⇐: Suppose that G contains a cycle c. We show that a depth-ﬁrst\\nsearch of G yields a back edge. Let v be the ﬁrst vertex to be discovered\\nin c, and let (u, v) be the preceding edge in c. At time v.d, the vertices of\\nc form a path of white vertices from v to u. By the white-path theorem,\\nvertex u becomes a descendant of v in the depth-ﬁrst forest. Therefore,\\n(u, v) is a back edge.\\n▪\\nTheorem 20.12\\nTOPOLOGICAL-SORT produces a topological sort of the directed\\nacyclic graph provided as its input.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 747}),\n",
              " Document(page_content='Proof\\xa0\\xa0\\xa0Suppose that DFS is run on a given dag G = (V, E) to determine\\nﬁnish times for its vertices. It sufﬁces to show that for any pair of\\ndistinct vertices u, v ∈ V, if G contains an edge from u to v, then v.f <\\nu.f. Consider any edge (u, v) explored by DFS(G). When this edge is\\nexplored, v cannot be gray, since then v would be an ancestor of u and\\n(u, v) would be a back edge, contradicting Lemma 20.11. Therefore, v\\nmust be either white or black. If v is white, it becomes a descendant of u,\\nand so v.f < u.f. If v is black, it has already been ﬁnished, so that v.f has\\nalready been set. Because the search is still exploring from u, it has yet\\nto assign a timestamp to u.f, so that the timestamp eventually assigned\\nto u.f is greater than v.f. Thus, v.f < u.f for any edge (u, v) in the dag,\\nproving the theorem.\\n▪\\nFigure 20.8 A dag for topological sorting.\\nExercises\\n20.4-1\\nShow the ordering of vertices produced by TOPOLOGICAL-SORT\\nwhen it is run on the dag of Figure 20.8. Assume that the for loop of\\nlines 5–7 of the DFS procedure considers the vertices in alphabetical\\norder, and assume that each adjacency list is ordered alphabetically.\\n20.4-2\\nGive a linear-time algorithm that, given a directed acyclic graph G = (V,\\nE) and two vertices a, b ∈ V, returns the number of simple paths from a\\nto b in G. For example, the directed acyclic graph of Figure 20.8\\ncontains exactly four simple paths from vertex p to vertex v: 〈p, o, v〉, 〈p,', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 748}),\n",
              " Document(page_content='o, r, y, v〉, 〈p, o, s, r, y, v〉, and 〈p, s, r, y, v〉. Your algorithm needs only to\\ncount the simple paths, not list them.\\n20.4-3\\nGive an algorithm that determines whether an undirected graph G = (V,\\nE) contains a simple cycle. Your algorithm should run in O(V) time,\\nindependent of |E|.\\n20.4-4\\nProve or disprove: If a directed graph G contains cycles, then the vertex\\nordering produced by TOPOLOGICAL-SORT(G) minimizes the\\nnumber of “bad” edges that are inconsistent with the ordering\\nproduced.\\n20.4-5\\nAnother way to topologically sort a directed acyclic graph G = (V, E) is\\nto repeatedly ﬁnd a vertex of in-degree 0, output it, and remove it and\\nall of its outgoing edges from the graph. Explain how to implement this\\nidea so that it runs in time O(V + E). What happens to this algorithm if\\nG has cycles?\\n20.5\\xa0\\xa0\\xa0\\xa0Strongly connected components\\nWe now consider a classic application of depth-ﬁrst search:\\ndecomposing a directed graph into its strongly connected components.\\nThis section shows how to do so using two depth-ﬁrst searches. Many\\nalgorithms that work with directed graphs begin with such a\\ndecomposition. After decomposing the graph into strongly connected\\ncomponents, such algorithms run separately on each one and then\\ncombine the solutions according to the structure of connections among\\ncomponents.\\nRecall from Appendix B that a strongly connected component of a\\ndirected graph G = (V, E) is a maximal set of vertices C ⊆ V such that\\nfor every pair of vertices u, v ∈ C, both u ⇝ v and v ⇝ u, that is, vertices\\nu and v are reachable from each other. Figure 20.9 shows an example.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 749}),\n",
              " Document(page_content='The algorithm for ﬁnding the strongly connected components of a\\ndirected graph G = (V, E) uses the transpose of G, which we deﬁned in\\nExercise 20.1-3 to be the graph GT = (V, ET), where ET = {(u, v) : (v, u)\\n∈ E}. That is, ET consists of the edges of G with their directions\\nreversed. Given an adjacency-list representation of G, the time to create\\nGT is Θ (V + E). The graphs G and GT have exactly the same strongly\\nconnected components: u and v are reachable from each other in G if\\nand only if they are reachable from each other in GT. Figure 20.9(b)\\nshows the transpose of the graph in Figure 20.9(a), with the strongly\\nconnected components shaded blue in both parts.\\nThe linear-time (i.e., Θ (V + E)-time) procedure STRONGLY-\\nCONNECTED-COMPONENTS on the next page computes the\\nstrongly connected components of a directed graph G = (V, E) using\\ntwo depth-ﬁrst searches, one on G and one on GT.\\nThe idea behind this algorithm comes from a key property of the\\ncomponent graph GSCC = (VSCC, ESCC), deﬁned as follows. Suppose\\nthat G has strongly connected components C1, C2, … , Ck. The vertex\\nset VSCC is {v1, v2, … , vk}, and it contains one vertex vi for each\\nstrongly connected component Ci of G. There is an edge (vi, vj) ∈\\nESCC if G contains a directed edge (x, y) for some x ∈ Ci and some y\\n∈ Cj. Looked at another way, if we contract all edges whose incident\\nvertices are within the same strongly connected component of G so that\\nonly a single vertex remains, the resulting graph is GSCC. Figure 20.9(c)\\nshows the component graph of the graph in Figure 20.9(a).', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 750}),\n",
              " Document(page_content='Figure 20.9 (a) A directed graph G. Each region shaded light blue is a strongly connected\\ncomponent of G. Each vertex is labeled with its discovery and ﬁnish times in a depth-ﬁrst\\nsearch, and tree edges are dark blue. (b) The graph GT, the transpose of G, with the depth-ﬁrst\\nforest computed in line 3 of STRONGLY-CONNECTED-COMPONENTS shown and tree\\nedges shaded dark blue. Each strongly connected component corresponds to one depth-ﬁrst\\ntree. Orange vertices b, c, g, and h are the roots of the depth-ﬁrst trees produced by the depth-\\nﬁrst search of GT. (c) The acyclic component graph GSCC obtained by contracting all edges\\nwithin each strongly connected component of G so that only a single vertex remains in each\\ncomponent.\\nSTRONGLY-CONNECTED-COMPONENTS(G)\\n1call DFS(G) to compute ﬁnish times u.f for each vertex u\\n2create GT\\n3call DFS(GT), but in the main loop of DFS, consider the vertices in\\norder of decreasing u.f (as computed in line 1)\\n4output the vertices of each tree in the depth-ﬁrst forest formed in\\nline 3 as a separate strongly connected component\\nThe following lemma gives the key property that the component\\ngraph is acyclic. We’ll see that the algorithm uses this property to visit\\nthe vertices of the component graph in topologically sorted order, by\\nconsidering vertices in the second depth-ﬁrst search in decreasing order\\nof the ﬁnish times that were computed in the ﬁrst depth-ﬁrst search.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 751}),\n",
              " Document(page_content='Lemma 20.13\\nLet C and C′ be distinct strongly connected components in directed\\ngraph G = (V, E), let u, v ∈ C, let u′, v′ ∈ C′, and suppose that G\\ncontains a path u ⇝ u′. Then G cannot also contain a path v′ ⇝ v.\\nProof\\xa0\\xa0\\xa0If G contains a path v′ ⇝ v, then it contains paths u ⇝ u′ ⇝ v′ and\\nv′ ⇝ v ⇝ u. Thus, u and v′ are reachable from each other, thereby\\ncontradicting the assumption that C and C′ are distinct strongly\\nconnected components.\\n▪\\nBecause the STRONGLY-CONNECTED-COMPONENTS\\nprocedure performs two depth-ﬁrst searches, there are two distinct sets\\nof discovery and ﬁnish times. In this section, discovery and ﬁnish times\\nalways refer to those computed by the ﬁrst call of DFS, in line 1.\\nThe notation for discovery and ﬁnish times extends to sets of\\nvertices. For a subset U of vertices, d(U) and f(U) are the earliest\\ndiscovery time and latest ﬁnish time, respectively, of any vertex in U:\\nd(U) = min {u.d : u ∈ U} and f(U) = max {u.f : u ∈ U}.\\nThe following lemma and its corollary give a key property relating\\nstrongly connected components and ﬁnish times in the ﬁrst depth-ﬁrst\\nsearch.\\nLemma 20.14\\nLet C and C′ be distinct strongly connected components in directed\\ngraph G = (V, E). Suppose that there is an edge (u, v) ∈ E, where u ∈ C′\\nand v ∈ C. Then f(C′) > f(C).\\nProof\\xa0\\xa0\\xa0We consider two cases, depending on which strongly connected\\ncomponent, C or C′, had the ﬁrst discovered vertex during the ﬁrst\\ndepth-ﬁrst search.\\nIf d(C′) < d(C), let x be the ﬁrst vertex discovered in C′. At time x.d,\\nall vertices in C and C′ are white. At that time, G contains a path from x\\nto each vertex in C′ consisting only of white vertices. Because (u, v) ∈ E,\\nfor any vertex w ∈ C, there is also a path in G at time x.d from x to w\\nconsisting only of white vertices: x ⇝ u → v ⇝ w. By the white-path', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 752}),\n",
              " Document(page_content='theorem, all vertices in C and C′ become descendants of x in the depth-\\nﬁrst tree. By Corollary 20.8, x has the latest ﬁnish time of any of its\\ndescendants, and so x.f = f(C′) > f(C).\\nOtherwise, d(C′) > d(C). Let y be the ﬁrst vertex discovered in C, so\\nthat y.d = d(C). At time y.d, all vertices in C are white and G contains a\\npath from y to each vertex in C consisting only of white vertices. By the\\nwhite-path theorem, all vertices in C become descendants of y in the\\ndepth-ﬁrst tree, and by Corollary 20.8, y.f = f(C). Because d(C′) > d(C)\\n= y.d, all vertices in C′ are white at time y.d. Since there is an edge (u, v)\\nfrom C′ to C, Lemma 20.13 implies that there cannot be a path from C\\nto C′. Hence, no vertex in C′ is reachable from y. At time y.f, therefore,\\nall vertices in C′ are still white. Thus, for any vertex w ∈ C′, we have w.f\\n> y.f, which implies that f(C′) > f(C).\\n▪\\nCorollary 20.15\\nLet C and C′ be distinct strongly connected components in directed\\ngraph G = (V, E), and suppose that f(C) > f(C′). Then ET contains no\\nedge (v, u) such that u = C′ and v ∈ C.\\nProof\\xa0\\xa0\\xa0The contrapositive of Lemma 20.14 says that if f(C′) < f(C), then\\nthere is no edge (u, v) ∈ E such that u ∈ C′ and v ∈ C. Because the\\nstrongly connected components of G and GT are the same, if there is no\\nsuch edge (u, v) ∈ E, then there is no edge (v, u) ∈ ET such that u ∈ C′\\nand v ∈ C.\\n▪\\nCorollary 20.15 provides the key to understanding why the strongly\\nconnected components algorithm works. Let’s examine what happens\\nduring the second depth-ﬁrst search, which is on GT. The search starts\\nfrom the vertex x whose ﬁnish time from the ﬁrst depth-ﬁrst search is\\nmaximum. This vertex belongs to some strongly connected component\\nC, and since x.f is maximum, f(C) is maximum over all strongly\\nconnected components. When the search starts from x, it visits all\\nvertices in C. By Corollary 20.15, GT contains no edges from C to any', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 753}),\n",
              " Document(page_content='other strongly connected component, and so the search from x never\\nvisits vertices in any other component. Thus, the tree rooted at x\\ncontains exactly the vertices of C. Having completed visiting all vertices\\nin C, the second depth-ﬁrst search selects as a new root a vertex from\\nsome other strongly connected component C′ whose ﬁnish time f(C′) is\\nmaximum over all components other than C. Again, the search visits all\\nvertices in C′. But by Corollary 20.15, if any edges in GT go from C′ to\\nany other component, they must go to C, which the second depth-ﬁrst\\nsearch has already visited. In general, when the depth-ﬁrst search of GT\\nin line 3 visits any strongly connected component, any edges out of that\\ncomponent must be to components that the search has already visited.\\nEach depth-ﬁrst tree, therefore, corresponds to exactly one strongly\\nconnected component. The following theorem formalizes this argument.\\nTheorem 20.16\\nThe STRONGLY-CONNECTED-COMPONENTS procedure\\ncorrectly computes the strongly connected components of the directed\\ngraph G provided as its input.\\nProof\\xa0\\xa0\\xa0We argue by induction on the number of depth-ﬁrst trees found\\nin the depth-ﬁrst search of GT in line 3 that the vertices of each tree\\nform a strongly connected component. The inductive hypothesis is that\\nthe ﬁrst k trees produced in line 3 are strongly connected components.\\nThe basis for the induction, when k = 0, is trivial.\\nIn the inductive step, we assume that each of the ﬁrst k depth-ﬁrst\\ntrees produced in line 3 is a strongly connected component, and we\\nconsider the (k + 1)st tree produced. Let the root of this tree be vertex u,\\nand let u be in strongly connected component C. Because of how the\\ndepth-ﬁrst search chooses roots in line 3, u.f = f(C) > f(C′) for any\\nstrongly connected component C′ other than C that has yet to be\\nvisited. By the inductive hypothesis, at the time that the search visits u,\\nall other vertices of C are white. By the white-path theorem, therefore,\\nall other vertices of C are descendants of u in its depth-ﬁrst tree.\\nMoreover, by the inductive hypothesis and by Corollary 20.15, any\\nedges in GT that leave C must be to strongly connected components that', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 754}),\n",
              " Document(page_content='have already been visited. Thus, no vertex in any strongly connected\\ncomponent other than C is a descendant of u during the depth-ﬁrst\\nsearch of GT. The vertices of the depth-ﬁrst tree in GT that is rooted at\\nu form exactly one strongly connected component, which completes the\\ninductive step and the proof.\\n▪\\nHere is another way to look at how the second depth-ﬁrst search\\noperates. Consider the component graph (GT)SCC of GT. If you map\\neach strongly connected component visited in the second depth-ﬁrst\\nsearch to a vertex of (GT)SCC, the second depth-ﬁrst search visits\\nvertices of (GT)SCC in the reverse of a topologically sorted order. If you\\nreverse the edges of (GT)SCC, you get the graph ((GT)SCC)T. Because\\n((GT)SCC)T = GSCC (see Exercise 20.5-4), the second depth-ﬁrst\\nsearch visits the vertices of GSCC in topologically sorted order.\\nExercises\\n20.5-1\\nHow can the number of strongly connected components of a graph\\nchange if a new edge is added?\\n20.5-2\\nShow how the procedure STRONGLY-CONNECTED-\\nCOMPONENTS works on the graph of Figure 20.6. Speciﬁcally, show\\nthe ﬁnish times computed in line 1 and the forest produced in line 3.\\nAssume that the loop of lines 5–7 of DFS considers vertices in\\nalphabetical order and that the adjacency lists are in alphabetical order.\\n20.5-3\\nProfessor Bacon rewrites the algorithm for strongly connected\\ncomponents to use the original (instead of the transpose) graph in the\\nsecond depth-ﬁrst search and scan the vertices in order of increasing\\nﬁnish times. Does this modiﬁed algorithm always produce correct\\nresults?', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 755}),\n",
              " Document(page_content='20.5-4\\nProve that for any directed graph G, the transpose of the component\\ngraph of GT is the same as the component graph of G. That is,\\n((GT)SCC)T = GSCC.\\n20.5-5\\nGive an O(V + E)-time algorithm to compute the component graph of a\\ndirected graph G = (V, E). Make sure that there is at most one edge\\nbetween two vertices in the component graph your algorithm produces.\\n20.5-6\\nGive an O(V + E)-time algorithm that, given a directed graph G = (V,\\nE), constructs another graph G′ = (V, E′) such that G and G′ have the\\nsame strongly connected components, G′ has the same component\\ngraph as G, and |E′| is as small as possible.\\n20.5-7\\nA directed graph G = (V, E) is semiconnected if, for all pairs of vertices\\nu, v ∈ V, we have u ⇝ v or v ⇝ u. Give an efﬁcient algorithm to\\ndetermine whether G is semiconnected. Prove that your algorithm is\\ncorrect, and analyze its running time.\\n20.5-8\\nLet G = (V, E) be a directed graph, and let l : V → ℝ be a function that\\nassigns a real-valued label l to each vertex. For vertices s, t ∈ V, deﬁne\\nGive an O(V + E)-time algorithm to ﬁnd vertices s and t such that Δl(s,\\nt) is maximum over all pairs of vertices. (Hint: Use Exercise 20.5-5.)\\nProblems\\n20-1\\xa0\\xa0\\xa0\\xa0\\xa0C lassifying edges by breadth-ﬁrst search', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 756}),\n",
              " Document(page_content='A depth-ﬁrst forest classiﬁes the edges of a graph into tree, back,\\nforward, and cross edges. A breadth-ﬁrst tree can also be used to\\nclassify the edges reachable from the source of the search into the same\\nfour categories.\\nFigure 20.10 The articulation points, bridges, and biconnected components of a connected,\\nundirected graph for use in Problem 20-2. The articulation points are the orange vertices, the\\nbridges are the dark blue edges, and the biconnected components are the edges in the light blue\\nregions, with a bcc numbering shown.\\na. Prove that in a breadth-ﬁrst search of an undirected graph, the\\nfollowing properties hold:\\n1. There are no back edges and no forward edges.\\n2. If (u, v) is a tree edge, then v.d = u.d + 1.\\n3. If (u, v) is a cross edge, then v.d = u.d or v.d = u.d + 1.\\nb. Prove that in a breadth-ﬁrst search of a directed graph, the following\\nproperties hold:\\n1. There are no forward edges.\\n2. If (u, v) is a tree edge, then v.d = u.d + 1.\\n3. If (u, v) is a cross edge, then v.d ≤ u.d + 1.\\n4. If (u, v) is a back edge, then 0 ≤ v.d ≤ u.d.\\n20-2\\xa0\\xa0\\xa0\\xa0\\xa0A rticulation points, bridges, and bi conne cted compone nts\\nLet G = (V, E) be a connected, undirected graph. An articulation point\\nof G is a vertex whose removal disconnects G. A bridge of G is an edge\\nwhose removal disconnects G. A biconnected component of G is a\\nmaximal set of edges such that any two edges in the set lie on a common\\nsimple cycle. Figure 20.10 illustrates these deﬁnitions. You can', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 757}),\n",
              " Document(page_content='determine articulation points, bridges, and biconnected components\\nusing depth-ﬁrst search. Let Gπ = (V, Eπ) be a depth-ﬁrst tree of G.\\na. Prove that the root of Gπ is an articulation point of G if and only if it\\nhas at least two children in Gπ.\\nb. Let v be a nonroot vertex of Gπ. Prove that v is an articulation point\\nof G if and only if v has a child s such that there is no back edge from\\ns or any descendant of s to a proper ancestor of v.\\nc. Let\\nShow how to compute v.low for all vertices v ∈ V in O(E) time.\\nd. Show how to compute all articulation points in O(E) time.\\ne. Prove that an edge of G is a bridge if and only if it does not lie on any\\nsimple cycle of G.\\nf. Show how to compute all the bridges of G in O(E) time.\\ng. Prove that the biconnected components of G partition the nonbridge\\nedges of G.\\nh. Give an O(E)-time algorithm to label each edge e of G with a positive\\ninteger e.bcc such that e.bcc = e′.bcc if and only if e and e′ belong to\\nthe same biconnected component.\\n20-3\\xa0\\xa0\\xa0\\xa0\\xa0E uler tour\\nAn Euler tour of a strongly connected, directed graph G = (V, E) is a\\ncycle that traverses each edge of G exactly once, although it may visit a\\nvertex more than once.\\na. Show that G has an Euler tour if and only if in-degree(v) = out-\\ndegree(v) for each vertex v ∈ V.\\nb. Describe an O(E)-time algorithm to ﬁnd an Euler tour of G if one\\nexists. (Hint: Merge edge-disjoint cycles.)', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 758}),\n",
              " Document(page_content='20-4\\xa0\\xa0\\xa0\\xa0\\xa0R eachability\\nLet G = (V, E) be a directed graph in which each vertex u ∈ V is labeled\\nwith a unique integer L(u) from the set {1, 2, … , |V|}. For each vertex u\\n∈ V, let R(u) = {v ∈ V : u ⇝ v} be the set of vertices that are reachable\\nfrom u. Deﬁne min(u) to be the vertex in R(u) whose label is minimum,\\nthat is, min(u) is the vertex v such that L(v) = min {L(w) : w ∈ R(u)}.\\nGive an O(V + E)-time algorithm that computes min(u) for all vertices u\\n∈ V.\\n20-5 Inserting and querying vertices in planar  graphs\\nA planar graph is an undirected graph that can be drawn in the plane\\nwith no edges crossing. Euler proved that every planar graph has |E| < 3\\n|V|.\\nConsider the following two operations on a planar graph G:\\nINSERT(G, v, neighbors) inserts a new vertex v into G, where\\nneighbors is an array (possibly empty) of vertices that have already\\nbeen inserted into G and will become all the neighbors of v in G\\nwhen v is inserted.\\nNEWEST-NEIGHBOR(G, v) returns the neighbor of vertex v\\nthat was most recently inserted into G, or NIL if v has no\\nneighbors.\\nDesign a data structure that supports these two operations such that\\nNEWEST-NEIGHBOR takes O(1) worst-case time and INSERT takes\\nO(1) amortized time. Note that the length of the array neighbors given\\nto INSERT may vary. (Hint: Use a potential function for the amortized\\nanalysis.)\\nChapter notes\\nEven [137] and Tarjan [429] are excellent references for graph\\nalgorithms.\\nBreadth-ﬁrst search was discovered by Moore [334] in the context of\\nﬁnding paths through mazes. Lee [280] independently discovered the', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 759}),\n",
              " Document(page_content='same algorithm in the context of routing wires on circuit boards.\\nHopcroft and Tarjan [226] advocated the use of the adjacency-list\\nrepresentation over the adjacency-matrix representation for sparse\\ngraphs and were the ﬁrst to recognize the algorithmic importance of\\ndepth-ﬁrst search. Depth-ﬁrst search has been widely used since the late\\n1950s, especially in artiﬁcial intelligence programs.\\nTarjan [426] gave a linear-time algorithm for ﬁnding strongly\\nconnected components. The algorithm for strongly connected\\ncomponents in Section 20.5 is adapted from Aho, Hopcroft, and\\nUllman [6], who credit it to S. R. Kosaraju (unpublished) and Sharir\\n[408]. Dijkstra [117, Chapter 25] also developed an algorithm for\\nstrongly connected components that is based on contracting cycles.\\nSubsequently, Gabow [163] rediscovered this algorithm. Knuth [259]\\nwas the ﬁrst to give a linear-time algorithm for topological sorting.\\n1 We distinguish between gray and black vertices to help us understand how breadth-ﬁrst search\\noperates. In fact, as Exercise 20.2-3 shows, we get the same result even if we do not distinguish\\nbetween gray and black vertices.\\n2 Chapters 22 and 23 generalize shortest paths to weighted graphs, in which every edge has a\\nreal-valued weight and the weight of a path is the sum of the weights of its constituent edges.\\nThe graphs considered in the present chapter are unweighted or, equivalently, all edges have unit\\nweight.\\n3 It may seem arbitrary that breadth-ﬁrst search is limited to only one source whereas depth-\\nﬁrst search may search from multiple sources. Although conceptually, breadth-ﬁrst search could\\nproceed from multiple sources and depth-ﬁrst search could be limited to one source, our\\napproach reﬂects how the results of these searches are typically used. Breadth-ﬁrst search\\nusually serves to ﬁnd shortest-path distances and the associated predecessor subgraph from a\\ngiven source. Depth-ﬁrst search is often a subroutine in another algorithm, as we’ll see later in\\nthis chapter.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 760}),\n",
              " Document(page_content='21\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0Minimum Spanning Trees\\nElectronic circuit designs often need to make the pins of several\\ncomponents electrically equivalent by wiring them together. To\\ninterconnect a set of n pins, the designer can use an arrangement of n −\\n1 wires, each connecting two pins. Of all such arrangements, the one\\nthat uses the least amount of wire is usually the most desirable.\\nTo model this wiring problem, use a connected, undirected graph G\\n= (V, E), where V is the set of pins, E is the set of possible\\ninterconnections between pairs of pins, and for each edge (u, v) ∈ E, a\\nweight w(u, v) speciﬁes the cost (amount of wire needed) to connect u\\nand v. The goal is to ﬁnd an acyclic subset T ⊆ E that connects all of\\nthe vertices and whose total weight\\nis minimized. Since T is acyclic and connects all of the vertices, it must\\nform a tree, which we call a spanning tree since it “spans” the graph G.\\nWe call the problem of determining the tree T the minimum-spanning-\\ntree problem.1 Figure 21.1 shows an example of a connected graph and\\na minimum spanning tree.\\nThis chapter studies two ways to solve the minimum-spanning-tree\\nproblem. Kruskal’s algorithm and Prim’s algorithm both run in O(E lg\\nV) time. Prim’s algorithm achieves this bound by using a binary heap as\\na priority queue. By using Fibonacci heaps instead (see page 478),\\nPrim’s algorithm runs in O(E + V lg V) time. This bound is better than\\nO(E lg V) whenever |E| grows asymptotically faster than |V|.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 761}),\n",
              " Document(page_content='Figure 21.1 A minimum spanning tree for a connected graph. The weights on edges are shown,\\nand the blue edges form a minimum spanning tree. The total weight of the tree shown is 37. This\\nminimum spanning tree is not unique: removing the edge (b, c) and replacing it with the edge (a,\\nh) yields another spanning tree with weight 37.\\nThe two algorithms are greedy algorithms, as described in Chapter\\n15. Each step of a greedy algorithm must make one of several possible\\nchoices. The greedy strategy advocates making the choice that is the best\\nat the moment. Such a strategy does not generally guarantee that it\\nalways ﬁnds globally optimal solutions to problems. For the minimum-\\nspanning-tree problem, however, we can prove that certain greedy\\nstrategies do yield a spanning tree with minimum weight. Although you\\ncan read this chapter independently of Chapter 15, the greedy methods\\npresented here are a classic application of the theoretical notions\\nintroduced there.\\nSection 21.1 introduces a “generic” minimum-spanning-tree method\\nthat grows a spanning tree by adding one edge at a time. Section 21.2\\ngives two algorithms that implement the generic method. The ﬁrst\\nalgorithm, due to Kruskal, is similar to the connected-components\\nalgorithm from Section 19.1. The second, due to Prim, resembles\\nDijkstra’s shortest-paths algorithm (Section 22.3).\\nBecause a tree is a type of graph, in order to be precise we must\\ndeﬁne a tree in terms of not just its edges, but its vertices as well.\\nBecause this chapter focuses on trees in terms of their edges, we’ll\\nimplicitly understand that the vertices of a tree T are those that some\\nedge of T is incident on.\\n21.1\\xa0\\xa0\\xa0\\xa0Growing a minimum spanni ng tree', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 762}),\n",
              " Document(page_content='The input to the minumum-spanning-tree problem is a connected,\\nundirected graph G = (V, E) with a weight function w : E → ℝ. The\\ngoal is to ﬁnd a minimum spanning tree for G. The two algorithms\\nconsidered in this chapter use a greedy approach to the problem,\\nalthough they differ in how they apply this approach.\\nThis greedy strategy is captured by the procedure GENERIC-MST\\non the facing page, which grows the minimum spanning tree one edge at\\na time. The generic method manages a set A of edges, maintaining the\\nfollowing loop invariant:\\nPrior to each iteration, A is a subset of some minimum\\nspanning tree.\\nGENERIC-MST(G, w)\\n1A = Ø\\n2while A does not form a spanning tree\\n3 ﬁnd an edge (u, v) that is safe for A\\n4 A = A ∪ {(u, v)}\\n5return A\\nEach step determines an edge (u, v) that the procedure can add to A\\nwithout violating this invariant, in the sense that A ∪ {(u, v)} is also a\\nsubset of a minimum spanning tree. We call such an edge a safe edge for\\nA, since it can be added safely to A while maintaining the invariant.\\nThis generic algorithm uses the loop invariant as follows:\\nInitialization: After line 1, the set A trivially satisﬁes the loop invariant.\\nMaintenance: The loop in lines 2–4 maintains the invariant by adding\\nonly safe edges.\\nTermination: All edges added to A belong to a minimum spanning tree,\\nand the loop must terminate by the time it has considered all edges.\\nTherefore, the set A returned in line 5 must be a minimum spanning\\ntree.\\nThe tricky part is, of course, ﬁnding a safe edge in line 3. One must\\nexist, since when line 3 is executed, the invariant dictates that there is a', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 763}),\n",
              " Document(page_content='spanning tree T such that A ⊆ T. Within the while loop body, A must be\\na proper subset of T, and therefore there must be an edge (u, v) ∈ T\\nsuch that (u, v) ∉ A and (u, v) is safe for A.\\nThe remainder of this section provides a rule (Theorem 21.1) for\\nrecognizing safe edges. The next section describes two algorithms that\\nuse this rule to ﬁnd safe edges efﬁciently.\\nWe ﬁrst need some deﬁnitions. Acut (S, V – S) of an undirected graph\\nG = (V, E) is a partition of V. Figure 21.2 illustrates this notion. We say\\nthat an edge (u, v) ∈ Ecrosses the cut (S, V – S) if one of its endpoints\\nbelongs to S and the other belongs to V – S. A cut respects a set A of\\nedges if no edge in A crosses the cut. An edge is a light edge crossing a\\ncut if its weight is the minimum of any edge crossing the cut. There can\\nbe more than one light edge crossing a cut in the case of ties. More\\ngenerally, we say that an edge is a light edge satisfying a given property\\nif its weight is the minimum of any edge satisfying the property.\\nThe following theorem gives the rule for recognizing safe edges.\\nTheorem 21.1\\nLet G = (V, E) be a connected, undirected graph with a real-valued\\nweight function w deﬁned on E. Let A be a subset of E that is included\\nin some minimum spanning tree for G, let (S, V – S) be any cut of G\\nthat respects A, and let (u, v) be a light edge crossing (S, V – S). Then,\\nedge (u, v) is safe for A.\\nFigure 21.2 A cut (S, V – S) of the graph from Figure 21.1. Orange vertices belong to the set S,\\nand tan vertices belong to V – S. The edges crossing the cut are those connecting tan vertices\\nwith orange vertices. The edge (d, c) is the unique light edge crossing the cut. Blue edges form a\\nsubset A of the edges. The cut (S, V – S) respects A, since no edge of A crosses the cut.\\nProof\\xa0\\xa0\\xa0Let T be a minimum spanning tree that includes A, and assume\\nthat T does not contain the light edge (u, v), since if it does, we are done.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 764}),\n",
              " Document(page_content='We’ll construct another minimum spanning tree T′ that includes A ∪\\n{(u, v)} by using a cut-and-paste technique, thereby showing that (u, v)\\nis a safe edge for A.\\nThe edge (u, v) forms a cycle with the edges on the simple path p\\nfrom u to v in T, as Figure 21.3 illustrates. Since u and v are on opposite\\nsides of the cut (S, V – S), at least one edge in T lies on the simple path\\np and also crosses the cut. Let (x, y) be any such edge. The edge (x, y) is\\nnot in A, because the cut respects A. Since (x, y) is on the unique simple\\npath from u to v in T, removing (x, y) breaks T into two components.\\nAdding (u, v) reconnects them to form a new spanning tree T′ = (T –\\n{(x, y)}) ∪  {(u, v)}.\\nWe next show that T′ is a minimum spanning tree. Since (u, v) is a\\nlight edge crossing (S, V – S) and (x, y) also crosses this cut, w(u, v) ≤\\nw(x, y). Therefore,\\nw(T′)=w(T) − w(x, y) + w(u, v)\\n≤w(T).\\nBut T is a minimum spanning tree, so that w(T) ≤ w(T′), and thus, T′\\nmust be a minimum spanning tree as well.\\nIt remains to show that (u, v) is actually a safe edge for A. We have A\\n⊆ T′, since A ⊆ T and (x, y) ∉ A, and thus, A ∪ {(u, v)} ⊆  T′.\\nConsequently, since T′ is a minimum spanning tree, (u, v) is safe for A.\\n▪\\nTheorem 21.1 provides insight into how the GENERIC-MST\\nmethod works on a connected graph G = (V, E). As the method\\nproceeds, the set A is always acyclic, since it is a subset of a minimum\\nspanning tree and a tree may not contain a cycle. At any point in the\\nexecution, the graph GA = (V, A) is a forest, and each of the connected\\ncomponents of GA is a tree. (Some of the trees may contain just one\\nvertex, as is the case, for example, when the method begins: A is empty\\nand the forest contains |V| trees, one for each vertex.) Moreover, any\\nsafe edge (u, v) for A connects distinct components of GA, since A ∪\\n{(u, v)} must be acyclic.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 765}),\n",
              " Document(page_content='Figure 21.3 The proof of Theorem 21.1. Orange vertices belong to S, and tan vertices belong to\\nV – S. Only edges in the minimum spanning tree T are shown, along with edge (u, v), which does\\nnot lie in T. The edges in A are blue, and (u, v) is a light edge crossing the cut (S, V – S). The\\nedge (x, y) is an edge on the unique simple path p from u to v in T. To form a minimum\\nspanning tree T′ that contains (u, v), remove the edge (x, y) from T and add the edge (u, v).\\nThe while loop in lines 2–4 of GENERIC-MST executes |V| – 1 times\\nbecause it ﬁnds one of the |V| – 1 edges of a minimum spanning tree in\\neach iteration. Initially, when A = Ø, there are |V| trees in GA, and each\\niteration reduces that number by 1. When the forest contains only a\\nsingle tree, the method terminates.\\nThe two algorithms in Section 21.2 use the following corollary to\\nTheorem 21.1.\\nCorollary 21.2\\nLet G = (V, E) be a connected, undirected graph with a real-valued\\nweight function w deﬁned on E. Let A be a subset of E that is included\\nin some minimum spanning tree for G, and let C = (VC, EC) be a\\nconnected component (tree) in the forest GA = (V, A). If (u, v) is a light\\nedge connecting C to some other component in GA, then (u, v) is safe\\nfor A.\\nProof\\xa0\\xa0\\xa0The cut (VC, V – VC) respects A, and (u, v) is a light edge for\\nthis cut. Therefore, (u, v) is safe for A.\\n▪\\nExercises', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 766}),\n",
              " Document(page_content='21.1-1\\nLet (u, v) be a minimum-weight edge in a connected graph G. Show that\\n(u, v) belongs to some minimum spanning tree of G.\\n21.1-2\\nProfessor Sabatier conjectures the following converse of Theorem 21.1.\\nLet G = (V, E) be a connected, undirected graph with a real-valued\\nweight function w deﬁned on E. Let A be a subset of E that is included\\nin some minimum spanning tree for G, let (S, V – S) be any cut of G\\nthat respects A, and let (u, v) be a safe edge for A crossing (S, V – S).\\nThen, (u, v) is a light edge for the cut. Show that the professor’s\\nconjecture is incorrect by giving a counterexample.\\n21.1-3\\nShow that if an edge (u, v) is contained in some minimum spanning tree,\\nthen it is a light edge crossing some cut of the graph.\\n21.1-4\\nGive a simple example of a connected graph such that the set of edges\\n{(u, v) : there exists a cut (S, V – S) such that (u, v) is a light edge\\ncrossing (S, V – S)} does not form a minimum spanning tree.\\n21.1-5\\nLet e be a maximum-weight edge on some cycle of connected graph G =\\n(V, E). Prove that there is a minimum spanning tree of G′ = (V, E – {e})\\nthat is also a minimum spanning tree of G. That is, there is a minimum\\nspanning tree of G that does not include e.\\n21.1-6\\nShow that a graph has a unique minimum spanning tree if, for every cut\\nof the graph, there is a unique light edge crossing the cut. Show that the\\nconverse is not true by giving a counterexample.\\n21.1-7\\nArgue that if all edge weights of a graph are positive, then any subset of\\nedges that connects all vertices and has minimum total weight must be a\\ntree. Give an example to show that the same conclusion does not follow\\nif we allow some weights to be nonpositive.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 767}),\n",
              " Document(page_content='21.1-8\\nLet T be a minimum spanning tree of a graph G, and let L be the sorted\\nlist of the edge weights of T. Show that for any other minimum\\nspanning tree T′ of G, the list L is also the sorted list of edge weights of\\nT′.\\n21.1-9\\nLet T be a minimum spanning tree of a graph G = (V, E), and let V′ be\\na subset of V. Let T′ be the subgraph of T induced by V′, and let G′ be\\nthe subgraph of G induced by V′. Show that if T′ is connected, then T′ is\\na minimum spanning tree of G ′ .\\n21.1-10\\nGiven a graph G and a minimum spanning tree T, suppose that the\\nweight of one of the edges in T decreases. Show that T is still a\\nminimum spanning tree for G. More formally, let T be a minimum\\nspanning tree for G with edge weights given by weight function w.\\nChoose one edge (x, y) ∈ T and a positive number k, and deﬁne the\\nweight function w′ by\\nShow that T is a minimum spanning tree for G with edge weights given\\nby w′.\\n★ 21.1-11\\nGiven a graph G and a minimum spanning tree T, suppose that the\\nweight of one of the edges not in T decreases. Give an algorithm for\\nﬁnding the minimum spanning tree in the modiﬁed graph.\\n21.2\\xa0\\xa0\\xa0\\xa0The algorithms of Kruskal and P rim\\nThe two minimum-spanning-tree algorithms described in this section\\nelaborate on the generic method. They each use a speciﬁc rule to\\ndetermine a safe edge in line 3 of GENERIC-MST. In Kruskal’s\\nalgorithm, the set A is a forest whose vertices are all those of the given', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 768}),\n",
              " Document(page_content='graph. The safe edge added to A is always a lowest-weight edge in the\\ngraph that connects two distinct components. In Prim’s algorithm, the\\nset A forms a single tree. The safe edge added to A is always a lowest-\\nweight edge connecting the tree to a vertex not in the tree. Both\\nalgorithms assume that the input graph is connected and represented by\\nadjacency lists.\\nFigure 21.4 The execution of Kruskal’s algorithm on the graph from Figure 21.1. Blue edges\\nbelong to the forest A being grown. The algorithm considers each edge in sorted order by\\nweight. A red arrow points to the edge under consideration at each step of the algorithm. If the\\nedge joins two distinct trees in the forest, it is added to the forest, thereby merging the two trees.\\nKruskal’s algorithm\\nKruskal’s algorithm ﬁnds a safe edge to add to the growing forest by\\nﬁnding, of all the edges that connect any two trees in the forest, an edge', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 769}),\n",
              " Document(page_content='(u, v) with the lowest weight. Let C1 and C2 denote the two trees that\\nare connected by (u, v). Since (u, v) must be a light edge connecting C1\\nto some other tree, Corollary 21.2 implies that (u, v) is a safe edge for\\nC1. Kruskal’s algorithm qualiﬁes as a greedy algorithm because at each\\nstep it adds to the forest an edge with the lowest possible weight.\\nFigure 21.4, continued Further steps in the execution of Kruskal’s algorithm.\\nLike the algorithm to compute connected components from Section\\n19.1, the procedure MST-KRUSKAL on the following page uses a\\ndisjoint-set data structure to maintain several disjoint sets of elements.\\nEach set contains the vertices in one tree of the current forest. The\\noperation FIND-SET(u) returns a representative element from the set\\nthat contains u. Thus, to determine whether two vertices u and v belong\\nto the same tree, just test whether FIND-SET(u) equals FIND-SET(v).\\nTo combine trees, Kruskal’s algorithm calls the UNION procedure.\\nFigure 21.4 shows how Kruskal’s algorithm works. Lines 1–3\\ninitialize the set A to the empty set and create |V| trees, one containing\\neach vertex. The for loop in lines 6–9 examines edges in order of weight,', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 770}),\n",
              " Document(page_content='from lowest to highest. The loop checks, for each edge (u, v), whether\\nthe endpoints u and v belong to the same tree. If they do, then the edge\\n(u, v) cannot be added to the forest without creating a cycle, and the\\nedge is ignored. Otherwise, the two vertices belong to different trees. In\\nthis case, line 8 adds the edge (u, v) to A, and line 9 merges the vertices\\nin the two trees.\\nMST-KRUSKAL(G, w)\\n\\xa0\\xa01A = Ø\\n\\xa0\\xa02for each vertex v ∈ G.V\\n\\xa0\\xa03MAKE-SET(v)\\n\\xa0\\xa04create a single list of the edges in G.E\\n\\xa0\\xa05sort the list of edges into monotonically increasing order by weight\\nw\\n\\xa0\\xa06for each edge (u, v) taken from the sorted list in order\\n\\xa0\\xa07if FIND-SET(u) ≠ FIND-SET(v)\\n\\xa0\\xa08 A = A ∪ {(u, v)}\\n\\xa0\\xa09 UNION(u, v)\\n10return A\\nThe running time of Kruskal’s algorithm for a graph G = (V, E)\\ndepends on the speciﬁc implementation of the disjoint-set data\\nstructure. Let’s assume that it uses the disjoint-set-forest\\nimplementation of Section 19.3 with the union-by-rank and path-\\ncompression heuristics, since that is the asymptotically fastest\\nimplementation known. Initializing the set A in line 1 takes O(1) time,\\ncreating a single list of edges in line 4 takes O(V + E) time (which is\\nO(E) because G is connected), and the time to sort the edges in line 5 is\\nO(E lg E). (We’ll account for the cost of the |V| MAKE-SET operations\\nin the for loop of lines 2–3 in a moment.) The for loop of lines 6–9\\nperforms O(E) FIND-SET and UNION operations on the disjoint-set\\nforest. Along with the |V| MAKE-SET operations, these disjoint-set\\noperations take a total of O((V + E) α(V)) time, where α  is the very\\nslowly growing function deﬁned in Section 19.4. Because we assume\\nthat G is connected, we have |E| ≥ |V| – 1, and so the disjoint-set', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 771}),\n",
              " Document(page_content='operations take O(E α(V)) time. Moreover, since α (|V|) = O(lg V) = O(lg\\nE), the total running time of Kruskal’s algorithm is O(E lg E).\\nObserving that |E| < |V|2, we have lg |E| = O(lg V), and so we can restate\\nthe running time of Kruskal’s algorithm as O(E lg V).\\nPrim’s algorithm\\nLike Kruskal’s algorithm, Prim’s algorithm is a special case of the\\ngeneric minimum-spanning-tree method from Section 21.1. Prim’s\\nalgorithm operates much like Dijkstra’s algorithm for ﬁnding shortest\\npaths in a graph, which we’ll see in Section 22.3. Prim’s algorithm has\\nthe property that the edges in the set A always form a single tree. As\\nFigure 21.5 shows, the tree starts from an arbitrary root vertex r and\\ngrows until it spans all the vertices in V. Each step adds to the tree A a\\nlight edge that connects A to an isolated vertex—one on which no edge\\nof A is incident. By Corollary 21.2, this rule adds only edges that are\\nsafe for A. Therefore, when the algorithm terminates, the edges in A\\nform a minimum spanning tree. This strategy qualiﬁes as greedy since at\\neach step it adds to the tree an edge that contributes the minimum\\namount possible to the tree’s weight.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 772}),\n",
              " Document(page_content='Figure 21.5 The execution of Prim’s algorithm on the graph from Figure 21.1. The root vertex is\\na. Blue vertices and edges belong to the tree being grown, and tan vertices have yet to be added\\nto the tree. At each step of the algorithm, the vertices in the tree determine a cut of the graph,\\nand a light edge crossing the cut is added to the tree. The edge and vertex added to the tree are\\nhighlighted in orange. In the second step (part (c)), for example, the algorithm has a choice of\\nadding either edge (b, c) or edge (a, h) to the tree since both are light edges crossing the cut.\\nIn the procedure MST-PRIM below, the connected graph G and the\\nroot r of the minimum spanning tree to be grown are inputs to the\\nalgorithm. In order to efﬁciently select a new edge to add into tree A,\\nthe algorithm maintains a min-priority queue Q of all vertices that are\\nnot in the tree, based on a key attribute. For each vertex v, the attribute', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 773}),\n",
              " Document(page_content='v.key is the minimum weight of any edge connecting v to a vertex in the\\ntree, where by convention, v.key = ∞ if there is no such edge. The\\nattribute v.π names the parent of v in the tree. The algorithm implicitly\\nmaintains the set A from GENERIC-MST as\\nA = {(v, v.π) : v ∈ V – {r} – Q},\\nwhere we interpret the vertices in Q as forming a set. When the\\nalgorithm terminates, the min-priority queue Q is empty, and thus the\\nminimum spanning tree A for G is\\nA = {(v, v.π) : v ∈ V – {r}}.\\nMST-PRIM(G, w, r)\\n\\xa0\\xa01for each vertex u ∈ G.V\\n\\xa0\\xa02u.key = ∞\\n\\xa0\\xa03u.π = NIL\\n\\xa0\\xa04r.key = 0\\n\\xa0\\xa05Q = Ø\\n\\xa0\\xa06for each vertex u ∈ G.V\\n\\xa0\\xa07INSERT(Q, u)\\n\\xa0\\xa08while Q ≠ Ø\\n\\xa0\\xa09u = EXTRACT-MIN(Q)// add u to the tree\\n10for each vertex v in\\nG.Adj[u]// update keys of u’s non-tree\\nneighbors\\n11 if v ∈ Q and w(u, v) < v.key\\n12 v.π = u\\n13 v.key = w(u, v)\\n14 DECREASE-KEY(Q, v, w(u, v))\\nFigure 21.5 shows how Prim’s algorithm works. Lines 1–7 set the key\\nof each vertex to ∞ (except for the root r, whose key is set to 0 to make it\\nthe ﬁrst vertex processed), set the parent of each vertex to NIL, and\\ninsert each vertex into the min-priority queue Q. The algorithm\\nmaintains the following three-part loop invariant:\\nPrior to each iteration of the while loop of lines 8–14,', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 774}),\n",
              " Document(page_content='1. A = {(v, v.π) : v ∈ V – {r} – Q}.\\n2. The vertices already placed into the minimum spanning\\ntree are those in V − Q.\\n3. For all vertices v ∈ Q, if v.π ≠ NIL, then v.key < ∞ and\\nv.key is the weight of a light edge (v, v.π) connecting v to\\nsome vertex already placed into the minimum spanning\\ntree.\\nLine 9 identiﬁes a vertex u ∈ Q incident on a light edge that crosses the\\ncut (V – Q, Q) (with the exception of the ﬁrst iteration, in which u = r\\ndue to lines 4–7). Removing u from the set Q adds it to the set V – Q of\\nvertices in the tree, thus adding the edge (u, u.π) to A. The for loop of\\nlines 10–14 updates the key and attributes of every vertex v adjacent to u\\nbut not in the tree, thereby maintaining the third part of the loop\\ninvariant. Whenever line 13 updates v.key, line 14 calls DECREASE-\\nKEY to inform the min-priority queue that v’s key has changed.\\nThe running time of Prim’s algorithm depends on the speciﬁc\\nimplementation of the min-priority queue Q. You can implement Q\\nwith a binary min-heap (see Chapter 6), including a way to map\\nbetween vertices and their corresponding heap elements. The BUILD-\\nMIN-HEAP procedure can perform lines 5–7 in O(V) time. In fact,\\nthere is no need to call BUILD-MIN-HEAP. You can just put the key\\nof r at the root of the min-heap, and because all other keys are ∞, they\\ncan go anywhere else in the min-heap. The body of the while loop\\nexecutes |V| times, and since each EXTRACT-MIN operation takes\\nO(lg V) time, the total time for all calls to EXTRACT-MIN is O(V lg\\nV). The for loop in lines 10–14 executes O(E) times altogether, since the\\nsum of the lengths of all adjacency lists is 2 |E|. Within the for loop, the\\ntest for membership in Q in line 11 can take constant time if you keep a\\nbit for each vertex that indicates whether it belongs to Q and update the\\nbit when the vertex is removed from Q. Each call to DECREASE-KEY\\nin line 14 takes O(lg V) time. Thus, the total time for Prim’s algorithm is\\nO(V lg V + E lg V) = O(E lg V), which is asymptotically the same as for\\nour implementation of Kruskal’s algorithm.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 775}),\n",
              " Document(page_content='You can further improve the asymptotic running time of Prim’s\\nalgorithm by implementing the min-priority queue with a Fibonacci\\nheap (see page 478). If a Fibonacci heap holds |V| elements, an\\nEXTRACT-MIN operation takes O(lg V) amortized time and each\\nINSERT and DECREASE-KEY operation takes only O(1) amortized\\ntime. Therefore, by using a Fibonacci heap to implement the min-\\npriority queue Q, the running time of Prim’s algorithm improves to\\nO(E+V lg V).\\nExercises\\n21.2-1\\nKruskal’s algorithm can return different spanning trees for the same\\ninput graph G, depending on how it breaks ties when the edges are\\nsorted. Show that for each minimum spanning tree T of G, there is a\\nway to sort the edges of G in Kruskal’s algorithm so that the algorithm\\nreturns T.\\n21.2-2\\nGive a simple implementation of Prim’s algorithm that runs in O(V2)\\ntime when the graph G = (V, E) is represented as an adjacency matrix.\\n21.2-3\\nFor a sparse graph G = (V, E), where |E| = Θ (V), is the implementation\\nof Prim’s algorithm with a Fibonacci heap asymptotically faster than\\nthe binary-heap implementation? What about for a dense graph, where\\n|E| = Θ (V2)? How must the sizes |E| and |V| be related for the Fibonacci-\\nheap implementation to be asymptotically faster than the binary-heap\\nimplementation?\\n21.2-4\\nSuppose that all edge weights in a graph are integers in the range from 1\\nto |V|. How fast can you make Kruskal’s algorithm run? What if the\\nedge weights are integers in the range from 1 to W for some constant\\nW?\\n21.2-5', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 776}),\n",
              " Document(page_content='Suppose that all edge weights in a graph are integers in the range from 1\\nto |V|. How fast can you make Prim’s algorithm run? What if the edge\\nweights are integers in the range from 1 to W for some constant W?\\n21.2-6\\nProfessor Borden proposes a new divide-and-conquer algorithm for\\ncomputing minimum spanning trees, which goes as follows. Given a\\ngraph G = (V, E), partition the set V of vertices into two sets V1 and V2\\nsuch that |V1| and |V2| differ by at most 1. Let E1 be the set of edges\\nthat are incident only on vertices in V1, and let E2 be the set of edges\\nthat are incident only on vertices in V2. Recursively solve a minimum-\\nspanning-tree problem on each of the two subgraphs G1 = (V1, E1) and\\nG2 = (V2, E2). Finally, select the minimum-weight edge in E that\\ncrosses the cut V1, V2), and use this edge to unite the resulting two\\nminimum spanning trees into a single spanning tree.\\nEither argue that the algorithm correctly computes a minimum\\nspanning tree of G, or provide an example for which the algorithm fails.\\n★ 21.2-7\\nSuppose that the edge weights in a graph are uniformly distributed over\\nthe half-open interval [0, 1). Which algorithm, Kruskal’s or Prim’s, can\\nyou make run faster?\\n★ 21.2-8\\nSuppose that a graph G has a minimum spanning tree already\\ncomputed. How quickly can you update the minimum spanning tree\\nupon adding a new vertex and incident edges to G?\\nProblems\\n21-1\\xa0\\xa0\\xa0\\xa0\\xa0Second-best minimum spanning tree\\nLet G = (V, E) be an undirected, connected graph whose weight\\nfunction is w : E → ℝ, and suppose that |E| ≥ |V| and all edge weights\\nare distinct.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 777}),\n",
              " Document(page_content='We deﬁne a second-best minimum spanning tree as follows. Let T be\\nthe set of all spanning trees of G, and let T be a minimum spanning tree\\nof G. Then a second-best minimum spanning tree is a spanning tree T′\\nsuch that w(T′) = min {w(T″) : T″ ∈ T − {T}}.\\na. Show that the minimum spanning tree is unique, but that the second-\\nbest minimum spanning tree need not be unique.\\nb. Let T be the minimum spanning tree of G. Prove that G contains\\nsome edge (u, v) ∈ T and some edge (x, y) ∉ T such that (T – {(u, v)})\\n∪ {(x, y)} is a second-best minimum spanning tree of G.\\nc. Now let T be any spanning tree of G and, for any two vertices u, v ∈\\nV, let max[u, v] denote an edge of maximum weight on the unique\\nsimple path between u and v in T. Describe an O(V2)-time algorithm\\nthat, given T, computes max[u, v] for all u, v ∈ V.\\nd. Give an efﬁcient algorithm to compute the second-best minimum\\nspanning tree of G.\\n21-2\\xa0\\xa0\\xa0\\xa0\\xa0M inimum spanning tree in sparse graphs\\nFor a very sparse connected graph G = (V, E), it is possible to further\\nimprove upon the O(E + V lg V) running time of Prim’s algorithm with\\na Fibonacci heap by preprocessing G to decrease the number of vertices\\nbefore running Prim’s algorithm. In particular, for each vertex u, choose\\nthe minimum-weight edge (u, v) incident on u, and put (u, v) into the\\nminimum spanning tree under construction. Then, contract all chosen\\nedges (see Section B.4). Rather than contracting these edges one at a\\ntime, ﬁrst identify sets of vertices that are united into the same new\\nvertex. Then create the graph that would have resulted from contracting\\nthese edges one at a time, but do so by “renaming” edges according to\\nthe sets into which their endpoints were placed. Several edges from the\\noriginal graph might be renamed the same as each other. In such a case,\\nonly one edge results, and its weight is the minimum of the weights of\\nthe corresponding original edges.\\nInitially, set the minimum spanning tree T being constructed to be\\nempty, and for each edge (u, v) ∈ E, initialize the two attributes (u,', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 778}),\n",
              " Document(page_content='v).orig = (u, v) and (u, v).c = w(u, v). Use the orig attribute to reference\\nthe edge from the initial graph that is associated with an edge in the\\ncontracted graph. The c attribute holds the weight of an edge, and as\\nedges are contracted, it is updated according to the above scheme for\\nchoosing edge weights. The procedure MST-REDUCE on the facing\\npage takes inputs G and T, and it returns a contracted graph G′ with\\nupdated attributes orig′ and c′. The procedure also accumulates edges of\\nG into the minimum spanning tree T.\\na. Let T be the set of edges returned by MST-REDUCE, and let A be\\nthe minimum spanning tree of the graph G′ formed by the call MST-\\nPRIM(G′, c′, r), where c′ is the weight attribute on the edges of G′.E\\nand r is any vertex in G′:V. Prove that T ∪ {(x, y).orig′ : (x, y) ∈ A} is\\na minimum spanning tree of G.\\nb. Argue that |G′.V| ≤ |V| /2.\\nc. Show how to implement MST-REDUCE so that it runs in O(E) time.\\n(Hint: Use simple data structures.)\\nd. Suppose that you run k phases of MST-REDUCE, using the output\\nG′ produced by one phase as the input G to the next phase and\\naccumulating edges in T. Argue that the overall running time of the k\\nphases is O(kE).\\ne. Suppose that after running k phases of MST-REDUCE, as in part\\n(d), you run Prim’s algorithm by calling MST-PRIM(G′, c′, r), where\\nG′, with weight attribute c′, is returned by the last phase and r is any\\nvertex in G′.V. Show how to pick k so that the overall running time is\\nO(E lg lg V). Argue that your choice of k minimizes the overall\\nasymptotic running time.\\nf. For what values of |E| (in terms of |V|) does Prim’s algorithm with\\npreprocessing asymptotically beat Prim’s algorithm without\\npreprocessing?\\nMST-REDUCE(G, T)\\n\\xa0\\xa01for each vertex v ∈ G.V\\n\\xa0\\xa02v.mark = FALSE', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 779}),\n",
              " Document(page_content='\\xa0\\xa03MAKE-SET(v)\\n\\xa0\\xa04for each vertex u ∈ G.V\\n\\xa0\\xa05if u.mark == FALSE\\n\\xa0\\xa06 choose v ∈ G.Adj[u] such that (u, v).c is minimized\\n\\xa0\\xa07 UNION(u, v)\\n\\xa0\\xa08 T = T ∪ {(u, v).orig}\\n\\xa0\\xa09 u.mark = TRUE\\n10 v.mark = TRUE\\n11G′.V = {FIND-SET(v) : v ∈ G.V}\\n12G′.E = Ø\\n13for each edge (x, y) ∈ G.E\\n14u = FIND-SET(x)\\n15v = FIND-SET(y)\\n16if u ≠ v\\n17 if (u, v) ∉ G′.E\\n18 G′.E = G′.E ∪ {(u, v)}\\n19 (u, v).orig′ = (x, y).orig\\n20 (u, v).c′ = (x, y).c\\n21 elseif (x, y).c < (u, v).c′\\n22 (u, v).orig′ = (x, y).orig\\n23 (u, v).c′ = (x, y).c\\n24construct adjacency lists G′.Adj for G′\\n25return G′ and T\\n21-3\\xa0\\xa0\\xa0\\xa0\\xa0A lternative minimum-spanni ng-tree algorithms\\nConsider the three algorithms MAYBE-MST-A, MAYBE-MST-B, and\\nMAYBE-MST-C on the next page. Each one takes a connected graph\\nand a weight function as input and returns a set of edges T. For each\\nalgorithm, either prove that T is a minimum spanning tree or prove that\\nT is not necessarily a minimum spanning tree. Also describe the most\\nefﬁcient implementation of each algorithm, regardless of whether it\\ncomputes a minimum spanning tree.\\n21-4\\xa0\\xa0\\xa0\\xa0\\xa0B ottleneck spanning tree', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 780}),\n",
              " Document(page_content='A bottleneck spanning tree T of an undirected graph G is a spanning tree\\nof G whose largest edge weight is minimum over all spanning trees of G.\\nThe value of the bottleneck spanning tree is the weight of the\\nmaximum-weight edge in T.\\nMAYBE-MST-A(G, w)\\n1sort the edges into monotonically decreasing order of edge weights\\nw\\n2T = E\\n3for each edge e, taken in monotonically decreasing order by weight\\n4 if T – {e} is a connected graph\\n5 T = T – {e}\\n6return T\\nMAYBE-MST-B(G, w)\\n1T = Ø\\n2for each edge e, taken in arbitrary order\\n3 if T ∪ {e} has no cycles\\n4 T = T ∪ {e}\\n5return T\\nMAYBE-MST-C(G, w)\\n1T = Ø\\n2for each edge e, taken in arbitrary order\\n3 T = T ∪ {e}\\n4 if T has a cycle c\\n5 let e′ be a maximum-weight edge on c\\n6 T = T – {e′}\\n7return T\\na. Argue that a minimum spanning tree is a bottleneck spanning tree.\\nPart (a) shows that ﬁnding a bottleneck spanning tree is no harder than\\nﬁnding a minimum spanning tree. In the remaining parts, you will show\\nhow to ﬁnd a bottleneck spanning tree in linear time.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 781}),\n",
              " Document(page_content='b. Give a linear-time algorithm that, given a gr aph G and an integer b,\\ndetermines whether the value of the bottleneck spanning tree is at\\nmost b.\\nc. Use your algorithm for part (b) as a subroutine in a linear-time\\nalgorithm for the bottleneck-spanning-tree problem. (Hint: You might\\nwant to use a subroutine that contracts sets of edges, as in the MST-\\nREDUCE procedure described in Problem 21-2.)\\nChapter notes\\nTarjan [429] surveys the minimum-spanning-tree problem and provides\\nexcellent advanced material. Graham and Hell [198] compiled a history\\nof the minimum-spanning-tree problem.\\nTarjan attributes the ﬁrst minimum-spanning-tree algorithm to a\\n1926 paper by O. Bor ůvka. Bor ůvka’s algorithm consists of running\\nO(lg V) iterations of the procedure MST-REDUCE described in\\nProblem 21-2. Kruskal’s algorithm was reported by Kruskal [272] in\\n1956. The algorithm commonly known as Prim’s algorithm was indeed\\ninvented by Prim [367], but it was also invented earlier by V. Jarník in\\n1930.\\nWhen |E| = Ω(V lg V), Prim’s algorithm, implemented with a\\nFibonacci heap, runs in O(E) time. For sparser graphs, using a\\ncombination of the ideas from Prim’s algorithm, Kruskal’s algorithm,\\nand Bor ůvka’s algorithm, together with advanced data structures,\\nFredman and Tarjan [156] give an algorithm that runs in O(E lg* V)\\ntime. Gabow, Galil, Spencer, and Tarjan [165] improved this algorithm\\nto run in O(E lg lg* V) time. Chazelle [83] gives an algorithm that runs\\nin O(E \\n(E, V)) time, where \\n(E, V) is the functional inverse of\\nAckermann’s function. (See the chapter notes for Chapter 19 for a brief\\ndiscussion of Ackermann’s function and its inverse.) Unlike previous\\nminimum-spanning-tree algorithms, Chazelle’s algorithm does not\\nfollow the greedy method. Pettie and Ramachandran [356] give an\\nalgorithm based on precomputed “MST decision trees” that also runs\\nin O(E \\n(E, V)) time.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 782}),\n",
              " Document(page_content='A related problem is spanning-tree veriﬁcation: given a graph G = (V,\\nE) and a tree T ⊆ E, determine whether T is a minimum spanning tree\\nof G. King [254] gives a linear-time algorithm to verify a spanning tree,\\nbuilding on earlier work of Komlós [269] and Dixon, Rauch, and Tarjan\\n[120].\\nThe above algorithms are all deterministic and fall into the\\ncomparison-based model described in Chapter 8. Karger, Klein, and\\nTarjan [243] give a randomized minimum-spanning-tree algorithm that\\nruns in O(V + E) expected time. This algorithm uses recursion in a\\nmanner similar to the linear-time selection algorithm in Section 9.3: a\\nrecursive call on an auxiliary problem identiﬁes a subset of the edges E′\\nthat cannot be in any minimum spanning tree. Another recursive call on\\nE – E′ then ﬁnds the minimum spanning tree. The algorithm also uses\\nideas from Bor ůvka’s algorithm and King’s algorithm for spanning-tree\\nveriﬁcation.\\nFredman and Willard [158] showed how to ﬁnd a minimum spanning\\ntree in O(V + E) time using a deterministic algorithm that is not\\ncomparison based. Their algorithm assumes that the data are b-bit\\nintegers and that the computer memory consists of addressable b-bit\\nwords.\\n1 The phrase “minimum spanning tree” is a shortened form of the phrase “minimum-weight\\nspanning tree.” There is no point in minimizing the number of edges in T, since all spanning\\ntrees have exactly |V| − 1 edges by Theorem B.2 on page 1169.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 783}),\n",
              " Document(page_content='22\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0Single-Source Shortest Paths\\nSuppose that you need to drive from Oceanside, New York, to\\nOceanside, California, by the shortest possible route. Your GPS\\ncontains information about the entire road network of the United\\nStates, including the road distance between each pair of adjacent\\nintersections. How can your GPS determine this shortest route?\\nOne possible way is to enumerate all the routes from Oceanside, New\\nYork, to Oceanside, California, add up the distances on each route, and\\nselect the shortest. But even disallowing routes that contain cycles, your\\nGPS would need to examine an enormous number of possibilities, most\\nof which are simply not worth considering. For example, a route that\\npasses through Miami, Florida, is a poor choice, because Miami is\\nseveral hundred miles out of the way.\\nThis chapter and Chapter 23 show how to solve such problems\\nefﬁciently. The input to a shortest-paths problem is a weighted, directed\\ngraph G = (V, E), with a weight function w : E → ℝ mapping edges to\\nreal-valued weights. The weight w(p) of path p = 〈v0, v1, … , vk〉 is the\\nsum of the weights of its constituent edges:\\nWe deﬁne the shortest-path weight δ(u, v) from u to v by\\n', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 784}),\n",
              " Document(page_content='A shortest path from vertex u to vertex v is then deﬁned as any path p\\nwith weight w(p) = δ (u, v).\\nIn the example of going from Oceanside, New York, to Oceanside,\\nCalifornia, your GPS models the road network as a graph: vertices\\nrepresent intersections, edges represent road segments between\\nintersections, and edge weights represent road distances. The goal is to\\nﬁnd a shortest path from a given intersection in Oceanside, New York\\n(say, Brower Avenue and Skillman Avenue) to a given intersection in\\nOceanside, California (say, Topeka Street and South Horne Street).\\nEdge weights can represent metrics other than distances, such as\\ntime, cost, penalties, loss, or any other quantity that accumulates\\nlinearly along a path and that you want to minimize.\\nThe breadth-ﬁrst-search algorithm from Section 20.2 is a shortest-\\npaths algorithm that works on unweighted graphs, that is, graphs in\\nwhich each edge has unit weight. Because many of the concepts from\\nbreadth-ﬁrst search arise in the study of shortest paths in weighted\\ngraphs, you might want to review Section 20.2 before proceeding.\\nVariants\\nThis chapter focuses on the single-source shortest-paths problem: given a\\ngraph G = (V, E), ﬁnd a shortest path from a given source vertex s ∈ V\\nto every vertex v ∈ V. The algorithm for the single-source problem can\\nsolve many other problems, including the following variants.\\nSingle-destination shortest-paths problem: Find a shortest path to a\\ngiven destination vertex t from each vertex v. By reversing the direction\\nof each edge in the graph, you can reduce this problem to a single-\\nsource problem.\\nSingle-pair shortest-path problem: Find a shortest path from u to v for\\ngiven vertices u and v. If you solve the single-source problem with\\nsource vertex u, you solve this problem also. Moreover, all known\\nalgorithms for this problem have the same worst-case asymptotic\\nrunning time as the best single-source algorithms.\\nAll-pairs shortest-paths problem: Find a shortest path from u to v for\\nevery pair of vertices u and v. Although you can solve this problem by', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 785}),\n",
              " Document(page_content='running a single-source algorithm once from each vertex, you often\\ncan solve it faster. Additionally, its structure is interesting in its own\\nright. Chapter 23 addresses the all-pairs problem in detail.\\nOptimal substructure of a shortest path\\nShortest-paths algorithms typically rely on the property that a shortest\\npath between two vertices contains other shortest paths within it. (The\\nEdmonds-Karp maximum-ﬂow algorithm in Chapter 24 also relies on\\nthis property.) Recall that optimal substructure is one of the key\\nindicators that dynamic programming (Chapter 14) and the greedy\\nmethod (Chapter 15) might apply. Dijkstra’s algorithm, which we shall\\nsee in Section 22.3, is a greedy algorithm, and the Floyd-Warshall\\nalgorithm, which ﬁnds a shortest path between every pair of vertices\\n(see Section 23.2), is a dynamic-programming algorithm. The following\\nlemma states the optimal-substructure property of shortest paths more\\nprecisely.\\nLemma 22.1 (Subpaths of shortest paths are shortest paths)\\nGiven a weighted, directed graph G = (V, E) with weight function w : E\\n→ ℝ let p = 〈v0, v1, … , vk〉 be a shortest path from vertex v0 to vertex\\nvk and, for any i and j such that 0 ≤ i ≤ j ≤ k, let pij = 〈vi, vi+1, … , vj〉 be\\nthe subpath of p from vertex vi to vertex vj. Then, pij is a shortest path\\nfrom vi to vj.\\nProof\\xa0\\xa0\\xa0Decompose path p into \\n , so that w(p) = w(p0i) +\\nw(pij) + w(pjk). Now, assume that there is a path \\n from vi to vj with\\nweight \\n . Then, \\n  is a path from v0 to vk whose\\nweight \\n  is less than w(p), which contradicts the\\nassumption that p is a shortest path from v0 to vk.\\n▪\\nNegative-weight edges', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 786}),\n",
              " Document(page_content='Some instances of the single-source shortest-paths problem may include\\nedges whose weights are negative. If the graph G = (V, E) contains no\\nnegative-weight cycles reachable from the source s, then for all v ∈ V,\\nthe shortest-path weight δ (s, v) remains well deﬁned, even if it has a\\nnegative value. If the graph contains a negative-weight cycle reachable\\nfrom s, however, shortest-path weights are not well deﬁned. No path\\nfrom s to a vertex on the cycle can be a shortest path—you can always\\nﬁnd a path with lower weight by following the proposed “shortest” path\\nand then traversing the negative-weight cycle. If there is a negative-\\nweight cycle on some path from s to v, we deﬁne δ (s, v) = −∞.\\nFigure 22.1 illustrates the effect of negative weights and negative-\\nweight cycles on shortest-path weights. Because there is only one path\\nfrom s to a (the path 〈s, a〉), we have δ (s, a) = w(s, a) = 3. Similarly, there\\nis only one path from s to b, and so δ (s, b) = w(s, a) + w(a, b) = 3 + (−4)\\n= −1. There are inﬁnitely many paths from s to c: 〈s, c〉, 〈s, c, d, c〉, 〈s, c,\\nd, c, d, c〉, and so on. Because the cycle 〈c, d, c〉 has weight 6 + (−3) = 3\\n> 0, the shortest path from s to c is 〈s, c〉, with weight δ (s, c) = w(s, c) =\\n5, and the shortest path from s to d is 〈s, c, d〉, with weight δ (s, d) = w(s,\\nc) + w(c, d) = 11. Analogously, there are inﬁnitely many paths from s to\\ne: 〈s, e〉, 〈s, e, f, e〉, 〈s, e, f, e, f, e〉, and so on. Because the cycle 〈e, f, e〉\\nhas weight 3 + (−6) = −3 < 0, however, there is no shortest path from s\\nto e. By traversing the negative-weight cycle 〈e, f, e〉 arbitrarily many\\ntimes, you can ﬁnd paths from s to e with arbitrarily large negative\\nweights, and so δ (s, e) = −∞. Similarly, δ (s, f) = −∞. Because g is\\nreachable from f, you can also ﬁnd paths with arbitrarily large negative\\nweights from s to g, and so δ (s, g) = −∞. Vertices h, i, and j also form a\\nnegative-weight cycle. They are not reachable from s, however, and so\\nδ(s, h) = δ (s, i) = δ (s, j) = ∞.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 787}),\n",
              " Document(page_content='Figure 22.1 Negative edge weights in a directed graph. The shortest-path weight from source s\\nappears within each vertex. Because vertices e and f form a negative-weight cycle reachable from\\ns, they have shortest-path weights of −∞. Because vertex g is reachable from a vertex whose\\nshortest-path weight is −∞, it, too, has a shortest-path weight of −∞. Vertices such as h, i, and j\\nare not reachable from s, and so their shortest-path weights are ∞, even though they lie on a\\nnegative-weight cycle.\\nSome shortest-paths algorithms, such as Dijkstra’s algorithm,\\nassume that all edge weights in the input graph are nonnegative, as in a\\nroad network. Others, such as the Bellman-Ford algorithm, allow\\nnegative-weight edges in the input graph and produce a correct answer\\nas long as no negative-weight cycles are reachable from the source.\\nTypically, if there is such a negative-weight cycle, the algorithm can\\ndetect and report its existence.\\nCycles\\nCan a shortest path contain a cycle? As we have just seen, it cannot\\ncontain a negative-weight cycle. Nor can it contain a positive-weight\\ncycle, since removing the cycle from the path produces a path with the\\nsame source and destination vertices and a lower path weight. That is, if\\np = 〈v0, v1, … , vk〉 is a path and c = 〈vi, vi+1, … , vj〉 is a positive-\\nweight cycle on this path (so that vi = vj and w(c) > 0), then the path p′\\n= 〈v0, v1, … , vi, vj+1, vj+2, … , vk〉 has weight w(p′) = w(p) − w(c) <\\nw(p), and so p cannot be a shortest path from v0 to vk.\\nThat leaves only 0-weight cycles. You can remove a 0-weight cycle\\nfrom any path to produce another path whose weight is the same. Thus,\\nif there is a shortest path from a source vertex s to a destination vertex v\\nthat contains a 0-weight cycle, then there is another shortest path from s', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 788}),\n",
              " Document(page_content='to v without this cycle. As long as a shortest path has 0-weight cycles,\\nyou can repeatedly remove these cycles from the path until you have a\\nshortest path that is cycle-free. Therefore, without loss of generality,\\nassume that shortest paths have no cycles, that is, they are simple paths.\\nSince any acyclic path in a graph G = (V, E) contains at most |V| distinct\\nvertices, it also contains at most |V| − 1 edges. Assume, therefore, that\\nany shortest path contains at most |V| − 1 edges.\\nRepresenting shortest paths\\nIt is usually not enough to compute only shortest-path weights. Most\\napplications of shortest paths need to know the vertices on shortest\\npaths as well. For example, if your GPS told you the distance to your\\ndestination but not how to get there, it would not be terribly useful. We\\nrepresent shortest paths similarly to how we represented breadth-ﬁrst\\ntrees in Section 20.2. Given a graph G = (V, E), maintain for each vertex\\nv ∈ V a predecessor v.π that is either another vertex or NIL. The\\nshortest-paths algorithms in this chapter set the π attributes so that the\\nchain of predecessors originating at a vertex v runs backward along a\\nshortest path from s to v. Thus, given a vertex v for which v.π ≠ NIL, the\\nprocedure PRINT-PATH(G, s, v) from Section 20.2 prints a shortest\\npath from s to v.\\nIn the midst of executing a shortest-paths algorithm, however, the π\\nvalues might not indicate shortest paths. The predecessor subgraph Gπ =\\n(Vπ, Eπ) induced by the π values is deﬁned the same for single-source\\nshortest paths as for breadth-ﬁrst search in equations (20.2) and (20.3)\\non page 561:\\nVπ = {v ∈ V : v.π ≠ NIL} ∪ {s},\\nEπ = {(v.π, v) ∈ E : v ∈ Vπ − {s}}.\\nWe’ll prove that the π values produced by the algorithms in this\\nchapter have the property that at termination Gπ is a “shortest-paths\\ntree”—informally, a rooted tree containing a shortest path from the\\nsource s to every vertex that is reachable from s. A shortest-paths tree is\\nlike the breadth-ﬁrst tree from Section 20.2, but it contains shortest', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 789}),\n",
              " Document(page_content='paths from the source deﬁned in terms of edge weights instead of\\nnumbers of edges. To be precise, let G = (V, E) be a weighted, directed\\ngraph with weight function w : E → ℝ, and assume that G contains no\\nnegative-weight cycles reachable from the source vertex s ∈ V, so that\\nshortest paths are well deﬁned. Ashortest-paths tree rooted at s is a\\ndirected subgraph G′ = (V′, E′), where V′ ⊆ V and E′ ⊆ E, such that\\n1. V′ is the set of vertices reachable from s in G,\\n2. G′ forms a rooted tree with root s, and\\n3. for all v ∈ V′, the unique simple path from s to v in G′ is a\\nshortest path from s to v in G.\\nFigure 22.2 (a) A weighted, directed graph with shortest-path weights from source s. (b) The\\nblue edges form a shortest-paths tree rooted at the source s. (c) Another shortest-paths tree with\\nthe same root.\\nShortest paths are not necessarily unique, and neither are shortest-\\npaths trees. For example, Figure 22.2 shows a weighted, directed graph\\nand two shortest-paths trees with the same root.\\nRelaxation\\nThe algorithms in this chapter use the technique of relaxation. For each\\nvertex v ∈ V, the single-source shortest paths algorithms maintain an\\nattribute v.d, which is an upper bound on the weight of a shortest path\\nfrom source s to v. We call v.d a shortest-path estimate. To initialize the\\nshortest-path estimates and predecessors, call the Θ (V)-time procedure\\nINITIALIZE-SINGLE-SOURCE. After initialization, we have v.π =\\nNIL for all v ∈ V, s.d = 0 and v.d = ∞ for v ∈ V − {s}.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 790}),\n",
              " Document(page_content='INITIALIZE-SINGLE-SOURCE(G, s)\\n1for each vertex v ∈ G.V\\n2 v.d = ∞\\n3 v.π = NIL\\n4s.d = 0\\nThe process of relaxing an edge (u, v) consists of testing whether\\ngoing through vertex u improves the shortest path to vertex v found so\\nfar and, if so, updating v.d and v.π. A relaxation step might decrease the\\nvalue of the shortest-path estimate v.d and update v’s predecessor\\nattribute v.π. The RELAX procedure on the following page performs a\\nrelaxation step on edge (u, v) in O(1) time. Figure 22.3 shows two\\nexamples of relaxing an edge, one in which a shortest-path estimate\\ndecreases and one in which no estimate changes.\\nFigure 22.3 Relaxing an edge (u, v) with weight w(u, v) = 2. The shortest-path estimate of each\\nvertex appears within the vertex. (a) Because v.d > u.d + w(u, v) prior to relaxation, the value of\\nv.d decreases. (b) Since we have v.d ≤ u.d + w(u, v) before relaxing the edge, the relaxation step\\nleaves v.d unchanged.\\nRELAX(u, v, w)\\n1if v.d > u.d + w(u, v)\\n2 v.d = u.d + w(u, v)\\n3 v.π = u\\nEach algorithm in this chapter calls INITIALIZE-SINGLE-\\nSOURCE and then repeatedly relaxes edges.1 Moreover, relaxation is\\nthe only means by which shortest-path estimates and predecessors', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 791}),\n",
              " Document(page_content='change. The algorithms in this chapter differ in how many times they\\nrelax each edge and the order in which they relax edges. Dijkstra’s\\nalgorithm and the shortest-paths algorithm for directed acyclic graphs\\nrelax each edge exactly once. The Bellman-Ford algorithm relaxes each\\nedge |V| − 1 times.\\nProperties of shortest paths and relaxation\\nTo prove the algorithms in this chapter correct, we’ll appeal to several\\nproperties of shortest paths and relaxation. We state these properties\\nhere, and Section 22.5 proves them formally. For your reference, each\\nproperty stated here includes the appropriate lemma or corollary\\nnumber from Section 22.5. The latter ﬁve of these properties, which\\nrefer to shortest-path estimates or the predecessor subgraph, implicitly\\nassume that the graph is initialized with a call to INITIALIZE-\\nSINGLE-SOURCE(G, s) and that the only way that shortest-path\\nestimates and the predecessor subgraph change are by some sequence of\\nrelaxation steps.\\nTriangle inequality (Lemma 22.10)\\nFor any edge (u, v) ∈ E, we have δ (s, v) ≤ δ(s, u) + w(u, v).\\nUpper-bound property (Lemma 22.11)\\nWe always have v.d ≥ δ(s, v) for all vertices v ∈ V, and once v.d\\nachieves the value δ (s, v), it never changes.\\nNo-path property (Corollary 22.12)\\nIf there is no path from s to v, then we always have v.d = δ(s, v) = ∞.\\nConvergence property (Lemma 22.14)\\nIf s ⇝ u → v is a shortest path in G for some u, v ∈ V, and if u.d =\\nδ(s, u) at any time prior to relaxing edge (u, v), then v.d = δ(s, v) at\\nall times afterward.\\nPath-relaxation property (Lemma 22.15)\\nIf p = 〈v0, v1, … , vk〉 is a shortest path from s = v0 to vk, and the\\nedges of p are relaxed in the order (v0, v1), (v1, v2), … , (vk−1, vk),\\nthen vk.d = δ (s, vk). This property holds regardless of any other', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 792}),\n",
              " Document(page_content='relaxation steps that occur, even if they are intermixed with\\nrelaxations of the edges of p.\\nPredecessor-subgraph property (Lemma 22.17)\\nOnce v.d = δ (s, v) for all v ∈ V, the predecessor subgraph is a\\nshortest-paths tree rooted at s.\\nChapter outline\\nSection 22.1 presents the Bellman-Ford algorithm, which solves the\\nsingle-source shortest-paths problem in the general case in which edges\\ncan have negative weight. The Bellman-Ford algorithm is remarkably\\nsimple, and it has the further beneﬁt of detecting whether a negative-\\nweight cycle is reachable from the source. Section 22.2 gives a linear-\\ntime algorithm for computing shortest paths from a single source in a\\ndirected acyclic graph. Section 22.3 covers Dijkstra’s algorithm, which\\nhas a lower running time than the Bellman-Ford algorithm but requires\\nthe edge weights to be nonnegative. Section 22.4 shows how to use the\\nBellman-Ford algorithm to solve a special case of linear programming.\\nFinally, Section 22.5 proves the properties of shortest paths and\\nrelaxation stated above.\\nThis chapter does arithmetic with inﬁnities, and so we need some\\nconventions for when ∞ or −∞ appears in an arithmetic expression. We\\nassume that for any real number a ≠ −∞, we have a + ∞ = ∞ + a = ∞.\\nAlso, to make our proofs hold in the presence of negative-weight cycles,\\nwe assume that for any real number a ≠ ∞, we have a + (−∞) = (−∞) + a\\n= −∞.\\nAll algorithms in this chapter assume that the directed graph G is\\nstored in the adjacency-list representation. Additionally, stored with\\neach edge is its weight, so that as each algorithm traverses an adjacency\\nlist, it can ﬁnd edge weights in O(1) time per edge.\\n22.1\\xa0\\xa0\\xa0\\xa0The Bellman-Ford algorithm\\nThe Bellman-Ford algorithm solves the single-source shortest-paths\\nproblem in the general case in which edge weights may be negative.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 793}),\n",
              " Document(page_content='Given a weighted, directed graph G = (V, E) with source vertex s and\\nweight function w : E → ℝ, the Bellman-Ford algorithm returns a\\nboolean value indicating whether there is a negative-weight cycle that is\\nreachable from the source. If there is such a cycle, the algorithm\\nindicates that no solution exists. If there is no such cycle, the algorithm\\nproduces the shortest paths and their weights.\\nThe procedure BELLMAN-FORD relaxes edges, progressively\\ndecreasing an estimate v.d on the weight of a shortest path from the\\nsource s to each vertex v ∈ V until it achieves the actual shortest-path\\nweight δ (s, v). The algorithm returns TRUE if and only if the graph\\ncontains no negative-weight cycles that are reachable from the source.\\nBELLMAN-FORD(G, w, s)\\n1INITIALIZE-SINGLE-SOURCE(G, s)\\n2for i = 1 to |G.V| − 1\\n3 for each edge (u, v) ∈ G.E\\n4 RELAX(u, v, w)\\n5for each edge (u, v) = G.E\\n6 if v.d > u.d + w(u, v)\\n7 return FALSE\\n8return TRUE\\nFigure 22.4 shows the execution of the Bellman-Ford algorithm on a\\ngraph with 5 vertices. After initializing the d and π values of all vertices\\nin line 1, the algorithm makes |V| − 1 passes over the edges of the graph.\\nEach pass is one iteration of the for loop of lines 2–4 and consists of\\nrelaxing each edge of the graph once. Figures 22.4(b)–(e) show the state\\nof the algorithm after each of the four passes over the edges. After\\nmaking |V| − 1 passes, lines 5–8 check for a negative-weight cycle and\\nreturn the appropriate boolean value. (We’ll see a little later why this\\ncheck works.)', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 794}),\n",
              " Document(page_content='Figure 22.4 The execution of the Bellman-Ford algorithm. The source is vertex s. The d values\\nappear within the vertices, and blue edges indicate predecessor values: if edge (u, v) is blue, then\\nv.π = u. In this particular example, each pass relaxes the edges in the order (t, x), (t, y), (t, z), (x,\\nt), (y, x), (y, z), (z, x), (z, s), (s, t), (s, y). (a) The situation just before the ﬁrst pass over the edges.\\n(b)–(e) The situation after each successive pass over the edges. Vertices whose shortest-path\\nestimates and predecessors have changed due to a pass are highlighted in orange. The d and π\\nvalues in part (e) are the ﬁnal values. The Bellman-Ford algorithm returns TRUE in this\\nexample.\\nThe Bellman-Ford algorithm runs in O(V2 + VE) time when the\\ngraph is represented by adjacency lists, since the initialization in line 1\\ntakes Θ (V) time, each of the |V| − 1 passes over the edges in lines 2–4\\ntakes Θ (V + E) time (examining |V| adjacency lists to ﬁnd the |E| edges),\\nand the for loop of lines 5–7 takes O(V + E) time. Fewer than |V| − 1\\npasses over the edges sometimes sufﬁce (see Exercise 22.1-3), which is\\nwhy we say O(V2+VE) time, rather than Θ (V2+VE) time. In the\\nfrequent case where |E| = Ω(V), we can express this running time as\\nO(VE). Exercise 22.1-5 asks you to make the Bellman-Ford algorithm\\nrun in O(VE) time even when |E| = o(V).\\nTo prove the correctness of the Bellman-Ford algorithm, we start by\\nshowing that if there are no negative-weight cycles, the algorithm\\ncomputes correct shortest-path weights for all vertices reachable from\\nthe source.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 795}),\n",
              " Document(page_content='Lemma 22.2\\nLet G = (V, E) be a weighted, directed graph with source vertex s and\\nweight function w : E → ℝ, and assume that G contains no negative-\\nweight cycles that are reachable from s. Then, after the |V| − 1 iterations\\nof the for loop of lines 2–4 of BELLMAN-FORD, v.d = δ(s, v) for all\\nvertices v that are reachable from s.\\nProof\\xa0\\xa0\\xa0We prove the lemma by appealing to the path-relaxation\\nproperty. Consider any vertex v that is reachable from s, and let p = 〈v0,\\nv1, … , vk〉, where v0 = s and vk = v, be any shortest path from s to v.\\nBecause shortest paths are simple, p has at most |V| − 1 edges, and so k\\n≤ |V| − 1. Each of the |V| − 1 iterations of the for loop of lines 2–4\\nrelaxes all |E| edges. Among the edges relaxed in the ith iteration, for i =\\n1, 2, … , k, is (vi−1, vi). By the path-relaxation property, therefore, v.d =\\nvk.d = δ(s, vk) = δ (s, v).\\n▪\\nCorollary 22.3\\nLet G = (V, E) be a weighted, directed graph with source vertex s and\\nweight function w : E → ℝ. Then, for each vertex v ∈ V, there is a path\\nfrom s to v if and only if BELLMAN-FORD terminates with v.d < ∞\\nwhen it is run on G.\\nProof\\xa0\\xa0\\xa0The proof is left as Exercise 22.1-2.\\n▪\\nTheorem 22.4 (Correctness of the Bellman-Ford algorithm)\\nLet BELLMAN-FORD be run on a weighted, directed graph G = (V,\\nE) with source vertex s and weight function w : E → ℝ. If G contains no\\nnegative-weight cycles that are reachable from s, then the algorithm\\nreturns TRUE, v.d = δ(s, v) for all vertices v ∈ V, and the predecessor\\nsubgraph Gπ is a shortest-paths tree rooted at s. If G does contain a\\nnegative-weight cycle reachable from s, then the algorithm returns\\nFALSE.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 796}),\n",
              " Document(page_content='Proof\\xa0\\xa0\\xa0Suppose that graph G contains no negative-weight cycles that\\nare reachable from the source s. We ﬁrst prove the claim that at\\ntermination, v.d = δ(s, v) for all vertices v ∈ V. If vertex v is reachable\\nfrom s, then Lemma 22.2 proves this claim. If v is not reachable from s,\\nthen the claim follows from the no-path property. Thus, the claim is\\nproven. The predecessor-subgraph property, along with the claim,\\nimplies that Gπ is a shortest-paths tree. Now we use the claim to show\\nthat BELLMAN-FORD returns TRUE. At termination, for all edges\\n(u, v) ∈ E we have\\nv.d=δ(s, v)\\n≤δ(s, u) + w(u, v) (by the triangle inequality)\\n=u.d + w(u, v),\\nand so none of the tests in line 6 causes BELLMAN-FORD to return\\nFALSE. Therefore, it returns TRUE.\\nNow, suppose that graph G contains a negative-weight cycle\\nreachable from the source s. Let this cycle be c = 〈v0, v1, … , vk〉, where\\nv0 = vk, in which case we have\\nAssume for the purpose of contradiction that the Bellman-Ford\\nalgorithm returns TRUE. Thus, vi.d ≤ vi−1.d + w(vi−1, vi) for i = 1, 2,\\n… , k. Summing the inequalities around cycle c gives\\nSince v0 = vk, each vertex in c appears exactly once in each of the\\nsummations \\n  and \\n , and so\\n', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 797}),\n",
              " Document(page_content='Moreover, by Corollary 22.3, vi.d is ﬁnite for i = 1, 2, … , k. Thus,\\nwhich contradicts inequality (22.1). We conclude that the Bellman-Ford\\nalgorithm returns TRUE if graph G contains no negative-weight cycles\\nreachable from the source, and FALSE otherwise.\\n▪\\nExercises\\n22.1-1\\nRun the Bellman-Ford algorithm on the directed graph of Figure 22.4,\\nusing vertex z as the source. In each pass, relax edges in the same order\\nas in the ﬁgure, and show the d and π values after each pass. Now,\\nchange the weight of edge (z, x) to 4 and run the algorithm again, using\\ns as the source.\\n22.1-2\\nProve Corollary 22.3.\\n22.1-3\\nGiven a weighted, directed graph G = (V, E) with no negative-weight\\ncycles, let m be the maximum over all vertices v ∈ V of the minimum\\nnumber of edges in a shortest path from the source s to v. (Here, the\\nshortest path is by weight, not the number of edges.) Suggest a simple\\nchange to the Bellman-Ford algorithm that allows it to terminate in m +\\n1 passes, even if m is not known in advance.\\n22.1-4\\nModify the Bellman-Ford algorithm so that it sets v.d to −∞ for all\\nvertices v for which there is a negative-weight cycle on some path from\\nthe source to v.\\n22.1-5\\nSuppose that the graph given as input to the Bellman-Ford algorithm is\\nrepresented with a list of |E| edges, where each edge indicates the', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 798}),\n",
              " Document(page_content='vertices it leaves and enters, along with its weight. Argue that the\\nBellman-Ford algorithm runs in O(VE) time without the constraint that\\n|E| = Ω(V). Modify the Bellman-Ford algorithm so that it runs in O(VE)\\ntime in all cases when the input graph is represented with adjacency\\nlists.\\n22.1-6\\nLet G = (V, E) be a weighted, directed graph with weight function w : E\\n→ ℝ. Give an O(VE)-time algorithm to ﬁnd, for all vertices v ∈ V, the\\nvalue δ *(v) = min { δ (u, v) : u ∈ V}.\\n22.1-7\\nSuppose that a weighted, directed graph G = (V, E) contains a negative-\\nweight cycle. Give an efﬁcient algorithm to list the vertices of one such\\ncycle. Prove that your algorithm is correct.\\n22.2\\xa0\\xa0\\xa0\\xa0Single-source shortest paths in directed acyclic graphs\\nIn this section, we introduce one further restriction on weighted,\\ndirected graphs: they are acyclic. That is, we are concerned with\\nweighted dags. Shortest paths are always well deﬁned in a dag, since\\neven if there are negative-weight edges, no negative-weight cycles can\\nexist. We’ll see that if the edges of a weighted dag G = (V, E) are relaxed\\naccording to a topological sort of its vertices, it takes only Θ (V + E)\\ntime to compute shortest paths from a single source.\\nThe algorithm starts by topologically sorting the dag (see Section\\n20.4) to impose a linear ordering on the vertices. If the dag contains a\\npath from vertex u to vertex v, then u precedes v in the topological sort.\\nThe DAG-SHORTEST-PATHS procedure makes just one pass over the\\nvertices in the topologically sorted order. As it processes each vertex, it\\nrelaxes each edge that leaves the vertex. Figure 22.5 shows the execution\\nof this algorithm.\\nDAG-SHORTEST-PATHS(G, w, s)\\n1topologically sort the vertices of G', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 799}),\n",
              " Document(page_content='2INITIALIZE-SINGLE-SOURCE(G, s)\\n3for each vertex u ∈ G.V, taken in topologically sorted order\\n4 for each vertex v in G.Adj[u]\\n5 RELAX(u, v, w)\\nLet’s analyze the running time of this algorithm. As shown in\\nSection 20.4, the topological sort of line 1 takes Θ (V + E) time. The call\\nof INITIALIZE-SINGLE-SOURCE in line 2 takes Θ (V) time. The for\\nloop of lines 3–5 makes one iteration per vertex. Altogether, the for loop\\nof lines 4–5 relaxes each edge exactly once. (We have used an aggregate\\nanalysis here.) Because each iteration of the inner for loop takes Θ (1)\\ntime, the total running time is Θ (V + E), which is linear in the size of an\\nadjacency-list representation of the graph.\\nThe following theorem shows that the DAG-SHORTEST-PATHS\\nprocedure correctly computes the shortest paths.\\nTheorem 22.5\\nIf a weighted, directed graph G = (V, E) has source vertex s and no\\ncycles, then at the termination of the DAG-SHORTEST-PATHS\\nprocedure, v.d = δ (s, v) for all vertices v ∈ V, and the predecessor\\nsubgraph Gπ is a shortest-paths tree.\\nProof\\xa0\\xa0\\xa0We ﬁrst show that v.d = δ (s, v) for all vertices v ∈ V at\\ntermination. If v is not reachable from s, then v.d = δ(s, v) = 1 by the no-\\npath property. Now, suppose that v is reachable from s, so that there is a\\nshortest path p = 〈v0, v1, … , vk〉, where v0 = s and vk = v. Because\\nDAG-SHORTEST-PATHS processes the vertices in topologically\\nsorted order, it relaxes the edges on p in the order (v0, v1), (v1, v2), … ,\\n(vk−1, vk). The path-relaxation property implies that vi.d = δ (s, vi) at\\ntermination for i = 0, 1, … , k. Finally, by the predecessor-subgraph\\nproperty, Gπ is a shortest-paths tree.\\n▪\\nA useful application of this algorithm arises in determining critical\\npaths in PERT chart2 analysis. A job consists of several tasks. Each task', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 800}),\n",
              " Document(page_content='takes a certain amount of time, and some tasks must be completed\\nbefore others can be started. For example, if the job is to build a house,\\nthen the foundation must be completed before starting to frame the\\nexterior walls, which must be completed before starting on the roof.\\nSome tasks require more than one other task to be completed before\\nthey can be started: before the drywall can be installed over the wall\\nframing, both the electrical system and plumbing must be installed. A\\ndag models the tasks and dependencies. Edges represent tasks, with the\\nweight of an edge indicating the time required to perform the task.\\nVertices represent “milestones,” which are achieved when all the tasks\\nrepresented by the edges entering the vertex have been completed. If\\nedge (u, v) enters vertex v and edge (v, x) leaves v, then task (u, v) must\\nbe completed before task (v, x) is started. A path through this dag\\nrepresents a sequence of tasks that must be performed in a particular\\norder. A critical path is a longest path through the dag, corresponding to\\nthe longest time to perform any sequence of tasks. Thus, the weight of a\\ncritical path provides a lower bound on the total time to perform all the\\ntasks, even if as many tasks as possible are performed simultaneously.\\nYou can ﬁnd a critical path by either\\nnegating the edge weights and running DAG-SHORTEST-\\nPATHS, or\\nrunning DAG-SHORTEST-PATHS, but replacing “∞” by “−∞”\\nin line 2 of INITIALIZE-SINGLE-SOURCE and “>” by “<” in\\nthe RELAX procedure.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 801}),\n",
              " Document(page_content='Figure 22.5 The execution of the algorithm for shortest paths in a directed acyclic graph. The\\nvertices are topologically sorted from left to right. The source vertex is s. The d values appear\\nwithin the vertices, and blue edges indicate the π values. (a) The situation before the ﬁrst\\niteration of the for loop of lines 3–5. (b)–(g) The situation after each iteration of the for loop of\\nlines 3–5. Blue vertices have had their outgoing edges relaxed. The vertex highlighted in orange\\nwas used as u in that iteration. Each edge highlighted in orange caused a d value to change when\\nit was relaxed in that iteration. The values shown in part (g) are the ﬁnal values.\\nExercises\\n22.2-1\\nShow the result of running DAG-SHORTEST-PATHS on the directed\\nacyclic graph of Figure 22.5, using vertex r as the source.\\n22.2-2\\nSuppose that you change line 3 of DAG-SHORTEST-PATHS to read\\n3for the ﬁrst |V| − 1 vertices, taken in topologically sorted order\\nShow that the procedure remains correct.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 802}),\n",
              " Document(page_content='22.2-3\\nAn alternative way to represent a PERT chart looks more like the dag of\\nFigure 20.7 on page 574. Vertices represent tasks and edges represent\\nsequencing constraints, that is, edge (u, v) indicates that task u must be\\nperformed before task v. Vertices, not edges, have weights. Modify the\\nDAG-SHORTEST-PATHS procedure so that it ﬁnds a longest path in\\na directed acyclic graph with weighted vertices in linear time.\\n★ 22.2-4\\nGive an efﬁcient algorithm to count the total number of paths in a\\ndirected acyclic graph. The count should include all paths between all\\npairs of vertices and all paths with 0 edges. Analyze your algorithm.\\n22.3\\xa0\\xa0\\xa0\\xa0Dijkstra’s algorithm\\nDijkstra’s algorithm solves the single-source shortest-paths problem on\\na weighted, directed graph G = (V, E), but it requires nonnegative\\nweights on all edges: w(u, v) ≥ 0 for each edge (u, v) ∈ E. As we shall see,\\nwith a good implementation, the running time of Dijkstra’s algorithm is\\nlower than that of the Bellman-Ford algorithm.\\nYou can think of Dijkstra’s algorithm as generalizing breadth-ﬁrst\\nsearch to weighted graphs. A wave emanates from the source, and the\\nﬁrst time that a wave arrives at a vertex, a new wave emanates from that\\nvertex. Whereas breadth-ﬁrst search operates as if each wave takes unit\\ntime to traverse an edge, in a weighted graph, the time for a wave to\\ntraverse an edge is given by the edge’s weight. Because a shortest path in\\na weighted graph might not have the fewest edges, a simple, ﬁrst-in,\\nﬁrst-out queue won’t sufﬁce for choosing the next vertex from which to\\nsend out a wave.\\nInstead, Dijkstra’s algorithm maintains a set S of vertices whose ﬁnal\\nshortest-path weights from the source s have already been determined.\\nThe algorithm repeatedly selects the vertex u ∈ V – S with the minimum\\nshortest-path estimate, adds u into S, and relaxes all edges leaving u.\\nThe procedure DIJKSTRA replaces the ﬁrst-in, ﬁrst-out queue of', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 803}),\n",
              " Document(page_content='breadth-ﬁrst search by a min-priority queue Q of vertices, keyed by their\\nd values.\\nDIJKSTRA(G, w, s)\\n\\xa0\\xa01INITIALIZE-SINGLE-SOURCE(G, s)\\n\\xa0\\xa02S = Ø\\n\\xa0\\xa03Q = Ø\\n\\xa0\\xa04for each vertex u ∈ G.V\\n\\xa0\\xa05INSERT(Q, u)\\n\\xa0\\xa06while Q ≠ Ø\\n\\xa0\\xa07u = EXTRACT-MIN(Q)\\n\\xa0\\xa08S = S ∪ {u}\\n\\xa0\\xa09for each vertex v in G.Adj[u]\\n10 RELAX(u, v, w)\\n11 if the call of RELAX decreased v.d\\n12 DECREASE-KEY(Q, v, v.d)\\nDijkstra’s algorithm relaxes edges as shown in Figure 22.6. Line 1\\ninitializes the d and π values in the usual way, and line 2 initializes the\\nset S to the empty set. The algorithm maintains the invariant that Q =\\nV − S at the start of each iteration of the while loop of lines 6–12. Lines\\n3–5 initialize the min-priority queue Q to contain all the vertices in V.\\nSince S = Ø at that time, the invariant is true upon ﬁrst reaching line 6.\\nEach time through the while loop of lines 6–12, line 7 extracts a vertex u\\nfrom Q = V − S and line 8 adds it to set S, thereby maintaining the\\ninvariant. (The ﬁrst time through this loop, u = s.) Vertex u, therefore,\\nhas the smallest shortest-path estimate of any vertex in V − S. Then,\\nlines 9–12 relax each edge (u, v) leaving u, thus updating the estimate v.d\\nand the predecessor v.π if the shortest path to v found so far improves\\nby going through u. Whenever a relaxation step changes the d and π\\nvalues, the call to DECREASE-KEY in line 12 updates the min-priority\\nqueue. The algorithm never inserts vertices into Q after the for loop of\\nlines 4–5, and each vertex is extracted from Q and added to S exactly\\nonce, so that the while loop of lines 6–12 iterates exactly |V| times.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 804}),\n",
              " Document(page_content='Figure 22.6 The execution of Dijkstra’s algorithm. The source s is the leftmost vertex. The\\nshortest-path estimates appear within the vertices, and blue edges indicate predecessor values.\\nBlue vertices belong to the set S, and tan vertices are in the min-priority queue Q = V − S. (a)\\nThe situation just before the ﬁrst iteration of the while loop of lines 6–12. (b)–(f) The situation\\nafter each successive iteration of the while loop. In each part, the vertex highlighted in orange\\nwas chosen as vertex u in line 7, and each edge highlighted in orange caused a d value and a\\npredecessor to change when the edge was relaxed. The d values and predecessors shown in part\\n(f) are the ﬁnal values.\\nBecause Dijkstra’s algorithm always chooses the “lightest” or\\n“closest” vertex in V − S to add to set S, you can think of it as using a\\ngreedy strategy. Chapter 15 explains greedy strategies in detail, but you\\nneed not have read that chapter to understand Dijkstra’s algorithm.\\nGreedy strategies do not always yield optimal results in general, but as\\nthe following theorem and its corollary show, Dijkstra’s algorithm does\\nindeed compute shortest paths. The key is to show that u.d = δ (s, u)\\neach time it adds a vertex u to set S.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 805}),\n",
              " Document(page_content='Figure 22.7 The proof of Theorem 22.6. Vertex u is selected to be added into set S in line 7 of\\nDIJKSTRA. Vertex y is the ﬁrst vertex on a shortest path from the source s to vertex u that is\\nnot in set S, and x ∈ S is y’s predecessor on that shortest path. The subpath from y to u may or\\nmay not re-enter set S.\\nTheorem 22.6 (Correctness of Dijkstra’s algorithm)\\nDijkstra’s algorithm, run on a weighted, directed graph G = (V, E) with\\nnonnegative weight function w and source vertex s, terminates with u.d\\n= δ(s, u) for all vertices u ∈ V.\\nProof\\xa0\\xa0\\xa0We will show that at the start of each iteration of the while loop\\nof lines 6–12, we have v.d = δ (s, v) for all v ∈ S. The algorithm\\nterminates when S = V, so that v.d = δ(s, v) for all v ∈ V.\\nThe proof is by induction on the number of iterations of the while\\nloop, which equals |S| at the start of each iteration. There are two bases:\\nfor |S| = 0, so that S = Ø and the claim is trivially true, and for |S| = 1,\\nso that S = {s} and s.d = δ(s, s) = 0.\\nFor the inductive step, the inductive hypothesis is that v.d = δ (s, v)\\nfor all v ∈ S. The algorithm extracts vertex u from V − S. Because the\\nalgorithm adds u into S, we need to show that u.d = δ(s, u) at that time.\\nIf there is no path from s to u, then we are done, by the no-path\\nproperty. If there is a path from s to u, then, as Figure 22.7 shows, let y\\nbe the ﬁrst vertex on a shortest path from s to u that is not in S, and let\\nx ∈ S be the predecessor of y on that shortest path. (We could have y =\\nu or x = s.) Because y appears no later than u on the shortest path and\\nall edge weights are nonnegative, we have δ (s, y) ≤ δ (s, u). Because the\\ncall of EXTRACT-MIN in line 7 returned u as having the minimum d\\nvalue in V − S, we also have u.d ≤ y.d, and the upper-bound property\\ngives δ (s, u) ≤ u.d.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 806}),\n",
              " Document(page_content='Since x ∈ S, the inductive hypothesis implies that x.d = δ (s, x).\\nDuring the iteration of the while loop that added x into S, edge (x, y)\\nwas relaxed. By the convergence property, y.d received the value of δ (s,\\ny) at that time. Thus, we have\\nδ(s, y) ≤ δ(s, u) ≤ u.d ≤ y.d and y.d = δ(s, y),\\nso that\\nδ(s, y) = δ (s, u) = u.d = y.d.\\nHence, u.d = δ(s, u), and by the upper-bound property, this value never\\nchanges again.\\n▪\\nCorollary 22.7\\nAfter Dijkstra’s algorithm is run on a weighted, directed graph G = (V,\\nE) with nonnegative weight function w and source vertex s, the\\npredecessor subgraph Gπ is a shortest-paths tree rooted at s.\\nProof\\xa0\\xa0\\xa0Immediate from Theorem 22.6 and the predecessor-subgraph\\nproperty.\\n▪\\nAnalysis\\nHow fast is Dijkstra’s algorithm? It maintains the min-priority queue Q\\nby calling three priority-queue operations: INSERT (in line 5),\\nEXTRACT-MIN (in line 7), and DECREASE-KEY (in line 12). The\\nalgorithm calls both INSERT and EXTRACT-MIN once per vertex.\\nBecause each vertex u ∈ V is added to set S exactly once, each edge in\\nthe adjacency list Adj[u] is examined in the for loop of lines 9–12 exactly\\nonce during the course of the algorithm. Since the total number of\\nedges in all the adjacency lists is |E|, this for loop iterates a total of |E|\\ntimes, and thus the algorithm calls DECREASE-KEY at most |E| times\\noverall. (Observe once again that we are using aggregate analysis.)\\nJust as in Prim’s algorithm, the running time of Dijkstra’s algorithm\\ndepends on the speciﬁc implementation of the min-priority queue Q. A\\nsimple implementation takes advantage of the vertices being numbered', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 807}),\n",
              " Document(page_content='1 to |V|: simply store v.d in the vth entry of an array. Each INSERT and\\nDECREASE-KEY operation takes O(1) time, and each EXTRACT-\\nMIN operation takes O(V) time (since it has to search through the\\nentire array), for a total time of O(V2 + E) = O(V2).\\nIf the graph is sufﬁciently sparse—in particular, E = o(V2/lg V)—you\\ncan improve the running time by implementing the min-priority queue\\nwith a binary min-heap that includes a way to map between vertices and\\ntheir corresponding heap elements. Each EXTRACT-MIN operation\\nthen takes O(lg V) time. As before, there are |V| such operations. The\\ntime to build the binary min-heap is O(V). (As noted in Section 21.2,\\nyou don’t even need to call BUILD-MIN-HEAP.) Each DECREASE-\\nKEY operation takes O(lg V) time, and there are still at most |E| such\\noperations. The total running time is therefore O((V + E) lg V), which is\\nO(E lg V) in the typical case that |E| = Ω(V). This running time improves\\nupon the straightforward O(V2)-time implementation if E = o(V2/lg V).\\nBy implementing the min-priority queue with a Fibonacci heap (see\\npage 478), you can improve the running time to O(V lg V + E). The\\namortized cost of each of the |V| EXTRACT-MIN operations is O(lg\\nV), and each DECREASE-KEY call, of which there are at most |E|,\\ntakes only O(1) amortized time. Historically, the development of\\nFibonacci heaps was motivated by the observation that Dijkstra’s\\nalgorithm typically makes many more DECREASE-KEY calls than\\nEXTRACT-MIN calls, so that any method of reducing the amortized\\ntime of each DECREASE-KEY operation to o(lg V) without increasing\\nthe amortized time of EXTRACT-MIN would yield an asymptotically\\nfaster implementation than with binary heaps.\\nDijkstra’s algorithm resembles both breadth-ﬁrst search (see Section\\n20.2) and Prim’s algorithm for computing minimum spanning trees (see\\nSection 21.2). It is like breadth-ﬁrst search in that set S corresponds to\\nthe set of black vertices in a breadth-ﬁrst search. Just as vertices in S\\nhave their ﬁnal shortest-path weights, so do black vertices in a breadth-\\nﬁrst search have their correct breadth-ﬁrst distances. Dijkstra’s\\nalgorithm is like Prim’s algorithm in that both algorithms use a min-\\npriority queue to ﬁnd the “lightest” vertex outside a given set (the set S\\nin Dijkstra’s algorithm and the tree being grown in Prim’s algorithm),', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 808}),\n",
              " Document(page_content='add this vertex into the set, and adjust the weights of the remaining\\nvertices outside the set accordingly.\\nExercises\\n22.3-1\\nRun Dijkstra’s algorithm on the directed graph of Figure 22.2, ﬁrst\\nusing vertex s as the source and then using vertex z as the source. In the\\nstyle of Figure 22.6, show the d and π values and the vertices in set S\\nafter each iteration of the while loop.\\n22.3-2\\nGive a simple example of a directed graph with negative-weight edges\\nfor which Dijkstra’s algorithm produces an incorrect answer. Why\\ndoesn’t the proof of Theorem 22.6 go through when negative-weight\\nedges are allowed?\\n22.3-3\\nSuppose that you change line 6 of Dijkstra’s algorithm to read\\n6\\xa0\\xa0\\xa0while |Q| > 1\\nThis change causes the while loop to execute |V| − 1 times instead of |V|\\ntimes. Is this proposed algorithm correct?\\n22.3-4\\nModify the DIJKSTRA procedure so that the priority queue Q is more\\nlike the queue in the BFS procedure in that it contains only vertices that\\nhave been reached from source s so far: Q ⊆ V − S and v ∈ Q implies\\nv.d ≠ ∞.\\n22.3-5\\nProfessor Gaedel has written a program that he claims implements\\nDijkstra’s algorithm. The program produces v.d and v.π for each vertex\\nv ∈ V. Give an O(V + E)-time algorithm to check the output of the\\nprofessor’s program. It should determine whether the d and π attributes\\nmatch those of some shortest-paths tree. You may assume that all edge\\nweights are nonnegative.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 809}),\n",
              " Document(page_content='22.3-6\\nProfessor Newman thinks that he has worked out a simpler proof of\\ncorrectness for Dijkstra’s algorithm. He claims that Dijkstra’s algorithm\\nrelaxes the edges of every shortest path in the graph in the order in\\nwhich they appear on the path, and therefore the path-relaxation\\nproperty applies to every vertex reachable from the source. Show that\\nthe professor is mistaken by constructing a directed graph for which\\nDijkstra’s algorithm relaxes the edges of a shortest path out of order.\\n22.3-7\\nConsider a directed graph G = (V, E) on which each edge (u, v) ∈ E has\\nan associated value r(u, v), which is a real number in the range 0 ≤ r(u, v)\\n≤ 1 that represents the reliability of a communication channel from\\nvertex u to vertex v. Interpret r(u, v) as the probability that the channel\\nfrom u to v will not fail, and assume that these probabilities are\\nindependent. Give an efﬁcient algorithm to ﬁnd the most reliable path\\nbetween two given vertices.\\n22.3-8\\nLet G = (V, E) be a weighted, directed graph with positive weight\\nfunction w : E → {1, 2, … , W} for some positive integer W, and assume\\nthat no two vertices have the same shortest-path weights from source\\nvertex s. Now deﬁne an unweighted, directed graph G′ = (V ∪ V′, E′) by\\nreplacing each edge (u, v) ∈ E with w(u, v) unit-weight edges in series.\\nHow many vertices does G′ have? Now suppose that you run a breadth-\\nﬁrst search on G′. Show that the order in which the breadth-ﬁrst search\\nof G′ colors vertices in V black is the same as the order in which\\nDijkstra’s algorithm extracts the vertices of V from the priority queue\\nwhen it runs on G.\\n22.3-9\\nLet G = (V, E) be a weighted, directed graph with nonnegative weight\\nfunction w : E → {0, 1, … , W} for some nonnegative integer W.\\nModify Dijkstra’s algorithm to compute the shortest paths from a given\\nsource vertex s in O(W V + E) time.\\n22.3-10', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 810}),\n",
              " Document(page_content='Modify your algorithm from Exercise 22.3-9 to run in O((V + E) lg W)\\ntime. (Hint: How many distinct shortest-path estimates can V − S\\ncontain at any point in time?)\\n22.3-11\\nSuppose that you are given a weighted, directed graph G = (V, E) in\\nwhich edges that leave the source vertex s may have negative weights, all\\nother edge weights are nonnegative, and there are no negative-weight\\ncycles. Argue that Dijkstra’s algorithm correctly ﬁnds shortest paths\\nfrom s in this graph.\\n22.3-12\\nSuppose that you have a weighted directed graph G = (V, E) in which all\\nedge weights are positive real values in the range [C, 2C] for some\\npositive constant C. Modify Dijkstra’s algorithm so that it runs in O(V\\n+ E) time.\\n22.4\\xa0\\xa0\\xa0\\xa0Difference constraints and s hortest paths\\nChapter 29 studies the general linear-programming problem, showing\\nhow to optimize a linear function subject to a set of linear inequalities.\\nThis section investigates a special case of linear programming that\\nreduces to ﬁnding shortest paths from a single source. The Bellman-\\nFord algorithm then solves the resulting single-source shortest-paths\\nproblem, thereby also solving the linear-programming problem.\\nLinear programming\\nIn the general linear-programming problem, the input is an m × n matrix\\nA, an m-vector b, and an n-vector c. The goal is to ﬁnd a vector x of n\\nelements that maximizes the objective function \\n  subject to the m\\nconstraints given by Ax ≤ b.\\nThe most popular method for solving linear programs is the simplex\\nalgorithm, which Section 29.1 discusses. Although the simplex\\nalgorithm does not always run in time polynomial in the size of its\\ninput, there are other linear-programming algorithms that do run in', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 811}),\n",
              " Document(page_content='polynomial time. We offer here two reasons to understand the setup of\\nlinear-programming problems. First, if you know that you can cast a\\ngiven problem as a polynomial-sized linear-programming problem, then\\nyou immediately have a polynomial-time algorithm to solve the\\nproblem. Second, faster algorithms exist for many special cases of linear\\nprogramming. For example, the single-pair shortest-path problem\\n(Exercise 22.4-4) and the maximum-ﬂow problem (Exercise 24.1-5) are\\nspecial cases of linear programming.\\nSometimes the objective function does not matter: it’s enough just to\\nﬁnd any feasible solution, that is, any vector x that satisﬁes Ax ≤ b, or to\\ndetermine that no feasible solution exists. This section focuses on one\\nsuch feasibility problem.\\nSystems of difference constraints\\nIn a system of difference constraints, each row of the linear-\\nprogramming matrix A contains one 1 and one −1, and all other entries\\nof A are 0. Thus, the constraints given by Ax ≤ b are a set of mdifference\\nconstraints involving n unknowns, in which each constraint is a simple\\nlinear inequality of the form\\nxj − xi ≤ bk,\\nwhere 1 ≤ i, j ≤ n, i ≠ j, and 1 ≤ k ≤ m.\\nFor example, consider the problem of ﬁnding a 5-vector x = (xi) that\\nsatisﬁes\\nThis problem is equivalent to ﬁnding values for the unknowns x1, x2,\\nx3, x4, x5, satisfying the following 8 difference constraints:', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 812}),\n",
              " Document(page_content='One solution to this problem is x = (−5, −3, 0, −1, −4), which you can\\nverify directly by checking each inequality. In fact, this problem has\\nmore than one solution. Another is x′ = (0, 2, 5, 4, 1). These two\\nsolutions are related: each component of x′ is 5 larger than the\\ncorresponding component of x. This fact is not mere coincidence.\\nLemma 22.8\\nLet x = (x1, x2, … , xn) be a solution to a system Ax ≤ b of difference\\nconstraints, and let d be any constant. Then x + d = (x1 + d, x2 + d, … ,\\nxn + d) is a solution to Ax ≤ b as well.\\nProof\\xa0\\xa0\\xa0For each xi and xj, we have (xj + d) − (xi + d) = xj − xi. Thus, if\\nx satisﬁes Ax ≤ b, so does x + d.\\n▪\\nSystems of difference constraints occur in various applications. For\\nexample, the unknowns xi might be times at which events are to occur.\\nEach constraint states that at least a certain amount of time, or at most\\na certain amount of time, must elapse between two events. Perhaps the\\nevents are jobs to be performed during the assembly of a product. If the\\nmanufacturer applies an adhesive that takes 2 hours to set at time x1\\nand has to wait until it sets to install a part at time x2, then there is a\\nconstraint that x2 ≥ x1 + 2 or, equivalently, that x1 − x2 ≤ −2.\\nAlternatively, the manufacturer might require the part to be installed\\nafter the adhesive has been applied but no later than the time that the\\nadhesive has set halfway. In this case, there is a pair of constraints x2 ≥\\nx1 and x2 ≤ x1 + 1 or, equivalently, x1 − x2 ≤ 0 and x2 − x1 ≤ 1.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 813}),\n",
              " Document(page_content='If all the constraints have nonnegative numbers on the right-hand\\nside—that is, if bi ≥ 0 for i = 1, 2, … , m—then ﬁnding a feasible\\nsolution is trivial: just set all the unknowns xi equal to each other. Then\\nall the differences are 0, and every constraint is satisﬁed. The problem of\\nﬁnding a feasible solution to a system of difference constraints is\\ninteresting only if at least one constraint has bi < 0.\\nConstraint graphs\\nWe can interpret systems of difference constraints from a graph-\\ntheoretic point of view. For a system Ax ≤ b of difference constraints,\\nlet’s view the m × n linear-programming matrix A as the transpose of an\\nincidence matrix (see Exercise 20.1-7) for a graph with n vertices and m\\nedges. Each vertex vi in the graph, for i = 1, 2, … , n, corresponds to one\\nof the n unknown variables xi. Each directed edge in the graph\\ncorresponds to one of the m inequalities involving two unknowns.\\nMore formally, given a system Ax ≤ b of difference constraints, the\\ncorresponding constraint graph is a weighted, directed graph G = (V, E),\\nwhere\\nV = {v0, v1, … , vn}\\nand\\nE ={(vi, vj) : xj − xi ≤ bk is a constraint}\\n\\xa0\\xa0\\xa0 ∪ {(v0, v1), (v0, v2), (v0, v3), … , (v0, vn)}.\\nThe constraint graph includes the additional vertex v0, as we shall see\\nshortly, to guarantee that the graph has some vertex that can reach all\\nother vertices. Thus, the vertex set V consists of a vertex vi for each\\nunknown xi, plus an additional vertex v0. The edge set E contains an\\nedge for each difference constraint, plus an edge (v0, vi) for each\\nunknown xi. If xj − xi ≤ bk is a difference constraint, then the weight of\\nedge (vi, vj) is w(vi, vj) = bk. The weight of each edge leaving v0 is 0.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 814}),\n",
              " Document(page_content='Figure 22.8 shows the constraint graph for the system (22.2)–(22.9) of\\ndifference constraints.\\nFigure 22.8 The constraint graph corresponding to the system (22.2)–(22.9) of difference\\nconstraints. The value of δ (v0, vi) appears in each vertex vi. One feasible solution to the system is\\nx = (−5, −3, 0, −1, −4).\\nThe following theorem shows how to solve a system of difference\\nconstraints by ﬁnding shortest-path weights in the corresponding\\nconstraint graph.\\nTheorem 22.9\\nGiven a system Ax ≤ b of difference constraints, let G = (V, E) be the\\ncorresponding constraint graph. If G contains no negative-weight cycles,\\nthen\\nis a feasible solution for the system. If G contains a negative-weight\\ncycle, then there is no feasible solution for the system.\\nProof\\xa0\\xa0\\xa0We ﬁrst show that if the constraint graph contains no negative-\\nweight cycles, then equation (22.10) gives a feasible solution. Consider\\nany edge (vi, vj) ∈ E. The triangle inequality implies that δ (v0, vj) ≤ δ(v0,\\nvi) + w(vi, vj), which is equivalent to δ (v0, vj)−δ(v0, vi) ≤ w(vi, vj). Thus,\\nletting xi = δ(v0, vi) and xj = δ(v0, vj) satisﬁes the difference constraint\\nxj − xi ≤ w(vi, vj) that corresponds to edge (vi, vj).\\nNow we show that if the constraint graph contains a negative-weight\\ncycle, then the system of difference constraints has no feasible solution.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 815}),\n",
              " Document(page_content='Without loss of generality, let the negative-weight cycle be c = 〈v1, v2,\\n… , vk〉, where v1 = vk. (The vertex v0 cannot be on cycle c, because it\\nhas no entering edges.) Cycle c corresponds to the following difference\\nconstraints:\\nx2 − x1≤w(v1, v2),\\nx3 − x2≤w(v2, v3),\\n⋮\\nxk−1 − xk−2≤w(vk−2, vk−1),\\nxk − xk−1≤w(vk−1, vk).\\nWe’ll assume that x has a solution satisfying each of these k inequalities\\nand then derive a contradiction. The solution must also satisfy the\\ninequality that results from summing the k inequalities together. In\\nsumming the left-hand sides, each unknown xi is added in once and\\nsubtracted out once (remember that v1 = vk implies x1 = xk), so that\\nthe left-hand side sums to 0. The right-hand side sums to the weight\\nw(c) of the cycle, giving 0 ≤ w(c). But since c is a negative-weight cycle,\\nw(c) < 0, and we obtain the contradiction that 0 ≤ w(c) < 0.\\n▪\\nSolving systems of difference constraints\\nTheorem 22.9 suggests how to use the Bellman-Ford algorithm to solve\\na system of difference constraints. Because the constraint graph\\ncontains edges from the source vertex v0 to all other vertices, any\\nnegative-weight cycle in the constraint graph is reachable from v0. If the\\nBellman-Ford algorithm returns TRUE, then the shortest-path weights\\ngive a feasible solution to the system. In Figure 22.8, for example, the\\nshortest-path weights provide the feasible solution x = (−5, −3, 0, −1,\\n−4), and by Lemma 22.8, x = (d − 5, d − 3, d, d − 1, d − 4) is also a\\nfeasible solution for any constant d. If the Bellman-Ford algorithm\\nreturns FALSE, there is no feasible solution to the system of difference\\nconstraints.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 816}),\n",
              " Document(page_content='A system of difference constraints with m constraints on n unknowns\\nproduces a graph with n + 1 vertices and n + m edges. Thus, the\\nBellman-Ford algorithm provides a way to solve the system in O((n + 1)\\n(n + m)) = O(n2 + nm) time. Exercise 22.4-5 asks you to modify the\\nalgorithm to run in O(nm) time, even if m is much less than n.\\nExercises\\n22.4-1\\nFind a feasible solution or determine that no feasible solution exists for\\nthe following system of difference constraints:\\nx1 − x2≤1,\\nx1 − x4≤−4,\\nx2 − x3≤2,\\nx2 − x5≤7,\\nx2 − x6≤5,\\nx3 − x6≤10,\\nx4 − x2≤2,\\nx5 − x1≤−1,\\nx5 − x4≤3,\\nx6 − x3≤−8.\\n22.4-2\\nFind a feasible solution or determine that no feasible solution exists for\\nthe following system of difference constraints:\\nx1 − x2≤4,\\nx1 − x5≤5,\\nx2 − x4≤−6,\\nx3 − x2≤1,\\n≤3,', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 817}),\n",
              " Document(page_content='x4 − x1\\nx4 − x3≤5,\\nx4 − x5≤10,\\nx5 − x3≤−4,\\nx5 − x4≤−8.\\n22.4-3\\nCan any shortest-path weight from the new vertex v0 in a constraint\\ngraph be positive? Explain.\\n22.4-4\\nExpress the single-pair shortest-path problem as a linear program.\\n22.4-5\\nShow how to modify the Bellman-Ford algorithm slightly so that when\\nusing it to solve a system of difference constraints with m inequalities on\\nn unknowns, the running time is O(nm).\\n22.4-6\\nConsider adding equality constraints of the form xi = xj + bk to a\\nsystem of difference constraints. Show how to solve this variety of\\nconstraint system.\\n22.4-7\\nShow how to solve a system of difference constraints by a Bellman-\\nFord-like algorithm that runs on a constraint graph without the extra\\nvertex v0.\\n★ 22.4-8\\nLet Ax ≤ b be a system of m difference constraints in n unknowns. Show\\nthat the Bellman-Ford algorithm, when run on the corresponding\\nconstraint graph, maximizes \\n  subject to Ax ≤ b and xi ≤ 0 for all xi.\\n★ 22.4-9', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 818}),\n",
              " Document(page_content='Show that the Bellman-Ford algorithm, when run on the constraint\\ngraph for a system Ax ≤ b of difference constraints, minimizes the\\nquantity (max {xi}−min {xi}) subject to Ax ≤ b. Explain how this fact\\nmight come in handy if the algorithm is used to schedule construction\\njobs.\\n22.4-10\\nSuppose that every row in the matrix A of a linear program Ax ≤ b\\ncorresponds to a difference constraint, a single-variable constraint of\\nthe form xi ≤ bk, or a single-variable constraint of the form −xi ≤ bk.\\nShow how to adapt the Bellman-Ford algorithm to solve this variety of\\nconstraint system.\\n22.4-11\\nGive an efﬁcient algorithm to solve a system Ax ≤ b of difference\\nconstraints when all of the elements of b are real-valued and all of the\\nunknowns xi must be integers.\\n★ 22.4-12\\nGive an efﬁcient algorithm to solve a system Ax ≤ b of difference\\nconstraints when all of the elements of b are real-valued and a speciﬁed\\nsubset of some, but not necessarily all, of the unknowns xi must be\\nintegers.\\n22.5\\xa0\\xa0\\xa0\\xa0Proofs of shortest-paths properties\\nThroughout this chapter, our correctness arguments have relied on the\\ntriangle inequality, upper-bound property, no-path property,\\nconvergence property, path-relaxation property, and predecessor-\\nsubgraph property. We stated these properties without proof on page\\n611. In this section, we prove them.\\nThe triangle inequality\\nIn studying breadth-ﬁrst search (Section 20.2), we proved as Lemma\\n20.1 a simple property of shortest distances in unweighted graphs. The', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 819}),\n",
              " Document(page_content='triangle inequality generalizes the property to weighted graphs.\\nLemma 22.10 (Triangle inequality)\\nLet G = (V, E) be a weighted, directed graph with weight function w : E\\n→ ℝ and source vertex s. Then, for all edges (u, v) ∈ E,\\nδ(s, v) ≤ δ(s, u) + w(u, v).\\nProof\\xa0\\xa0\\xa0Suppose that p is a shortest path from source s to vertex v. Then\\np has no more weight than any other path from s to v. Speciﬁcally, path\\np has no more weight than the particular path that takes a shortest path\\nfrom source s to vertex u and then takes edge (u, v).\\nExercise 22.5-3 asks you to handle the case in which there is no\\nshortest path from s to v.\\n▪\\nEffects of relaxation on shortest-path estimates\\nThe next group of lemmas describes how shortest-path estimates are\\naffected by executing a sequence of relaxation steps on the edges of a\\nweighted, directed graph that has been initialized by INITIALIZE-\\nSINGLE-SOURCE.\\nLemma 22.11 (Upper-bound property)\\nLet G = (V, E) be a weighted, directed graph with weight function w : E\\n→ ℝ. Let s ∈ V be the source vertex, and let the graph be initialized by\\nINITIALIZE-SINGLE-SOURCE(G, s). Then, v.d ≥ δ(s, v) for all v ∈\\nV, and this invariant is maintained over any sequence of relaxation steps\\non the edges of G. Moreover, once v.d achieves its lower bound δ (s, v), it\\nnever changes.\\nProof\\xa0\\xa0\\xa0We prove the invariant v.d ≥ δ(s, v) for all vertices v ∈ V by\\ninduction over the number of relaxation steps.\\nFor the base case, v.d ≥ δ(s, v) holds after initialization, since if v.d =\\n∞, then v.d ≥ δ(s, v) for all v ∈ V − {s}, and since s.d = 0 ≥ δ (s, s). (Note\\nthat δ (s, s) = −∞ if s is on a negative-weight cycle and that δ (s, s) = 0\\notherwise.)', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 820}),\n",
              " Document(page_content='For the inductive step, consider the relaxation of an edge (u, v). By\\nthe inductive hypothesis, x.d ≥ δ (s, x) for all x ∈ V prior to the\\nrelaxation. The only d value that may change is v.d. If it changes, we\\nhave\\nv.d=u.d + w(u, v)\\n≥δ(s, u) + w(u, v)(by the inductive hypothesis)\\n≥δ(s, v) (by the triangle inequality),\\nand so the invariant is maintained.\\nThe value of v.d never changes once v.d = δ (s, v) because, having\\nachieved its lower bound, v.d cannot decrease since we have just shown\\nthat v.d ≥ δ(s, v), and it cannot increase because relaxation steps do not\\nincrease d values.\\n▪\\nCorollary 22.12 (No-path property)\\nSuppose that in a weighted, directed graph G = (V, E) with weight\\nfunction w : E → ℝ, no path connects a source vertex s ∈ V to a given\\nvertex v ∈ V. Then, after the graph is initialized by INITIALIZE-\\nSINGLE-SOURCE(G, s), we have v.d = δ(s, v) = ∞, and this equation is\\nmaintained as an invariant over any sequence of relaxation steps on the\\nedges of G.\\nProof\\xa0\\xa0\\xa0By the upper-bound property, we always have ∞ = δ (s, v) ≤ v.d,\\nand thus v.d = ∞ = δ (s, v).\\n▪\\nLemma 22.13\\nLet G = (V, E) be a weighted, directed graph with weight function w : E\\n→ ℝ, and let (u, v) ∈ E. Then, immediately after edge (u, v) is relaxed\\nby a call of RELAX(u, v, w), we have v.d ≤ u.d + w(u, v).\\nProof\\xa0\\xa0\\xa0If, just prior to relaxing edge (u, v), we have v.d > u.d + w(u, v),\\nthen v.d = u.d + w(u, v) afterward. If, instead, v.d ≤ u.d + w(u, v) just\\nbefore the relaxation, then neither u.d nor v.d changes, and so v.d ≤ u.d +\\nw(u, v) afterward.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 821}),\n",
              " Document(page_content='▪\\nLemma 22.14 (Convergence property)\\nLet G = (V, E) be a weighted, directed graph with weight function w : E\\n→ ℝ, let s ∈ V be a source vertex, and let s ⇝ u → v be a shortest path\\nin G for some vertices u, v ∈ V. Suppose that G is initialized by\\nINITIALIZE-SINGLE-SOURCE(G, s) and then a sequence of\\nrelaxation steps that includes the call RELAX(u, v, w) is executed on the\\nedges of G. If u.d = δ(s, u) at any time prior to the call, then v.d = δ(s, v)\\nat all times after the call.\\nProof\\xa0\\xa0\\xa0By the upper-bound property, if u.d = δ(s, u) at some point prior\\nto relaxing edge (u, v), then this equation holds thereafter. In particular,\\nafter edge (u, v) is relaxed, we have\\nv.d≤u.d + w(u, v) (by Lemma 22.13)\\n=δ(s, u) + w(u, v)\\n=δ(s, u) (by Lemma 22.1 on page 606).\\nThe upper-bound property gives v.d ≥ δ(s, v), from which we conclude\\nthat v.d = δ(s, v), and this equation is maintained thereafter.\\n▪\\nLemma 22.15 (Path-relaxation property)\\nLet G = (V, E) be a weighted, directed graph with weight function w : E\\n→ ℝ, and let s ∈ V be a source vertex. Consider any shortest path p =\\n〈v0, v1, … , vk〉 from s = v0 to vk. If G is initialized by INITIALIZE-\\nSINGLE-SOURCE(G, s) and then a sequence of relaxation steps occurs\\nthat includes, in order, relaxing the edges (v0, v1), (v1, v2), … , (vk−1,\\nvk), then vk.d = δ (s, vk) after these relaxations and at all times\\nafterward. This property holds no matter what other edge relaxations\\noccur, including relaxations that are intermixed with relaxations of the\\nedges of p.\\nProof\\xa0\\xa0\\xa0We show by induction that after the ith edge of path p is relaxed,\\nwe have vi.d = δ(s, vi). For the base case, i = 0, and before any edges of p', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 822}),\n",
              " Document(page_content='have been relaxed, we have from the initialization that v0.d = s.d = 0 =\\nδ(s, s). By the upper-bound property, the value of s.d never changes\\nafter initialization.\\nFor the inductive step, assume that vi−1.d = δ (s, vi−1). What\\nhappens when edge (vi−1, vi) is relaxed? By the convergence property,\\nafter this relaxation, we have vi.d = δ (s, vi), and this equation is\\nmaintained at all times thereafter.\\n▪\\nRelaxation and shortest-paths trees\\nWe now show that once a sequence of relaxations has caused the\\nshortest-path estimates to converge to shortest-path weights, the\\npredecessor subgraph Gπ induced by the resulting π values is a shortest-\\npaths tree for G. We start with the following lemma, which shows that\\nthe predecessor subgraph always forms a rooted tree whose root is the\\nsource.\\nLemma 22.16\\nLet G = (V, E) be a weighted, directed graph with weight function w : E\\n→ ℝ let s ∈ V be a source vertex, and assume that G contains no\\nnegative-weight cycles that are reachable from s. Then, after the graph is\\ninitialized by INITIALIZE-SINGLE-SOURCE(G, s), the predecessor\\nsubgraph Gπ forms a rooted tree with root s, and any sequence of\\nrelaxation steps on edges of G maintains this property as an invariant.\\nProof\\xa0\\xa0\\xa0Initially, the only vertex in Gπ is the source vertex, and the\\nlemma is trivially true. Consider a predecessor subgraph Gπ that arises\\nafter a sequence of relaxation steps. We ﬁrst prove that Gπ is acyclic.\\nSuppose for the sake of contradiction that some relaxation step creates\\na cycle in the graph Gπ. Let the cycle be c = 〈v0, v1, … , vk〉, where vk =\\nv0. Then, vi.π = vi−1 for i = 1, 2, … , k and, without loss of generality,\\nassume that relaxing edge (vk−1, vk) created the cycle in Gπ.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 823}),\n",
              " Document(page_content='We claim that all vertices on cycle c are reachable from the source\\nvertex s. Why? Each vertex on c has a non-NIL predecessor, and so each\\nvertex on c was assigned a ﬁnite shortest-path estimate when it was\\nassigned its non-NIL π value. By the upper-bound property, each vertex\\non cycle c has a ﬁnite shortest-path weight, which means that it is\\nreachable from s.\\nWe’ll examine the shortest-path estimates on cycle c immediately\\nbefore the call RELAX(vk−1, vk, w) and show that c is a negative-\\nweight cycle, thereby contradicting the assumption that G contains no\\nnegative-weight cycles that are reachable from the source. Just before\\nthe call, we have vi.π = vi−1 for i = 1, 2, … , k − 1. Thus, for i = 1, 2, … ,\\nk − 1, the last update to vi.d was by the assignment vi.d =\\nvi−1.d+w(vi−1, vi). If vi−1.d changed since then, it decreased. Therefore,\\njust before the call RELAX(vk−1, vk, w), we have\\nBecause vk.π is changed by the call RELAX(vk−1, vk, w), immediately\\nbeforehand we also have the strict inequality\\nvk.d > vk−1.d + wvk−1, vk):\\nSumming this strict inequality with the k − 1 inequalities (22.11), we\\nobtain the sum of the shortest-path estimates around cycle c:\\nBut\\n', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 824}),\n",
              " Document(page_content='Figure 22.9 Showing that a simple path in Gπ from source vertex s to vertex v is unique. If Gπ\\ncontains two paths p1 (s ⇝ u ⇝ x → z ⇝ v) and p2 (s ⇝ u ⇝ y → z ⇝ v), where x ≠ y, then z.π = x\\nand z.π = y, a contradiction.\\nsince each vertex in the cycle c appears exactly once in each summation.\\nThis equation implies\\nThus, the sum of weights around the cycle c is negative, which provides\\nthe desired contradiction.\\nWe have now proven that Gπ is a directed, acyclic graph. To show\\nthat it forms a rooted tree with root s, it sufﬁces (see Exercise B.5-2 on\\npage 1175) to prove that for each vertex v ∈ Vπ, there is a unique simple\\npath from s to v in Gπ.\\nThe vertices in Vπ are those with non-NIL values, plus s. Exercise\\n22.5-6 asks you to prove that a path from s exists to each vertex in Vπ.\\nTo complete the proof of the lemma, we now show that for any\\nvertex v ∈ Vπ, the graph Gπ contains at most one simple path from s to\\nv. Suppose otherwise. That is, suppose that, as Figure 22.9 illustrates,\\nGπ contains two simple paths from s to some vertex v: p1, which we\\ndecompose into s ⇝ u ⇝ x → z ⇝ v, and p2, which we decompose into s\\n⇝ u ⇝ y → z ⇝ v, where x ≠ y (though u could be s and z could be v).\\nBut then, z.π = x and z.π = y, which implies the contradiction that x =\\ny. We conclude that Gπ contains a unique simple path from s to v, and\\nthus Gπ forms a rooted tree with root s.\\n▪\\nWe can now show that if all vertices have been assigned their true\\nshortest-path weights after a sequence of relaxation steps, then the', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 825}),\n",
              " Document(page_content='predecessor subgraph Gπ is a shortest-paths tree.\\nLemma 22.17 (Predecessor-subgraph pr operty)\\nLet G = (V, E) be a weighted, directed graph with weight function w : E\\n→ ℝ, let s ∈ V be a source vertex, and assume that G contains no\\nnegative-weight cycles that are reachable from s. Then, after a call to\\nINITIALIZE-SINGLE-SOURCE(G, s) followed by any sequence of\\nrelaxation steps on edges of G that produces v.d = δ(s, v) for all v ∈ V,\\nthe predecessor subgraph Gπ is a shortest-paths tree rooted at s.\\nProof\\xa0\\xa0\\xa0We must prove that the three properties of shortest-paths trees\\ngiven on page 608 hold for Gπ. To show the ﬁrst property, we must show\\nthat Vπ is the set of vertices reachable from s. By deﬁnition, a shortest-\\npath weight δ (s, v) is ﬁnite if and only if v is reachable from s, and thus\\nthe vertices that are reachable from s are exactly those with ﬁnite d\\nvalues. But a vertex v ∈ V − {s} has been assigned a ﬁnite value for v.d\\nif and only if v.π ≠ NIL, since both assignments occur in RELAX. Thus,\\nthe vertices in Vπ are exactly those reachable from s.\\nThe second property, that Gπ forms a rooted tree with root s, follows\\ndirectly from Lemma 22.16.\\nIt remains, therefore, to prove the last property of shortest-paths\\ntrees: for each vertex v ∈ Vπ, the unique simple path \\n  in Gπ is a\\nshortest path from s to v in G. Let p = 〈v0, v1, … , vk〉, where v0 = s and\\nvk = v. Consider an edge (vi−1, vi) in path p. Because this edge belongs\\nto Gπ, the last relaxation that changed vi.d must have been of this edge.\\nAfter that relaxation, we had vi.d = vi−1.d + (vi−1, vi). Subsequently, an\\nedge entering vi−1 could have been relaxed, causing vi−1.d to decrease\\nfurther, but without changing vi.d. Therefore, we have vi.d ≥ vi−1.d +\\nw(vi−1, vi). Thus, for i = 1, 2, … , k, we have both vi.d = δ(s, vi) and vi.d\\n≥ vi−1.d + w(vi−1, vi), which together imply w(vi−1, vi) ≤ δ (s, vi) − δ (s,\\nvi−1). Summing the weights along path p yields', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 826}),\n",
              " Document(page_content='Thus, we have w(p) ≤ δ (s, vk). Since δ (s, vk) is a lower bound on the\\nweight of any path from s to vk, we conclude that w(p) = δ (s, vk), and p\\nis a shortest path from s to v = vk.\\n▪\\nExercises\\n22.5-1\\nGive two shortest-paths trees for the directed graph of Figure 22.2 on\\npage 609 other than the two shown.\\n22.5-2\\nGive an example of a weighted, directed graph G = (V, E) with weight\\nfunction w : E → ℝ and source vertex s such that G satisﬁes the\\nfollowing property: For every edge (u, v) ∈ E, there is a shortest-paths\\ntree rooted at s that contains (u, v) and another shortest-paths tree\\nrooted at s that does not contain (u, v).\\n22.5-3\\nModify the proof of Lemma 22.10 to handle cases in which shortest-\\npath weights are ∞ or −∞.\\n22.5-4\\nLet G = (V, E) be a weighted, directed graph with source vertex s, and\\nlet G be initialized by INITIALIZE-SINGLE-SOURCE(G, s). Prove\\nthat if a sequence of relaxation steps sets s.π to a non-NIL value, then G\\ncontains a negative-weight cycle.\\n22.5-5', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 827}),\n",
              " Document(page_content='Let G = (V, E) be a weighted, directed graph with no negative-weight\\nedges. Let s ∈ V be the source vertex, and suppose that v.π is allowed to\\nbe the predecessor of v on any shortest path to v from source s if v ∈ V\\n− {s} is reachable from s, and NIL otherwise. Give an example of such a\\ngraph G and an assignment of π values that produces a cycle in Gπ. (By\\nLemma 22.16, such an assignment cannot be produced by a sequence of\\nrelaxation steps.)\\n22.5-6\\nLet G = (V, E) be a weighted, directed graph with weight function w : E\\n→ ℝ and no negative-weight cycles. Let s ∈ V be the source vertex, and\\nlet G be initialized by INITIALIZE-SINGLE-SOURCE(G, s). Use\\ninduction to prove that for every vertex v ∈ Vπ, there exists a path from\\ns to v in Gπ and that this property is maintained as an invariant over\\nany sequence of relaxations.\\n22.5-7\\nLet G = (V, E) be a weighted, directed graph that contains no negative-\\nweight cycles. Let s ∈ V be the source vertex, and let G be initialized by\\nINITIALIZESINGLE-SOURCE(G, s). Prove that there exists a\\nsequence of |V| − 1 relaxation steps that produces v.d = δ(s, v) for all v ∈\\nV.\\n22.5-8\\nLet G be an arbitrary weighted, directed graph with a negative-weight\\ncycle reachable from the source vertex s. Show how to construct an\\ninﬁnite sequence of relaxations of the edges of G such that every\\nrelaxation causes a shortest-path estimate to change.\\nProblems\\n22-1\\xa0\\xa0\\xa0\\xa0\\xa0Y en’s improvement to Bellman-Ford\\nThe Bellman-Ford algorithm does not specify the order in which to\\nrelax edges in each pass. Consider the following method for deciding\\nupon the order. Before the ﬁrst pass, assign an arbitrary linear order v1,', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 828}),\n",
              " Document(page_content='v2, … , v|V| to the vertices of the input graph G = (V, E). Then partition\\nthe edge set E into Ef ∪ Eb, where Ef = {(vi, vj) ∈ E : i < j} and Eb =\\n{(vi, vj) ∈ E : i > j}. (Assume that G contains no self-loops, so that\\nevery edge belongs to either Ef or Eb.) Deﬁne Gf = (V, Ef) and Gb = (V,\\nEb).\\na. Prove that Gf is acyclic with topological sort 〈v1, v2, … , v|V|〉 and\\nthat Gb is acyclic with topological sort 〈v|V|, v|V|−1, … , v1〉.\\nSuppose that each pass of the Bellman-Ford algorithm relaxes edges in\\nthe following way. First, visit each vertex in the order v1, v2, … , v|V|,\\nrelaxing edges of Ef that leave the vertex. Then visit each vertex in the\\norder v|V|, v|V|−1, …, v1, relaxing edges of Eb that leave the vertex.\\nb. Prove that with this scheme, if G contains no negative-weight cycles\\nthat are reachable from the source vertex s, then after only ⌈|V| / 2 ⌉\\npasses over the edges, v.d = δ(s, v) for all vertices v ∈ V.\\nc. Does this scheme improve the asymptotic running time of the\\nBellman-Ford algorithm?\\n22-2\\xa0\\xa0\\xa0\\xa0\\xa0N esting boxes\\nA d-dimensional box with dimensions (x1, x2, … , xd) nests within\\nanother box with dimensions (y1, y2, … , yd) if there exists a\\npermutation π on {1, 2, … , d} such that xπ(1) < y1, xπ(2) < y2, … ,\\nxπ(d) < yd.\\na. Argue that the nesting relation is transitive.\\nb. Describe an efﬁcient method to determine whether one d-dimensional\\nbox nests inside another.\\nc. You are given a set of n d-dimensional boxes {B1, B2, … , Bn}. Give\\nan efﬁcient algorithm to ﬁnd the longest sequence \\n  of\\nboxes such that \\n nests within \\n for j = 1, 2, … , k − 1. Express the\\nrunning time of your algorithm in terms of n and d.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 829}),\n",
              " Document(page_content='22-3\\xa0\\xa0\\xa0\\xa0\\xa0A rbitrage\\nArbitrage is the use of discrepancies in currency exchange rates to\\ntransform one unit of a currency into more than one unit of the same\\ncurrency. For example, suppose that one U.S. dollar buys 64 Indian\\nrupees, one Indian rupee buys 1:8 Japanese yen, and one Japanese yen\\nbuys 0:009 U.S. dollars. Then, by converting currencies, a trader can\\nstart with 1 U.S. dollar and buy 64 × 1.8 × 0.009 = 1.0368 U.S. dollars,\\nthus turning a proﬁt of 3.68%.\\nSuppose that you are given n currencies c1, c2, … , cn and an n × n\\ntable R of exchange rates, such that 1 unit of currency ci buys R[i, j]\\nunits of currency cj.\\na. Give an efﬁcient algorithm to determine whether there exists a\\nsequence of currencies \\n  such that\\nR[i1, i2] · R[i2, i3] … R[ik−1, ik] · R[ik, i1] > 1.\\nAnalyze the running time of your algorithm.\\nb. Give an efﬁcient algorithm to print out such a sequence if one exists.\\nAnalyze the running time of your algorithm.\\n22-4\\xa0\\xa0\\xa0\\xa0\\xa0G abow’s scaling algorithm for single-source shortest paths\\nA scaling algorithm solves a problem by initially considering only the\\nhighest-order bit of each relevant input value, such as an edge weight,\\nassuming that these values are nonnegative integers. The algorithm then\\nreﬁnes the initial solution by looking at the two highest-order bits. It\\nprogressively looks at more and more high-order bits, reﬁning the\\nsolution each time, until it has examined all bits and computed the\\ncorrect solution.\\nThis problem examines an algorithm for computing the shortest\\npaths from a single source by scaling edge weights. The input is a\\ndirected graph G = (V, E) with nonnegative integer edge weights w. Let\\nW = max {w(u, v) : (u, v) = E} be the maximum weight of any edge. In\\nthis problem, you will develop an algorithm that runs in O(E lg W) time.\\nAssume that all vertices are reachable from the source.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 830}),\n",
              " Document(page_content='The scaling algorithm uncovers the bits in the binary representation\\nof the edge weights one at a time, from the most signiﬁcant bit to the\\nleast signiﬁcant bit. Speciﬁcally, let k = ⌈lg(W + 1) ⌉ be the number of\\nbits in the binary representation of W, and for i = 1, 2, … , k, let wi(u,v)\\n= ⌊w(u,v)/2k–i⌋. That is, wi (u, v) is the “scaled-down” version of w(u, v)\\ngiven by the i most signiﬁcant bits of w(u, v). (Thus, wk(u, v) = w(u, v)\\nfor all (u, v) ∈ E.) For example, if k = 5 and w(u, v) = 25, which has the\\nbinary representation 〈11001〉, then w3(u, v) = 〈110〉 = 6. Also with k =\\n5, if w(u, v) = 〈00100〉 = 4, then w4(u, v) = 〈0010〉 = 2. Deﬁne δi(u, v) as\\nthe shortest-path weight from vertex u to vertex v using weight function\\nwi, so that δk(u, v) = δ (u, v) for all u, v ∈ V. For a given source vertex s,\\nthe scaling algorithm ﬁrst computes the shortest-path weights δ1(s, v)\\nfor all v ∈ V, then computes δ2(s, v) for all v ∈ V, and so on, until it\\ncomputes δk(s, v) for all v ∈ V. Assume throughout that |E| ≥ |V| − 1.\\nYou will show how to compute δi from δi−1 in O(E) time, so that the\\nentire algorithm takes O(kE) = O(E lg W) time.\\na. Suppose that for all vertices v ∈ V, we have δ (s, v) ≤ |E|. Show how to\\ncompute δ (s, v) for all v ∈ V in O(E) time.\\nb. Show how to compute δ1(s, v) for all v ∈ V in O(E) time.\\nNow focus on computing δi from δi−1.\\nc. Prove that for i = 2, 3, … , k, either wi(u, v) = 2wi−1(u, v) or wi(u, v) =\\n2wi−1(u, v) + 1. Then prove that\\n2δi−1(s, v) ≤ δi(s, v) ≤ 2δi−1(s, v) + |V| − 1\\nfor all v ∈ V.\\nd. Deﬁne, for i = 2, 3, … , k and all (u, v) ∈ E,\\nŵi(u, v) = wi(u, v) + 2 δi−1(s, u) − 2 δi−1(s, v).', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 831}),\n",
              " Document(page_content='Prove that for i = 2, 3, … , k and all u, v ∈ V, the “reweighted” value\\nŵi(u, v) of edge (u, v) is a nonnegative integer.\\ne. Now deﬁne \\n  as the shortest-path weight from s to v using the\\nweight function ŵi. Prove that for i = 2, 3, … , k and all v ∈ V,\\nand that \\n .\\nf. Show how to compute δi(s, v) from δi−1(s, v) for all v ∈ V in O(E)\\ntime. Conclude that you can compute δ (s, v) for all v ∈ V in O(E lg\\nW) time.\\n22-5\\xa0\\xa0\\xa0\\xa0\\xa0K arp’s minimum mean-weight cycle algorithm\\nLet G = (V, E) be a directed graph with weight function w : E → ℝ, and\\nlet n = |V|. We deﬁne the mean weight of a cycle c = 〈e1, e2, … , ek〉 of\\nedges in E to be\\nLet μ* = min {μ(c) : c is a directed cycle in G}. We call a cycle c for\\nwhich μ(c) = μ* a minimum mean-weight cycle. This problem investigates\\nan efﬁcient algorithm for computing μ*.\\nAssume without loss of generality that every vertex v ∈ V is\\nreachable from a source vertex s ∈ V. Let δ (s, v) be the weight of a\\nshortest path from s to v, and let δk(s, v) be the weight of a shortest path\\nfrom s to v consisting of exactly k edges. If there is no path from s to v\\nwith exactly k edges, then δk(s, v) = ∞.\\na. Show that if μ* = 0, then G contains no negative-weight cycles and\\nδ(s, v) = min { δk(s, v) : 0 ≤ k ≤ n − 1} for all vertices v ∈ V.\\nb. Show that if μ* = 0, then\\n', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 832}),\n",
              " Document(page_content='for all vertices v ∈ V. (Hint: Use both properties from part (a).)\\nc. Let c be a 0-weight cycle, and let u and v be any two vertices on c.\\nSuppose that μ* = 0 and that the weight of the simple path from u to v\\nalong the cycle is x. Prove that δ (s, v) = δ (s, u) + x. (Hint: The weight\\nof the simple path from v to u along the cycle is −x.)\\nd. Show that if μ* = 0, then on each minimum mean-weight cycle there\\nexists a vertex v such that\\n(Hint: Show how to extend a shortest path to any vertex on a\\nminimum mean-weight cycle along the cycle to make a shortest path\\nto the next vertex on the cycle.)\\ne. Show that if μ* = 0, then the minimum value of\\ntaken over all vertices v ∈ V, equals 0.\\nf. Show that if you add a constant t to the weight of each edge of G,\\nthen μ* increases by t. Use this fact to show that μ* equals the\\nminimum value of\\ntaken over all vertices v ∈ V.\\ng. Give an O(VE)-time algorithm to compute μ*.\\n22-6\\xa0\\xa0\\xa0\\xa0\\xa0B itonic shortest paths\\nA sequence is bitonic if it monotonically increases and then\\nmonotonically decreases, or if by a circular shift it monotonically\\nincreases and then monotonically decreases. For example the sequences\\n〈1, 4, 6, 8, 3, −2〉, 〈9, 2, −4, −10, −5〉, and 〈1, 2, 3, 4〉 are bitonic, but 〈1,', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 833}),\n",
              " Document(page_content='3, 12, 4, 2, 10〉 is not bitonic. (See Problem 14-3 on page 407 for the\\nbitonic euclidean traveling-salesperson problem.)\\nSuppose that you are given a directed graph G = (V, E) with weight\\nfunction w : E → ℝ, where all edge weights are unique, and you wish to\\nﬁnd single-source shortest paths from a source vertex s. You are given\\none additional piece of information: for each vertex v ∈ V, the weights\\nof the edges along any shortest path from s to v form a bitonic sequence.\\nGive the most efﬁcient algorithm you can to solve this problem, and\\nanalyze its running time.\\nChapter notes\\nThe shortest-path problem has a long history that is nicely desribed in\\nan article by Schrijver [400]. He credits the general idea of repeatedly\\nexecuting edge relaxations to Ford [148]. Dijkstra’s algorithm [116]\\nappeared in 1959, but it contained no mention of a priority queue. The\\nBellman-Ford algorithm is based on separate algorithms by Bellman\\n[45] and Ford [149]. The same algorithm is also attributed to Moore\\n[334]. Bellman describes the relation of shortest paths to difference\\nconstraints. Lawler [276] describes the linear-time algorithm for shortest\\npaths in a dag, which he considers part of the folklore.\\nWhen edge weights are relatively small nonnegative integers, more\\nefﬁcient algorithms result from using min-priority queues that require\\ninteger keys and rely on the sequence of values returned by the\\nEXTRACT-MIN calls in Dijkstra’s algorithm monotonically increasing\\nover time. Ahuja, Mehlhorn, Orlin, and Tarjan [8] give an algorithm\\nthat runs in \\n  time on graphs with nonnegative edge weights,\\nwhere W is the largest weight of any edge in the graph. The best bounds\\nare by Thorup [436], who gives an algorithm that runs in O(E lg lg V)\\ntime, and by Raman [375], who gives an algorithm that runs in O(E + V\\nmin {(lg V)1/3+ ε, (lg W)1/4+ ε}) time. These two algorithms use an\\namount of space that depends on the word size of the underlying\\nmachine. Although the amount of space used can be unbounded in the', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 834}),\n",
              " Document(page_content='size of the input, it can be reduced to be linear in the size of the input\\nusing randomized hashing.\\nFor undirected graphs with integer weights, Thorup [435] gives an\\nalgorithm that runs in O(V + E) time for single-source shortest paths. In\\ncontrast to the algorithms mentioned in the previous paragraph, the\\nsequence of values returned by EXTRACT-MIN calls does not\\nmonotonically increase over time, and so this algorithm is not an\\nimplementation of Dijkstra’s algorithm. Pettie and Ramachandran [357]\\nremove the restriction of integer weights on undirected graphs. Their\\nalgorithm entails a preprocessing phase, followed by queries for speciﬁc\\nsource vertices. Preprocessing takes O(MST(V, E) + min {V lg V, V lg\\nlg r}) time, where MST(V, E) is the time to compute a minimum\\nspanning tree and r is the ratio of the maximum edge weight to the\\nminimum edge weight. After preprocessing, each query takes \\n time, where \\n  is the inverse of Ackermann’s function.\\n(See the chapter notes for Chapter 19 for a brief discussion of\\nAckermann’s function and its inverse.)\\nFor graphs with negative edge weights, an algorithm due to Gabow\\nand Tarjan [167] runs in \\n  time, and one by Goldberg [186]\\nruns in \\n  time, where W = max {|w(u, v)| : (u, v) ∈ E}. There\\nhas also been some progress based on methods that use continuous\\noptimization and electrical ﬂows. Cohen et al. [98] give such an\\nalgorithm, which is randomized and runs in Õ(E10/7 lg W) expected\\ntime (see Problem 3-6 on page 73 for the deﬁntion of Õ-notation). There\\nis also a pseudopolyomial-time algorithm based on fast matrix\\nmultiplication. Sankowski [394] and Yuster and Zwick [465] designed an\\nalgorithm for shortest paths that runs in Õ(W Vω) time, where two n ×\\nn matrices can be multiplied in O(nω) time, giving a faster algorithm\\nthan the previously mentioned algorithms for small values of W on\\ndense graphs.\\nCherkassky, Goldberg, and Radzik [89] conducted extensive\\nexperiments comparing various shortest-path algorithms. Shortest-path\\nalgorithms are widely used in real-time navigation and route-planning\\napplications. Typically based on Dijkstra’s algorithm, these algorithms\\nuse many clever ideas to be able to compute shortest paths on networks', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 835}),\n",
              " Document(page_content='with many millions of vertices and edges in fractions of a second. Bast\\net al. [36] survey many of these developments.\\n1 It may seem strange that the term “relaxation” is used for an operation that tightens an upper\\nbound. The use of the term is historical. The outcome of a relaxation step can be viewed as a\\nrelaxation of the constraint v.d ≤ u.d + w(u, v), which, by the triangle inequality (Lemma 22.10\\non page 633), must be satisﬁed if u.d = δ(s, u) and v.d = δ(s, v). That is, if v.d ≤ u.d + w(u, v), there\\nis no “pressure” to satisfy this constraint, so the constraint is “relaxed.”\\n2 “PERT” is an acronym for “program evaluation and review technique.”', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 836}),\n",
              " Document(page_content='23\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0All-Pairs Shortest Paths\\nIn this chapter, we turn to the problem of ﬁnding shortest paths between\\nall pairs of vertices in a graph. A classic application of this problem\\noccurs in computing a table of distances between all pairs of cities for a\\nroad atlas. Classic perhaps, but not a true application of ﬁnding shortest\\npaths between all pairs of vertices. After all, a road map modeled as a\\ngraph has one vertex for every road intersection and one edge wherever\\na road connects intersections. A table of intercity distances in an atlas\\nmight include distances for 100 cities, but the United States has\\napproximately 300,000 signal-controlled intersections1 and many more\\nuncontrolled intersections.\\nA legitimate application of all-pairs shortest paths is to determine\\nthe diameter of a network: the longest of all shortest paths. If a directed\\ngraph models a communication network, with the weight of an edge\\nindicating the time required for a message to traverse a communication\\nlink, then the diameter gives the longest possible transit time for a\\nmessage in the network.\\nAs in Chapter 22, the input is a weighted, directed graph G = (V, E)\\nwith a weight function w : E → ℝ that maps edges to real-valued\\nweights. Now the goal is to ﬁnd, for every pair of vertices u, v ∈ V, a\\nshortest (least-weight) path from u to v, where the weight of a path is\\nthe sum of the weights of its constituent edges. For the all-pairs\\nproblem, the output typically takes a tabular form in which the entry in\\nu’s row and v’s column is the weight of a shortest path from u to v.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 837}),\n",
              " Document(page_content='You can solve an all-pairs shortest-paths problem by running a\\nsingle-source shortest-paths algorithm |V| times, once with each vertex\\nas the source. If all edge weights are nonnegative, you can use Dijkstra’s\\nalgorithm. If you implement the min-priority queue with a linear array,\\nthe running time is O(V3 + VE) which is O(V3). The binary min-heap\\nimplementation of the min-priority queue yields a running time of\\nO(V(V + E) lg V). If |E| = Ω(V), the running time becomes O(VE lg V),\\nwhich is faster than O(V3) if the graph is sparse. Alternatively, you can\\nimplement the min-priority queue with a Fibonacci heap, yielding a\\nrunning time of O(V2 lg V + VE).\\nIf the graph contains negative-weight edges, Dijkstra’s algorithm\\ndoesn’t work, but you can run the slower Bellman-Ford algorithm once\\nfrom each vertex. The resulting running time is O(V2E), which on a\\ndense graph is O(V4). This chapter shows how to guarantee a much\\nbetter asymptotic running time. It also investigates the relation of the\\nall-pairs shortest-paths problem to matrix multiplication.\\nUnlike the single-source algorithms, which assume an adjacency-list\\nrepresentation of the graph, most of the algorithms in this chapter\\nrepresent the graph by an adjacency matrix. (Johnson’s algorithm for\\nsparse graphs, in Section 23.3, uses adjacency lists.) For convenience, we\\nassume that the vertices are numbered 1, 2, … , |V|, so that the input is\\nan n × n matrix W = (wij) representing the edge weights of an n-vertex\\ndirected graph G = (V, E), where\\nThe graph may contain negative-weight edges, but we assume for the\\ntime being that the input graph contains no negative-weight cycles.\\nThe tabular output of each of the all-pairs shortest-paths algorithms\\npresented in this chapter is an n × n matrix. The (i, j) entry of the output\\nmatrix contains δ (i, j), the shortest-path weight from vertex i to vertex j,\\nas in Chapter 22.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 838}),\n",
              " Document(page_content='A full solution to the all-pairs shortest-paths problem includes not\\nonly the shortest-path weights but also a predecessor matrix Π = (πij),\\nwhere πij is NIL if either i = j or there is no path from i to j, and\\notherwise πij is the predecessor of j on some shortest path from i. Just as\\nthe predecessor subgraph Gπ from Chapter 22 is a shortest-paths tree\\nfor a given source vertex, the subgraph induced by the ith row of the Π\\nmatrix should be a shortest-paths tree with root i. For each vertex i ∈\\nV, the predecessor subgraph of G for i is Gπ,i = (Vπ,i, Eπ,i), where\\nVπ,i={j ∈ V : πij ≠ NIL} ∪ {i},\\nEπ,i={(πij, j) : j ∈ Vπ,i − {i}}.\\nIf Gπ,i is a shortest-paths tree, then PRINT-ALL-PAIRS-SHORTEST-\\nPATH on the following page, which is a modiﬁed version of the\\nPRINT-PATH procedure from Chapter 20, prints a shortest path from\\nvertex i to vertex j.\\nIn order to highlight the essential features of the all-pairs algorithms\\nin this chapter, we won’t cover how to compute predecessor matrices\\nand their properties as extensively as we dealt with predecessor\\nsubgraphs in Chapter 22. Some of the exercises cover the basics.\\nPRINT-ALL-PAIRS-SHORTEST-PATH(Π, i, j)\\n1if i == j\\n2 print i\\n3elseif πij == NIL\\n4 print “no path from” i “to” j “exists”\\n5else PRINT-ALL-PAIRS-SHORTEST-PATH(Π, i, πij)\\n6 print j\\nChapter outline\\nSection 23.1 presents a dynamic-programming algorithm based on\\nmatrix multiplication to solve the all-pairs shortest-paths problem. The\\ntechnique of “repeated squaring” yields a running time of Θ (V3 lg V).', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 839}),\n",
              " Document(page_content='Section 23.2 gives another dynamic-programming algorithm, the Floyd-\\nWarshall algorithm, which runs in Θ (V3) time. Section 23.2 also covers\\nthe problem of ﬁnding the transitive closure of a directed graph, which\\nis related to the all-pairs shortest-paths problem. Finally, Section 23.3\\npresents Johnson’s algorithm, which solves the all-pairs shortest-paths\\nproblem in O(V2 lg V + VE) time and is a good choice for large, sparse\\ngraphs.\\nBefore proceeding, we need to establish some conventions for\\nadjacency-matrix representations. First, we generally assume that the\\ninput graph G = (V, E) has n vertices, so that n = |V|. Second, we use the\\nconvention of denoting matrices by uppercase letters, such as W, L, or\\nD, and their individual elements by subscripted lowercase letters, such\\nas wij, lij, or dij. Finally, some matrices have parenthesized superscripts,\\nas in \\n  or \\n , to indicate iterates.\\n23.1\\xa0\\xa0\\xa0\\xa0Shortest paths and matrix multiplication\\nThis section presents a dynamic-programming algorithm for the all-\\npairs shortest-paths problem on a directed graph G = (V, E). Each\\nmajor loop of the dynamic program invokes an operation similar to\\nmatrix multiplication, so that the algorithm looks like repeated matrix\\nmultiplication. We’ll start by developing a Θ (V4)-time algorithm for the\\nall-pairs shortest-paths problem, and then we’ll improve its running\\ntime to Θ (V3 lg V).\\nBefore proceeding, let’s brieﬂy recap the steps given in Chapter 14 for\\ndeveloping a dynamic-programming algorithm:\\n1. Characterize the structure of an optimal solution.\\n2. Recursively deﬁne the value of an optimal solution.\\n3. Compute the value of an optimal solution in a bottom-up\\nfashion.\\nWe reserve the fourth step—constructing an optimal solution from\\ncomputed information—for the exercises.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 840}),\n",
              " Document(page_content='The structure of a shortest path\\nLet’s start by characterizing the structure of an optimal solution.\\nLemma 22.1 tells us that all subpaths of a shortest path are shortest\\npaths. Consider a shortest path p from vertex i to vertex j, and suppose\\nthat p contains at most r edges. Assuming that there are no negative-\\nweight cycles, r is ﬁnite. If i = j, then p has weight 0 and no edges. If\\nvertices i and j are distinct, then decompose path p into \\n , where\\npath p′ now contains at most r − 1 edges. Lemma 22.1 says that p′ is a\\nshortest path from i to k, and so δ (i, j) = δ (i, k) + wkj.\\nA recursive solution to the all-pairs shortest-paths problem\\nNow, let \\n be the minimum weight of any path from vertex i to vertex j\\nthat contains at most r edges. When r = 0, there is a shortest path from i\\nto j with no edges if and only if i = j, yielding\\nFor r ≥ 1, one way to achieve a minimum-weight path from i to j with at\\nmost r edges is by taking a path containing at most r − 1 edges, so that \\n. Another way is by taking a path of at most r − 1 edges from i\\nto some vertex k and then taking the edge (k, j), so that \\n .\\nTherefore, to examine paths from i to j consisting of at most r edges, try\\nall possible predecessors k of j, giving the recursive deﬁnition\\nThe last equality follows from the observation that wjj = 0 for all j.\\nWhat are the actual shortest-path weights δ (i, j)? If the graph\\ncontains no negative-weight cycles, then whenever δ (i, j) < ∞, there is a\\nshortest path from vertex i to vertex j that is simple. (A path p from i to j\\nthat is not simple contains a cycle. Since each cycle’s weight is\\nnonnegative, removing all cycles from the path leaves a simple path with\\nweight no greater than p’s weight.) Because any simple path contains at', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 841}),\n",
              " Document(page_content='most n − 1 edges, a path from vertex i to vertex j with more than n − 1\\nedges cannot have lower weight than a shortest path from i to j. The\\nactual shortest-path weights are therefore given by\\nComputing the shortest-path weights bottom up\\nTaking as input the matrix W = (wij), let’s see how to compute a series\\nof matrices L(0), L(1), … , L(n−1), where \\n  for r = 0, 1, … , n −\\n1. The initial matrix is L(0) given by equation (23.2). The ﬁnal matrix\\nL(n−1) contains the actual shortest-path weights.\\nThe heart of the algorithm is the procedure EXTEND-SHORTEST-\\nPATHS, which implements equation (23.3) for all i and j. The four\\ninputs are the matrix L(r−1) computed so far; the edge-weight matrix\\nW; the output matrix L(r), which will hold the computed result and\\nwhose elements are all initialized to ∞ before invoking the procedure;\\nand the number n of vertices. The superscripts r and r − 1 help to make\\nthe correspondence of the pseudocode with equation (23.3) plain, but\\nthey play no actual role in the pseudocode. The procedure extends the\\nshortest paths computed so far by one more edge, producing the matrix\\nL(r) of shortest-path weights from the matrix L(r−1) computed so far.\\nIts running time is Θ (n3) due to the three nested for loops.\\nEXTEND-SHORTEST-PATHS(L(r−1), W, L(r), n)\\n1// Assume that the elements of L(r) are initialized to ∞.\\n2for i = 1 to n\\n3 for j = 1 to n\\n4 for k = 1 to n\\n5\\nLet’s now understand the relation of this computation to matrix\\nmultiplication. Consider how to compute the matrix product C = A · B', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 842}),\n",
              " Document(page_content='of two n × n matrices A and B. The straightforward method used by\\nMATRIX-MULTIPLY on page 81 uses a triply nested loop to\\nimplement equation (4.1), which we repeat here for convenience:\\nfor i, j = 1, 2, … , n. Now make the substitutions\\nl(r−1)\\xa0→\\xa0a,\\nw\\xa0→\\xa0b,\\nl(r)\\xa0→\\xa0c,\\nmin\\xa0→\\xa0+,\\n+\\xa0→\\xa0.\\nin equation (23.3). You get equation (23.5)! Making these changes to\\nEXTEND-SHORTEST-PATHS, and also replacing ∞ (the identity for\\nmin) by 0 (the identity for +), yields the procedure MATRIX-\\nMULTIPLY. We can see that the procedure EXTEND-SHORTEST-\\nPATHS(L(r−1), W, L(r), n) computes the matrix “product” L(r) =\\nL(r−1). W using this unusual deﬁnition of matrix multiplication.2\\nThus, we can solve the all-pairs shortest-paths problem by repeatedly\\nmultiplying matrices. Each step extends the shortest-path weights\\ncomputed so far by one more edge using EXTEND-SHORTEST-\\nPATHS(L(r−1), W, L(r), n) to perform the matrix multiplication.\\nStarting with the matrix L(0), we produce the following sequence of n −\\n1 matrices corresponding to powers of W:\\nL(1)=L(0) · W=W1,\\nL(2)=L(1) · W=W2,\\nL(3)=L(2) · W=W3,\\n⋮\\nL(n−1)=L(n−2) · W=Wn−1.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 843}),\n",
              " Document(page_content='At the end, the matrix L(n−1) = Wn−1 contains the shortest-path\\nweights.\\nThe procedure SLOW-APSP on the next page computes this\\nsequence in Θ (n4) time. The procedure takes the n × n matrices W and\\nL(0) as inputs, along with n. Figure 23.1 illustrates its operation. The\\npseudocode uses two n × n matrices L and M to store powers of W,\\ncomputing M = L · W on each iteration. Line 2 initializes L = L(0). For\\neach iteration r, line 4 initializes M = ∞, where ∞ in this context is a\\nmatrix of scalar ∞ values. The rth iteration starts with the invariant L =\\nL(r−1) = Wr−1. Line 6 computes M = L · W = L(r−1) · W = Wr−1 · W\\n= Wr = L(r) so that the invariant can be restored for the next iteration\\nby line 7, which sets L = M. At the end, the matrix L = L(n−1) = Wn−1\\nof shortest-path weights is returned. The assignments to n × n matrices\\nin lines 2, 4, and 7 implicitly run doubly nested loops that take Θ (n2)\\ntime for each assignment. The n − 1 invocations of EXTEND-\\nSHORTEST-PATHS, each of which takes Θ (n3) time, dominate the\\ncomputation, yielding a total running time of Θ (n4).\\nFigure 23.1 A directed graph and the sequence of matrices L(r) computed by SLOW-APSP. You\\nmight want to verify that L(5), deﬁned as L(4) · W, equals L(4), and thus L(r) = L(4) for all r ≥\\n4.\\nSLOW-APSP(W, L(0), n)\\n1let L = (lij) and M = (mij) be new n × n matrices', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 844}),\n",
              " Document(page_content='2L = L(0)\\n3for r = 1 to n − 1\\n4 M = ∞\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0// initialize M\\n5 // Compute the matrix “product” M = L · W.\\n6 EXTEND-SHORTEST-PATHS(L, W, M, n)\\n7 L = M\\n8return L\\nImproving the running time\\nBear in mind that the goal is not to compute all the L(r) matrices: only\\nthe matrix L(n−1) matters. Recall that in the absence of negative-weight\\ncycles, equation (23.4) implies L(r) = L(n−1) for all integers r ≥ n − 1.\\nJust as traditional matrix multiplication is associative, so is matrix\\nmultiplication deﬁned by the EXTEND-SHORTEST-PATHS\\nprocedure (see Exercise 23.1-4). In fact, we can compute L(n−1) with\\nonly ⌈lg(n – 1) ⌉ matrix products by using the technique of repeated\\nsquaring:\\nSince 2⌈lg(n – 1) ⌉ ≥ n – 1, the ﬁnal product is \\n .\\nThe procedure FASTER-APSP implements this idea. It takes just\\nthe n × n matrix W and the size n as inputs. Each iteration of the while\\nloop of lines 4–8 starts with the invariant L = Wr, which it squares\\nusing EXTEND-SHORTEST-PATHS to obtain the matrix M = L2 =\\n(Wr)2 = W2r. At the end of each iteration, the value of r doubles, and L\\nfor the next iteration becomes M, restoring the invariant. Upon exiting\\nthe loop when r ≥ n − 1, the procedure returns L = Wr = L(r) = L(n−1)\\nby equation (23.4). As in SLOW-APSP, the assignments to n × n', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 845}),\n",
              " Document(page_content='matrices in lines 2, 5, and 8 implicitly run doubly nested loops, taking\\nΘ(n2) time for each assignment.\\nFASTER-APSP(W, n)\\n1let L and M be new n × n matrices\\n2L = W\\n3r = 1\\n4while r < n − 1\\n5 M = ∞ // initialize M\\n6 EXTEND-SHORTEST-\\nPATHS(L, L, M, n)// compute M = L2\\n7 r = 2r\\n8 L = M // ready for the next\\niteration\\n9return L\\nBecause each of the ⌈lg(n – 1) ⌉ matrix products takes Θ (n3) time,\\nFASTER-APSP runs in Θ (n3 lg n) time. The code is tight, containing\\nno elaborate data structures, and the constant hidden in the Θ -notation\\nis therefore small.\\nExercises\\n23.1-1\\nRun SLOW-APSP on the weighted, directed graph of Figure 23.2,\\nshowing the matrices that result for each iteration of the loop. Then do\\nthe same for FASTER-APSP.\\nFigure 23.2 A weighted, directed graph for use in Exercises 23.1-1, 23.2-1, and 23.3-1.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 846}),\n",
              " Document(page_content='23.1-2\\nWhy is it convenient for both SLOW-APSP and FASTER-APSP that\\nwii = 0 for i = 1, 2, … , n?\\n23.1-3\\nWhat does the matrix\\nused in the shortest-paths algorithms correspond to in regular matrix\\nmultiplication?\\n23.1-4\\nShow that matrix multiplication deﬁned by EXTEND-SHORTEST-\\nPATHS is associative.\\n23.1-5\\nShow how to express the single-source shortest-paths problem as a\\nproduct of matrices and a vector. Describe how evaluating this product\\ncorresponds to a Bellman-Ford-like algorithm (see Section 22.1).\\n23.1-6\\nArgue that we don’t need the matrix M in SLOW-APSP because by\\nsubstituting L for M and leaving out the initialization of M, the code\\nstill works correctly. (Hint: Relate line 5 of EXTEND-SHORTEST-\\nPATHS to RELAX on page 610.) Do we need the matrix M in\\nFASTER-APSP?\\n23.1-7\\nSuppose that you also want to compute the vertices on shortest paths in\\nthe algorithms of this section. Show how to compute the predecessor\\nmatrix Π  from the completed matrix L of shortest-path weights in O(n3)\\ntime.\\n23.1-8', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 847}),\n",
              " Document(page_content='You can also compute the vertices on shortest paths along with\\ncomputing the shortest-path weights. Deﬁne \\n as the predecessor of\\nvertex j on any minimum-weight path from vertex i to vertex j that\\ncontains at most r edges. Modify the EXTEND-SHORTEST-PATHS\\nand SLOW-APSP procedures to compute the matrices Π(1), Π(2), … ,\\nΠ(n−1) as they compute the matrices L(1), L(2), … , L(n−1).\\n23.1-9\\nModify FASTER-APSP so that it can determine whether the graph\\ncontains a negative-weight cycle.\\n23.1-10\\nGive an efﬁcient algorithm to ﬁnd the length (number of edges) of a\\nminimum-length negative-weight cycle in a gr aph.\\n23.2\\xa0\\xa0\\xa0\\xa0The Floyd-Warshall algorithm\\nHaving already seen one dynamic-programming solution to the all-pairs\\nshortest-paths problem, in this section we’ll see another: the Floyd-\\nWarshall algorithm, which runs in Θ (V3) time. As before, negative-\\nweight edges may be present, but not negative-weight cycles. As in\\nSection 23.1, we develop the algorithm by following the dynamic-\\nprogramming process. After studying the resulting algorithm, we\\npresent a similar method for ﬁnding the transitive closure of a directed\\ngraph.\\nThe structure of a shortest path\\nIn the Floyd-Warshall algorithm, we characterize the structure of a\\nshortest path differently from how we characterized it in Section 23.1.\\nThe Floyd-Warshall algorithm considers the intermediate vertices of a\\nshortest path, where an intermediate vertex of a simple path p = 〈v1, v2,\\n… , vl〉 is any vertex of p other than v1 or vl, that is, any vertex in the set\\n{v2, v3, … , vl−1}.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 848}),\n",
              " Document(page_content='The Floyd-Warshall algorithm relies on the following observation.\\nNumbering the vertices of G by V = {1, 2, … , n}, take a subset {1, 2, …\\n, k} of vertices for some 1 ≤ k ≤ n. For any pair of vertices i, j ∈ V,\\nconsider all paths from i to j whose intermediate vertices are all drawn\\nfrom {1, 2, … , k}, and let p be a minimum-weight path from among\\nthem. (Path p is simple.) The Floyd-Warshall algorithm exploits a\\nrelationship between path p and shortest paths from i to j with all\\nintermediate vertices in the set {1, 2, … , k − 1}. The details of the\\nrelationship depend on whether k is an intermediate vertex of path p or\\nnot.\\nFigure 23.3 Optimal substructure used by the Floyd-Warshall algorithm. Path p is a shortest\\npath from vertex i to vertex j, and k is the highest-numbered intermediate vertex of p. Path p1,\\nthe portion of path p from vertex i to vertex k, has all intermediate vertices in the set {1, 2, … , k\\n− 1}. The same holds for path p2 from vertex k to vertex j.\\nIf k is not an intermediate vertex of path p, then all intermediate\\nvertices of path p belong to the set {1, 2, … , k − 1}. Thus a\\nshortest path from vertex i to vertex j with all intermediate\\nvertices in the set {1, 2, … , k − 1} is also a shortest path from i to\\nj with all intermediate vertices in the set {1, 2, … , k}.\\nIf k is an intermediate vertex of path p, then decompose p into \\n, as Figure 23.3 illustrates. By Lemma 22.1, p1 is a\\nshortest path from i to k with all intermediate vertices in the set\\n{1, 2, … , k}. In fact, we can make a slightly stronger statement.\\nBecause vertex k is not an intermediate vertex of path p1, all\\nintermediate vertices of p1 belong to the set {1, 2, … , k − 1}.\\nTherefore p1 is a shortest path from i to k with all intermediate\\nvertices in the set {1, 2, … , k − 1}. Likewise, p2 is a shortest path', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 849}),\n",
              " Document(page_content='from vertex k to vertex j with all intermediate vertices in the set\\n{1, 2, … , k − 1}.\\nA recursive solution to the all-pairs shortest-paths problem\\nThe above observations suggest a recursive formulation of shortest-path\\nestimates that differs from the one in Section 23.1. Let \\n be the weight\\nof a shortest path from vertex i to vertex j for which all intermediate\\nvertices belong to the set {1, 2, … , k}. When k = 0, a path from vertex i\\nto vertex j with no intermediate vertex numbered higher than 0 has no\\nintermediate vertices at all. Such a path has at most one edge, and hence\\n. Following the above discussion, deﬁne \\n recursively by\\nBecause for any path, all intermediate vertices belong to the set {1, 2, …\\n, n}, the matrix \\n  gives the ﬁnal answer: \\n  for all i, j ∈ V.\\nComputing the shortest-path weights bottom up\\nBased on recurrence (23.6), the bottom-up procedure FLOYD-\\nWARSHALL computes the values \\n in order of increasing values of k.\\nIts input is an n × n matrix W deﬁned as in equation (23.1). The\\nprocedure returns the matrix D(n) of shortest-path weights. Figure 23.4\\nshows the matrices D(k) computed by the Floyd-Warshall algorithm for\\nthe graph in Figure 23.1.\\nFLOYD-WARSHALL(W, n)\\n1D(0) = W\\n2for k = 1 to n\\n3 let \\n  be a new n × n matrix\\n4 for i = 1 to n\\n5 for j = 1 to n\\n6\\n7return D(n)', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 850}),\n",
              " Document(page_content='The running time of the Floyd-Warshall algorithm is determined by\\nthe triply nested for loops of lines 2–6. Because each execution of line 6\\ntakes O(1) time, the algorithm runs in Θ (n3) time. As in the ﬁnal\\nalgorithm in Section 23.1, the code is tight, with no elaborate data\\nstructures, and so the constant hidden in the Θ -notation is small. Thus,\\nthe Floyd-Warshall algorithm is quite practical for even moderate-sized\\ninput graphs.\\nConstructing a shortest path\\nThere are a variety of different methods for constructing shortest paths\\nin the Floyd-Warshall algorithm. One way is to compute the matrix D\\nof shortest-path weights and then construct the predecessor matrix Π\\nfrom the D matrix. Exercise 23.1-7 asks you to implement this method\\nso that it runs in O(n3) time. Given the predecessor matrix Π , the\\nPRINT-ALL-PAIRS-SHORTEST-PATH procedure prints the vertices\\non a given shortest path.\\nAlternatively, the predecessor matrix … can be computed while the\\nalgorithm computes the matrices D(0), D(1), … , D(n). Speciﬁcally,\\ncompute a sequence of matrices Π(0), Π(1), … , Π(n), where Π  = Π(n)\\nand \\n is the predecessor of vertex j on a shortest path from vertex i\\nwith all intermediate vertices in the set {1, 2, … , k}.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 851}),\n",
              " Document(page_content='Figure 23.4 The sequence of matrices D(k) and Π(k) computed by the Floyd-Warshall algorithm\\nfor the graph in Figure 23.1.\\nHere’s a recursive formulation of \\n. When k = 0, a shortest path\\nfrom i to j has no intermediate vertices at all, and so\\nFor k ≥ 1, if the path has k as an intermediate vertex, so that it is i ⇝ k\\n⇝ j where k ≠ j, then choose as the predecessor of j on this path the\\nsame vertex as the predecessor of j chosen on a shortest path from k\\nwith all intermediate vertices in the set {1, 2, … , k − 1}. Otherwise,\\nwhen the path from i to j does not have k as an intermediate vertex,', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 852}),\n",
              " Document(page_content='choose the same predecessor of j as on a shortest path from i with all\\nintermediate vertices in the set {1, 2, … , k − 1}. Formally, for k ≥ 1,\\nExercise 23.2-3 asks you to show how to incorporate the Π(k) matrix\\ncomputations into the FLOYD-WARSHALL procedure. Figure 23.4\\nshows the sequence of Π(k) matrices that the resulting algorithm\\ncomputes for the graph of Figure 23.1. The exercise also asks for the\\nmore difﬁcult task of proving that the predecessor subgraph Gπ,i is a\\nshortest-paths tree with root i. Exercise 23.2-7 asks for yet another way\\nto reconstruct shortest paths.\\nTransitive closure of a directed graph\\nGiven a directed graph G = (V, E) with vertex set V = {1, 2, … , n}, you\\nmight wish to determine simply whether G contains a path from i to j\\nfor all vertex pairs i, j ∈ V, without regard to edge weights. We deﬁne\\nthe transitive closure of G as the graph G* = (V, E*), where\\nE* = {(i, j) : there is a path from vertex i to vertex j in G}.\\nOne way to compute the transitive closure of a graph in Θ (n3) time is\\nto assign a weight of 1 to each edge of E and run the Floyd-Warshall\\nalgorithm. If there is a path from vertex i to vertex j, you get dij < n.\\nOtherwise, you get dij = ∞.\\nThere is another, similar way to compute the transitive closure of G\\nin Θ (n3) time, which can save time and space in practice. This method\\nsubstitutes the logical operations ∨  (logical OR) and ∧  (logical AND)\\nfor the arithmetic operations min and + in the Floyd-Warshall\\nalgorithm. For i, j, k = 1, 2, … , n, deﬁne \\n to be 1 if there exists a path\\nin graph G from vertex i to vertex j with all intermediate vertices in the\\nset {1, 2, … , k}, and 0 otherwise. To construct the transitive closure G*', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 853}),\n",
              " Document(page_content='= (V, E*), put edge (i, j) into E* if and only if \\n . A recursive\\ndeﬁnition of \\n, analogous to recurrence (23.6), is\\nFigure 23.5 A directed graph and the matrices T(k) computed by the transitive-closure\\nalgorithm.\\nand for k ≥ 1,\\nAs in the Floyd-Warshall algorithm, the TRANSITIVE-CLOSURE\\nprocedure computes the matrices \\n  in order of increasing k.\\nTRANSITIVE-CLOSURE(G, n)\\n\\xa0\\xa01let \\n  be a new n × n matrix\\n\\xa0\\xa02for i = 1 to n\\n\\xa0\\xa03for j = 1 to n\\n\\xa0\\xa04 if i == j or (i, j) ∈ G.E\\n\\xa0\\xa05\\n\\xa0\\xa06 else \\n\\xa0\\xa07for k = 1 to n\\n\\xa0\\xa08let \\n  be a new n × n matrix\\n\\xa0\\xa09for i = 1 to n\\n10 for j = 1 to n\\n11\\n12return T(n)', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 854}),\n",
              " Document(page_content='Figure 23.5 shows the matrices T(k) computed by the\\nTRANSITIVE-CLOSURE procedure on a sample graph. The\\nTRANSITIVE-CLOSURE procedure, like the Floyd-Warshall\\nalgorithm, runs in Θ (n3) time. On some computers, though, logical\\noperations on single-bit values execute faster than arithmetic operations\\non integer words of data. Moreover, because the direct transitive-closure\\nalgorithm uses only boolean values rather than integer values, its space\\nrequirement is less than the Floyd-Warshall algorithm’s by a factor\\ncorresponding to the size of a word of computer storage.\\nExercises\\n23.2-1\\nRun the Floyd-Warshall algorithm on the weighted, directed graph of\\nFigure 23.2. Show the matrix D(k) that results for each iteration of the\\nouter loop.\\n23.2-2\\nShow how to compute the transitive closure using the technique of\\nSection 23.1.\\n23.2-3\\nModify the FLOYD-WARSHALL procedure to compute the Π(k)\\nmatrices according to equations (23.7) and (23.8). Prove rigorously that\\nfor all i ∈ V, the predecessor subgraph Gπ,i is a shortest-paths tree with\\nroot i. (Hint: To show that Gπ,i is acyclic, ﬁrst show that \\n  implies \\n, according to the deﬁnition of \\n. Then adapt the proof of\\nLemma 22.16.)\\n23.2-4\\nAs it appears on page 657, the Floyd-Warshall algorithm requires Θ (n3)\\nspace, since it creates \\n for i, j, k = 1, 2, … , n. Show that the procedure', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 855}),\n",
              " Document(page_content='FLOYD-WARSHALL′, which simply drops all the superscripts, is\\ncorrect, and thus only Θ (n2) space is required.\\nFLOYD-WARSHALL′(W, n)\\n1D = W\\n2for k = 1 to n\\n3 for i = 1 to n\\n4 for j = 1 to n\\n5 dij = min {dij, dik + dkj}\\n6return D\\n23.2-5\\nConsider the following change to how equation (23.8) handles equality:\\nIs this alternative deﬁnition of the predecessor matrix Π  correct?\\n23.2-6\\nShow how to use the output of the Floyd-Warshall algorithm to detect\\nthe presence of a negative-weight cycle.\\n23.2-7\\nAnother way to reconstruct shortest paths in the Floyd-Warshall\\nalgorithm uses values \\n for i,j,k = 1, 2, … , n, where \\n is the highest-\\nnumbered intermediate vertex of a shortest path from i to j in which all\\nintermediate vertices lie in the set {1, 2, … , k}. Give a recursive\\nformulation for \\n, modify the FLOYD-WARSHALL procedure to\\ncompute the \\n  values, and rewrite the PRINT-ALL-PAIRS-\\nSHORTEST-PATH procedure to take the matrix \\n  as an input.\\nHow is the matrix Φ  like the s table in the matrix-chain multiplication\\nproblem of Section 14.2?\\n23.2-8', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 856}),\n",
              " Document(page_content='Give an O(VE)-time algorithm for computing the transitive closure of a\\ndirected graph G = (V, E). Assume that |V| = O(E) and that the graph is\\nrepresented with adjacency lists.\\n23.2-9\\nSuppose that it takes f(|V|, |E|) time to compute the transitive closure of\\na directed acyclic graph, where f is a monotonically increasing function\\nof both |V| and |E|. Show that the time to compute the transitive closure\\nG* = (V, E*) of a general directed graph G = (V, E) is then f(|V|, |E|) +\\nO(V + E*).\\n23.3\\xa0\\xa0\\xa0\\xa0Johnson’s algorithm for sparse graphs\\nJohnson’s algorithm ﬁnds shortest paths between all pairs in O(V2 lg V\\n+ VE) time. For sparse graphs, it is asymptotically faster than either\\nrepeated squaring of matrices or the Floyd-Warshall algorithm. The\\nalgorithm either returns a matrix of shortest-path weights for all pairs\\nof vertices or reports that the input graph contains a negative-weight\\ncycle. Johnson’s algorithm uses as subroutines both Dijkstra’s algorithm\\nand the Bellman-Ford algorithm, which Chapter 22 describes.\\nJohnson’s algorithm uses the technique of reweighting, which works\\nas follows. If all edge weights w in a graph G = (V, E) are nonnegative,\\nDijkstra’s algorithm can ﬁnd shortest paths between all pairs of vertices\\nby running it once from each vertex. With the Fibonacci-heap min-\\npriority queue, the running time of this all-pairs algorithm is O(V2 lg V\\n+ VE). If G has negative-weight edges but no negative-weight cycles,\\nﬁrst compute a new set of nonnegative edge weights so that Dijkstra’s\\nalgorithm applies. The new set of edge weights ŵ must satisfy two\\nimportant properties:\\n1. For all pairs of vertices u, v ∈ V, a path p is a shortest path from\\nu to v using weight function w if and only if p is also a shortest\\npath from u to v using weight function ŵ.\\n2. For all edges (u, v), the new weight ŵ(u, v) is nonnegative.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 857}),\n",
              " Document(page_content='As we’ll see in a moment, preprocessing G to determine the new weight\\nfunction ŵ takes O(VE) time.\\nPreserving shortest paths by reweighting\\nThe following lemma shows how to reweight the edges to satisfy the ﬁrst\\nproperty above. We use δ  to denote shortest-path weights derived from\\nweight function w and \\n to denote shortest-path weights derived from\\nweight function ŵ.\\nLemma 23.1 (Reweighting does not chang e shortest paths)\\nGiven a weighted, directed graph G = (V, E) with weight function w : E\\n→ ℝ, let h : V → ℝ be any function mapping vertices to real numbers.\\nFor each edge (u, v) ∈ E, deﬁne\\nLet p = 〈v0, v1, … , vk〉 be any path from vertex v0 to vertex vk. Then p\\nis a shortest path from v0 to vk with weight function w if and only if it is\\na shortest path with weight function ŵ. That is, w(p) = δ (v0, vk) if and\\nonly if \\n . Furthermore, G has a negative-weight cycle using\\nweight function w if and only if G has a negative-weight cycle using\\nweight function ŵ.\\nProof\\xa0\\xa0\\xa0We start by showing that\\nWe have\\n', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 858}),\n",
              " Document(page_content='Therefore, any path p from v0 to vk has ŵ(p) = w(p) + h(v0) − h(vk).\\nBecause h(v0) and h(vk) do not depend on the path, if one path from v0\\nto vk is shorter than another using weight function w, then it is also\\nshorter using ŵ. Thus, w(p) = δ (v0, vk) if and only if \\n .\\nFinally, we show that G has a negative-weight cycle using weight\\nfunction w if and only if G has a negative-weight cycle using weight\\nfunction ŵ. Consider any cycle c = 〈v0, v1, … , vk〉, where v0 = vk. By\\nequation (23.11),\\nŵ(c)=w(c) + h(v0) + h(vk)\\n=w(c),\\nand thus c has negative weight using w if and only if it has negative\\nweight using ŵ.\\n▪\\nProducing nonnegative weights by reweighting\\nOur next goal is to ensure that the second property holds: ŵ(u, v) must\\nbe nonnegative for all edges (u, v) = E. Given a weighted, directed graph\\nG = (V, E) with weight function w : E → ℝ, we’ll see how to make a new\\ngraph G′ = (V′, E′), where V′ = V ∪ {s} for some new vertex s ∉ V and\\nE′ = E ∪ {(s, v) : v = V }. To incorporate the new vertex s, extend the\\nweight function w so that w(s, v) = 0 for all v ∈ V. Since no edges enter\\ns, no shortest paths in G′, other than those with source s, contain s.\\nMoreover, G′ has no negative-weight cycles if and only if G has no\\nnegative-weight cycles. Figure 23.6(a) shows the graph G′ corresponding\\nto the graph G of Figure 23.1.\\nNow suppose that G and G′ have no negative-weight cycles. Deﬁne\\nthe function h(v) = δ (s, v) for all v ∈ V′. By the triangle inequality\\n(Lemma 22.10 on page 633), we have h(v) ≤ h(u) + w(u, v) for all edges\\n(u, v) ∈ E′. Thus, by deﬁning reweighted edge weights ŵ according to\\nequation (23.10), we have ŵ(u, v) = w(u, v) + h(u) − h(v) ≥ 0, thereby', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 859}),\n",
              " Document(page_content='satisfying the second property. Figure 23.6(b) shows the graph G′ from\\nFigure 23.6(a) with reweighted edges.\\nComputing all-pairs shortest paths\\nJohnson’s algorithm to compute all-pairs shortest paths uses the\\nBellman-Ford algorithm (Section 22.1) and Dijkstra’s algorithm\\n(Section 22.3) as subroutines. The pseudocode appears in the procedure\\nJOHNSON on page 666. It assumes implicitly that the edges are stored\\nin adjacency lists. The algorithm returns the usual |V| × |V| matrix D =\\n(dij), where dij = δ (i, j), or it reports that the input graph contains a\\nnegative-weight cycle. As is typical for an all-pairs shortest-paths\\nalgorithm, it assumes that the vertices are numbered from 1 to |V|.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 860}),\n",
              " Document(page_content='Figure 23.6 Johnson’s all-pairs shortest-paths algorithm run on the graph of Figure 23.1. Vertex\\nnumbers appear outside the vertices. (a) The graph G′ with the original weight function w. The\\nnew vertex s is blue. Within each vertex v is h(v) = δ (s, v). (b) After reweighting each edge (u, v)\\nwith weight function ŵ(u, v) = w(u, v) + h(u) − h(v). (c)–(g) The result of running Dijkstra’s\\nalgorithm on each vertex of G using weight function ŵ. In each part, the source vertex u is blue,\\nand blue edges belong to the shortest-paths tree computed by the algorithm. Within each vertex\\nv are the values \\n  and δ (u, v), separated by a slash. The value duv = δ (u, v) is equal to \\n.\\nJOHNSON(G, w)\\n\\xa0\\xa01compute G′, where G′.V = G.V ∪ {s},\\nG′.E = G.E ∪ {(s, v) : v ∈ G.V}, and', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 861}),\n",
              " Document(page_content='w(s, v) = 0 for all v ∈ G.V\\n\\xa0\\xa02if BELLMAN-FORD(G′, w, s) == FALSE\\n\\xa0\\xa03print “the input graph contains a negative-weight cycle”\\n\\xa0\\xa04else for each vertex v ∈ G′.V\\n\\xa0\\xa05 set h(v) to the value of δ (s, v)\\ncomputed by the Bellman-Ford algorithm\\n\\xa0\\xa06for each edge (u, v) ∈ G′.E\\n\\xa0\\xa07 ŵ(u, v) = w(u, v) + h(u) − h(v)\\n\\xa0\\xa08let D = (duv) be a new n × n matrix\\n\\xa0\\xa09for each vertex u ∈ G.V\\n10 run DIJKSTRA(G, ŵ, u) to compute \\n  for all v ∈ G.V\\n11 for each vertex v ∈ G.V\\n12\\n13return D\\nThe JOHNSON procedure simply performs the actions speciﬁed\\nearlier. Line 1 produces G′. Line 2 runs the Bellman-Ford algorithm on\\nG′ with weight function w and source vertex s. If G′, and hence G,\\ncontains a negative-weight cycle, line 3 reports the problem. Lines 4–12\\nassume that G′ contains no negative-weight cycles. Lines 4–5 set h(v) to\\nthe shortest-path weight δ (s, v) computed by the Bellman-Ford\\nalgorithm for all v ∈ V′. Lines 6–7 compute the new weights ŵ. For\\neach pair of vertices u, v ∈ V, the for loop of lines 9–12 computes the\\nshortest-path weight \\n  by calling Dijkstra’s algorithm once from\\neach vertex in V. Line 12 stores in matrix entry duv the correct shortest-\\npath weight δ (u, v), calculated using equation (23.11). Finally, line 13\\nreturns the completed D matrix. Figure 23.6 depicts the execution of\\nJohnson’s algorithm.\\nIf the min-priority queue in Dijkstra’s algorithm is implemented by a\\nFibonacci heap, Johnson’s algorithm runs in O(V2 lg V + VE) time. The\\nsimpler binary min-heap implementation yields a running time of O(VE\\nlg V), which is still asymptotically faster than the Floyd-Warshall\\nalgorithm if the graph is sparse.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 862}),\n",
              " Document(page_content='Exercises\\n23.3-1\\nUse Johnson’s algorithm to ﬁnd the shortest paths between all pairs of\\nvertices in the graph of Figure 23.2. Show the values of h and ŵ\\ncomputed by the algorithm.\\n23.3-2\\nWhat is the purpose of adding the new vertex s to V, yielding V′?\\n23.3-3\\nSuppose that w(u, v) ≥ 0 for all edges (u, v) ∈ E. What is the relationship\\nbetween the weight functions w and ŵ?\\n23.3-4\\nProfessor Greenstreet claims that there is a simpler way to reweight\\nedges than the method used in Johnson’s algorithm. Letting w* = min\\n{w(u, v) : (u, v) ∈ E}, just deﬁne ŵ(u, v) = w(u, v) − w* for all edges (u,\\nv) ∈ E. What is wrong with the professor’s method of reweighting?\\n23.3-5\\nShow that if G contains a 0-weight cycle c, then ŵ(u, v) = 0 for every\\nedge (u, v) in c.\\n23.3-6\\nProfessor Michener claims that there is no need to create a new source\\nvertex in line 1 of JOHNSON. He suggests using G′ = G instead and\\nletting s be any vertex. Give an example of a weighted, directed graph G\\nfor which incorporating the professor’s idea into JOHNSON causes\\nincorrect answers. Assume that ∞ − ∞ is undeﬁned, and in particular, it\\nis not 0. Then show that if G is strongly connected (every vertex is\\nreachable from every other vertex), the results returned by JOHNSON\\nwith the professor’s modiﬁcation are correct.\\nProblems', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 863}),\n",
              " Document(page_content='23-1\\xa0\\xa0\\xa0\\xa0\\xa0Transitive closure of a dynam ic graph\\nYou wish to maintain the transitive closure of a directed graph G = (V,\\nE) as you insert edges into E. That is, after inserting an edge, you update\\nthe transitive closure of the edges inserted so far. Start with G having no\\nedges initially, and represent the transitive closure by a boolean matrix.\\na. Show how to update the transitive closure G* = (V, E*) of a graph G\\n= (V, E) in O(V2) time when a new edge is added to G.\\nb. Give an example of a graph G and an edge e such that Ω(V2) time is\\nrequired to update the transitive closure after inserting e into G, no\\nmatter what algorithm is used.\\nc. Give an algorithm for updating the transitive closure as edges are\\ninserted into the graph. For any sequence of r insertions, your\\nalgorithm should run in time \\n , where ti is the time to\\nupdate the transitive closure upon inserting the ith edge. Prove that\\nyour algorithm attains this time bound.\\n23-2\\xa0\\xa0\\xa0\\xa0\\xa0Shortest paths in ϵ-dense graphs\\nA graph G = (V, E) is ϵ-dense if |E| = Θ (V1+ ϵ) for some constant in the\\nrange 0 < ϵ ≤ 1. d-ary min-heaps (see Problem 6-2 on page 179) provide\\na way to match the running times of Fibonacci-heap-based shortest-\\npath algorithms on ϵ-dense graphs without using as complicated a data\\nstructure.\\na. What are the asymptotic running times for the operations INSERT,\\nEXTRACT-MIN, and DECREASE-KEY, as a function of d and the\\nnumber n of elements in a d-ary min-heap? What are these running\\ntimes if you choose d = Θ (nπ) for some constant 0 < α ≤ 1? Compare\\nthese running times to the amortized costs of these operations for a\\nFibonacci heap.\\nb. Show how to compute shortest paths from a single source on an ϵ-\\ndense directed graph G = (V, E) with no negative-weight edges in\\nO(E) time. (Hint: Pick d as a function of ϵ.)', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 864}),\n",
              " Document(page_content='c. Show how to solve the all-pairs shortest-paths problem on an ϵ-dense\\ndirected graph G = (V, E) with no negative-weight edges in O(VE)\\ntime.\\nd. Show how to solve the all-pairs shortest-paths problem in O(VE) time\\non an ϵ-dense directed graph G = (V, E) that may have negative-\\nweight edges but has no negative-weight cycles.\\nChapter notes\\nLawler [276] has a good discussion of the all-pairs shortest-paths\\nproblem. He attributes the matrix-multiplication algorithm to the\\nfolklore. The Floyd-Warshall algorithm is due to Floyd [144], who\\nbased it on a theorem of Warshall [450] that describes how to compute\\nthe transitive closure of boolean matrices. Johnson’s algorithm is taken\\nfrom [238].\\nSeveral researchers have given improved algorithms for computing\\nshortest paths via matrix multiplication. Fredman [153] shows how to\\nsolve the all-pairs shortest paths problem using O(V5/2) comparisons\\nbetween sums of edge weights and obtains an algorithm that runs in\\nO(V3lg lg V/lg V/1/3) time, which is slightly better than the running\\ntime of the Floyd-Warshall algorithm. This bound has been improved\\nseveral times, and the fastest algorithm is now by Williams [457], with a\\nrunning time of \\n .\\nAnother line of research demonstrates how to apply algorithms for\\nfast matrix multiplication (see the chapter notes for Chapter 4) to the\\nall-pairs shortest paths problem. Let O(nω) be the running time of the\\nfastest algorithm for multiplying two n × n matrices. Galil and Margalit\\n[170, 171] and Seidel [403] designed algorithms that solve the all-pairs\\nshortest paths problem in undirected, unweighted graphs in (Vωp(V))\\ntime, where p(n) denotes a particular function that is\\npolylogarithmically bounded in n. In dense graphs, these algorithms are\\nfaster than the O(VE) time needed to perform |V| breadth-ﬁrst searches.\\nSeveral researchers have extended these results to give algorithms for', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 865}),\n",
              " Document(page_content='solving the all-pairs shortest paths problem in undirected graphs in\\nwhich the edge weights are integers in the range {1, 2, … , W}. The\\nasymptotically fastest such algorithm, by Shoshan and Zwick [410],\\nruns in O(W Vωp(V W)) time. In directed graphs, the best algorithm to\\ndate is due to Zwick [467] and runs in Õ(W1/(4− ω )V2+1/(4− ω )) time.\\nKarger, Koller, and Phillips [244] and independently McGeoch [320]\\nhave given a time bound that depends on E*, the set of edges in E that\\nparticipate in some shortest path. Given a graph with nonnegative edge\\nweights, their algorithms run in O(VE* + V2 lg V) time and improve\\nupon running Dijkstra’s algorithm |V| times when |E*| = o(E). Pettie\\n[355] uses an approach based on component hierarchies to achieve a\\nrunning time of O(VE + V2 lg lg V), and the same running time is also\\nachieved by Hagerup [205].\\nBaswana, Hariharan, and Sen [37] examined decremental\\nalgorithms, which allow a sequence of intermixed edge deletions and\\nqueries, for maintaining all-pairs shortest paths and transitive-closure\\ninformation. When a path exists, their randomized transitive-closure\\nalgorithm can fail to report it with probability 1/nc for an arbitrary c >\\n0. The query times are O(1) with high probability. For transitive closure,\\nthe amortized time for each update is O(V4/3 lg1/3V). By comparison,\\nProblem 23-1, in which edges are inserted, asks for an incremental\\nalgorithm. For all-pairs shortest paths, the update times depend on the\\nqueries. For queries just giving the shortest-path weights, the amortized\\ntime per update is O(V3/E lg2V). To report the actual shortest path, the\\namortized update time is min \\n . Demetrescu and\\nItaliano [111] showed how to handle update and query operations when\\nedges are both inserted and deleted, as long as the range of edge weights\\nis bounded.\\nAho, Hopcroft, and Ullman [5] deﬁned an algebraic structure known\\nas a “closed semiring,” which serves as a general framework for solving\\npath problems in directed graphs. Both the Floyd-Warshall algorithm\\nand the transitive-closure algorithm from Section 23.2 are instantiations\\nof an all-pairs algorithm based on closed semirings. Maggs and Plotkin', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 866}),\n",
              " Document(page_content='[309] showed how to ﬁnd minimum spanning trees using a closed\\nsemiring.\\n1 According to a report cited by U.S. Department of Transportation Federal Highway\\nAdministration, “a reasonable ‘rule of thumb’ is one signalized intersection per 1,000\\npopulation.”\\n2 An algebraic semiring contains operations ⊕ , which is commutative with identity I ⊕, and ⊕ ,\\nwith identity I ⊕, where ⊕  distributes over ⊕  on both the left and right, and where I ⊕ ⊕x =\\nx ⊕I ⊕ = I ⊕ for all x. Standard matrix multiplication, as in MATRIX-MULTIPLY, uses the\\nsemiring with + for ⊕ , ⊕  for ⊕ , 0 for I ⊕, and 1 for I ⊕. The procedure EXTEND-SHORTEST-\\nPATHS uses another semiring, known as the tropical semiring, with min for ⊕ , + for ⊕ , ∞ for\\nI ⊕, and 0 for I ⊕.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 867}),\n",
              " Document(page_content='24\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0Maximum Flow\\nJust as you can model a road map as a directed graph in order to ﬁnd\\nthe shortest path from one point to another, you can also interpret a\\ndirected graph as a “ﬂow network” and use it to answer questions about\\nmaterial ﬂows. Imagine a material coursing through a system from a\\nsource, where the material is produced, to a sink, where it is consumed.\\nThe source produces the material at some steady rate, and the sink\\nconsumes the material at the same rate. The “ﬂow” of the material at\\nany point in the system is intuitively the rate at which the material\\nmoves. Flow networks can model many problems, including liquids\\nﬂowing through pipes, parts through assembly lines, current through\\nelectrical networks, and information through communication networks.\\nYou can think of each directed edge in a ﬂow network as a conduit\\nfor the material. Each conduit has a stated capacity, given as a\\nmaximum rate at which the material can ﬂow through the conduit, such\\nas 200 gallons of liquid per hour through a pipe or 20 amperes of\\nelectrical current through a wire. Vertices are conduit junctions, and\\nother than the source and sink, material ﬂows through the vertices\\nwithout collecting in them. In other words, the rate at which material\\nenters a vertex must equal the rate at which it leaves the vertex. We call\\nthis property “ﬂow conservation,” and it is equivalent to Kirchhoff’s\\ncurrent law when the material is electrical current.\\nThe goal of the maximum-ﬂow problem is to compute the greatest\\nrate for shipping material from the source to the sink without violating\\nany capacity constraints. It is one of the simplest problems concerning\\nﬂow networks and, as we shall see in this chapter, this problem can be', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 868}),\n",
              " Document(page_content='solved by efﬁcient algorithms. Moreover, other network-ﬂow problems\\nare solvable by adapting the basic techniques used in maximum-ﬂow\\nalgorithms.\\nThis chapter presents two general methods for solving the\\nmaximum-ﬂow problem. Section 24.1 formalizes the notions of ﬂow\\nnetworks and ﬂows, formally deﬁning the maximum-ﬂow problem.\\nSection 24.2 describes the classical method of Ford and Fulkerson for\\nﬁnding maximum ﬂows. We ﬁnish up with a simple application of this\\nmethod, ﬁnding a maximum matching in an undirected bipartite graph,\\nin Section 24.3. (Section 25.1 will give a more efﬁcient algorithm that is\\nspeciﬁcally designed to ﬁnd a maximum matching in a bipartite graph.)\\n24.1\\xa0\\xa0\\xa0\\xa0Flow networks\\nThis section gives a graph-theoretic deﬁnition of ﬂow networks,\\ndiscusses their properties, and deﬁnes the maximum-ﬂow problem\\nprecisely. It also introduces some helpful notation.\\nFlow networks and ﬂows\\nA ﬂow network\\xa0G = (V, E) is a directed graph in which each edge (u, v)\\n∈ E has a nonnegative capacity\\xa0c(u, v) ≥ 0. We further require that if E\\ncontains an edge (u, v), then there is no edge (v, u) in the reverse\\ndirection. (We’ll see shortly how to work around this restriction.) If (u,\\nv) ∉ E, then for convenience we deﬁne c(u, v) = 0, and we disallow self-\\nloops. Each ﬂow network contains two distinguished vertices: a source\\xa0s\\nand a sink\\xa0t. For convenience, we assume that each vertex lies on some\\npath from the source to the sink. That is, for each vertex v ∈ V, the ﬂow\\nnetwork contains a path s ⇝ v ⇝ t. Because each vertex other than s has\\nat least one entering edge, we have |E| ≥ |V | − 1. Figure 24.1 shows an\\nexample of a ﬂow network.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 869}),\n",
              " Document(page_content='Figure 24.1 (a) A ﬂow network G = (V, E) for the Lucky Puck Company’s trucking problem.\\nThe Vancouver factory is the source s, and the Winnipeg warehouse is the sink t. The company\\nships pucks through intermediate cities, but only c(u, v) crates per day can go from city u to city\\nv. Each edge is labeled with its capacity. (b) A ﬂow f in G with value |f | = 19. Each edge (u, v) is\\nlabeled by f (u, v)/c(u, v). The slash notation merely separates the ﬂow and capacity and does not\\nindicate division.\\nWe are now ready to deﬁne ﬂows more formally. Let G = (V, E) be a\\nﬂow network with a capacity function c. Let s be the source of the\\nnetwork, and let t be the sink. A ﬂow in G is a real-valued function f : V\\n× V → ℝ that satisﬁes the following two properties:\\nCapacity constraint: For all u, v ∈ V, we require\\n0 ≤ f(u, v) ≤ c(u, v).\\nThe ﬂow from one vertex to another must be nonnegative and must\\nnot exceed the given capacity.\\nFlow conservation: For all u ∈ V − {s, t}, we require\\nThe total ﬂow into a vertex other than the source or sink must equal\\nthe total ﬂow out of that vertex—informally, “ﬂow in equals ﬂow\\nout.”\\nWhen (u, v) ∉ E, there can be no ﬂow from u to v, and f (u, v) = 0.\\nWe call the nonnegative quantity f (u, v) the ﬂow from vertex u to\\nvertex v. The value |f | of a ﬂow f is deﬁned as\\n', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 870}),\n",
              " Document(page_content='that is, the total ﬂow out of the source minus the ﬂow into the source.\\n(Here, the |·| notation denotes ﬂow value, not absolute value or\\ncardinality.) Typically, a ﬂow network does not have any edges into the\\nsource, and the ﬂow into the source, given by the summation Σv ∈V\\xa0f(v,\\ns), is 0. We include it, however, because when we introduce residual\\nnetworks later in this chapter, the ﬂow into the source can be positive. In\\nthe maximum-ﬂow problem, the input is a ﬂow network G with source s\\nand sink t, and the goal is to ﬁnd a ﬂow of maximum value.\\nAn example of ﬂow\\nA ﬂow network can model the trucking problem shown in Figure\\n24.1(a). The Lucky Puck Company has a factory (source s) in\\nVancouver that manufactures hockey pucks, and it has a warehouse\\n(sink t) in Winnipeg that stocks them. Lucky Puck leases space on\\ntrucks from another ﬁrm to ship the pucks from the factory to the\\nwarehouse. Because the trucks travel over speciﬁed routes (edges)\\nbetween cities (vertices) and have a limited capacity, Lucky Puck can\\nship at most c(u, v) crates per day between each pair of cities u and v in\\nFigure 24.1(a). Lucky Puck has no control over these routes and\\ncapacities, and so the company cannot alter the ﬂow network shown in\\nFigure 24.1(a). They need to determine the largest number p of crates\\nper day that they can ship and then to produce this amount, since there\\nis no point in producing more pucks than they can ship to their\\nwarehouse. Lucky Puck is not concerned with how long it takes for a\\ngiven puck to get from the factory to the warehouse. They care only that\\np crates per day leave the factory and p crates per day arrive at the\\nwarehouse.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 871}),\n",
              " Document(page_content='Figure 24.2 Converting a network with antiparallel edges to an equivalent one with no\\nantiparallel edges. (a) A ﬂow network containing both the edges (v1, v2) and (v2, v1). (b) An\\nequivalent network with no antiparallel edges. A new vertex v′ was added, and edge (v1, v2) was\\nreplaced by the pair of edges (v1, v′) and (v′, v2), both with the same capacity as (v1, v2).\\nA ﬂow in this network models the “ﬂow” of shipments because the\\nnumber of crates shipped per day from one city to another is subject to\\na capacity constraint. Additionally, the model must obey ﬂow\\nconservation, for in a steady state, the rate at which pucks enter an\\nintermediate city must equal the rate at which they leave. Otherwise,\\ncrates would accumulate at intermediate cities.\\nModeling problems with antiparallel edges\\nSuppose that the trucking ﬁrm offers Lucky Puck the opportunity to\\nlease space for 10 crates in trucks going from Edmonton to Calgary. It\\nmight seem natural to add this opportunity to our example and form\\nthe network shown in Figure 24.2(a). This network suffers from one\\nproblem, however: it violates the original assumption that if edge (v1,\\nv2) ∈ E, then (v2, v1) ∉ E. We call the two edges (v1, v2) and (v2, v1)\\nantiparallel. Thus, to model a ﬂow problem with antiparallel edges, the\\nnetwork must be transformed into an equivalent one containing no\\nantiparallel edges. Figure 24.2(b) displays this equivalent network. To\\ntransform the network, choose one of the two antiparallel edges, in this\\ncase (v1, v2), and split it by adding a new vertex v′ and replacing edge\\n(v1, v2) with the pair of edges (v1, v′) and (v′, v2). Also set the capacity\\nof both new edges to the capacity of the original edge. The resulting\\nnetwork satisﬁes the property that if an edge belongs to the network, the', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 872}),\n",
              " Document(page_content='reverse edge does not. As Exercise 24.1-1 asks you to prove, the\\nresulting network is equivalent to the original one.\\nFigure 24.3 Converting a multiple-source, multiple-sink maximum-ﬂow problem into a problem\\nwith a single source and a single sink. (a) A ﬂow network with three sources S = {s1, s2, s3} and\\ntwo sinks T = {t1, t2}. (b) An equivalent single-source, single-sink ﬂow network. Add a\\nsupersource s and an edge with inﬁnite capacity from s to each of the multiple sources. Also add\\na supersink t and an edge with inﬁnite capacity from each of the multiple sinks to t.\\nNetworks with multiple sources and s inks\\nA maximum-ﬂow problem may have several sources and sinks, rather\\nthan just one of each. The Lucky Puck Company, for example, might\\nactually have a set of m factories {s1, s2, …, sm} and a set of n\\nwarehouses {t1, t2, …, tn}, as shown in Figure 24.3(a). Fortunately, this\\nproblem is no harder than ordinary maximum ﬂow.\\nThe problem of determining a maximum ﬂow in a network with\\nmultiple sources and multiple sinks reduces to an ordinary maximum-\\nﬂow problem. Figure 24.3(b) shows how to convert the network from (a)\\nto an ordinary ﬂow network with only a single source and a single sink.\\nAdd a supersource\\xa0s and add a directed edge (s, si) with capacity c(s, si)\\n= ∞ for each i = 1, 2, …, m. Similarly, create a new supersink\\xa0t and add a\\ndirected edge (ti, t) with capacity c(ti, t) = ∞ for each i = 1, 2, …, n.\\nIntuitively, any ﬂow in the network in (a) corresponds to a ﬂow in the\\nnetwork in (b), and vice versa. The single supersource s provides as\\nmuch ﬂow as desired for the multiple sources si, and the single supersink', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 873}),\n",
              " Document(page_content='t likewise consumes as much ﬂow as desired for the multiple sinks ti.\\nExercise 24.1-2 asks you to prove formally that the two problems are\\nequivalent.\\nExercises\\n24.1-1\\nShow that splitting an edge in a ﬂow network yields an equivalent\\nnetwork. More formally, suppose that ﬂow network G contains edge (u,\\nv), and deﬁne a new ﬂow network G′ by creating a new vertex x and\\nreplacing (u, v) by new edges (u, x) and (x, v) with c(u, x) = c(x, v) = c(u,\\nv). Show that a maximum ﬂow in G′ has the same value as a maximum\\nﬂow in G.\\n24.1-2\\nExtend the ﬂow properties and deﬁnitions to the multiple-source,\\nmultiple-sink problem. Show that any ﬂow in a multiple-source,\\nmultiple-sink ﬂow network corresponds to a ﬂow of identical value in\\nthe single-source, single-sink network obtained by adding a supersource\\nand a supersink, and vice versa.\\n24.1-3\\nSuppose that a ﬂow network G = (V, E) violates the assumption that the\\nnetwork contains a path s ⇝ v ⇝ t for all vertices v ∈ V. Let u be a\\nvertex for which there is no path s ⇝ u ⇝ t. Show that there must exist a\\nmaximum ﬂow f in G such that f (u, v) = f (v, u) = 0 for all vertices v ∈\\nV.\\n24.1-4\\nLet f be a ﬂow in a network, and let α be a real number. The scalar ﬂow\\nproduct, denoted αf, is a function from V × V to ℝ deﬁned by\\n( αf)(u, v) = α · f (u, v).\\nProve that the ﬂows in a network form a convex set. That is, show that if\\nf1 and f2 are ﬂows, then so is αf1 + (1 − α) f2 for all α in the range 0 ≤ α\\n≤ 1.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 874}),\n",
              " Document(page_content='24.1-5\\nState the maximum-ﬂow problem as a linear-programming problem.\\n24.1-6\\nProfessor Adam has two children who, unfortunately, dislike each other.\\nThe problem is so severe that not only do they refuse to walk to school\\ntogether, but in fact each one refuses to walk on any block that the\\nother child has stepped on that day. The children have no problem with\\ntheir paths crossing at a corner. Fortunately both the professor’s house\\nand the school are on corners, but beyond that he is not sure if it is\\ngoing to be possible to send both of his children to the same school. The\\nprofessor has a map of his town. Show how to formulate the problem of\\ndetermining whether both his children can go to the same school as a\\nmaximum-ﬂow problem.\\n24.1-7\\nSuppose that, in addition to edge capacities, a ﬂow network has vertex\\ncapacities. That is each vertex v has a limit l(v) on how much ﬂow can\\npass through v. Show how to transform a ﬂow network G = (V, E) with\\nvertex capacities into an equivalent ﬂow network G′ = (V′, E′) without\\nvertex capacities, such that a maximum ﬂow in G′ has the same value as\\na maximum ﬂow in G. How many vertices and edges does G′ have?\\n24.2\\xa0\\xa0\\xa0\\xa0The Ford-Fulkerson method\\nThis section presents the Ford-Fulkerson method for solving the\\nmaximum-ﬂow problem. We call it a “method” rather than an\\n“algorithm” because it encompasses several implementations with\\ndiffering running times. The Ford-Fulkerson method depends on three\\nimportant ideas that transcend the method and are relevant to many\\nﬂow algorithms and problems: residual networks, augmenting paths,\\nand cuts. These ideas are essential to the important max-ﬂow min-cut\\ntheorem (Theorem 24.6), which characterizes the value of a maximum\\nﬂow in terms of cuts of the ﬂow network. We end this section by\\npresenting one speciﬁc implementation of the Ford-Fulkerson method\\nand analyzing its running time.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 875}),\n",
              " Document(page_content='The Ford-Fulkerson method iteratively increases the value of the\\nﬂow. It starts with f (u, v) = 0 for all u, v ∈ V, giving an initial ﬂow of\\nvalue 0. Each iteration increases the ﬂow value in G by ﬁnding an\\n“augmenting path” in an associated “residual network” Gf. The edges\\nof the augmenting path in Gf indicate on which edges in G to update the\\nﬂow in order to increase the ﬂow value. Although each iteration of the\\nFord-Fulkerson method increases the value of the ﬂow, we’ll see that the\\nﬂow on any particular edge of G may increase or decrease. Although it\\nmight seem counterintuitive to decrease the ﬂow on an edge, doing so\\nmay enable ﬂow to increase on other edges, allowing more ﬂow to travel\\nfrom the source to the sink. The Ford-Fulkerson method, given in the\\nprocedure FORD-FULKERSON-METHOD, repeatedly augments the\\nﬂow until the residual network has no more augmenting paths. The\\nmax-ﬂow min-cut theorem shows that upon termination, this process\\nyields a maximum ﬂow.\\nFORD-FULKERSON-METHOD (G, s, t)\\n1initialize ﬂow f to 0\\n2while there exists an augmenting path p in the residual network Gf\\n3 augment ﬂow f along p\\n4return\\xa0f\\nIn order to implement and analyze the Ford-Fulkerson method, we\\nneed to introduce several additional concepts.\\nResidual networks\\nIntuitively, given a ﬂow network G and a ﬂow f, the residual network Gf\\nconsists of edges whose capacities represent how the ﬂow can change on\\nedges of G. An edge of the ﬂow network can admit an amount of\\nadditional ﬂow equal to the edge’s capacity minus the ﬂow on that edge.\\nIf that value is positive, that edge goes into Gf with a “residual capacity”\\nof cf (u, v) = c(u, v) − f (u, v). The only edges of G that belong to Gf are', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 876}),\n",
              " Document(page_content='those that can admit more ﬂow. Those edges (u, v) whose ﬂow equals\\ntheir capacity have cf (u, v) = 0, and they do not belong to Gf.\\nYou might be surprised that the residual network Gf can also contain\\nedges that are not in G. As an algorithm manipulates the ﬂow, with the\\ngoal of increasing the total ﬂow, it might need to decrease the ﬂow on a\\nparticular edge in order to increase the ﬂow elsewhere. In order to\\nrepresent a possible decrease in the positive ﬂow f (u, v) on an edge in G,\\nthe residual network Gf contains an edge (v, u) with residual capacity cf\\n(v, u) = f (u, v)—that is, an edge that can admit ﬂow in the opposite\\ndirection to (u, v), at most canceling out the ﬂow on (u, v). These reverse\\nedges in the residual network allow an algorithm to send back ﬂow it\\nhas already sent along an edge. Sending ﬂow back along an edge is\\nequivalent to decreasing the ﬂow on the edge, which is a necessary\\noperation in many algorithms.\\nMore formally, for a ﬂow network G = (V, E) with source s, sink t,\\nand a ﬂow f, consider a pair of vertices u, v ∈ V. We deﬁne the residual\\ncapacity\\xa0cf (u, v) by\\nIn a ﬂow network, (u, v) ∈ E implies (v, u) ∉ E, and so exactly one case\\nin equation (24.2) applies to each ordered pair of vertices.\\nAs an example of equation (24.2), if c(u, v) = 16 and f (u, v) = 11,\\nthen f (u, v) can increase by up to cf (u, v) = 5 units before exceeding the\\ncapacity constraint on edge (u, v). Alternatively, up to 11 units of ﬂow\\ncan return from v to u, so that cf (v, u) = 11.\\nGiven a ﬂow network G = (V, E) and a ﬂow f, the residual network of\\nG induced by f is Gf = (V, Ef), where\\n', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 877}),\n",
              " Document(page_content='Figure 24.4 (a) The ﬂow network G and ﬂow f of Figure 24.1(b). (b) The residual network Gf\\nwith augmenting path p, having residual capacity cf (p) = cf (v2, v3) = 4, in blue. Edges with\\nresidual capacity equal to 0, such as (v1, v3), are not shown, a convention we follow in the\\nremainder of this section. (c) The ﬂow in G that results from augmenting along path p by its\\nresidual capacity 4. Edges carrying no ﬂow, such as (v3, v2), are labeled only by their capacity,\\nanother convention we follow throughout. (d) The residual network induced by the ﬂow in (c).\\nThat is, as promised above, each edge of the residual network, or\\nresidual edge, can admit a ﬂow that is greater than 0. Figure 24.4(a)\\nrepeats the ﬂow network G and ﬂow f of Figure 24.1(b), and Figure\\n24.4(b) shows the corresponding residual network Gf. The edges in Ef\\nare either edges in E or their reversals, and thus\\n|Ef| ≤ 2 |E|.\\nObserve that the residual network Gf is similar to a ﬂow network\\nwith capacities given by cf. It does not satisfy the deﬁnition of a ﬂow\\nnetwork, however, because it could contain antiparallel edges. Other\\nthan this difference, a residual network has the same properties as a ﬂow\\nnetwork, and we can deﬁne a ﬂow in the residual network as one that\\nsatisﬁes the deﬁnition of a ﬂow, but with respect to capacities cf in the\\nresidual network Gf.\\nA ﬂow in a residual network provides a roadmap for adding ﬂow to\\nthe original ﬂow network. If f is a ﬂow in G and f′ is a ﬂow in the', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 878}),\n",
              " Document(page_content='corresponding residual network Gf, we deﬁne f ↑ f′, the augmentation of\\nﬂow f by f ′, to be a function from V × V to ℝ, deﬁned by\\nThe intuition behind this deﬁnition follows the deﬁnition of the\\nresidual network. The ﬂow on (u, v) increases by f ′(u, v), but decreases\\nby f ′(v, u) because pushing ﬂow on the reverse edge in the residual\\nnetwork signiﬁes decreasing the ﬂow in the original network. Pushing\\nﬂow on the reverse edge in the residual network is also known as\\ncancellation. For example, suppose that 5 crates of hockey pucks go\\nfrom u to v and 2 crates go from v to u. That is equivalent (from the\\nperspective of the ﬁnal result) to sending 3 crates from u to v and none\\nfrom v to u. Cancellation of this type is crucial for any maximum-ﬂow\\nalgorithm.\\nThe following lemma shows that augmenting a ﬂow in G by a ﬂow in\\nGf yields a new ﬂow in G with a greater ﬂow value.\\nLemma 24.1\\nLet G = (V, E) be a ﬂow network with source s and sink t, and let f be a\\nﬂow in G. Let Gf be the residual network of G induced by f, and let f ′ be\\na ﬂow in Gf. Then the function f ↑ f ′ deﬁned in equation (24.4) is a ﬂow\\nin G with value |f ↑ f ′| = |f | + |f ′|.\\nProof\\xa0\\xa0\\xa0We ﬁrst verify that f ↑ f ′ obeys the capacity constraint for each\\nedge in E and ﬂow conservation at each vertex in V − {s, t}.\\nFor the capacity constraint, ﬁrst observe that if (u, v) ∈ E, then cf (v,\\nu) = f (u, v). Because f ′ is a ﬂow in Gf, we have f ′(v, u) ≤ cf (v, u), which\\ngives f ′(v, u) ≤ f (u, v). Therefore,\\n(f ↑ f′)(u, v)=f (u, v) + f ′(u, v) − f ′(v, u)(by equation (24.4))\\n≥f (u, v) + f ′(u, v) − f (u, v)(because f ′(v, u) ≤ f (u, v))\\n=f ′(u, v)\\n≥0.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 879}),\n",
              " Document(page_content='In addition,\\n(f ↑ f′)(u, v)\\n=f (u, v) + f ′(u, v) − f ′(v, u)(by equation (24.4))\\n≤f (u, v) + f ′(u, v) (because ﬂows are nonnegative)\\n≤f (u, v) + cf (u, v) (capacity constraint)\\n=f (u, v) + c(u, v) − f (u, v)(deﬁnition of cf)\\n=c(u, v).\\nTo show that ﬂow conservation holds and that |f ↑ f ′| = |f | + |f ′|, we\\nﬁrst prove the claim that for all u ∈ V, we have\\nBecause we disallow antiparallel edges in G (but not in Gf), we know\\nthat for each vertex u, there can be an edge (u, v) or (v, u) in G, but never\\nboth. For a ﬁxed vertex u, deﬁne Vl(u) = {v : (u, v) ∈ E} to be the set of\\nvertices with edges in G leaving u, and deﬁne Ve(u) = {v : (v, u) ∈ E} to\\nbe the set of vertices with edges in G entering u. We have Vl(u) ∪ Ve(u)\\n⊆ V and, because G contains no antiparallel edges, Vl(u) ∩ Ve(u) = ∅ .\\nBy the deﬁnition of ﬂow augmentation in equation (24.4), only vertices\\nv in Vl(u) can have positive (f ↑ f′)(u, v), and only vertices v in Ve(u) can\\nhave positive (f ↑ f ′)(v, u). Starting from the left-hand side of equation\\n(24.5), we use this fact and then reorder and group terms, giving', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 880}),\n",
              " Document(page_content='In equation (24.6), all four summations can extend to sum over V, since\\neach additional term has value 0. (Exercise 24.2-1 asks you to prove this\\nformally.) Taking all four summations over V, instead of just subsets of\\nV, proves the claim in equation (24.5).\\nNow we are ready to prove ﬂow conservation for f ↑ f ′ and that |f ↑ f′|\\n= | f | + |f ′|. For the latter property, let u = s in equation (24.5). Then, we\\nhave\\nFor ﬂow conservation, observe that for any vertex u that is neither s nor\\nt, ﬂow conservation for f and f ′ means that the right-hand side of\\nequation (24.5) is 0, and thus Σv ∈V (f ↑ f′)(u, v) = Σv ∈V (f ↑ f′)(v, u).\\n▪\\nAugmenting paths\\nGiven a ﬂow network G = (V, E) and a ﬂow f, an augmenting path\\xa0p is a\\nsimple path from s to t in the residual network Gf. By the deﬁnition of\\nthe residual network, the ﬂow on an edge (u, v) of an augmenting path', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 881}),\n",
              " Document(page_content='may increase by up to cf (u, v) without violating the capacity constraint\\non whichever of (u, v) and (v, u) belongs to the original ﬂow network G.\\nThe blue path in Figure 24.4(b) is an augmenting path. Treating the\\nresidual network Gf in the ﬁgure as a ﬂow network, the ﬂow through\\neach edge of this path can increase by up to 4 units without violating a\\ncapacity constraint, since the smallest residual capacity on this path is cf\\n(v2, v3) = 4. We call the maximum amount by which we can increase the\\nﬂow on each edge in an augmenting path p the residual capacity of p,\\ngiven by\\ncf (p) = min {cf (u, v) : (u, v) is in p}.\\nThe following lemma, which Exercise 24.2-7 asks you to prove, makes\\nthe above argument more precise.\\nLemma 24.2\\nLet G = (V, E) be a ﬂow network, let f be a ﬂow in G, and let p be an\\naugmenting path in Gf. Deﬁne a function fp : V × V → ℝ by\\nThen, fp is a ﬂow in Gf with value |fp| = cf (p) > 0.\\n▪\\nThe following corollary shows that augmenting f by fp produces\\nanother ﬂow in G whose value is closer to the maximum. Figure 24.4(c)\\nshows the result of augmenting the ﬂow f from Figure 24.4(a) by the\\nﬂow fp in Figure 24.4(b), and Figure 24.4(d) shows the ensuing residual\\nnetwork.\\nCorollary 24.3\\nLet G = (V, E) be a ﬂow network, let f be a ﬂow in G, and let p be an\\naugmenting path in Gf. Let fp be deﬁned as in equation (24.7), and\\nsuppose that f is augmented by fp. Then the function f ↑ fp is a ﬂow in G\\nwith value |f ↑ fp| = |f| + |fp| > |f|.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 882}),\n",
              " Document(page_content='Proof\\xa0\\xa0\\xa0Immediate from Lemmas 24.1 and 24.2.\\n▪\\nCuts of ﬂow networks\\nThe Ford-Fulkerson method repeatedly augments the ﬂow along\\naugmenting paths until it has found a maximum ﬂow. How do we know\\nthat when the algorithm terminates, it has actually found a maximum\\nﬂow? The max-ﬂow min-cut theorem, which we will prove shortly, tells\\nus that a ﬂow is maximum if and only if its residual network contains no\\naugmenting path. To prove this theorem, though, we must ﬁrst explore\\nthe notion of a cut of a ﬂow network.\\nA cut (S, T) of ﬂow network G = (V, E) is a partition of V into S and\\nT = V − S such that s ∈ S and t ∈ T. (This deﬁnition is similar to the\\ndeﬁnition of “cut” that we used for minimum spanning trees in Chapter\\n21, except that here we are cutting a directed graph rather than an\\nundirected graph, and we insist that s ∈ S and t ∈ T.) If f is a ﬂow, then\\nthe net ﬂow\\xa0f(S, T) across the cut (S, T) is deﬁned to be\\nThe capacity of the cut (S, T) is\\nA minimum cut of a network is a cut whose capacity is minimum over all\\ncuts of the network.\\nYou probably noticed that the deﬁnitions of ﬂow across a cut and\\ncapacity of a cut differ in that ﬂow counts edges going in both directions\\nacross the cut, but capacity counts only edges going from the source\\nside of the cut toward the sink side. This asymmetry is intentional and\\nimportant. The reason for this difference will become apparent later in\\nthis section.\\nFigure 24.5 shows the cut ({s, v1, v2}, {v3, v4, t}) in the ﬂow network\\nof Figure 24.1(b). The net ﬂow across this cut is\\nf (v1, v3) + f (v2, v4) − f (v3, v2)=12 + 11 − 4', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 883}),\n",
              " Document(page_content='=19,\\nand the capacity of this cut is\\nFigure 24.5 A cut (S, T) in the ﬂow network of Figure 24.1(b), where S = {s, v1, v2} and T =\\n{v3, v4, t}. The vertices in S are orange, and the vertices in T are tan. The net ﬂow across (S, T)\\nis f (S, T) = 19, and the capacity is c(S, T) = 26.\\nc(v1, v3) + c(v2, v4)=12 + 14\\n=26.\\nThe following lemma shows that, for a given ﬂow f, the net ﬂow\\nacross any cut is the same, and it equals |f|, the value of the ﬂow.\\nLemma 24.4\\nLet f be a ﬂow in a ﬂow network G with source s and sink t, and let (S,\\nT) be any cut of G. Then the net ﬂow across (S, T) is f (S, T) =| f|.\\nProof\\xa0\\xa0\\xa0For any vertex u ∈ V − {s, t}, rewrite the ﬂow-conservation\\ncondition as\\nTaking the deﬁnition of| f| from equation (24.1) and adding the left-\\nhand side of equation (24.10), which equals 0, summed over all vertices\\nin S − {s}, gives\\nExpanding the right-hand summation and regrouping terms yields', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 884}),\n",
              " Document(page_content='Because V = S ∪ T and S ∩ T = ∅ , splitting each summation over V\\ninto summations over S and T gives\\nThe two summations within the parentheses are actually the same, since\\nfor all vertices x, y ∈ S, the term f (x, y) appears once in each\\nsummation. Hence, these summations cancel, yielding\\n▪\\nA corollary to Lemma 24.4 shows how cut capacities bound the\\nvalue of a ﬂow.\\nCorollary 24.5\\nThe value of any ﬂow f in a ﬂow network G is bounded from above by\\nthe capacity of any cut of G.\\nProof\\xa0\\xa0\\xa0Let (S, T) be any cut of G and let f be any ﬂow. By Lemma 24.4\\nand the capacity constraint,', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 885}),\n",
              " Document(page_content='▪\\nCorollary 24.5 yields the immediate consequence that the value of a\\nmaximum ﬂow in a network is bounded from above by the capacity of a\\nminimum cut of the network. The important max-ﬂow min-cut\\ntheorem, which we now state and prove, says that the value of a\\nmaximum ﬂow is in fact equal to the capacity of a minimum cut.\\nTheorem 24.6 (Max-ﬂow min-cut theorem)\\nIf f is a ﬂow in a ﬂow network G = (V, E) with source s and sink t, then\\nthe following conditions are equivalent:\\n1. f is a maximum ﬂow in G.\\n2. The residual network Gf contains no augmenting paths.\\n3. |f| = c(S, T) for some cut (S, T) of G.\\nProof\\xa0\\xa0\\xa0(1) ⇒  (2): Suppose for the sake of contradiction that f is a\\nmaximum ﬂow in G but that Gf has an augmenting path p. Then, by\\nCorollary 24.3, the ﬂow found by augmenting f by fp, where fp is given\\nby equation (24.7), is a ﬂow in G with value strictly greater than |f|,\\ncontradicting the assumption that f is a maximum ﬂow.\\n(2) ⇒  (3): Suppose that Gf has no augmenting path, that is, that Gf\\ncontains no path from s to t. Deﬁne\\nS = {v ∈ V : there exists a path from s to v in Gf }\\nand T = V − S. The partition (S, T) is a cut: we have s ∈ S trivially and\\nt ∉ S because there is no path from s to t in Gf. Now consider a pair of\\nvertices u ∈ S and v ∈ T. If (u, v) ∈ E, we must have f (u, v) = c(u, v),', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 886}),\n",
              " Document(page_content='since otherwise (u, v) ∈ Ef, which would place v in set S. If (v, u) ∈ E,\\nwe must have f (v, u) = 0, because otherwise cf (u, v) = f (v, u) would be\\npositive and we would have (u, v) ∈ Ef, which again would place v in S.\\nOf course, if neither (u, v) nor (v, u) belongs to E, then f (u, v) = f (v, u)\\n= 0. We thus have\\nBy Lemma 24.4, therefore, |f| = f (S, T) = c(S, T).\\n(3) ⇒  (1): By Corollary 24.5, |f| ≤ c(S, T) for all cuts (S, T). The\\ncondition |f| = c(S, T) thus implies that f is a maximum ﬂow.\\n▪\\nThe basic Ford-Fulkerson algorithm\\nEach iteration of the Ford-Fulkerson method ﬁnds some augmenting\\npath p and uses p to modify the ﬂow f. As Lemma 24.2 and Corollary\\n24.3 suggest, replacing f by f ↑ fp produces a new ﬂow whose value is |f|\\n+ |fp|. The procedure FORD-FULKERSON on the next page\\nimplements the method by updating the ﬂow attribute (u, v).f for each\\nedge (u, v) ∈ E.1 It assumes implicitly that (u, v).f = 0 if (u, v) ∉ E. The\\nprocedure also assumes that the capacities c(u, v) come with the ﬂow\\nnetwork, and that c(u, v) = 0 if (u, v) ∉ E. The procedure computes the\\nresidual capacity cf (u, v) in accordance with the formula (24.2). The\\nexpression cf (p) in the code is just a temporary variable that stores the\\nresidual capacity of the path p.\\nFORD-FULKERSON (G, s, t)\\n1for each edge (u, v) ∈ G.E\\n2 (u, v).f = 0\\n3while there exists a path p from s to t in the residual network Gf\\n4 cf (p) = min {cf (u, v) : (u, v) is in p}', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 887}),\n",
              " Document(page_content='5 for each edge (u, v) in p\\n6 if (u, v) ∈ G.E\\n7 (u, v).f = (u, v).f + cf (p)\\n8 else (v, u).f = (v, u).f − cf (p)\\n9return\\xa0f\\nThe FORD-FULKERSON procedure simply expands on the\\nFORD-FULKERSON-METHOD pseudocode given earlier. Figure\\n24.6 shows the result of each iteration in a sample run. Lines 1–2\\ninitialize the ﬂow f to 0. The while loop of lines 3–8 repeatedly ﬁnds an\\naugmenting path p in Gf and augments ﬂow f along p by the residual\\ncapacity cf (p). Each residual edge in path p is either an edge in the\\noriginal network or the reversal of an edge in the original network.\\nLines 6–8 update the ﬂow in each case appropriately, adding ﬂow when\\nthe residual edge is an original edge and subtracting it otherwise. When\\nno augmenting paths exist, the ﬂow f is a maximum ﬂow.\\nAnalysis of Ford-Fulkerson\\nThe running time of FORD-FULKERSON depends on the\\naugmenting path p and how it’s found in line 3. If the edge capacities are\\nirrational numbers, it’s possible to choose the augmenting path so that\\nthe algorithm never terminates: the value of the ﬂow increases with\\nsuccessive augmentations, but never converges to the maximum ﬂow\\nvalue. The good news is that if the algorithm ﬁnds the augmenting path\\nby using a breadth-ﬁrst search (which we saw in Section 20.2), it runs in\\npolynomial time. Before proving this result, we obtain a simple bound\\nfor the case in which all capacities are integers and the algorithm ﬁnds\\nany augmenting path.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 888}),\n",
              " Document(page_content='Figure 24.6 The execution of the basic Ford-Fulkerson algorithm. (a)–(e) Successive iterations of\\nthe while loop. The left side of each part shows the residual network Gf from line 3 with a blue\\naugmenting path p. The right side of each part shows the new ﬂow f that results from\\naugmenting f by fp. The residual network in (a) is the input ﬂow network G. (f) The residual\\nnetwork at the last while loop test. It has no augmenting paths, and the ﬂow f shown in (e) is\\ntherefore a maximum ﬂow. The value of the maximum ﬂow found is 23.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 889}),\n",
              " Document(page_content='Figure 24.7 (a) A ﬂow network for which FORD-FULKERSON can take Θ (E | f*|) time, where\\nf* is a maximum ﬂow, shown here with |f*| = 2,000,000. The blue path is an augmenting path\\nwith residual capacity 1. (b) The resulting residual network, with another augmenting path\\nwhose residual capacity is 1. (c) The resulting residual network.\\nIn practice, the maximum-ﬂow problem often arises with integer\\ncapacities. If the capacities are rational numbers, an appropriate scaling\\ntransformation can make them all integers. If f* denotes a maximum\\nﬂow in the transformed network, then a straightforward implementation\\nof FORD-FULKERSON executes the while loop of lines 3–8 at most\\n|f*| times, since the ﬂow value increases by at least 1 unit in each\\niteration.\\nA good implementation should perform the work done within the\\nwhile loop efﬁciently. It should represent the ﬂow network G = (V, E)\\nwith the right data structure and ﬁnd an augmenting path by a linear-\\ntime algorithm. Let’s assume that the implementation keeps a data\\nstructure corresponding to a directed graph G′ = (V, E′), where E′ = {(u,\\nv) : (u, v) ∈ E or (v, u) ∈ E}. Edges in the network G are also edges in\\nG′, making it straightforward to maintain capacities and ﬂows in this\\ndata structure. Given a ﬂow f on G, the edges in the residual network Gf\\nconsist of all edges (u, v) of G′ such that cf (u, v) > 0, where cf conforms\\nto equation (24.2). The time to ﬁnd a path in a residual network is\\ntherefore O(V + E′) = O(E) using either depth-ﬁrst search or breadth-\\nﬁrst search. Each iteration of the while loop thus takes O(E) time, as\\ndoes the initialization in lines 1–2, making the total running time of the\\nFORD-FULKERSON algorithm O(E |f*|).\\nWhen the capacities are integers and the optimal ﬂow value |f*| is\\nsmall, the running time of the Ford-Fulkerson algorithm is good.\\nFigure 24.7(a) shows an example of what can happen on a simple ﬂow', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 890}),\n",
              " Document(page_content='network for which |f*| is large. A maximum ﬂow in this network has\\nvalue 2,000,000: 1,000,000 units of ﬂow traverse the path s → u → t, and\\nanother 1,000,000 units traverse the path s → v → t. If the ﬁrst\\naugmenting path found by FORD-FULKERSON is s → u → v → t,\\nshown in Figure 24.7(a), the ﬂow has value 1 after the ﬁrst iteration.\\nThe resulting residual network appears in Figure 24.7(b). If the second\\niteration ﬁnds the augmenting path s → v → u → t, as shown in Figure\\n24.7(b), the ﬂow then has value 2. Figure 24.7(c) shows the resulting\\nresidual network. If the algorithm continues alternately choosing the\\naugmenting paths s → u → v → t and s → v → u → t, it performs a total\\nof 2,000,000 augmentations, increasing the ﬂow value by only 1 unit in\\neach.\\nThe Edmonds-Karp algorithm\\nIn the example of Figure 24.7, the algorithm never chooses the\\naugmenting path with the fewest edges. It should have. By using\\nbreadth-ﬁrst search to ﬁnd an augmenting path in the residual network,\\nthe algorithm runs in polynomial time, independent of the maximum\\nﬂow value. We call the Ford-Fulkerson method so implemented the\\nEdmonds-Karp algorithm.\\nLet’s now prove that the Edmonds-Karp algorithm runs in O(VE2)\\ntime. The analysis depends on the distances to vertices in the residual\\nnetwork Gf. The notation δf (u, v) denotes the shortest-path distance\\nfrom u to v in Gf, where each edge has unit distance.\\nLemma 24.7\\nIf the Edmonds-Karp algorithm is run on a ﬂow network G = (V, E)\\nwith source s and sink t, then for all vertices v ∈ V − {s, t}, the shortest-\\npath distance δf (s, v) in the residual network Gf increases\\nmonotonically with each ﬂow augmentation.\\nProof\\xa0\\xa0\\xa0We’ll suppose that a ﬂow augmentation occurs that causes the\\nshortest-path distance from s to some vertex v ∈ V − {s, t} to decrease\\nand then derive a contradiction. Let f be the ﬂow just before an\\naugmentation that decreases some shortest-path distance, and let f′ be', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 891}),\n",
              " Document(page_content='the ﬂow just afterward. Let v be a vertex with the minimum δf ′ (s, v)\\nwhose distance was decreased by the augmentation, so that δf′ (s, v) < δf\\n(s, v). Let p = s ⇝ u → v be a shortest path from s to v in Gf′, so that (u,\\nv) ∈ Ef′ and\\nBecause of how we chose v, we know that the distance of vertex u from\\nthe source s did not decrease, that is,\\nWe claim that (u, v) ∉ Ef. Why? If we have (u, v) ∈ Ef, then we also\\nhave\\nδf (s, v)≤ δf (s, u) + 1(by Lemma 22.10, the triangle inequality)\\n≤ δf′ (s, u) + 1(by inequality (24.12))\\n= δf′ (s, v) (by equation (24.11)),\\nwhich contradicts our assumption that δf′ (s, v) < δf (s, v).\\nHow can we have (u, v) ∉ Ef and (u, v) ∈ Ef′? The augmentation\\nmust have increased the ﬂow from v to u, so that edge (v, u) was in the\\naugmenting path. The augmenting path was a shortest path from s to t\\nin Gf, and since any subpath of a shortest path is itself a shortest path,\\nthis augmenting path includes a shortest path from s to u in Gf that has\\n(v, u) as its last edge. Therefore,\\nδf (s, v)= δf (s, u) − 1\\n≤ δf′ (s, u) − 1(by inequality (24.12))\\n= δf′ (s, v) − 2(by equation (24.11)),\\nso that δf′ (s, v) > δf (s, v), contradicting our assumption that δf′ (s, v) <\\nδf (s, v). We conclude that our assumption that such a vertex v exists is\\nincorrect.\\n▪', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 892}),\n",
              " Document(page_content='The next theorem bounds the number of iterations of the Edmonds-\\nKarp algorithm.\\nTheorem 24.8\\nIf the Edmonds-Karp algorithm is run on a ﬂow network G = (V, E)\\nwith source s and sink t, then the total number of ﬂow augmentations\\nperformed by the algorithm is O(VE).\\nProof\\xa0\\xa0\\xa0We say that an edge (u, v) in a residual network Gf is critical on\\nan augmenting path p if the residual capacity of p is the residual\\ncapacity of (u, v), that is, if cf (p) = cf (u, v). After ﬂow is augmented\\nalong an augmenting path, any critical edge on the path disappears\\nfrom the residual network. Moreover, at least one edge on any\\naugmenting path must be critical. We’ll show that each of the |E| edges\\ncan become critical at most |V|/2 times.\\nLet u and v be vertices in V that are connected by an edge in E. Since\\naugmenting paths are shortest paths, when (u, v) is critical for the ﬁrst\\ntime, we have\\nδf (s, v) = δf (s, u) + 1.\\nOnce the ﬂow is augmented, the edge (u, v) disappears from the residual\\nnetwork. It cannot reappear later on another augmenting path until\\nafter the ﬂow from u to v is decreased, which occurs only if (v, u)\\nappears on an augmenting path. If f ′ is the ﬂow in G when this event\\noccurs, then we have\\nδf′ (s, u) = δf′ (s, v) + 1.\\nSince δf (s, v) ≤ δf′ (s, v) by Lemma 24.7, we have\\nδf′ (s, u)= δf′ (s, v) + 1\\n≥ δf (s, v) + 1\\n= δf (s, u) + 2.\\nConsequently, from the time (u, v) becomes critical to the time when\\nit next becomes critical, the distance of u from the source increases by at', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 893}),\n",
              " Document(page_content='least 2. The distance of u from the source is initially at least 0. Because\\nedge (u, v) is on an augmenting path, and augmenting paths end at t, we\\nknow that u cannot be t, so that in any residual network that has a path\\nfrom s to u, the shortest such path has at most |V| − 2 edges. Thus, after\\nthe ﬁrst time that (u, v) becomes critical, it can become critical at most\\n(|V| − 2)/2 = |V|/2 − 1 times more, for a total of at most |V|/2 times. Since\\nthere are O(E) pairs of vertices that can have an edge between them in a\\nresidual network, the total number of critical edges during the entire\\nexecution of the Edmonds-Karp algorithm is O(VE). Each augmenting\\npath has at least one critical edge, and hence the theorem follows.\\n▪\\nBecause each iteration of FORD-FULKERSON takes O(E) time\\nwhen it uses breadth-ﬁrst search to ﬁnd the augmenting path, the total\\nrunning time of the Edmonds-Karp algorithm is O(VE2).\\nExercises\\n24.2-1\\nProve that the summations in equation (24.6) equal the summations on\\nthe right-hand side of equation (24.5).\\n24.2-2\\nIn Figure 24.1(b), what is the net ﬂow across the cut ({s, v2, v4}, {v1, v3,\\nt})? What is the capacity of this cut?\\n24.2-3\\nShow the execution of the Edmonds-Karp algorithm on the ﬂow\\nnetwork of Figure 24.1(a).\\n24.2-4\\nIn the example of Figure 24.6, what is the minimum cut corresponding\\nto the maximum ﬂow shown? Of the augmenting paths appearing in the\\nexample, which one cancels ﬂow?\\n24.2-5', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 894}),\n",
              " Document(page_content='The construction in Section 24.1 to convert a ﬂow network with\\nmultiple sources and sinks into a single-source, single-sink network adds\\nedges with inﬁnite capacity. Prove that any ﬂow in the resulting network\\nhas a ﬁnite value if the edges of the original network with multiple\\nsources and sinks have ﬁnite capacity.\\n24.2-6\\nSuppose that each source si in a ﬂow network with multiple sources and\\nsinks produces exactly pi units of ﬂow, so that Σv ∈V\\xa0f (si, v) = pi.\\nSuppose also that each sink tj consumes exactly qj units, so that Σv ∈V\\xa0f\\n(v, tj) = qj, where Σi\\xa0pi = Σj\\xa0qj. Show how to convert the problem of\\nﬁnding a ﬂow f that obeys these additional constraints into the problem\\nof ﬁnding a maximum ﬂow in a single-source, single-sink ﬂow network.\\n24.2-7\\nProve Lemma 24.2.\\n24.2-8\\nSuppose that we redeﬁne the residual network to disallow edges into s.\\nArgue that the procedure FORD-FULKERSON still correctly\\ncomputes a maximum ﬂow.\\n24.2-9\\nSuppose that both f and f ′ are ﬂows in a ﬂow network. Does the\\naugmented ﬂow f ↑ f ′ satisfy the ﬂow conservation property? Does it\\nsatisfy the capacity constraint?\\n24.2-10\\nShow how to ﬁnd a maximum ﬂow in a ﬂow network G = (V, E) by a\\nsequence of at most |E| augmenting paths. (Hint: Determine the paths\\nafter ﬁnding the maximum ﬂow.)\\n24.2-11\\nThe edge connectivity of an undirected graph is the minimum number k\\nof edges that must be removed to disconnect the graph. For example,\\nthe edge connectivity of a tree is 1, and the edge connectivity of a cyclic\\nchain of vertices is 2. Show how to determine the edge connectivity of', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 895}),\n",
              " Document(page_content='an undirected graph G = (V, E) by running a maximum-ﬂow algorithm\\non at most |V| ﬂow networks, each having O(V + E) vertices and O(E)\\nedges.\\n24.2-12\\nYou are given a ﬂow network G, where G contains edges entering the\\nsource s. Let f be a ﬂow in G with |f| ≥ 0 in which one of the edges (v, s)\\nentering the source has f (v, s) = 1. Prove that there must exist another\\nﬂow f ′ with f ′(v, s) = 0 such that |f| = |f′|. Give an O(E)-time algorithm\\nto compute f′, given f and assuming that all edge capacities are integers.\\n24.2-13\\nSuppose that you wish to ﬁnd, among all minimum cuts in a ﬂow\\nnetwork G with integer capacities, one that contains the smallest\\nnumber of edges. Show how to modify the capacities of G to create a\\nnew ﬂow network G′ in which any minimum cut in G′ is a minimum cut\\nwith the smallest number of edges in G.\\n24.3\\xa0\\xa0\\xa0\\xa0Maximum bipartite matching\\nSome combinatorial problems can be cast as maximum-ﬂow problems,\\nsuch as the multiple-source, multiple-sink maximum-ﬂow problem from\\nSection 24.1. Other combinatorial problems seem on the surface to have\\nlittle to do with ﬂow networks, but they can in fact be reduced to\\nmaximum-ﬂow problems. This section presents one such problem:\\nﬁnding a maximum matching in a bipartite graph. In order to solve this\\nproblem, we’ll take advantage of an integrality property provided by the\\nFord-Fulkerson method. We’ll also see how to use the Ford-Fulkerson\\nmethod to solve the maximum-bipartite-matching problem on a graph\\nG = (V, E) in O(VE) time. Section 25.1 will present an algorithm\\nspeciﬁcally designed to solve this problem.\\nThe maximum-bipartite-matching pr oblem\\nGiven an undirected graph G = (V, E), a matching is a subset of edges\\nM ⊆ E such that for all vertices v ∈ V, at most one edge of M is', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 896}),\n",
              " Document(page_content='incident on v. We say that a vertex v ∈ V is matched by the matching M\\nif some edge in M is incident on v, and otherwise, v is unmatched. A\\nmaximum matching is a matching of maximum cardinality, that is, a\\nmatching M such that for any matching M′, we have |M| ≥ |M′|. In this\\nsection, we restrict our attention to ﬁnding maximum matchings in\\nbipartite graphs: graphs in which the vertex set can be partitioned into\\nV = L ∪ R, where L and R are disjoint and all edges in E go between L\\nand R. We further assume that every vertex in V has at least one\\nincident edge. Figure 24.8 illustrates the notion of a matching in a\\nbipartite graph.\\nThe problem of ﬁnding a maximum matching in a bipartite graph\\nhas many practical applications. As an example, consider matching a set\\nL of machines with a set R of tasks to be performed simultaneously. An\\nedge (u, v) in E signiﬁes that a particular machine u ∈ L is capable of\\nperforming a particular task v ∈ R. A maximum matching provides\\nwork for as many machines as possible.\\nFigure 24.8 A bipartite graph G = (V, E) with vertex partition V = L ∪ R. (a) A matching with\\ncardinality 2, indicated by blue edges. (b) A maximum matching with cardinality 3. (c) The\\ncorresponding ﬂow network G′ with a maximum ﬂow shown. Each edge has unit capacity. Blue\\nedges have a ﬂow of 1, and all other edges carry no ﬂow. The blue edges from L to R correspond\\nto those in the maximum matching from (b).\\nFinding a maximum bipartite matching\\nThe Ford-Fulkerson method provides a basis for ﬁnding a maximum\\nmatching in an undirected bipartite graph G = (V, E) in time', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 897}),\n",
              " Document(page_content='polynomial in |V| and |E|. The trick is to construct a ﬂow network in\\nwhich ﬂows correspond to matchings, as shown in Figure 24.8(c). We\\ndeﬁne the corresponding ﬂow network\\xa0G′ = (V′, E′) for the bipartite\\ngraph G as follows. Let the source s and sink t be new vertices not in V,\\nand let V′ = V ∪ {s, t}. If the vertex partition of G is V = L ∪ R, the\\ndirected edges of G′ are the edges of E, directed from L to R, along with\\n|V | new directed edges:\\nE′={(s, u) : u ∈ L}\\n∪ {(u, v) : u ∈ L, v ∈ R, and (u, v) ∈ E}\\n∪ {(v, t) : v ∈ R}.\\nTo complete the construction, assign unit capacity to each edge in E′.\\nSince each vertex in V has at least one incident edge, |E| ≥ |V|/2. Thus, |E|\\n≤ |E′| = |E| + |V| ≤ 3 |E|, and so |E′| = Θ (E).\\nThe following lemma shows that a matching in G corresponds\\ndirectly to a ﬂow in G’s corresponding ﬂow network G′. We say that a\\nﬂow f on a ﬂow network G = (V, E) is integer-valued if f (u, v) is an\\ninteger for all (u, v) ∈ V × V.\\nLemma 24.9\\nLet G = (V, E) be a bipartite graph with vertex partition V = L ∪ R,\\nand let G′ = (V′, E′) be its corresponding ﬂow network. If M is a\\nmatching in G, then there is an integer-valued ﬂow f in G′ with value |f|\\n= |M|. Conversely, if f is an integer-valued ﬂow in G′, then there is a\\nmatching M in G with cardinality |M| = |f| consisting of edges (u, v) ∈ E\\nsuch that f (u, v) > 0.\\nProof\\xa0\\xa0\\xa0We ﬁrst show that a matching M in G corresponds to an integer-\\nvalued ﬂow f in G′. Deﬁne f as follows. If (u, v) ∈ M, then f (s, u) = f (u,\\nv) = f (v, t) = 1. For all other edges (u, v) ∈ E′, deﬁne f (u, v) = 0. It is\\nsimple to verify that f satisﬁes the capacity constraint and ﬂow\\nconservation.\\nIntuitively, each edge (u, v) ∈ M corresponds to 1 unit of ﬂow in G′\\nthat traverses the path s → u → v → t. Moreover, the paths induced by\\nedges in M are vertex-disjoint, except for s and t. The net ﬂow across cut', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 898}),\n",
              " Document(page_content='(L ∪ {s}, R ∪ {t}) is equal to |M|, and thus, by Lemma 24.4, the value\\nof the ﬂow is |f| = |M|.\\nTo prove the converse, let f be an integer-valued ﬂow in G′ and, as in\\nthe statement of the lemma, let\\nM = {(u, v) : u ∈ L, v ∈ R, and f (u, v) > 0}.\\nEach vertex u ∈ L has only one entering edge, namely (s, u), and its\\ncapacity is 1. Thus, each u ∈ L has at most 1 unit of ﬂow entering it,\\nand if 1 unit of ﬂow does enter, by ﬂow conservation, 1 unit of ﬂow must\\nleave. Furthermore, since the ﬂow f is integer-valued, for each u ∈ L, the\\n1 unit of ﬂow can enter on at most one edge and can leave on at most\\none edge. Thus, 1 unit of ﬂow enters u if and only if there is exactly one\\nvertex v ∈ R such that f (u, v) = 1, and at most one edge leaving each u\\n∈ L carries positive ﬂow. A symmetric argument applies to each v ∈ R.\\nThe set M is therefore a matching.\\nTo see that |M| = |f|, observe that of the edges (u, v) ∈ E′ such that u\\n∈ L and v ∈ R,\\nConsequently, f (L ∪ {s}, R ∪ {t}), the net ﬂow across cut (L ∪ {s}, R ∪\\n{t}), is equal to |M|. Lemma 24.4 gives that |f| = f (L ∪ {s}, R ∪ {t}) =\\n|M|.\\n▪\\nBased on Lemma 24.9, we would like to conclude that a maximum\\nmatching in a bipartite graph G corresponds to a maximum ﬂow in its\\ncorresponding ﬂow network G′, and therefore running a maximum-ﬂow\\nalgorithm on G′ provides a maximum matching in G. The only hitch in\\nthis reasoning is that the maximum-ﬂow algorithm might return a ﬂow\\nin G′ for which some f (u, v) is not an integer, even though the ﬂow value\\n|f| must be an integer. The following theorem shows that the Ford-\\nFulkerson method cannot produce a solution with this problem.\\nTheorem 24.10 (Integrality theorem)', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 899}),\n",
              " Document(page_content='If the capacity function c takes on only integer values, then the\\nmaximum ﬂow f produced by the Ford-Fulkerson method has the\\nproperty that |f| is an integer. Moreover, for all vertices u and v, the\\nvalue of f (u, v) is an integer.\\nProof\\xa0\\xa0\\xa0Exercise 24.3-2 asks you to provide the proof by induction on\\nthe number of iterations.\\n▪\\nWe can now prove the following corollary to Lemma 24. 9.\\nCorollary 24.11\\nThe cardinality of a maximum matching M in a bipartite graph G\\nequals the value of a maximum ﬂow f in its corresponding ﬂow network\\nG′.\\nProof\\xa0\\xa0\\xa0We use the nomenclature from Lemma 24.9. Suppose that M is a\\nmaximum matching in G and that the corresponding ﬂow f in G′ is not\\nmaximum. Then there is a maximum ﬂow f′ in G′ such that |f′| > |f|.\\nSince the capacities in G′ are integer-valued, by Theorem 24.10, we can\\nassume that f′ is integer-valued. Thus, f′ corresponds to a matching M′\\nin G with cardinality |M′| = |f′| > |f| = |M|, contradicting our assumption\\nthat M is a maximum matching. In a similar manner, we can show that\\nif f is a maximum ﬂow in G′, its corresponding matching is a maximum\\nmatching on G.\\n▪\\nThus, to ﬁnd a maximum matching in a bipartite undirected graph\\nG, create the ﬂow network G′, run the Ford-Fulkerson method on G′,\\nand convert the integer-valued maximum ﬂow found into a maximum\\nmatching for G. Since any matching in a bipartite graph has cardinality\\nat most min {|L|, |R|} = O(V), the value of the maximum ﬂow in G′ is\\nO(V). Therefore, ﬁnding a maximum matching in a bipartite graph\\ntakes O(VE′) = O(VE) time, since |E′| = Θ (E).\\nExercises\\n24.3-1', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 900}),\n",
              " Document(page_content='Run the Ford-Fulkerson algorithm on the ﬂow network in Figure\\n24.8(c) and show the residual network after each ﬂow augmentation.\\nNumber the vertices in L top to bottom from 1 to 5 and in R top to\\nbottom from 6 to 9. For each iteration, pick the augmenting path that is\\nlexicographically smallest.\\n24.3-2\\nProve Theorem 24.10. Use induction on the number of iterations of the\\nFord-Fulkerson method.\\n24.3-3\\nLet G = (V, E) be a bipartite graph with vertex partition V = L ∪ R,\\nand let G′ be its corresponding ﬂow network. Give a good upper bound\\non the length of any augmenting path found in G′ during the execution\\nof FORD-FULKERSON.\\nProblems\\n24-1\\xa0\\xa0\\xa0\\xa0\\xa0E scape problem\\nAn n×n\\xa0grid is an undirected graph consisting of n rows and n columns\\nof vertices, as shown in Figure 24.9. We denote the vertex in the ith row\\nand the j th column by (i, j). All vertices in a grid have exactly four\\nneighbors, except for the boundary vertices, which are the points (i, j)\\nfor which i = 1, i = n, j = 1, or j = n.\\nGiven m ≤ n2 starting points (x1, y1), (x2, y2), …, (xm, ym) in the\\ngrid, the escape problem is to determine whether there are m vertex-\\ndisjoint paths from the starting points to any m different points on the\\nboundary. For example, the grid in Figure 24.9(a) has an escape, but the\\ngrid in Figure 24.9(b) does not.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 901}),\n",
              " Document(page_content='Figure 24.9 Grids for the escape problem. Starting points are blue, and other grid vertices are\\ntan. (a) A grid with an escape, shown by blue paths. (b) A grid with no escape.\\na. Consider a ﬂow network in which vertices, as well as edges, have\\ncapacities. That is, the total positive ﬂow entering any given vertex is\\nsubject to a capacity constraint. Show how to reduce the problem of\\ndetermining the maximum ﬂow in a network with edge and vertex\\ncapacities to an ordinary maximum-ﬂow problem on a ﬂow network\\nof comparable size.\\nb. Describe an efﬁcient algorithm to solve the escape problem, and\\nanalyze its running time.\\n24-2\\xa0\\xa0\\xa0\\xa0\\xa0M inimum path cover\\nA path cover of a directed graph G = (V, E) is a set P of vertex-disjoint\\npaths such that every vertex in V is included in exactly one path in P.\\nPaths may start and end anywhere, and they may be of any length,\\nincluding 0. A minimum path cover of G is a path cover containing the\\nfewest possible paths.\\na. Give an efﬁcient algorithm to ﬁnd a minimum path cover of a\\ndirected acyclic graph G = (V, E). (Hint: Assuming that V = {1, 2, …,\\nn}, construct a ﬂow network based on the graph G′ = (V′, E′), where\\nV′ = {x0, x1, …, xn} ∪ {y0, y1, …, yn},\\nE′ = {(x0, xi) : i ∈ V } ∪ {(yi, y0) : i ∈ V } ∪ {(xi, yj) : (i, j) ∈ E},\\nand run a maximum-ﬂow algorithm.)', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 902}),\n",
              " Document(page_content='b. Does your algorithm work for directed graphs that contain cycles?\\nExplain.\\n24-3\\xa0\\xa0\\xa0\\xa0\\xa0H iring consulting experts\\nProfessor Fieri wants to open a consulting company for the food\\nindustry. He has identiﬁed n important food categories, which he\\nrepresents by the set C = {C1, C2, …, Cn}. In each category Ck, he can\\nhire an expert in that category for ek > 0 dollars. The consulting\\ncompany has lined up a set J = {J1, J2, …, Jm} of potential jobs. In\\norder to perform job Ji, the company needs to have hired experts in a\\nsubset Ri ⊆ C of categories. Each expert can work on multiple jobs\\nsimultaneously. If the company chooses to accept job Ji, it must have\\nhired experts in all categories in Ri, and it takes in revenue of pi > 0\\ndollars.\\nProfessor Fieri’s job is to determine which categories to hire experts\\nin and which jobs to accept in order to maximize the net revenue, which\\nis the total income from jobs accepted minus the total cost of employing\\nthe experts.\\nConsider the following ﬂow network G. It contains a source vertex s,\\nvertices C1, C2, …, Cn, vertices J1, J2, …, Jm, and a sink vertex t. For\\nk = 1, 2 …, n, the ﬂow network contains an edge (s, Ck) with capacity\\nc(s, Ck) = ek, and for i = 1, 2, …, m, the ﬂow network contains an edge\\n(Ji, t) with capacity c(Ji, t) = pi. For k = 1, 2, …, n and i = 1, 2, …, m, if\\nCk ∈ Ri, then G contains an edge (Ck, Ji) with capacity c(Ck, Ji) = ∞.\\na. Show that if Ji ∈ T for a ﬁnite-capacity cut (S, T) of G, then Ck ∈ T\\nfor each Ck ∈ Ri.\\nb. Show how to determine the maximum net revenue from the capacity\\nof a minimum cut of G and the given pi values.\\nc. Give an efﬁcient algorithm to determine which jobs to accept and\\nwhich experts to hire. Analyze the running time of your algorithm in\\nterms of m, n, and \\n .', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 903}),\n",
              " Document(page_content='24-4\\xa0\\xa0\\xa0\\xa0\\xa0U pdating maximum ﬂow\\nLet G = (V, E) be a ﬂow network with source s, sink t, and integer\\ncapacities. Suppose that you are given a maximum ﬂow in G.\\na. Suppose that the capacity of a single edge (u, v) ∈ E increases by 1.\\nGive an O(V + E)-time algorithm to update the maximum ﬂow.\\nb. Suppose that the capacity of a single edge (u, v) ∈ E decreases by 1.\\nGive an O(V + E)-time algorithm to update the maximum ﬂow.\\n24-5\\xa0\\xa0\\xa0\\xa0\\xa0M aximum ﬂow by scaling\\nLet G = (V, E) be a ﬂow network with source s, sink t, and an integer\\ncapacity c(u, v) on each edge (u, v) ∈ E. Let C = max {c(u, v) : (u, v) ∈\\nE}.\\na. Argue that a minimum cut of G has capacity at most C |E|.\\nb. For a given number K, show how to ﬁnd an augmenting path of\\ncapacity at least K in O(E) time, if such a path exists.\\nThe procedure MAX-FLOW-BY-SCALING appearing on the\\nfollowing page modiﬁes the basic FORD-FULKERSON-METHOD\\nprocedure to compute a maximum ﬂow in G.\\nc. Argue that MAX-FLOW-BY-SCALING returns a maximum ﬂow.\\nd. Show that the capacity of a minimum cut of the residual network Gf\\nis less than 2K |E| each time line 4 executes.\\ne. Argue that the inner while loop of lines 5–6 executes O(E) times for\\neach value of K.\\nMAX-FLOW-BY-SCALING (G, s, t)\\n1C = max {c(u, v) : (u, v) ∈ E}\\n2initialize ﬂow f to 0\\n3K = 2⌊lg C ⌋\\n4while\\xa0K ≥ 1\\n5 while there exists an augmenting path p of capacity at least K\\n6 augment ﬂow f along p', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 904}),\n",
              " Document(page_content='7 K = K/2\\n8return\\xa0f\\nf. Conclude that MAX-FLOW-BY-SCALING can be implemented so\\nthat it runs in O(E2 lg C) time.\\n24-6\\xa0\\xa0\\xa0\\xa0\\xa0W idest augmenting path\\nThe Edmonds-Karp algorithm implements the Ford-Fulkerson\\nalgorithm by always choosing a shortest augmenting path in the\\nresidual network. Suppose instead that the Ford-Fulkerson algorithm\\nchooses a widest augmenting path: an augmenting path with the greatest\\nresidual capacity. Assume that G = (V, E) is a ﬂow network with source\\ns and sink t, that all capacities are integer, and that the largest capacity\\nis C. In this problem, you will show that choosing a widest augmenting\\npath results in at most |E| ln |f*| augmentations to ﬁnd a maximum ﬂow\\nf*.\\na. Show how to adjust Dijkstra’s algorithm to ﬁnd the widest\\naugmenting path in the residual network.\\nb. Show that a maximum ﬂow in G can be formed by successive ﬂow\\naugmentations along at most |E| paths from s to t.\\nc. Given a ﬂow f, argue that the residual network Gf has an augmenting\\npath p with residual capacity cf (p) ≥ (|f*| − |f|)/|E|.\\nd. Assuming that each augmenting path is a widest augmenting path, let\\nfi be the ﬂow after augmenting the ﬂow by the ith augmenting path,\\nwhere f0 has f (u, v) = 0 for all edges (u, v). Show that |f*| − |fi| ≤ |f*| (1\\n− 1/|E|)i.\\ne. Show that |f* | − |fi| < |f*| e−i/|E|.\\nf. Conclude that after the ﬂow is augmented at most |E| ln |f*| times, the\\nﬂow is a maximum ﬂow.\\n24-7\\xa0\\xa0\\xa0\\xa0\\xa0G lobal minimum cut', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 905}),\n",
              " Document(page_content='A global cut in an undirected graph G = (V, E) is a partition (see page\\n1156) of V into two nonempty sets V1 and V2. This deﬁnition is like the\\ndeﬁnition of cut that we have used in this chapter, except that we no\\nlonger have distinguished vertices s and t. Any edge (u, v) with u ∈ V1\\nand v ∈ V2 is said to cross the cut.\\nWe can extend this deﬁnition of a cut to a multigraph G = (V, E) (see\\npage 1167), and we denote by c(u, v) the number of edges in the\\nmultigraph with endpoints u and v. A global cut in a multigraph is still a\\npartition of the vertices, and the value of a global cut (V1, V2) is \\n. A solution to the global-minimum-cut\\nproblem is a cut (V1, V2) such that c(V1, V2) is minimum. Let μ(G)\\ndenote the value of a global minimum cut in a gr aph or multigraph G.\\na. Show how to ﬁnd a global minimum cut of a gr aph G = (V, E) by\\nsolving \\n maximum-ﬂow problems, each with a different pair of\\nvertices as the source and sink, and taking the mininum value of the\\ncuts found.\\nb. Give an algorithm to ﬁnd a global minimum cut by solving only Θ (V)\\nmaximum-ﬂow problems. What is the running time of your algorithm?\\nThe remainder of this problem develops an algorithm for the global-\\nminimum-cut problem that does not use any maximum-ﬂow\\ncomputations. It uses the notion of an edge contraction, deﬁned on\\npage 1168, with one crucial difference. The algorithm maintains a\\nmultigraph, so that upon contracting an edge (u, v), it creates a new\\nvertex x, and for any other vertex y ∈ V, the number of edges between x\\nand y is c(u, y) + c(v, y). The algorithm does not maintain self-loops,\\nand so it sets c(x, x) to 0. Denote by G/(u, v) the multigraph that results\\nfrom contracting edge (u, v) in multigraph G.\\nConsider what can happen to the minimum cut when an edge is\\ncontracted. Assume that, at all points, the minimum cut in a multigraph\\nG is unique. We’ll remove this assumption later.\\nc. Show that for any edge (u, v), we have μ(G/(u, v)) ≤ μ(G). Under what\\nconditions is μ(G/(u, v)) < μ(G)?', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 906}),\n",
              " Document(page_content='Next, you will show that if you pick an edge uniformly at random,\\nthe probability that it belongs to the minimum cut is small.\\nd. Show that for any multigraph G = (V, E), the value of the global\\nminimum cut is at most the average degree of a vertex: that μ(G) ≤ 2\\n|E|/|V|, where |E| denotes the total number of edges in the multigraph.\\ne. Using the results from parts (c) and (d), show that, if we pick an edge\\n(u, v) uniformly at random, then the probability that (u, v) belongs to\\nthe minimum cut is at most 2/V.\\nConsider the algorithm that repeatedly chooses an edge at random\\nand contracts it until the multigraph has exactly two vertices, say u and\\nv. At that point, the multigraph corresponds to a cut in the original\\ngraph, with vertex u representing all the nodes in one side of the original\\ngraph, and v representing all the vertices on the other side. The number\\nof edges given by c(u, v) corresponds exactly to the number of edges\\ncrossing the corresponding cut in the original graph. We call this\\nalgorithm the contraction algorithm.\\nf. Suppose that the contraction algorithm terminates with a multigraph\\nwhose only vertices are u and v. Show that \\n .\\ng. Prove that if the contraction algorithm repeats \\n  times, then the\\nprobability that at least one of the runs returns the minimum cut is at\\nleast 1 − 1/|V|.\\nh. Give a detailed implementation of the contraction algorithm that\\nruns in O(V2) time.\\ni. Combine the previous parts and remove the assumption that the\\nminimum cut must be unique, to conclude that running the\\ncontraction algorithm \\n  times yields an algorithm that runs in\\nO(V4 lg V) time and returns a minimum cut with probability at least 1\\n− 1/V.\\nChapter notes', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 907}),\n",
              " Document(page_content='Ahuja, Magnanti, and Orlin [7], Even [137], Lawler [276],\\nPapadimitriou and Steiglitz [353], Tarjan [429], and Williamson [458]\\nare good references for network ﬂows and related algorithms. Schrijver\\n[399] has written an interesting review of historical developments in the\\nﬁeld of network ﬂows.\\nThe Ford-Fulkerson method is due to Ford and Fulkerson [149],\\nwho originated the formal study of many of the problems in the area of\\nnetwork ﬂow, including the maximum-ﬂow and bipartite-matching\\nproblems. Many early implementations of the Ford-Fulkerson method\\nfound augmenting paths using breadth-ﬁrst search. Edmonds and Karp\\n[132], and independently Dinic [119], proved that this strategy yields a\\npolynomial-time algorithm. A related idea, that of using “blocking\\nﬂows,” was also ﬁrst developed by Dinic [119].\\nA class of algorithms known as push-relabel algorithms, due to\\nGoldberg [185] and Goldberg and Tarjan [188], takes a different\\napproach from the Ford-Fulkerson method. Push-relabel algorithms\\nallow ﬂow conservation to be violated at vertices other than the source\\nand sink as they execute. Using an idea ﬁrst developed by Karzonov\\n[251], they allow a preﬂow in which the ﬂow into a vertex may exceed the\\nﬂow out of the vertex. Such a vertex is said to be overﬂowing. Initially,\\nevery edge leaving the source is ﬁlled to capacity, so that all neighbors of\\nthe source are overﬂowing. In a push-relabel algorithm, each vertex is\\nassigned an integer height. An overﬂowing vertex may push ﬂow to a\\nneighboring vertex to which it has a residual edge provided that it is\\nhigher than the neighbor. If all residual edges from an overﬂowing\\nvertex go to neighbors with equal or greater heights, then the vertex\\nmay increase its height. Once all vertices other than the sink are no\\nlonger overﬂowing, the preﬂow is not only a legal ﬂow, but also a\\nmaximum ﬂow.\\nGoldberg and Tarjan [188] gave an O(V3)-time algorithm that uses a\\nqueue to maintain the set of overﬂowing vertices, as well as an\\nalgorithm that uses dynamic trees to achieve a running time of O(VE\\nlg(V2/E + 2)). Several other researchers developed improved variants\\nand implementations [9, 10, 15, 86, 87, 255, 358], the fastest of which,\\nby King, Rao, and Tarjan [255], runs in O(VE logE/(V lg V)\\xa0V) time.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 908}),\n",
              " Document(page_content='Another efﬁcient algorithm for maximum ﬂow, by Goldberg and\\nRao [187], runs in O (min{V2/3, E1/2} E lg (V2/E + 2) lg C) time, where\\nC is the maximum capacity any edge. Orlin [350] gave an algorithm in\\nthe same spirit as this algorithm that runs in O(VE + E31/16 lg2\\xa0V)\\ntime. Combining it with the algorithm of King, Rao, and Tarjan results\\nin an O(VE)-time algorithm.\\nA different approach to maximum ﬂows and related problems is to\\nuse techniques from continuous optimization including electrical ﬂows\\nand interior-point methods. The ﬁrst breakthrough in this line of work\\nis due to Madry [308], who gave an Õ(E10/7)-time algorithm for unit-\\ncapacity maximum ﬂow and bipartite maximum matching. (See\\nProblem 3-6 on page 73 for a deﬁnition of Õ.) There has been a series of\\npapers in this area for matchings, maximum ﬂows, and minimum-cost\\nﬂows. The fastest algorithm to date in this line of work for maximum\\nﬂow is due to Lee and Sidford [285], taking \\n  time. If the\\ncapacities are not too large, this algorithm is faster than the O(VE)-time\\nalgorithm mentioned above. Another algorithm, due to Liu and Sidford\\n[303] runs in Õ(E11/8C1/4) time, where C is the maximum capacity of\\nany edge. This algorithm does not run in polynomial time, but for small\\nenough capacities, it is faster than the previous ones.\\nIn practice, push-relabel algorithms currently dominate algorithms\\nbased on augmenting paths, continuous-optimization, and linear\\nprogramming for the maximum-ﬂow problem [88].\\n1 Recall from Section 20.1 that we represent an attribute f for edge (u, v) with the same style of\\nnotation—(u, v).f—that we use for an attribute of any other object.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 909}),\n",
              " Document(page_content='25\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0Matchings in Bipartite Graphs\\nMany real-world problems can be modeled as ﬁnding matchings in an\\nundirected graph. For an undirected graph G = (V, E), a matching is a\\nsubset of edges M ⊆ E such that every vertex in V has at most one\\nincident edge in M.\\nFor example, consider the following scenario. You have one or more\\npositions to ﬁll and several candidates to interview. According to your\\nschedule, you are able to interview candidates at certain time slots. You\\nask the candidates to indicate the subsets of time slots at which they are\\navailable. How can you schedule the interviews so that each time slot\\nhas at most one candidate scheduled, while maximizing the number of\\ncandidates that you can interview? You can model this scenario as a\\nmatching problem on a bipartite graph in which each vertex represents\\neither a candidate or a time slot, with an edge between a candidate and\\na time slot if the candidate is available then. If an edge is included in the\\nmatching, that means you are scheduling a particular candidate for a\\nparticular time slot. Your goal is to ﬁnd a maximum matching: a\\nmatching of maximum cardinality. One of the authors of this book was\\nfaced with exactly this situation when hiring teaching assistants for a\\nlarge class. He used the Hopcroft-Karp algorithm in Section 25.1 to\\nschedule the interviews.\\nAnother application of matching is the U.S. National Resident\\nMatching Program, in which medical students are matched to hospitals\\nwhere they will be stationed as medical residents. Each student ranks\\nthe hospitals by preference, and each hospital ranks the students. The\\ngoal is to assign students to hospitals so that there is never a student', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 910}),\n",
              " Document(page_content='and a hospital that both have regrets because the student was not\\nassigned to the hospital, yet each ranked the other higher than who or\\nwhere they were assigned. This scenario is perhaps the best-known real-\\nworld example of the “stable-marriage problem,” which Section 25.2\\nexamines.\\nYet another instance where matching comes into play occurs when\\nworkers must be assigned to tasks in order to maximize the overall\\neffectiveness of the assignment. For each worker and each task, the\\nworker has some quantiﬁed effectiveness for that task. Assuming that\\nthere are equal numbers of workers and tasks, the goal is to ﬁnd a\\nmatching with the maximum total effectiveness. Such a situation is an\\nexample of an assignment problem, which Section 25.3 shows how to\\nsolve.\\nThe algorithms in this chapter ﬁnd matchings in bipartite graphs. As\\nin Section 24.3, the input is an undirected graph G = (V, E), where V =\\nL ∪ R, the vertex sets L and R are disjoint, and every edge in E is\\nincident on one vertex in L and one vertex in R. A matching, therefore,\\nmatches vertices in L with vertices in R. In some applications, the sets L\\nand R have equal cardinality, and in other applications they need not be\\nthe same size.\\nAn undirected graph need not be bipartite for the concept of\\nmatching to apply. Matching in general undirected graphs has\\napplications in areas such as scheduling and computational chemistry. It\\nmodels problems in which you want to pair up entities, represented by\\nvertices. Two vertices are adjacent if they represent compatible entities,\\nand you need to ﬁnd a large set of compatible pairs. Maximum-\\nmatching and maximum-weight matching problems on general graphs\\ncan be solved by polynomial-time algorithms whose running times are\\nsimilar to those for bipartite matching, but the algorithms are\\nsigniﬁcantly more complicated. Exercise 25.2-5 discusses the general\\nversion of the stable-marriage problem, known as the “stable-\\nroommates problem.” Although matching applies to general undirected\\ngraphs, this chapter deals only with bipartite graphs.\\n25.1\\xa0\\xa0\\xa0\\xa0Maximum bipartite matching (revisited)', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 911}),\n",
              " Document(page_content='Section 24.3 demonstrated one way to ﬁnd a maximum matching in a\\nbipartite graph, by ﬁnding a maximum ﬂow. This section provides a\\nmore efﬁcient method, the Hopcroft-Karp algorithm, which runs in \\n time. Figure 25.1(a) shows a matching in an undirected\\nbipartite graph. A vertex that has an incident edge in matching M is\\nmatched under M, and otherwise, it is unmatched. A maximal matching\\nis a matching M to which no other edges can be added, that is, for every\\nedge e ∈ E − M, the edge set M ∪ {e} fails to be a matching. A\\nmaximum matching is always maximal, but the reverse does not always\\nhold.\\nMany algorithms to ﬁnd maximum matchings, the Hopcroft-Karp\\nalgorithm included, work by incrementally increasing the size of a\\nmatching. Given a matching M in an undirected graph G = (V, E), an\\nM-alternating path is a simple path whose edges alternate between being\\nin M and being in E − M. Figure 25.1(b) depicts an M-augmenting path\\n(sometimes called an augmenting path with respect to M): an M-\\nalternating path whose ﬁrst and last edges belong to E − M. Since an\\nM-augmenting path contains one more edge in E − M than in M, it\\nmust consist of an odd number of edges.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 912}),\n",
              " Document(page_content='Figure 25.1 A bipartite graph, where V = L ∪ R, L = {l1, l2, … , l7}, and R = {r1, r2, … , r8}.\\n(a) A matching M with cardinality 4, highlighted in blue. Matched vertices are blue, and\\nunmatched vertices are tan. (b) The ﬁve edges highlighted in orange form an M-augmenting\\npath P going between vertices l6 and r8. (c) The set of edges M′ = M ⊕ P highlighted in blue is a\\nmatching containing one more edge than M and adding l6 and r8 to the matched vertices. This\\nmatching is not a maximum matching (see Exercise 25.1-1).\\nFigure 25.1(c) demonstrates the following lemma, which shows that\\nby removing from matching M the edges in an M-augmenting path that\\nbelong to M and adding to M the edges in the M-augmenting path that\\nare not in M, the result is a new matching with one more edge than M.\\nSince a matching is a set of edges, the lemma relies on the notion of the\\nsymmetric difference of two sets: X ⊕ Y = (X − Y) ∪ (Y − X), that is, the\\nelements that belong to X or Y, but not both. Alternatively, you can\\nthink of X ⊕ Y as (X ∪ Y)−(X ∩ Y). The operator ⊕  is commutative\\nand associative. Furthermore, X ⊕ X = Ø and X ⊕ Ø = Ø ⊕  X = X for\\nany set X, so that the empty set is the identity for ⊕ .\\nLemma 25.1\\nLet M be a matching in any undirected graph G = (V, E), and let P be\\nan M-augmenting path. Then the set of edges M′ = M ⊕ P is also a', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 913}),\n",
              " Document(page_content='matching in G with |M′| = |M| + 1.\\nProof\\xa0\\xa0\\xa0Let P contain q edges, so that ⌈q/2 ⌉ edges belong to E − M and\\n⌊q/2 ⌋ edges belong to M, and let these q edges be (v1, v2), (v2, v3), … ,\\n(vq, vq+1). Because P is an M-augmenting path, vertices v1 and vq+1\\nare unmatched under M and all other vertices in P are matched. Edges\\n(v1, v2), (v3, v4), … , (vq, vq+1) belong to E − M, and edges (v2, v3),\\n(v4, v5), … , (vq−1, vq) belong to M. The symmetric difference M′ = M\\n⊕ P reverses these roles, so that edges (v1, v2), (v3, v4), … , (vq, vq+1)\\nbelong to M′ and (v2, v3), (v4, v5), … , (vq−1, vq) belong to E − M′.\\nEach vertex v1, v2, … , vq, vq+1 is matched under M′, which gains one\\nadditional edge relative to M, and no other vertices or edges in G are\\naffected by the change from M to M′. Hence, M′ is a matching in G, and\\n|M′| = |M| + 1.\\n▪\\nSince taking the symmetric difference of a matching M with an M-\\naugmenting path increases the size of the matching by 1, the following\\ncorollary shows that taking the symmetric difference of M with k\\nvertex-disjoint M-augmenting paths increases the size of the matching\\nby k.\\nCorollary 25.2\\nLet M be a matching in any undirected graph G = (V, E) and P1, P2, …\\n, Pk be vertex-disjoint M-augmenting paths. Then the set of edges M′ =\\nM ⊕ (P1 ∪ P2 ∪ … ∪  Pk) is a matching in G with |M′| = |M| + k.\\nProof\\xa0\\xa0\\xa0Since the M-augmenting paths P1, P2, … , Pk are vertex-\\ndisjoint, we have that P1 ∪ P2 ∪ ⋯ ∪ Pk = P1 ⊕ P2 ⊕ ⋯ ⊕ Pk. Because\\nthe operator ⊕  is associative, we have\\nM ⊕ (P1 ∪ P2 ∪ ⋯ ∪\\nPk)=M ⊕ (P1 ⊕ P2 ⊕ ⋯ ⊕ Pk)\\n=', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 914}),\n",
              " Document(page_content='( ⋯ ((M ⊕ P1) ⊕ P2) ⊕ ⋯ ⊕ Pk−1) ⊕\\nPk.\\nA simple induction on i using Lemma 25.1 shows that M ⊕ (P1 ∪ P2 ∪\\n⋯ ∪ Pi−1) is a matching in G containing |M| + i − 1 edges and that\\npath Pi is an augmenting path with respect to M ⊕ (P1 ∪ P2 ∪ ⋯ ∪\\nPi−1). Each of these augmenting paths increases the size of the\\nmatching by 1, and so |M′| = |M| + k.\\n▪\\nAs the Hopcroft-Karp algorithm goes from matching to matching, it\\nwill be useful to consider the symmetric difference between two\\nmatchings.\\nLemma 25.3\\nLet M and M* be matchings in graph G = (V, E), and consider the\\ngraph G′ = (V, E′), where E′ = M ⊕ M*. Then, G′ is a disjoint union of\\nsimple paths, simple cycles, and/or isolated vertices. The edges in each\\nsuch simple path or simple cycle alternate between M and M*. If |M*| >\\n|M|, then G′ contains at least |M*|−|M| vertex-disjoint M-augmenting\\npaths.\\nProof\\xa0\\xa0\\xa0Each vertex in G′ has degree 0, 1, or 2, since at most two edges of\\nE′ can be incident on a vertex: at most one edge from M and at most\\none edge from M*. Therefore, each connected component of G′ is either\\na singleton vertex, an even-length simple cycle with edges alternately in\\nM and M*, or a simple path with edges alternately in M and M*. Since\\nE′=M ⊕ M*\\n=(M ∪ M*) − (M ∩ M*)\\nand |M*| > |M|, the edge set E′ must contain |M*| − |M| more edges\\nfrom M* than from M. Because each cycle in G′ has an even number of\\nedges drawn alternately from M and M*, each cycle has an equal\\nnumber of edges from M and M*. Therefore, the simple paths in G′\\naccount for there being |M*| − |M| more edges from M* than M. Each', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 915}),\n",
              " Document(page_content='path containing a different number of edges from M and M* either\\nstarts and ends with edges from M, containing one more edge from M\\nthan from M*, or starts and ends with edges from M*, containing one\\nmore edge from M* than from M. Because E′ contains |M*| − |M| more\\nedges from M* than from M, there are at least |M*| − |M| paths of the\\nlatter type, and each one is an M-augmenting path. Because each vertex\\nhas at most two incident edges from E′, these paths must be vertex-\\ndisjoint.\\n▪\\nIf an algorithm ﬁnds a maximum matching by incrementally\\nincreasing the size of the matching, how does it determine when to stop?\\nThe following corollary gives the answer: when there are no augmenting\\npaths.\\nCorollary 25.4\\nMatching M in graph G = (V, E) is a maximum matching if and only if\\nG contains no M-augmenting path.\\nProof\\xa0\\xa0\\xa0We prove the contrapositive of both directions of the lemma\\nstatement. The contrapositive of the forward direction is\\nstraightforward. If there is an M-augmenting path P in G, then by\\nLemma 25.1, the matching M ⊕ P contains one more edge than M,\\nmeaning that M could not be a maximum matching.\\nTo show the contrapositive of the backward direction—if M is not a\\nmaximum matching, then G contains an M-augmenting path—let M*\\nbe a maximum matching in Lemma 25.3, so that |M*| > |M|. Then G\\ncontains at least |M*| − |M| > 0 vertex-disjoint M-augmenting paths.\\n▪\\nWe already have learned enough to create a maximum-matching\\nalgorithm that runs in O(VE) time. Start with the matching M empty.\\nThen repeatedly run a variant of either breadth-ﬁrst search or depth-\\nﬁrst search from an unmatched vertex that takes alternating paths until\\nyou ﬁnd another unmatched vertex. Use the resulting M-augmenting\\npath to increase the size of M by 1.\\nThe Hopcroft-Karp algorithm', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 916}),\n",
              " Document(page_content='The Hopcroft-Karp algorithm improves the running time to \\n .\\nThe procedure HOPCROFT-KARP is given an undirected bipartite\\ngraph, and it uses Corollary 25.2 to repeatedly increase the size of the\\nmatching M it ﬁnds. Corollary 25.4 proves that the algorithm is correct,\\nsince it terminates once there are no M-augmenting paths. It remains to\\nshow that the algorithm does run in \\n  time. We’ll see that the\\nrepeat loop of lines 2–5 iterates \\n  times and how to implement line\\n3 so that it runs in O(E) time in each iteration.\\nHOPCROFT-KARP (G)\\n1M = Ø\\n2repeat\\n3 let P = {P1, P2, … , Pk} be a maximal set of vertex-disjoint\\nshortest M-augmenting paths\\n4 M = M ⊕ (P1 ∪ P2 ∪ ⋯ ∪ Pk)\\n5until\\xa0 P == Ø\\n6return\\xa0M\\nLet’s ﬁrst see how to ﬁnd a maximal set of vertex-disjoint shortest\\nM-augmenting paths in O(E) time. There are three phases. The ﬁrst\\nphase forms a directed version GM of the undirected bipartite graph G.\\nThe second phase creates a directed acyclic graph H from GM via a\\nvariant of breadth-ﬁrst search. The third phase ﬁnds a maximal set of\\nvertex-disjoint shortest M-augmenting paths by running a variant of\\ndepth-ﬁrst search on the transpose HT of H. (Recall that the transpose\\nof a directed graph reverses the direction of each edge. Since H is\\nacyclic, so is HT.)\\nGiven a matching M, you can think of an M-augmenting path P as\\nstarting at an unmatched vertex in L, traversing an odd number of\\nedges, and ending at an unmatched vertex in R. The edges in P traversed\\nfrom L to R must belong to E − M, and the edges in P traversed from R\\nto L must belong to M. The ﬁrst phase, therefore, creates the directed\\ngraph GM by directing the edges accordingly: GM = (V, EM), where', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 917}),\n",
              " Document(page_content='Figure 25.2 (a) The directed graph GM created in the ﬁrst phase for the undirected bipartite\\ngraph G and matching M in Figure 25.1(a). Breadth-ﬁrst distances from any unmatched vertex\\nin L appear next to each vertex. (b) The dag H created from GM in the second phase. Because\\nthe smallest distance to an unmatched vertex in R is 3, vertices l7 and r8, with distances greater\\nthan 3, are not in H.\\nEM={(l, r) : l ∈ L, r ∈ R, and (l, r) ∈ E − M }(edges from L to R)\\n∪ {(r, l) : r ∈ R, l ∈ L, and (l, r) ∈ M } (edges from R to L).\\nFigure 25.2(a) shows the graph GM for the graph G and matching M in\\nFigure 25.1(a).\\nThe dag H = (VH, EH) created by the second phase has layers of\\nvertices. Figure 25.2(b) shows the dag H corresponding to the directed\\ngraph GM in part (a) of the ﬁgure. Each layer contains only vertices\\nfrom L or only vertices from R, alternating from layer to layer. The\\nlayer that a vertex resides in is given by that vertex’s minimum breadth-', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 918}),\n",
              " Document(page_content='ﬁrst distance in GM from any unmatched vertex in L. Vertices in L\\nappear in even-numbered layers, and vertices in R appear in odd-\\nnumbered layers. Let q denote the smallest distance in GM of any\\nunmatched vertex in R. Then, the last layer in H contains the vertices in\\nR with distance q. Vertices whose distance exceeds q do not appear in\\nVH. (The graph H in Figure 25.2(b) omits vertices l7 and r8 because\\ntheir distances from any unmatched vertex in L exceed q = 3.) The edges\\nin EH form a subset of EM:\\nEH = {(l, r) ∈ EM : r.d ≤ q and r.d = l.d + 1} ∪  {(r, l) ∈ EM : l.d ≤ q},\\nwhere the attribute d of a vertex gives the vertex’s breadth-ﬁrst distance\\nin GM from any unmatched vertex in L. Edges that do not go between\\ntwo consecutive layers are omitted from EH.\\nTo determine the breadth-ﬁrst distances of vertices, run breadth-ﬁrst\\nsearch on the graph GM, but starting from all the unmatched vertices in\\nL. (In the BFS procedure on page 556, replace the root vertex s by the\\nset of unmatched vertices in L.) The predecessor attributes π computed\\nby the BFS procedure are not needed here, since H is a dag and not\\nnecessarily a tree.\\nEvery path in H from a vertex in layer 0 to an unmatched vertex in\\nlayer q corresponds to a shortest M-augmenting path in the original\\nbipartite graph G. Just use the undirected versions of the directed edges\\nin H. Moreover, every shortest M-augmenting path in G is present in H.\\nThe third phase identiﬁes a maximal set of vertex-disjoint shortest\\nM-augmenting paths. As Figure 25.3 shows, it starts by creating the\\ntranspose HT of H. Then, for each unmatched vertex r in layer q, it\\nperforms a depth-ﬁrst search starting from r until it either reaches a\\nvertex in layer 0 or has exhausted all possible paths without reaching a\\nvertex in layer 0. Instead of maintaining discovery and ﬁnish times, the\\ndepth-ﬁrst search just needs to keep track of the predecessor attributes π\\nin the depth-ﬁrst tree of each search. Upon reaching a vertex in layer 0,\\ntracing back along the predecessors identiﬁes an M-augmenting path.\\nEach vertex is searched from only when it is ﬁrst discovered in any\\nsearch. If the search from a vertex r in layer q cannot ﬁnd a path of', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 919}),\n",
              " Document(page_content='undiscovered vertices to an undiscovered vertex in layer 0, then no M-\\naugmenting path including r goes into the maximal set.\\nFigure 25.3 shows the result of the third phase. The ﬁrst depth-ﬁrst\\nsearch starts from vertex r1. It identiﬁes the M-augmenting path 〈(r1,\\nl3), (l3, r3), (r3, l1)〉, which is highlighted in orange, and discovers\\nvertices r1, l3, r3, and l1. The next depth-ﬁrst search starts from vertex\\nr4. This search ﬁrst examines the edge (r4, l3), but because l3 was\\nalready discovered, it backtracks and examines edge (r4, l5). From\\nthere, it continues and identiﬁes the M-augmenting path 〈(r4, l5), (l5,\\nr7), (r7, l6)〉, which is highlighted in yellow, and discovers vertices r4, l5,\\nr7, and l6. The depth-ﬁrst search from vertex r6 gets stuck at vertices l3\\nand l5, which have already been discovered, and so this search fails to\\nﬁnd a path of undiscovered vertices to a vertex in layer 0. There is no\\ndepth-ﬁrst search from vertex r5 because it is matched, and depth-ﬁrst\\nsearches start from unmatched vertices. Therefore, the maximal set of\\nvertex-disjoint shortest M-augmenting paths found contains just the\\ntwo M-augmenting paths (〈r1, l3), (l3, r3), (r3, l1)〉 and 〈(r4, l5), (l5, r7),\\n(r7, l6)〉.\\nYou might have noticed that in this example, this maximal set of two\\nvertex-disjoint shortest M-augmenting paths is not a maximum set. The\\ngraph contains three vertex-disjoint shortest M-augmenting paths: 〈(r1,\\nl2), (l2, r2), (r2, l1)〉, 〈(r4, l3), (l3, r3), (r3, l4)〉, and 〈(r6, l5), (l5, r7), (r7,\\nl6)〉. No matter: the algorithm requires the set of vertex-disjoint shortest\\nM-augmenting paths found in line 3 of HOPCROFT-KARP to be only\\nmaximal, not necessarily maximum.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 920}),\n",
              " Document(page_content='Figure 25.3 The transpose HT of the dag H created in the third phase. The ﬁrst depth-ﬁrst\\nsearch, starting from vertex r1, identiﬁes the M-augmenting path 〈(r1, l3), (l3, r3), (r3, l1)〉\\nhighlighted in orange, and it discovers vertices r1, l3, r3, l1. The second depth-ﬁrst search,\\nstarting from vertex r4, identiﬁes the M-augmenting path 〈(r4, l5), (l5, r7), (r7, l6)〉 highlighted\\nin yellow, discovering vertices r4, l5, r7, l6.\\nIt remains to show that all three phases of line 3 take O(E) time. We\\nassume that in the original bipartite graph G, each vertex has at least\\none incident edge so that |V| = O(E), which in turn implies that |V| + |E|\\n= O(E). The ﬁrst phase creates the directed graph GM by simply\\ndirecting each edge of G, so that |VM| = |V| and |EM = |E|. The second\\nphase performs a breadth-ﬁrst search on GM, taking O(VM + EM) =\\nO(EM) = O(E) time. In fact, it can stop once the ﬁrst distance in the\\nqueue within the breadth-ﬁrst search exceeds the shortest distance q to\\nan unmatched vertex in R. The dag H has |VH| ≤ |VM| and |EH| ≤ |EM|,\\nso that it takes O(VH + EH) = O(E) time to construct. Finally, the third\\nphase performs depth-ﬁrst searches from the unmatched vertices in\\nlayer q. Once a vertex is discovered, it is not searched from again, and so\\nthe analysis of depth-ﬁrst search from Section 20.3 applies here: O(VH\\n+ EH) = O(E). Hence, all three phases take just O(E) time.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 921}),\n",
              " Document(page_content='Once the maximal set of vertex-disjoint shortest M-augmenting\\npaths have been found in line 3, updating the matching in line 4 takes\\nO(E) time, as it is just a matter of going through the edges of the M-\\naugmenting paths and adding edges to and removing edges from the\\nmatching M. Thus, each iteration of the repeat loop of lines 2–5 can run\\nin O(E) time.\\nIt remains to show that the repeat loop iterates \\n  times. We\\nstart with the following lemma, which shows that after each iteration of\\nthe repeat loop, the length of an augmenting path increases.\\nLemma 25.5\\nLet G = (V, E) be an undirected bipartite graph with matching M, and\\nlet q be the length of a shortest M-augmenting path. Let P = {P1, P2,\\n… , Pk} be a maximal set of vertex-disjoint M-augmenting paths of\\nlength q. Let M′ = M ⊕ (P1 ∪ P2 ∪ ⋯ ∪ Pk), and suppose that P is a\\nshortest M′-augmenting path. Then P has more than q edges.\\nProof\\xa0\\xa0\\xa0We consider separately the cases in which P is vertex-disjoint\\nfrom the augmenting paths in P and in which it is not vertex-disjoint.\\nFirst, assume that P is vertex-disjoint from the augmenting paths in\\nP. Then, P contains edges that are in M but are not in any of P1, P2, …\\n, Pk, so that P is also an M-augmenting path. Since P is disjoint from\\nP1, P2, … , Pk but is also an M-augmenting path, and since P is a\\nmaximal set of shortest M-augmenting paths, P must be longer than\\nany of the augmenting paths in P, each of which has length q.\\nTherefore, P has more than q edges.\\nNow, assume that P visits at least one vertex from the M-augmenting\\npaths in P. By Corollary 25.2, M ′ is a matching in G with |M′| = |M| +\\nk. Since P is an M′-augmenting path, by Lemma 25.1, M′ ⊕ P is a\\nmatching with |M′ ⊕ P| = |M′| + 1 = |M| + k + 1. Now let A = M ⊕ M′\\n⊕ P. We claim that A = (P1 ∪ P2 ∪ ⋯ ∪ Pk) ⊕ P:\\nA=M ⊕ M′ ⊕ P\\n=', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 922}),\n",
              " Document(page_content='M ⊕ (M ⊕  (P1 ∪ P2 ∪ ⋯ ∪ Pk)) ⊕ P\\n=(M ⊕ M) ⊕ (P1 ∪ P2 ∪ ⋯ ∪ Pk) ⊕ P(associativity of ⊕ )\\n=Ø ⊕ (P1 ∪ P2 ∪ ⋯ ∪ Pk) ⊕ P (X ⊕ X = Ø for all X)\\n=(P1 ∪ P2 ∪ ⋯ ∪ Pk) ⊕ P (Ø ⊕  X = X for all X).\\nLemma 25.3 with M* = M′ ⊕ P gives that A contains at least |M′ ⊕ P|\\n− |M| = k + 1 vertex-disjoint M-augmenting paths. Since each such M-\\naugmenting path has at least q edges, we have |A| ≥ (k + 1)q = kq + q.\\nNow we claim that P shares at least one edge with some M-\\naugmenting path in P. Under the matching M′, every vertex in each M-\\naugmenting path in P is matched. (Only the ﬁrst and last vertex in each\\nM-augmenting path Pi is unmatched under M, and under M ⊕ Pi, all\\nvertices in Pi are matched. Because the M-augmenting paths in P are\\nvertex-disjoint, no other path in P can affect whether the vertices in Pi\\nare matched. That is, the vertices in Pi are matched under (M ⊕ Pi) ⊕\\nPj if and only if they are matched under M ⊕ Pi, for any other path Pj\\n∈ P.) Suppose that P shares a vertex v with some path Pi ∈ P. Vertex v\\ncannot be an endpoint of P, because the endpoints of P are unmatched\\nunder M′. Therefore, v has an incident edge in P that belongs to M′.\\nSince any vertex has at most one incident edge in a matching, this edge\\nmust also belong to Pi, thus proving the claim.\\nBecause A = (P1 ∪ P2 ∪ ⋯ ∪ Pk) ⊕ P and P shares at least one\\nedge with some Pi ∈ P, we have that |A| < |P1 ∪ P2 ∪ ⋯ ∪ Pk| + |P|.\\nThus, we have\\nkq + q≤|A|\\n<|P1 ∪ P2 ∪ ⋯ ∪ Pk| + |P|\\n=kq + |P|,\\nso that q < |P|. We conclude that P contains more than q edges.\\n▪', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 923}),\n",
              " Document(page_content='The next lemma bounds the size of a maximum matching, based on\\nthe length of a shortest augmenting path.\\nLemma 25.6\\nLet M be a matching in graph G = (V, E), and let a shortest M-\\naugmenting path in G contain q edges. Then the size of a maximum\\nmatching in G is at most |M| + |V| / (q + 1).\\nProof\\xa0\\xa0\\xa0Let M* be a maximum matching in G. By Lemma 25.3, G\\ncontains at least |M*| − |M| vertex-disjoint M-augmenting paths. Each\\nof these paths contains at least q edges, and hence at least q + 1 vertices.\\nBecause these paths are vertex-disjoint, we have (|M*|−|M|)(q+1) ≤ |V|,\\nso that |M*| ≤ |M| + |V|/(q+1).\\n▪\\nThe ﬁnal lemma bounds the number of iterations of the repeat loop\\nof lines 2–5.\\nLemma 25.7\\nWhen the HOPCROFT-KARP procedure runs on an undirected\\nbipartite graph G = (V, E), the repeat loop of lines 2–5 iterates \\ntimes.\\nProof\\xa0\\xa0\\xa0By Lemma 25.5, the length q of the shortest M-augmenting\\npaths found in line 3 increases from iteration to iteration. After \\niterations, therefore, we must have \\n . Consider the situation\\nafter the ﬁrst time line 4 executes with M-augmenting paths whose\\nlength is at least \\n . Since the size of a matching increases by at\\nleast one edge per iteration, Lemma 25.6 implies that the number of\\nadditional iterations before achieving a m aximum matching is at most\\nHence, the total number of loop iterations is less than \\n .\\n▪', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 924}),\n",
              " Document(page_content='Thus, we have the following bound on the running time of the\\nHOPCROFT-KARP procedure.\\nTheorem 25.8\\nThe procedure HOPCROFT-KARP runs in \\n  time on an\\nundirected bipartite graph G = (V, E).\\nProof\\xa0\\xa0\\xa0By Lemma 25.7 the repeat loop iterates \\n  times, and we\\nhave seen how to implement each iteration in O(E) time.\\n▪\\nExercises\\n25.1-1\\nUse the Hopcroft-Karp algorithm to ﬁnd a maximum matching for the\\ngraph in Figure 25.1.\\n25.1-2\\nHow are M-augmenting paths and augmenting paths in ﬂow networks\\nsimilar? How do they differ?\\n25.1-3\\nWhat is the advantage of searching in the transpose HT from\\nunmatched vertices in layer q (the ﬁrst layer that contains an unmatched\\nvertex in R) to layer 0 versus searching in the dag H from layer 0 to\\nlayer q?\\n25.1-4\\nShow how to bound the number of iterations of the the repeat loop of\\nlines 2–5 of HOPCROFT-KARP by \\n .\\n★ 25.1-5\\nA perfect matching is a matching under which every vertex is matched.\\nLet G = (V, E) be an undirected bipartite graph with vertex partition V\\n= L ∪ R, where |L| = |R|. For any X ⊆ V, deﬁne the neighborhood of X\\nas', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 925}),\n",
              " Document(page_content='N(X) = {y ∈ V : (x, y) ∈ E for some x ∈ X},\\nthat is, the set of vertices adjacent to some member of X. Prove Hall’s\\ntheorem: there exists a perfect matching in G if and only if |A| ≤ |N(A)|\\nfor every subset A ⊆ L.\\n25.1-6\\nIn a d-regular graph, every vertex has degree d. If G = (V, E) is bipartite\\nwith vertex partition V = L ∪ R and also d-regular, then |L| = |R|. Use\\nHall’s theorem (see Exercise 25.1-5) to prove that every d-regular\\nbipartite graph contains a perfect matching. Then use that result to\\nprove that every d-regular bipartite graph contains d disjoint perfect\\nmatchings.\\n25.2\\xa0\\xa0\\xa0\\xa0The stable-marriage problem\\nIn Section 25.1, the goal was to ﬁnd a maximum matching in an\\nundirected bipartite graph. If you know that the graph G = (V, E) with\\nvertex partition V = L ∪ R is a complete bipartite graph1—containing\\nan edge from every vertex in L to every vertex in R—then you can ﬁnd a\\nmaximum matching by a simple greedy algorithm.\\nWhen a graph can have several matchings, you might want to decide\\nwhich matchings are most desirable. In Section 25.3, we’ll add weights\\nto the edges and ﬁnd a matching of maximum weight. In this section,\\nwe will instead add some information to each vertex in a complete\\nbipartite graph: a ranking of the vertices in the other side. That is, each\\nvertex in L has an ordered list of all the vertices in R, and vice-versa. To\\nkeep things simple, let’s assume that L and R each contain n vertices.\\nThe goal here is to match each vertex in L with a vertex in R in a\\n“stable” way.\\nThis problem derives its name, the stable-marriage problem, from the\\nnotion of heterosexual marriage, viewing L as a set of women and R as\\na set of men.2 Each woman ranks all the men in terms of desirability,\\nand each man does the same with all the women. The goal is to pair up\\nwomen and men (a matching) so that if a woman and a man are not', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 926}),\n",
              " Document(page_content='matched to each other, then at least one of them prefers their assigned\\npartner.\\nIf a woman and a man are not matched to each other but each\\nprefers the other over their assigned partner, they form a blocking pair.\\nA blocking pair has incentive to opt out of the assigned pairing and get\\ntogether on their own. If that were to occur, then this pair would block\\nthe matching from being “stable.” A stable matching, therefore, is a\\nmatching that has no blocking pair. If there is a blocking pair, then the\\nmatching is unstable.\\nLet’s look at an example with four women—W anda, Emma, Lacey,\\nand Karen—and four men—Oscar, Davis, Brent, and Hank—having\\nthe following preferences:\\nWanda:Brent, Hank, Oscar, Davis\\nEmma:Davis, Hank, Oscar, Brent\\nLacey:Brent, Davis, Hank, Oscar\\nKaren:Brent, Hank, Davis, Oscar\\nOscar:Wanda, Karen, Lacey, Emma\\nDavis:Wanda, Lacey, Karen, Emma\\nBrent:Lacey, Karen, Wanda, Emma\\nHank:Lacey, Wanda, Emma, Karen\\nA stable matching comprises the following pairs:\\nLacey and Brent\\nWanda and Hank\\nKaren and Davis\\nEmma and Oscar\\nYou can verify that this matching has no blocking pair. For example,\\neven though Karen prefers Brent and Hank to her partner Davis, Brent\\nprefers his partner Lacey to Karen, and Hank prefers his partner\\nWanda to Karen, so that neither Karen and Brent nor Karen and Hank\\nform a blocking pair. In fact, this stable matching is unique. Suppose\\ninstead that the last two pairs were\\nEmma and Davis', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 927}),\n",
              " Document(page_content='Karen and Oscar\\nThen Karen and Davis would be a blocking pair, because they were not\\npaired together, Karen prefers Davis to Oscar, and Davis prefers Karen\\nto Emma. Therefore, this matching is not stable.\\nStable matchings need not be unique. For example, suppose that\\nthere are three women—Monica, Phoebe, and Rachel—an d three men\\n—Chandler, Joey, and Ross—with these preferences:\\nMonica:Chandler, Joey, Ross\\nPhoebe:Joey, Ross, Chandler\\nRachel:Ross, Chandler, Joey\\nChandler:\\xa0Phoebe, Rachel, Monica\\nJoey: Rachel, Monica, Phoebe\\nRoss: Monica, Phoebe, Rachel\\nIn this case, there are three stable matchings:\\nMatching 1 Matching 2 Matching 3\\nMonica and ChandlerPhoebe and ChandlerRachel and Chandler\\nPhoebe and Joey Rachel and Joey Monica and Joey\\nRachel and Ross Monica and Ross Phoebe and Ross\\nIn matching 1, all women get their ﬁrst choice and all men get their last\\nchoice. Matching 2 is the opposite, with all men getting their ﬁrst choice\\nand all women getting their last choice. When all the women or all the\\nmen get their ﬁrst choice, there plainly cannot be a blocking pair. In\\nmatching 3, everyone gets their second choice. You can verify that there\\nare no blocking pairs.\\nYou might wonder whether it is always possible to come up with a\\nstable matching no matter what rankings each participant provides. The\\nanswer is yes. (Exercise 25.2-3 asks you to show that even in the scenario\\nof the National Resident Matching Program, where each hospital takes\\non multiple students, it is always possible to devise a stable assignment.)\\nA simple algorithm known as the Gale-Shapley algorithm always ﬁnds\\na stable matching. The algorithm has two variants, which mirror each', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 928}),\n",
              " Document(page_content='other: “woman-oriented” and “man-oriented.” Let’s examine the\\nwoman-oriented version. Each participant is either “free” or “engaged.”\\nEveryone starts out free. Engagements occur when a free woman\\nproposes to a man. When a man is ﬁrst proposed to, he goes from free\\nto engaged, and he always stays engaged, though not necessarily to the\\nsame woman. If an engaged man receives a proposal from a woman\\nwhom he prefers to the woman he’s currently engaged to, that\\nengagement is broken, the woman to whom he had been engaged\\nbecomes free, and the man and the woman whom he prefers become\\nengaged. Each woman proposes to the men in her preference list, in\\norder, until the last time she becomes engaged. When a woman is\\nengaged, she temporarily stops proposing, but if she becomes free again,\\nshe continues down her list. Once everyone is engaged, the algorithm\\nterminates. The procedure GALE-SHAPLEY on the next page makes\\nthis process more concrete. The procedure allows for some choice: any\\nfree woman may be selected in line 2. We’ll see that the procedure\\nproduces a stable matching regardless of the order in which line 2\\nchooses free women. For the man-oriented version, just reverse the roles\\nof men and women in the procedure.\\nLet’s see how the GALE-SHAPLEY procedure executes on the\\nexample with Wanda, Emma, Lacey, Karen, Oscar, Davis, Brent, and\\nHank. After everyone is initialized to free, here is one possible version\\nof what can occur in successive iterations of the while loop of lines 2–9:\\n1. Wanda proposes to Brent. Brent is free, so that Wanda and\\nBrent become engaged and no longer free.\\n2. Emma proposes to Davis. Davis is free, so that Emma and Davis\\nbecome engaged and no longer free.\\n3. Lacey proposes to Brent. Brent is engaged to Wanda, but he\\nprefers Lacey. Brent breaks the engagement to Wanda, who\\nbecomes free. Lacey and Brent become engaged, with Lacey no\\nlonger free.\\n4. Karen proposes to Brent. Brent is engaged to Lacey, whom he\\nprefers to Karen. Brent rejects Karen, who remains free.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 929}),\n",
              " Document(page_content='5. Karen proposes to Hank. Hank is free, so that Karen and Hank\\nbecome engaged and no longer free.\\n6. Wanda proposes to Hank. Hank is engaged to Karen, but he\\nprefers Wanda. Hank breaks the engagement with Karen, who\\nbecomes free. Wanda and Hank become engaged, with Wanda\\nno longer free.\\n7. Karen proposes to Davis. Davis is engaged to Emma, but he\\nprefers Karen. Davis breaks the engagement to Emma, who\\nbecomes free. Karen and Davis become engaged, with Karen no\\nlonger free.\\n8. Emma proposes to Hank. Hank is engaged to Wanda, whom he\\nprefers to Emma. Hank rejects Emma, who remains free.\\n9. Emma proposes to Oscar. Oscar is free, so that Emma and Oscar\\nbecome engaged and no longer free.\\nGALE-SHAPLEY (men, women, rankings)\\n\\xa0\\xa01assign each woman and man as free\\n\\xa0\\xa02while some woman w is free\\n\\xa0\\xa03let m be the ﬁrst man on w’s ranked list to whom she has not\\nproposed\\n\\xa0\\xa04if\\xa0m is free\\n\\xa0\\xa05 w and m become engaged to each other (and not free)\\n\\xa0\\xa06elseif\\xa0m ranks w higher than the woman w′ he is currently\\nengaged to\\n\\xa0\\xa07 m breaks the engagement to w′, who becomes free\\n\\xa0\\xa08 w and m become engaged to each other (and not free)\\n\\xa0\\xa09else\\xa0m rejects w, with w remaining free\\n10return the stable matching consisting of the engaged pairs\\nAt this point, everyone is engaged and nobody is free, so the while loop\\nterminates. The procedure returns the stable matching we saw earlier.\\nThe following theorem shows that not only does GALE-SHAPLEY\\nterminate, but that it always returns a stable matching, thereby proving\\nthat a stable matching always exists.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 930}),\n",
              " Document(page_content='Theorem 25.9\\nThe procedure GALE-SHAPLEY always terminates and returns a\\nstable matching.\\nProof\\xa0\\xa0\\xa0Let’s ﬁrst show that the while loop of lines 2–9 always\\nterminates, so that the procedure terminates. The proof is by\\ncontradiction. If the loop fails to terminate, it is because some woman\\nremains free. In order for a woman to remain free, she must have\\nproposed to all the men and been rejected by each one. In order for a\\nman to reject a woman, he must be already engaged. Therefore, all the\\nmen are engaged. Once engaged, a man stays engaged (though not\\nnecessarily to the same woman). There are an equal number n of women\\nand men, however, which means that every woman is engaged, leading\\nto the contradiction that no women are free. We must also show that the\\nwhile loop makes a bounded number of iterations. Since each of the n\\nwomen goes through her ranking of the n men in order, possibly not\\nreaching the end of her list, the total number of iterations is at most n2.\\nTherefore, the while loop always terminates, and the procedure returns a\\nmatching.\\nWe need to show that there are no blocking pairs. We ﬁrst observe\\nthat once a man m is engaged to a woman w, all subsequent actions for\\nm occur in lines 6–8. Therefore, once a man is engaged, he stays\\nengaged, and any time he breaks an engagement to a woman w, it’s for a\\nwoman whom he prefers to w. Suppose that a woman w is matched with\\na man m, but she prefers man m′. We’ll show that w and m′ is not a\\nblocking pair, because m′ does not prefer w to his partner. Because w\\nranks m′ higher than m, she must have proposed to m′ before proposing\\nto m, and m′ either rejected her proposal or accepted it and later broke\\nthe engagement. If m′ rejected the proposal from w, it is because he was\\nalready engaged to some woman he prefers to w. If m′ accepted and\\nlater broke the engagement, he was at some point engaged to w but later\\naccepted a proposal from a woman he prefers to w. In either case, he\\nultimately ends up with a partner whom he prefers to w. We conclude\\nthat even though w might prefer m′ to her partner m, it is not also the\\ncase that m′ prefers w to his partner. Therefore, the procedure returns a\\nmatching containing no blocking pairs.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 931}),\n",
              " Document(page_content='▪\\nExercise 25.2-1 asks you to provide the proof of the following\\ncorollary.\\nCorollary 25.10\\nGiven preference rankings for n women and n men, the Gale-Shapley\\nalgorithm can be implemented to run in O(n2) time.\\n▪\\nBecause line 2 can choose any free woman, you might wonder\\nwhether different choices can produce different stable matchings. The\\nanswer is no: as the following theorem shows, every execution of the\\nGALE-SHAPLEY produces exactly the same result. Moreover, the\\nstable matching returned is optimal for the women.\\nTheorem 25.11\\nRegardless of how women are chosen in line 2 of GALE-SHAPLEY,\\nthe procedure always returns the same stable matching, and in this\\nstable matching, each woman has the best partner possible in any stable\\nmatching.\\nProof\\xa0\\xa0\\xa0The proof that each woman has the best partner possible in any\\nstable matching is by contradiction. Suppose that the GALE-\\nSHAPLEY procedure returns a stable matching M, but that there is a\\ndifferent stable matching M′ in which some woman w prefers her\\npartner m′ to the partner m she has in M. Because w ranks m′ higher\\nthan m, she must have proposed to m′ before proposing to m. Then\\nthere is a woman w′ whom m′ prefers to w, and m′ was already engaged\\nto w′ when w proposed or m′ accepted the proposal from w and later\\nbroke the engagement in favor of w′. Either way, there is a moment\\nwhen m′ decided against w in favor of w′. Now suppose, without loss of\\ngenerality, that this moment was the ﬁrst time that any man rejected a\\npartner who belongs to some stable matching.\\nWe claim that w′ cannot have a partner m″ in a stable matching\\nwhom she prefers to m′. If there were such a man m″, then in order for\\nw′ to propose to m′, she would have proposed to m″ and been rejected at', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 932}),\n",
              " Document(page_content='some point before proposing to m′. If m′ accepted the proposal from w\\nand later broke it to accept w′, then since this was the ﬁrst rejection in a\\nstable matching, we get the contradiction that m″ could not have\\nrejected w′ beforehand. If m″ was already engaged to w′ when w\\nproposed, then again, m″ could not have rejected w′ beforehand, thus\\nproving the claim.\\nSince w′ does not prefer anyone to m′ in a stable matching and w′ is\\nnot matched with m′ in M′ (because m′ is matched with w in M′), w′\\nprefers m′ to her partner in M′. Since w′ prefers m′ over her partner in\\nM′ and m′ prefers w′ over his partner w in M′, the pair w′ and m′ is a\\nblocking pair in M′. Because M′ has a blocking pair, it cannot be a\\nstable matching, thereby contradicting the assumption that there exists\\nsome stable matching in which each woman has the best partner\\npossible other than the matching M returned by GALE-SHAPLEY.\\nWe put no condition on the execution of the procedure, which means\\nthat all possible orders in which line 2 selects women result in the same\\nstable matching being returned.\\n▪\\nCorollary 25.12\\nThere can be stable matchings that the GALE-SHAPLEY procedure\\ndoes not return.\\nProof\\xa0\\xa0\\xa0Theorem 25.11 says that for a given set of rankings, GALE-\\nSHAPLEY returns just one matching, no matter how it chooses women\\nin line 2. The earlier example of three women and three men with three\\ndifferent stable matchings shows that there can be multiple stable\\nmatchings for a given set of rankings. A call of GALE-SHAPLEY is\\ncapable of returning only one of these stable matchings.\\n▪\\nAlthough the GALE-SHAPLEY procedure gives the best possible\\noutcome for the women, the following corollary shows that it also\\nproduces the worst possible outcome for the men.\\nCorollary 25.13', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 933}),\n",
              " Document(page_content='In the stable matching returned by the procedure GALE-SHAPLEY,\\neach man has the worst partner possible in any stable matching.\\nProof\\xa0\\xa0\\xa0Let M be the matching returned by a call to GALE-SHAPLEY.\\nSuppose that there is another stable matching M′ and a man m who\\nprefers his partner w in M to his partner w′ in M′. Let the partner of w\\nin M′ be m′. By Theorem 25.11, m is the best partner that w can have in\\nany stable matching, which means that w prefers m to m′. Since m\\nprefers w to w′, the pair w and m is a blocking pair in M′, contradicting\\nthe assumption that M′ is a stable matching.\\n▪\\nExercises\\n25.2-1\\nDescribe how to implement the Gale-Shapley algorithm so that it runs\\nin O(n2) time.\\n25.2-2\\nIs it possible to have an unstable matching with just two women and two\\nmen? If so, provide and justify an example. If not, argue why not.\\n25.2-3\\nThe National Resident Matching Program differs from the scenario for\\nthe stable-marriage problem set out in this section in two ways. First, a\\nhospital may be matched with more than one student, so that hospital h\\ntakes rh ≥ 1 students. Second, the number of students might not equal\\nthe number of hospitals. Describe how to modify the Gale-Shapley\\nalgorithm to ﬁt the requirements of the National Resident Matching\\nProgram.\\n25.2-4\\nProve the following property, which is known as weak Pareto optimality:\\nLet M be the stable matching produced by the GALE-\\nSHAPLEY procedure, with women proposing to men. Then,\\nfor a given instance of the stable-marriage problem there is no', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 934}),\n",
              " Document(page_content='matching—stable or unstable—such that every woman has a\\npartner whom she prefers to her partner in the stable matching\\nM.\\n25.2-5\\nThe stable-roommates problem is similar to the stable-marriage problem,\\nexcept that the graph is a complete graph, not bipartite, with an even\\nnumber of vertices. Each vertex represents a person, and each person\\nranks all the other people. The deﬁnitions of blocking pairs and stable\\nmatching extend in the natural way: a blocking pair comprises two\\npeople who both prefer each other to their current partner, and a\\nmatching is stable if there are no blocking pairs. For example, consider\\nfour people—Wendy, Xenia, Yolanda, and Zelda—w ith the following\\npreference lists:\\nWendy:Xenia, Yolanda, Zelda\\nXenia:Wendy, Zelda, Yolanda\\nYolanda:\\xa0Wendy, Zelda, Xenia\\nZelda:Xenia, Yolanda, Wendy\\nYou can verify that the following matching is stable:\\nWendy and Xenia\\nYolanda and Zelda\\nUnlike the stable-marriage problem, the stable-roommates problem can\\nhave inputs for which no stable matching exists. Find such an input and\\nexplain why no stable matching exists.\\n25.3\\xa0\\xa0\\xa0\\xa0The Hungarian algorithm for the assignment problem\\nLet us once again add some information to a complete bipartite graph\\nG = (V, E), where V = L ∪ R. This time, instead of having the vertices\\nof each side rank the vertices on the other side, we assign a weight to\\neach edge. Again, let’s assume that the vertex sets L and R each contain\\nn vertices, so that the graph contains n2 edges. For l ∈ L and r ∈ R,', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 935}),\n",
              " Document(page_content='denote the weight of edge (l, r) by w(l, r), which represents the utility\\ngained by matching vertex l with vertex r.\\nThe goal is to ﬁnd a perfect matching M* (see Exercises 25.1-5 and\\n25.1-6) whose edges have the maximum total weight over all perfect\\nmatchings. That is, letting w(M) = ∑(l,r) ∈M\\xa0w(l, r) denote the total\\nweight of the edges in matching M, we want to ﬁnd a perfect matching\\nM* such that\\nw(M*) = max {w(M) : M is a perfect matching}.\\nWe call ﬁnding such a maximum-weight perfect matching the\\nassignment problem. A solution to the assignment problem is a perfect\\nmatching that maximizes the total utility. Like the stable-marriage\\nproblem, the assignment problem ﬁnds a matching that is “good,” but\\nwith a different deﬁnition of good: maximizing total value rather than\\nachieving stability.\\nAlthough you could enumerate all n! perfect matchings to solve the\\nassignment problem, an algorithm known as the Hungarian algorithm\\nsolves it much faster. This section will prove an O(n4) time bound, and\\nProblem 25-2 asks you to reﬁne the algorithm to reduce the running\\ntime to O(n3). Instead of working with the complete bipartite graph G,\\nthe Hungarian algorithm works with a subgraph of G called the\\n“equality subgraph.” The equality subgraph, which is deﬁned below,\\nchanges over time and has the beneﬁcial property that any perfect\\nmatching in the equality subgraph is also an optimal solution to the\\nassignment problem.\\nThe equality subgraph depends on assigning an attribute h to each\\nvertex. We call h the label of a vertex, and we say that h is a feasible\\nvertex labeling of G if\\nl.h + r.h ≥ w(l, r) for all l ∈ L and r ∈ R.\\nA feasible vertex labeling always exists, such as the default vertex\\nlabeling given by\\n', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 936}),\n",
              " Document(page_content='Given a feasible vertex labeling h, the equality subgraph\\xa0Gh = (V, Eh) of\\nG consists of the same vertices as G and the subset of edges\\nEh = {(l, r) ∈ E : l.h + r.h = w(l, r)}.\\nThe following theorem ties together a perfect matching in an\\nequality subgraph and an optimal solution to the assignment problem.\\nTheorem 25.14\\nLet G = (V, E), where V = L ∪ R, be a complete bipartite graph where\\neach edge (l, r) ∈ E has weight w(l, r). Let h be a feasible vertex labeling\\nof G and Gh be the equality subgraph of G. If Gh contains a perfect\\nmatching M*, then M* is an optimal solution to the assignment\\nproblem on G.\\nProof\\xa0\\xa0\\xa0If Gh contains a perfect matching M*, then because Gh and G\\nhave the same sets of vertices, M* is also a perfect matching in G.\\nBecause each edge of M* belongs to Gh and each vertex has exactly one\\nincident edge from any perfect matching, we have\\nLetting M be any perfect matching in G, we have\\nThus, we have', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 937}),\n",
              " Document(page_content='so that M* is a maximum-weight perfect matching in G.\\n▪\\nThe goal now becomes ﬁnding a perfect matching in an equality\\nsubgraph. Which equality subgraph? It does not matter! We have free\\nrein to not only choose an equality subgraph, but to change which\\nequality subgraph we choose as we go along. We just need to ﬁnd some\\nperfect matching in some equality subgraph.\\nTo understand the equality subgraph better, consider again the proof\\nof Theorem 25.14 and, in the second half, let M be any matching. The\\nproof is still valid, in particular, inequality (25.3): the weight of any\\nmatching is always at most the sum of the vertex labels. If we choose any\\nset of vertex labels that deﬁne an equality subgraph, then a maximum-\\ncardinality matching in this equality subgraph has total value at most\\nthe sum of the vertex labels. If the set of vertex labels is the “right” one,\\nthen it will have total value equal to w(M*), and a maximum-cardinality\\nmatching in the equality subgraph is also a maximum-weight perfect\\nmatching. The Hungarian algorithm repeatedly modiﬁes the matching\\nand the vertex labels in order to achieve this goal.\\nThe Hungarian algorithm starts with any feasible vertex labeling h\\nand any matching M in the equality subgraph Gh. It repeatedly ﬁnds an\\nM-augmenting path P in Gh and, using Lemma 25.1, updates the\\nmatching to be M ⊕ P, thereby incrementing the size of the matching.\\nAs long as there is some equality subgraph that contains an M-\\naugmenting path, the size of the matching can increase, until a perfect\\nmatching is achieved.\\nFour questions arise:\\n1. What initial feasible vertex labeling should the algorithm start\\nwith? Answer: the default vertex labeling given by equations\\n(25.1) and (25.2).\\n2. What initial matching in Gh should the algorithm start with?\\nShort answer: any matching, even an empty matching, but a', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 938}),\n",
              " Document(page_content='greedy maximal matching works well.\\n3. If an M-augmenting path exists in Gh, how to ﬁnd it? Short\\nanswer: use a variant of breadth-ﬁrst search similar to the\\nsecond phase of the procedure used in the Hopcroft-Karp\\nalgorithm to ﬁnd a maximal set of shortest M-augmenting paths.\\n4. What if the search for an M-augmenting path fails? Short\\nanswer: update the feasible vertex labeling to bring in at least one\\nnew edge.\\nWe’ll elaborate on the short answers using the example that starts in\\nFigure 25.4. Here, L = {l1, l2, … , l7} and R = {r1, r2, … , r7}. The\\nedge weights appear in the matrix shown in part (a), where the weight\\nw(li, rj) appears in row i and column j. The feasible vertex labels, given\\nby the default vertex labeling, appear to the left of and above the\\nmatrix. Matrix entries in red indicate edges (li, rj) for which li.h + rj.h =\\nw(li, rj), that is, edges in the equality subgraph Gh appearing in part (b)\\nof the ﬁgure.\\nGreedy maximal bipartite matching\\nThere are several ways to implement a greedy method to ﬁnd a maximal\\nbipartite matching. The procedure GREEDY-BIPARTITE-\\nMATCHING shows one. Edges in Figure 25.4(b) highlighted in blue\\nindicate the initial greedy maximal matching in Gh. Exercise 25.3-2 asks\\nyou to show that the GREEDY-BIPARTITE-MATCHING procedure\\nreturns a matching that is at least half the size of a maximum matching.\\nGREEDY-BIPARTITE-MATCHING (G)\\n1M = Ø\\n2for each vertex l ∈ L\\n3 if\\xa0l has an unmatched neighbor in R\\n4 choose any such unmatched neighbor r ∈ R\\n5 M = M ∪ {(l, r)}\\n6return\\xa0M', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 939}),\n",
              " Document(page_content='Figure 25.4 The start of the Hungarian algorithm. (a) The matrix of edge weights for a bipartite\\ngraph with L = {l1, l2, … , l7}. The value in row i and column j indicates w(li, rj). Feasible\\nvertex labels appear above and next to the matrix. Red entries correspond to edges in the\\nequality subgraph. (b) The equality subgraph Gh. Edges highlighted in blue belong to the initial\\ngreedy maximal matching M. Blue vertices are matched, and tan vertices are unmatched. (c) The\\ndirected equality subgraph GM,h created from Gh by directing edges in M from R to L and all\\nother edges from L to R.\\nFinding an M-augmenting path in Gh\\nTo ﬁnd an M-augmenting path in the equality subgraph Gh with a\\nmatching M, the Hungarian algorithm ﬁrst creates the directed equality\\nsubgraph\\xa0GM,h from Gh, just as the Hopcroft-Karp algorithm creates\\nGM from G. As in the Hopcroft-Karp algorithm, you can think of an\\nM-augmenting path as starting from an unmatched vertex in L, ending\\nat an unmatched vertex in R, taking unmatched edges from L to R, and\\ntaking matched edges from R to L. Thus, GM,h = (V, EM,h), where\\nEM,h={(l, r) : l ∈ L, r ∈ R, and (l, r) ∈ Eh − M }(edges from L to R)', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 940}),\n",
              " Document(page_content='∪ {(r, l) : r ∈ R, l ∈ L, and (l, r) ∈ M } (edges from R to L).\\nBecause an M-augmenting path in the directed equality subgraph GM.h\\nis also an M-augmenting path in the equality subgraph Gh, it sufﬁces to\\nﬁnd M-augmenting paths in GM.h. Figure 25.4(c) shows the directed\\nequality subgraph GM,h corresponding to the equality subgraph Gh\\nand matching M from part (b) of the ﬁgure.\\nWith the directed equality subgraph GM,h in hand, the Hungarian\\nalgorithm searches for an M-augmenting path from any unmatched\\nvertex in L to any unmatched vertex in R. Any exhaustive graph-search\\nmethod sufﬁces. Here, we’ll use breadth-ﬁrst search, starting from all\\nthe unmatched vertices in L (just as the Hopcroft-Karp algorithm does\\nwhen creating the dag H), but stopping upon ﬁrst discovering some\\nunmatched vertex in R. Figure 25.5 shows the idea. To start from all the\\nunmatched vertices in L, initialize the ﬁrst-in, ﬁrst-out queue with all\\nthe unmatched vertices in L, rather than just one source vertex. Unlike\\nthe dag H in the Hopcroft-Karp algorithm, here each vertex needs just\\none predecessor, so that the breadth-ﬁrst search creates a breadth-ﬁrst\\nforest\\xa0F = (VF, EF). Each unmatched vertex in L is a root in F.\\nIn Figure 25.5(g), the breadth-ﬁrst search has found the M-\\naugmenting path 〈(l4, r2), (r2, l1), (l1, r3), (r3, l6), (l6, r5)〉. Figure\\n25.6(a) shows the new matching created by taking the symmetric\\ndifference of the matching M in Figure 25.5(a) with this M-augmenting\\npath.\\nWhen the search for an M-augmenting path fails\\nHaving updated the matching M from an M-augmenting path, the\\nHungarian algorithm updates the directed equality subgraph GM,h\\naccording to the new matching and then starts a new breadth-ﬁrst\\nsearch from all the unmatched vertices in L. Figure 25.6 shows the start\\nof this process, picking up from Figure 25.5.\\nIn Figure 25.6(d), the queue contains vertices l4 and l3. Neither of\\nthese vertices has an edge that leaves it, however, so that once these\\nvertices are removed from the queue, the queue becomes empty. The', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 941}),\n",
              " Document(page_content='search terminates at this point, before discovering an unmatched vertex\\nin R to yield an M-augmenting path. Whenever this situation occurs,\\nthe most recently discovered vertices must belong to L. Why? Whenever\\nan unmatched vertex in R is discovered, the search has found an M-\\naugmenting path, and when a matched vertex in R is discovered, it has\\nan unvisited neighbor in L, which the search can then discover.\\nRecall that we have the freedom to work with any equality subgraph.\\nWe can change the directed equality subgraph “on the ﬂy,” as long we\\ndo not counteract the work already done. The Hungarian algorithm\\nupdates the feasible vertex labeling h to fulﬁll the following criteria:\\n1. No edge in the breadth-ﬁrst forest F leaves the directed equality\\nsubgraph.\\n2. No edge in the matching M leaves the directed equality\\nsubgraph.\\n3. At least one edge (l, r), where l ∈ L ∩ VF and r ∈ R − VF goes\\ninto Eh, and hence into EM,h. Therefore, at least one vertex in R\\nwill be newly discovered.\\nThus, at least one new edge enters the directed equality subgraph, and\\nany edge that leaves the directed equality subgraph belongs to neither\\nthe matching M nor the breadth-ﬁrst forest F. Newly discovered vertices\\nin R are enqueued, but their distances are not necessarily 1 greater than\\nthe distances of the most recently discovered vertices in L.\\n', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 942}),\n",
              " Document(page_content='Figure 25.5 Finding an M-augmenting path in GM,h by breadth-ﬁrst search. (a) The directed\\nequality subgraph GM,h from Figure 25.4(c). (b)–(g) Successive versions of the breadth-ﬁrst\\nforest F, shown as the vertices at each distance from the roots—the unmatched vertices in L—\\nare discovered. In parts (b)–(f), the layer of vertices closest to the bottom of the ﬁgure are those\\nin the ﬁrst-in, ﬁrst-out queue. For example, in part (b), the queue contains the roots 〈l4, l5, l7〉,\\nand in part (e), the queue contains 〈r3, r4〉, at distance 3 from the roots. In part (g), the\\nunmatched vertex r5 is discovered, so the breadth-ﬁrst search terminates. The path 〈(l4, r2), (r2,\\nl1), (l1, r3), (r3, l6), (l6, r5)〉, highlighted in orange in parts (a) and (g), is an M-augmenting\\npath. Taking its symmetric difference with the matching M yields a new matching with one more\\nedge than M.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 943}),\n",
              " Document(page_content='Figure 25.6 (a) The new matching M and the new directed equality subgraph GM.h after\\nupdating the matching in Figure 25.5(a) with the M-augmenting path in Figure 25.5(g). (b)–(d)\\nSuccessive versions of the breadth-ﬁrst forest F in a new breadth-ﬁrst search with roots l5 and\\nl7. After the vertices l4 and l3 in part (d) have been removed from the queue, the queue becomes\\nempty before the search can discover an unmatched vertex in R.\\nTo update the feasible vertex labeling, the Hungarian algorithm ﬁrst\\ncomputes the value\\nwhere FL = L ∩ VF and FR = R ∩ VF denote the vertices in the\\nbreadth-ﬁrst forest F that belong to L and R, respectively. That is, δ is\\nthe smallest difference by which an edge incident on a vertex in FL\\nmissed being in the current equality subgraph Gh. The Hungarian\\nalgorithm then creates a new feasible vertex labeling, say h′, by\\nsubtracting δ from l.h for all vertices l ∈ FL and adding δ to r.h for all\\nvertices r ∈ FR:', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 944}),\n",
              " Document(page_content='The following lemma shows that these changes achieve the three criteria\\nabove.\\nLemma 25.15\\nLet h be a feasible vertex labeling for the complete bipartite graph G\\nwith equality subgraph Gh, and let M be a matching for Gh and F be a\\nbreadth-ﬁrst forest being constructed for the directed equality subgraph\\nGM,h. Then, the labeling h′ in equation (25.5) is a feasible vertex\\nlabeling for G with the following properties:\\n1. If (u, v) is an edge in the breadth-ﬁrst forest F for GM,h, then (u,\\nv) ∈ EM,h ′.\\n2. If (l, r) belongs to the matching M for Gh, then (r, l) ∈ EM,h ′.\\n3. There exist vertices l ∈ FL and r ∈ R − FR such that (l, r) ∉\\nEM,h but (l, r) ∈ EM,h ′.\\nProof\\xa0\\xa0\\xa0We ﬁrst show that h′ is a feasible vertex labeling for G. Because h\\nis a feasible vertex labeling, we have l.h + r.h ≥ w(l, r) for all l ∈ L and r\\n∈ R. In order for h′ to not be a feasible vertex labeling, we would need\\nl.h′ + r.h′ < l.h + r.h for some l ∈ L and r ∈ R. The only way this could\\noccur would be for some l ∈ FL and r ∈ R − FR. In this instance, the\\namount of the decrease equals δ, so that l.h′ + r.h′ = l.h − δ + r.h. By\\nequation (25.4), we have that l.h− δ+r.h ≥ w(l, r) for any l ∈ FL and r ∈\\nR−FR, so that l.h′+r.h′ ≥ w(l, r). For all other edges, we have l.h′ + r.h′ ≥\\nl.h+r.h ≥ w(l, r). Thus, h′ is a feasible vertex labeling.\\nNow we show that each of the three desired properties holds:\\n1. If l ∈ FL and r ∈ FR, then we have l.h′+r.h′ = l.h+r.h because δ\\nis added to the label of l and subtracted from the label of r.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 945}),\n",
              " Document(page_content='Therefore, if an edge belongs to F for the directed graph GM,h, it\\nalso belongs to GM,h′.\\n2. We claim that at the time the Hungarian algorithm computes the\\nnew feasible vertex labeling h′, for every edge (l, r) ∈ M, we have\\nl ∈ FL if and only if r ∈ FR. To see why, consider a matched\\nvertex r and let (l, r) ∈ M. First suppose that r ∈ FR, so that the\\nsearch discovered r and enqueued it. When r was removed from\\nthe queue, l was discovered, so l ∈ FL. Now suppose that r ∉\\nFR, so r is undiscovered. We will show that l ∉ FL. The only\\nedge in GM,h that enters l is (r, l), and since r is undiscovered,\\nthe search has not taken this edge; if l ∈ FL, it is not because of\\nthe edge (r, l). The only other way that a vertex in L can be in FL\\nis if it is a root of the search, but only unmatched vertices in L\\nare roots and l is matched. Thus, l ∉ FL, and the claim is\\nproved.\\nWe already saw that l ∈ FL and r ∈ FR implies l.h′ + r.h′ = l.h +\\nr.h. For the opposite case, when l ∈ L − FL and R ∈ R − FR, we\\nhave that l.h′ = l.h and r.h′ = r.h, so that again l.h′ + r.h′ = l.h +\\nr.h. Thus, if edge (l, r) is in the matching M for the equality\\ngraph Gh, then (r, l) ∈ EM,h′.\\n3. Let (l, r) be an edge not in Eh such that l ∈ FL, r ∈ R − FR, and\\nδ = l.h + r.h − w(l, r). By the deﬁnition of δ, there is at least one\\nsuch edge. Then, we have\\nl.h′ + r.h′=l.h − δ + r.h\\n=l.h − (l.h + r.h − w(l, r)) + r.h\\n=w(l, r),\\nand thus (l, r) ∈ Eh′. Since (l, r) is not in Eh, it is not in the\\nmatching M, so that in EM,h′ it must be directed from L to R.\\nThus, (l, r) ∈ EM,h′.\\n▪', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 946}),\n",
              " Document(page_content='It is possible for an edge to belong to EM,h but not to EM,h′. By\\nLemma 25.15, any such edge belongs neither to the matching M nor to\\nthe breadth-ﬁrst forest F at the time that the new feasible vertex labeling\\nh′ is computed. (See Exercise 25.3-3.)\\nGoing back to Figure 25.6(d), the queue became empty before an M-\\naugmenting path was found. Figure 25.7 shows the next steps taken by\\nthe algorithm. The value of δ = 1 is achieved by the edge (l5, r3) because\\nin Figure 25.4(a), l5.h + r3.h − w(l5, r3) = 6 + 0 − 5 = 1. In Figure\\n25.7(a), the values of l3.h, l4.h, l5.h, and l7.h have decreased by 1 and\\nthe values of r2.h and r7.h have increased by 1 because these vertices are\\nin F. As a result, the edges (l1, r2) and (l6, r7) leave GM,h and the edge\\n(l5, r3) enters. Figure 25.7(b) shows the new directed equality subgraph\\nGM,h. With edge (l5, r3) now in GM,h, Figure 25.7(c) shows that this\\nedge is added to the breadth-ﬁrst forest F, and r3 is added to the queue.\\nParts (c)–(f) show the breadth-ﬁrst forest continuing to be built until in\\npart (f), the queue once again becomes empty after vertex l2, which has\\nno edges leaving, is removed. Again, the algorithm must update the\\nfeasible vertex labeling and the directed equality subgraph. Now the\\nvalue of δ = 1 is achieved by three edges: (l1, r6), (l5, r6), and (l7, r6).\\nAs Figure 25.8 shows in parts (a) and (b), these edges enter GM,h,\\nand edge (l6, r3) leaves. Part (c) shows that edge (l1, r6) is added to the\\nbreadth-ﬁrst forest. (Either of edges (l5, r6) or (l7, r6) could have been\\nadded instead.) Because r6 is unmatched, the search has found the M-\\naugmenting path 〈(l5, r3), (r3, l1), (l1, r6)〉, highlighted in orange.\\nFigure 25.9(a) shows GM,h after the matching M has been updated\\nby taking its symmetric difference with the M-augmenting path. The\\nHungarian algorithm starts its last breadth-ﬁrst search, with vertex l7 as\\nthe only root. The search proceeds as shown in parts (b)–(h) of the\\nﬁgure, until the queue becomes empty after removing l4. This time, we\\nﬁnd that δ = 2, achieved by the ﬁve edges (l2, r5), (l3, r1), (l4, r5), (l5,\\nr1), and (l5, r5), each of which enters GM,h. Figure 25.10(a) shows the', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 947}),\n",
              " Document(page_content='results of decreasing the feasible vertex label of each vertex in FL by 2\\nand increasing the feasible vertex label of each vertex in FR by 2, and\\nFigure 25.10(b) shows the resulting directed equality subgraph GM,h.\\nPart (c) shows that edge (l3, r1) is added to the breadth-ﬁrst forest.\\nSince r1 is an unmatched vertex, the search terminates, having found the\\nM-augmenting path 〈(l7, r7), (r7, l3), (l3, r1)〉, highlighted in orange. If\\nr1 had been matched, vertex r5 would also have been added to the\\nbreadth-ﬁrst forest, with any of l2, l4, or l5 as its parent.\\nFigure 25.7 Updating the feasible vertex labeling and the directed equality subgraph GM,h when\\nthe queue becomes empty before ﬁnding an M-augmenting path. (a) With δ = 1, the values of\\nl3.h, l4.h, l5.h, and l7.h decreased by 1 and r2.h and r7.h increased by 1. Edges (l1, r2) and (l6,\\nr7) leave GM,h, and edge (l5, r3) enters. These changes are highlighted in yellow. (b) The\\nresulting directed equality subgraph GM,h. (c)–(f) With edge (l5, r3) added to the breadth-ﬁrst\\nforest and r3 added to the queue, the breadth-ﬁrst search continues until the queue once again\\nbecomes empty in part (f).\\nAfter updating the matching M, the algorithm arrives at the perfect\\nmatching shown for the equality subgraph Gh in Figure 25.11. By', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 948}),\n",
              " Document(page_content='Theorem 25.14, the edges in M form an optimal solution to the original\\nassignment problem given in the matrix. Here, the weights of edges (l1,\\nr6), (l2, r4), (l3, r1), (l4, r2), (l5, r3), (l6, r5), and (l7, r7) sum to 65,\\nwhich is the maximum weight of any matching.\\nThe weight of the maximum-weight matching equals the sum of all\\nthe feasible vertex labels. These problems—maximizing the weight of a\\nmatching and minimizing the sum of the feasible vertex labels—are\\n“duals” of each other, in a similar vein to how the value of a maximum\\nﬂow equals the capacity of a minimum cut. Section 29.3 explores duality\\nin more depth.\\nFigure 25.8 Another update to the feasible vertex labeling and directed equality subgraph GM,h\\nbecause the queue became empty before ﬁnding an M-augmenting path. (a) With δ = 1, the\\nvalues of l1.h, l2.h, l3.h, l4.h, l5.h, and l7.h decrease by 1, and r2.h, r3.h, r4.h, and r7.h increase\\nby 1. Edge (l6, r3) leaves GM,h, and edges (l1, r6), (l5, r6) and (l7, r6) enter. (b) The resulting\\ndirected equality subgraph GM,h. (c) With edge (l1, r6) added to the breadth-ﬁrst forest and r6\\nunmatched, the search terminates, having found the M-augmenting path 〈(l5, r3), (r3, l1), (l1,\\nr6)〉, highlighted in orange in parts (b) and (c).', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 949}),\n",
              " Document(page_content='The Hungarian algorithm\\nThe procedure HUNGARIAN on page 737 and its subroutine FIND-\\nAUGMENTING-PATH on page 738 follow the steps we have just seen.\\nThe third property in Lemma 25.15 ensures that in line 23 of FIND-\\nAUGMENTING-PATH the queue Q is nonempty. The pseudocode\\nuses the attribute π to indicate predecessor vertices in the breadth-ﬁrst\\nforest. Instead of coloring vertices, as in the BFS procedure on page\\n556, the search puts the discovered vertices into the sets FL and FR.\\nBecause the Hungarian algorithm does not need breadth-ﬁrst distances,\\nthe pseudocode omits the d attribute computed by the BFS procedure.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 950}),\n",
              " Document(page_content='Figure 25.9 (a) The new matching M and the new directed equality subgraph GM,h after\\nupdating the matching in Figure 25.8 with the M-augmenting path in Figure 25.8 parts (b) and\\n(c). (b)–(h) Successive versions of the breadth-ﬁrst forest F in a new breadth-ﬁrst search with\\nroot l7. After the vertex l4 in part (h) has been removed from the queue, the queue becomes\\nempty before the search discovers an unmatched vertex in R.\\nNow, let’s see why the Hungarian algorithm runs in O(n4) time,\\nwhere |V| = n/2 and |E| = n2 in the original graph G. (Below we outline\\nhow to reduce the running time to O(n3).) You can go through the\\npseudocode of HUNGARIAN to verify that lines 1–6 and 11 take\\nO(n2) time. The while loop of lines 7–10 iterates at most n times, since\\neach iteration increases the size of the matching M by 1. Each test in', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 951}),\n",
              " Document(page_content='line 7 can take constant time by just checking whether |M| < n, each\\nupdate of M in line 9 takes O(n) time, and the updates in line 10 take\\nO(n2) time.\\nTo achieve the O(n4) time bound, it remains to show that each call of\\nFIND-AUGMENTING-PATH runs in O(n3) time. Let’s call each\\nexecution of lines 10–22 a growth step. Ignoring the growth steps, you\\ncan verify that FIND-AUGMENTING-PATH is a breadth-ﬁrst search.\\nWith the sets FL and FR represented appropriately, the breadth-ﬁrst\\nsearch takes O(V + E) = O(n2) time. Within a call of FIND-\\nAUGMENTING-PATH, at most n growth steps can occur, since each\\ngrowth step is guaranteed to discover at least one vertex in R. Since\\nthere are at most n2 edges in GM,h, the for loop of lines 16–22 iterates\\nat most n2 times per call of FIND-AUGMENTING-PATH. The\\nbottleneck is lines 10 and 15, which take O(n2) time, so that FIND-\\nAUGMENTING-PATH takes O(n3) time.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 952}),\n",
              " Document(page_content='Figure 25.10 Updating the feasible vertex labeling and directed equality subgraph GM,h. (a)\\nHere, δ = 2, so the values of l1.h, l2.h, l3.h, l4.h, l5.h, and l7.h decreased by 2, and the values of\\nr2.h, r3.h, r4.h, r6.h, and r7.h increased by 2. Edges (l2, r5), (l3, r1), (l4, r5), (l5, r1), and (l5, r5)\\nenter GM,h. (b) The resulting directed graph GM,h. (c) With edge (l3, r1) added to the breadth-\\nﬁrst forest and r1 unmatched, the search terminates, having found the M-augmenting path 〈(l7,\\nr7), (r7, l3), (l3, r1)〉, highlighted in orange in parts (b) and (c).\\nExercise 25.3-5 asks you to show that reconstructing the directed\\nequality subgraph GM,h in line 15 is actually unnecessary, so that its\\ncost can be eliminated. Reducing the cost of computing δ in line 10 to\\nO(n) takes a little more effort and is the subject of Problem 25-2. With\\nthese changes, each call of FIND-AUGMENTING-PATH takes O(n2)\\ntime, so that the Hungarian algorithm runs in O(n3) time.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 953}),\n",
              " Document(page_content='Figure 25.11 The ﬁnal matching, shown for the equality subgraph Gh with blue edges and blue\\nentries in the matrix. The weights of the edges in the matching sum to 65, which is the maximum\\nfor any matching in the original complete bipartite graph G, as well as the sum of all the ﬁnal\\nfeasible vertex labels.\\nHUNGARIAN (G)\\n\\xa0\\xa01for each vertex l ∈ L\\n\\xa0\\xa02l.h = max {w(l, r) : r ∈ R} // from equation (25.1)\\n\\xa0\\xa03for each vertex r ∈ R\\n\\xa0\\xa04r.h = 0 // from equation (25.2)\\n\\xa0\\xa05let M be any matching in Gh (such as the matching returned by\\nGREEDY-BIPARTITE-MATCHING)\\n\\xa0\\xa06from G, M, and h, form the equality subgraph Gh\\nand the directed equality subgraph GM,h\\n\\xa0\\xa07while\\xa0M is not a perfect matching in Gh\\n\\xa0\\xa08P = FIND-AUGMENTING-PATH (GM,h)\\n\\xa0\\xa09M = M ⊕ P\\n10', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 954}),\n",
              " Document(page_content='update the equality subgraph Gh\\nand the directed equality subgraph GM,h\\n11return\\xa0M\\nFIND-AUGMENTING-PATH (GM,h)\\n\\xa0\\xa01Q = Ø\\n\\xa0\\xa02FL = Ø\\n\\xa0\\xa03FR = Ø\\n\\xa0\\xa04for each unmatched vertex l ∈ L\\n\\xa0\\xa05l.π = NIL\\n\\xa0\\xa06ENQUEUE (Q, l)\\n\\xa0\\xa07FL = FL ∪ {l} // forest F starts with unmatched\\nvertices in L\\n\\xa0\\xa08repeat\\n\\xa0\\xa09if\\xa0Q is empty // ran out of vertices to search from?\\n10 δ = min {l.h + r.h − w(l, r) : l ∈ FL and r ∈ R − FR}\\n11 for each vertex l ∈ FL\\n12 l.h = l.h − δ // relabel according to equation\\n(25.5)\\n13 for each vertex r ∈ FR\\n14 r.h = r.h + δ // relabel according to equation\\n(25.5)\\n15 from G, M, and h, form a new directed equality graph GM,h\\n16 for each new edge (l, r)\\nin GM,h// continue search with\\nnew edges\\n17 if\\xa0r ∉ FR\\n18 r.π = l // discover r, add it to\\nF\\n19 if\\xa0r is unmatched\\n20 an M-augmenting path has been found\\n20 (exit the repeat loop)\\n21 else ENQUEUE // can search from r', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 955}),\n",
              " Document(page_content='(Q, r) later\\n22 FR = FR ∪ {r}\\n23u = DEQUEUE (Q) // search from u\\n24for each neighbor v of u in GM,h\\n25 if\\xa0v ∈ L\\n26 v.π = u\\n27 FL = FL ∪ {v} // discover v, add it to\\nF\\n28 ENQUEUE (Q, v) // can search from v\\nlater\\n29 elseif\\xa0v ∉ FR //\\xa0v ∈ R, do same as\\nlines 18–22\\n30 v.π = u\\n31 if\\xa0v is unmatched\\n32 an M-augmenting path has been found\\n(exit the repeat loop)\\n33 else ENQUEUE (Q, v)\\n34 FR = FR ∪ {v}\\n35until an M-augmenting path has been found\\n36using the predecessor attributes π, construct an M-augmenting path\\nP by tracing back from the unmatched vertex in R\\n37return\\xa0P\\nExercises\\n25.3-1\\nThe FIND-AUGMENTING-PATH procedure checks in two places\\n(lines 19 and 31) whether a vertex it discovers in R is unmatched. Show\\nhow to rewrite the pseudocode so that it checks for an unmatched\\nvertex in R in only one place. What is the downside of doing so?\\n25.3-2\\nShow that for any bipartite graph, the GREEDY-BIPARTITE-\\nMATCHING procedure on page 726 returns a matching at least half\\nthe size of a maximum matching.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 956}),\n",
              " Document(page_content='25.3-3\\nShow that if an edge (l, r) belongs to the directed equality subgraph\\nGM,h but is not a member of GM,h′, where h′ is given by equation\\n(25.5), then l ∈ L − FL and r ∈ FR at the time that h′ is computed.\\n25.3-4\\nAt line 29 in the FIND-AUGMENTING-PATH procedure, it has\\nalready been established that v ∈ R. This line checks to see whether v is\\nalready discovered by testing whether v ∈ FR. Why doesn’t the\\nprocedure need to check whether v is already discovered for the case\\nwhen v ∈ L, in lines 26–28?\\n25.3-5\\nProfessor Hrabosky asserts that the directed equality subgraph GM,h\\nmust be constructed and maintained by the Hungarian algorithm, so\\nthat line 6 of HUNGARIAN and line 15 of FIND-AUGMENTING-\\nPATH are required. Argue that the professor is incorrect by showing\\nhow to determine whether an edge belongs to EM,h without explicitly\\nconstructing GM,h.\\n25.3-6\\nHow can you modify the Hungarian algorithm to ﬁnd a matching of\\nvertices in L to vertices in R that minimizes, rather than maximizes, the\\nsum of the edge weights in the matching?\\n25.3-7\\nHow can an assignment problem with |L| ≠ |R| be modiﬁed so that the\\nHungarian algorithm solves it?\\nProblems\\n25-1\\xa0\\xa0\\xa0\\xa0\\xa0Perfect matchings in a regular bipartite graph\\na. Problem 20-3 asked about Euler tours in directed graphs. Prove that a\\nconnected, undirected graph G = (V, E) has an Euler tour—a cycle', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 957}),\n",
              " Document(page_content='traversing each edge exactly once, though it may visit a vertex multiple\\ntimes—if and only if the degree of every vertex in V is even.\\nb. Assuming that G is connected, undirected, and every vertex in V has\\neven degree, give an O(E)-time algorithm to ﬁnd an Euler tour of G,\\nas in Problem 20-3(b).\\nc. Exercise 25.1-6 states that if G = (V, E) is a d-regular bipartite graph,\\nthen it contains d disjoint perfect matchings. Suppose that d is an\\nexact power of 2. Give an algorithm to ﬁnd all d disjoint perfect\\nmatchings in a d-regular bipartite graph in Θ (E lg d) time.\\n25-2\\xa0\\xa0\\xa0\\xa0\\xa0R educing the running time of the Hungarian al gorithm to O(n3)\\nIn this problem, you will show how to reduce the running time of the\\nHungarian algorithm from O(n4) to O(n3) by showing how to reduce\\nthe running time of the FIND-AUGMENTING-PATH procedure\\nfrom O(n3) to O(n2). Exercise 25.3-5 demonstrates that line 6 of\\nHUNGARIAN and line 15 of FIND-AUGMENTING-PATH are\\nunnecessary. Now you will show how to reduce the running time of each\\nexecution of line 10 in FIND-AUGMENTING-PATH to O(n).\\nFor each vertex r ∈ R − FR, deﬁne a new attribute r. σ where\\nr. σ = min {l.h + r.h − w(l, r) : l ∈ FL}.\\nThat is, r. σ indicates how close r is to being adjacent to some vertex l ∈\\nFL in the directed equality subgraph Gm,h. Initially, before placing any\\nvertices into FL, set r. σ to ∞ for all r ∈ R.\\na. Show how to compute δ in line 10 in O(n) time, based on the σ\\nattribute.\\nb. Show how to update all the σ attributes in O(n) time after δ has been\\ncomputed.\\nc. Show that updating all the σ attributes when FL changes takes O(n2)\\ntime per call of FIND-AUGMENTING-PATH.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 958}),\n",
              " Document(page_content='d. Conclude that the HUNGARIAN procedure can be implemented to\\nrun in O(n3) time.\\n25-3\\xa0\\xa0\\xa0\\xa0\\xa0O ther matching problems\\nThe Hungarian algorithm ﬁnds a maximum-weight perfect matching in\\na complete bipartite graph. It is possible to use the Hungarian\\nalgorithm to solve problems in other graphs by modifying the input\\ngraph, running the Hungarian algorithm, and then possibly modifying\\nthe output. Show how to solve the following matching problems in this\\nmanner.\\na. Give an algorithm to ﬁnd a maximum-weight matching in a weighted\\nbipartite graph that is not necessarily complete and with all edge\\nweights positive.\\nb. Redo part (a), but with edge weights allowed to also be 0 or negative.\\nc. A cycle cover in a directed graph, not necessarily bipartite, is a set of\\nedge-disjoint directed cycles such that each vertex lies on at most one\\ncycle. Given nonnegative edge weights w(u, v), let C be the set of edges\\nin a cycle cover, and deﬁne w(C) = ∑(u,v) ∈C\\xa0w(u, v) to be the weight\\nof the cycle cover. Give an algorithm to ﬁnd a maximum-weight cycle\\ncover.\\n25-4\\xa0\\xa0\\xa0\\xa0\\xa0Fractional matchings\\nIt is possible to deﬁne a fractional matching. Given a graph G = (V, E),\\nwe deﬁne a fractional matching x as a function x : E → [0, 1] (real\\nnumbers between 0 and 1, inclusive) such that for every vertex u ∈ V,\\nwe have ∑(u,v) ∈E\\xa0x(u, v) ≤ 1. The value of a fractional matching is ∑(u,\\nv) ∈E\\xa0x(u, v). The deﬁnition of a fractional matching is identical to that\\nof a matching, except that a matching has the additional constraint that\\nx(u, v) ∈ {0, 1} for all edges (u, v) ∈ E. Given a graph, we let M*\\ndenote a maximum matching and x* denote a fractional matching with\\nmaximum value.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 959}),\n",
              " Document(page_content='a. Argue that, for any bipartite graph, we must have ∑(u, v) ∈E\\xa0x*(u, v) ≥\\n|M*|.\\nb. Prove that, for any bipartite graph, we must have ∑(u, v) ∈E\\xa0x*(e) ≤\\n|M*|. (Hint: Give an algorithm that converts a fractional matching\\nwith an integer value to a matching.) Conclude that the maximum\\nvalue of a fractional matching in a bipartite graph is the same as the\\nsize of the maximum cardinality matching.\\nc. We can deﬁne a fractional matching in a weighted graph in the same\\nmanner: the value of the matching is now ∑(u, v) ∈E\\xa0w(u, v) x(u, v).\\nExtend the results of the previous parts to show that in a weighted\\nbipartite graph, the maximum value of a weighted fractional matching\\nis equal to the value of a maximum weighted matching.\\nd. In a general graph, the analogous results do not necessarily hold.\\nGive an example of a small graph that is not bipartite for which the\\nfractional matching with maximum value is not a maximum\\nmatching.\\n25-5\\xa0\\xa0\\xa0\\xa0\\xa0C omputing vertex labels\\nYou are given a complete bipartite graph G = (V, E) with edge weights\\nw(l, r) for all (l, r) ∈ E. You are also given a maximum-weight perfect\\nmatching M* for G. You wish to compute a feasible vertex labeling h\\nsuch that M* is a perfect matching in the equality subgraph Gh. That is,\\nyou want to compute a labeling h of vertices such that\\n(Requirement (25.6) holds for all edges, and the stronger requirement\\n(25.7) holds for all edges in M*.) Give an algorithm to compute the\\nfeasible vertex labeling h, and prove that it is correct. (Hint: Use the\\nsimilarity between conditions (25.6) and (25.7) and some of the\\nproperties of shortest paths proved in Chapter 22, in particular the\\ntriangle inequality (Lemma 22.10) and the convergence property\\n(Lemma 22.14.))', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 960}),\n",
              " Document(page_content='Chapter notes\\nMatching algorithms have a long history and have been central to many\\nbreakthroughs in algorithm design and analysis. The book by Lovász\\nand Plummer [306] is an excellent reference on matching problems, and\\nthe chapter on matching in the book by Ahuja, Magnanti and Orlin [10]\\nalso has extensive references.\\nThe Hopcroft-Karp algorithm is by Hopcroft and Karp [224].\\nMadry [308] gave an Õ(E10/7)-time algorithm, which is asymptotically\\nfaster than Hopcroft-Karp for sparse graphs.\\nCorollary 25.4 is due to Berge [53], and it also holds in graphs that\\nare not bipartite. Matching in general graphs requires more complicated\\nalgorithms. The ﬁrst polynomial-time algorithm, running in O(V\\xa04)\\ntime, is due to Edmonds [130] (in a paper that also introduced the\\nnotion of a polynomial-time algorithm). Like the bipartite case, this\\nalgorithm also uses augmenting paths, although the algorithm for\\nﬁnding augmenting paths in general graphs is more involved than the\\none for bipartite graphs. Subsequently, several \\n -time algorithms\\nappeared, including ones by Gabow and Tarjan [168] as part of an\\nalgorithm for weighted matching and a simpler one by Gabow [164].\\nThe Hungarian algorithm is described in the book by Bondy and\\nMurty [67] and is based on work by Kuhn [273] and Munkres [337].\\nKuhn adopted the name “Hungarian algorithm” because the algorithm\\nderived from work by the Hungarian mathematicians D. K őnig and J.\\nEgervéry. The algorithm is an early example of a primal-dual algorithm.\\nA faster algorithm that runs in \\n  time, where the edge\\nweights are integers from 0 to W, was given by Gabow and Tarjan [167],\\nand an algorithm with the same time bound for maximum-weight\\nmatching in general graphs was given by Duan, Pettie, and Su [127].\\nThe stable-marriage problem was ﬁrst deﬁned and analyzed by Gale\\nand Shapley [169]. The stable-marriage problem has numerous variants.\\nThe books by Gusﬁeld and Irving [203], Knuth [266], and Manlove\\n[313] serve as excellent sources for cataloging and solving them.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 961}),\n",
              " Document(page_content='1 The deﬁnition of a complete bipartite graph differs from the deﬁnition of complete graph\\ngiven on page 1167 because in a bipartite graph, there are no edges between vertices in L and no\\nedges between vertices in R.\\n2 Although marriage norms are changing, it’s traditional to view the stable-marriage problem\\nthrough the lens of heterosexual marriage.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 962}),\n",
              " Document(page_content='Part VII\\xa0\\xa0\\xa0\\xa0Selected Topics', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 963}),\n",
              " Document(page_content='\\xa0\\n\\xa0\\nIntroduction\\nThis part contains a selection of algorithmic topics that extend and\\ncomplement earlier material in this book. Some chapters introduce new\\nmodels of computation such as circuits or parallel computers. Others\\ncover specialized domains such as matrices or number theory. The last\\ntwo chapters discuss some of the known limitations to the design of\\nefﬁcient algorithms and introduce techniques for coping with those\\nlimitations.\\nChapter 26 presents an algorithmic model for parallel computing\\nbased on task-parallel computing, and more speciﬁcally, fork-join\\nparallelism. The chapter introduces the basics of the model, showing\\nhow to quantify parallelism in terms of the measures of work and span.\\nIt then investigates several interesting fork-join algorithms, including\\nalgorithms for matrix multiplication and merge sorting.\\nAn algorithm that receives its input over time, rather than having the\\nentire input available at the start, is called an “online” algorithm.\\nChapter 27 examines techniques used in online algorithms, starting with\\nthe “toy” problem of how long to wait for an elevator before taking the\\nstairs. It then studies the “move-to-front” heuristic for maintaining a\\nlinked list and ﬁnishes with the online version of the caching problem\\nwe saw back in Section 15.4. The analyses of these online algorithms are\\nremarkable in that they prove that these algorithms, which do not know\\ntheir future inputs, perform within a constant factor of optimal\\nalgorithms that know the future inputs.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 964}),\n",
              " Document(page_content='Chapter 28 studies efﬁcient algorithms for operating on matrices. It\\npresents two general methods—LU decomposition and LUP\\ndecomposition—for solving linear equations by Gaussian elimination in\\nO(n3) time. It also shows that matrix inversion and matrix\\nmultiplication can be performed equally fast. The chapter concludes by\\nshowing how to compute a least-squares approximate solution when a\\nset of linear equations has no exact solution.\\nChapter 29 studies how to model problems as linear programs, where\\nthe goal is to maximize or minimize an objective, given limited resources\\nand competing constraints. Linear programming arises in a variety of\\npractical application areas. The chapter also addresses the concept of\\n“duality” which, by establishing that a maximization problem and\\nminimization problem have the same objective value, helps to show that\\nsolutions to each are optimal.\\nChapter 30 studies operations on polynomials and shows how to use\\na well-known signal-processing technique—the fast Fourier transform\\n(FFT)—to multiply two degree-n polynomials in O(n lg n) time. It also\\nderives a parallel circuit to compute the FFT.\\nChapter 31 presents number-theoretic algorithms. After reviewing\\nelementary number theory, it presents Euclid’s algorithm for computing\\ngreatest common divisors. Next, it studies algorithms for solving\\nmodular linear equations and for raising one number to a power\\nmodulo another number. Then, it explores an important application of\\nnumber-theoretic algorithms: the RSA public-key cryptosystem. This\\ncryptosystem can be used not only to encrypt messages so that an\\nadversary cannot read them, but also to provide digital signatures. The\\nchapter ﬁnishes with the Miller-Rabin randomized primality test, which\\nenables ﬁnding large primes efﬁciently—an  essential requirement for the\\nRSA system.\\nChapter 32 studies the problem of ﬁnding all occurrences of a given\\npattern string in a given text string, a problem that arises frequently in\\ntext-editing programs. After examining the naive approach, the chapter\\npresents an elegant approach due to Rabin and Karp. Then, after\\nshowing an efﬁcient solution based on ﬁnite automata, the chapter\\npresents the Knuth-Morris-Pratt algorithm, which modiﬁes the', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 965}),\n",
              " Document(page_content='automaton-based algorithm to save space by cleverly preprocessing the\\npattern. The chapter ﬁnishes by studying sufﬁx arrays, which can not\\nonly ﬁnd a pattern in a text string, but can do quite a bit more, such as\\nﬁnding the longest repeated substring in a text and ﬁnding the longest\\ncommon substring appearing in two texts.\\nChapter 33 examines three algorithms within the expansive ﬁeld of\\nmachine learning. Machine-learning algorithms are designed to take in\\nvast amounts of data, devise hypotheses about patterns in the data, and\\ntest these hypotheses. The chapter starts with k-means clustering, which\\ngroups data elements into k classes based on how similar they are to\\neach other. It then shows how to use the technique of multiplicative\\nweights to make predictions accurately based on a set of “experts” of\\nvarying quality. Perhaps surprisingly, even without knowing which\\nexperts are reliable and which are not, you can predict almost as\\naccurately as the most reliable expert. The chapter ﬁnishes with gradient\\ndescent, an optimization technique that ﬁnds a local minimum value for\\na function. Gradient descent has many applications, including ﬁnding\\nparameter settings for many machine-learning models.\\nChapter 34 concerns NP-complete problems. Many interesting\\ncomputational problems are NP-complete, but no polynomial-time\\nalgorithm is known for solving any of them. This chapter presents\\ntechniques for determining when a problem is NP-complete, using them\\nto prove several classic problems NP-complete: determining whether a\\ngraph has a hamiltonian cycle (a cycle that includes every vertex),\\ndetermining whether a boolean formula is satisﬁable (whether there\\nexists an assignment of boolean values to its variables that causes the\\nformula to evaluate to TRUE), and determining whether a given set of\\nnumbers has a subset that adds up to a given target value. The chapter\\nalso proves that the famous traveling-salesperson problem (ﬁnd a\\nshortest route that starts and ends at the same location and visits each\\nof a set of locations once) is NP-complete.\\nChapter 35 shows how to ﬁnd approximate solutions to NP-\\ncomplete problems efﬁciently by using approximation algorithms. For\\nsome NP-complete problems, approximate solutions that are near\\noptimal are quite easy to produce, but for others even the best\\napproximation algorithms known work progressively more poorly as the', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 966}),\n",
              " Document(page_content='problem size increases. Then, there are some problems for which\\ninvesting increasing amounts of computation time yields increasingly\\nbetter approximate solutions. This chapter illustrates these possibilities\\nwith the vertex-cover problem (unweighted and weighted versions), an\\noptimization version of 3-CNF satisﬁability, the traveling-salesperson\\nproblem, the set-covering problem, and the subset-sum problem.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 967}),\n",
              " Document(page_content='26\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0Parallel Algorithms\\nThe vast majority of algorithms in this book are serial algorithms\\nsuitable for running on a uniprocessor computer that executes only one\\ninstruction at a time. This chapter extends our algorithmic model to\\nencompass parallel algorithms, where multiple instructions can execute\\nsimultaneously. Speciﬁcally, we’ll explore the elegant model of task-\\nparallel algorithms, which are amenable to algorithmic design and\\nanalysis. Our study focuses on fork-join parallel algorithms, the most\\nbasic and best understood kind of task-parallel algorithm. Fork-join\\nparallel algorithms can be expressed cleanly using simple linguistic\\nextensions to ordinary serial code. Moreover, they can be implemented\\nefﬁciently in practice.\\nParallel computers—computers with multiple processing units—are\\nubiquitous. Handheld, laptop, desktop, and cloud machines are all\\nmulticore computers, or simply, multicores, containing multiple\\nprocessing “cores.” Each processing core is a full-ﬂedged processor that\\ncan directly access any location in a common shared memory.\\nMulticores can be aggregated into larger systems, such as clusters, by\\nusing a network to interconnect them. These multicore clusters usually\\nhave a distributed memory, where one multicore’s memory cannot be\\naccessed directly by a processor in another multicore. Instead, the\\nprocessor must explicitly send a message over the cluster network to a\\nprocessor in the remote multicore to request any data it requires. The\\nmost powerful clusters are supercomputers, comprising many thousands\\nof multicores. But since shared-memory programming tends to be\\nconceptually easier than distributed-memory programming, and', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 968}),\n",
              " Document(page_content='multicore machines are widely available, this chapter focuses on parallel\\nalgorithms for multicores.\\nOne approach to programming multicores is thread parallelism. This\\nprocessor-centric parallel-programming model employs a software\\nabstraction of “virtual processors,” or threads that share a common\\nmemory. Each thread maintains its own program counter and can\\nexecute code independently of the other threads. The operating system\\nloads a thread onto a processing core for execution and switches it out\\nwhen another thread needs to run.\\nUnfortunately, programming a shared-memory parallel computer\\nusing threads tends to be difﬁcult and error-prone. One reason is that it\\ncan be complicated to dynamically partition the work among the\\nthreads so that each thread receives approximately the same load. For\\nany but the simplest of applications, the programmer must use complex\\ncommunication protocols to implement a scheduler that load-balances\\nthe work.\\nTask-parallel programming\\nThe difﬁculty of thread programming has led to the creation of task-\\nparallel platforms, which provide a layer of software on top of threads\\nto coordinate, schedule, and manage the processors of a multicore.\\nSome task-parallel platforms are built as runtime libraries, but others\\nprovide full-ﬂedged parallel languages with compiler and runtime\\nsupport.\\nTask-parallel programming allows parallelism to be speciﬁed in a\\n“processor-oblivious” fashion, where the programmer identiﬁes what\\ncomputational tasks may run in parallel but does not indicate which\\nthread or processor performs the task. Thus, the programmer is freed\\nfrom worrying about communication protocols, load balancing, and\\nother vagaries of thread programming. The task-parallel platform\\ncontains a scheduler, which automatically load-balances the tasks across\\nthe processors, thereby greatly simplifying the programmer’s chore.\\nTask-parallel algorithms provide a natural extension to ordinary serial\\nalgorithms, allowing performance to be reasoned about mathematically\\nusing “work/span analysis.”', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 969}),\n",
              " Document(page_content='Fork-join parallelism\\nAlthough the functionality of task-parallel environments is still evolving\\nand increasing, almost all support fork-join parallelism, which is\\ntypically embodied in two linguistic features: spawning and parallel\\nloops. Spawning allows a subroutine to be “forked”: executed like a\\nsubroutine call, except that the caller can continue to execute while the\\nspawned subroutine computes its result. A parallel loop is like an\\nordinary for loop, except that multiple iterations of the loop can execute\\nat the same time.\\nFork-join parallel algorithms employ spawning and parallel loops to\\ndescribe parallelism. A key aspect of this parallel model, inherited from\\nthe task-parallel model but different from the thread model, is that the\\nprogrammer does not specify which tasks in a computation must run in\\nparallel, only which tasks may run in parallel. The underlying runtime\\nsystem uses threads to load-balance the tasks across the processors. This\\nchapter investigates parallel algorithms described in the fork-join\\nmodel, as well as how the underlying runtime system can schedule task-\\nparallel computations (which include fork-join computations)\\nefﬁciently.\\nFork-join parallelism offers several important advantages:\\nThe fork-join programming model is a simple extension of the\\nfamiliar serial programming model used in most of this book. To\\ndescribe a fork-join parallel algorithm, the pseudocode in this\\nbook needs just three added keywords: parallel, spawn, and sync.\\nDeleting these parallel keywords from the parallel pseudocode\\nresults in ordinary serial pseudocode for the same problem, which\\nwe call the “serial projection” of the parallel algorithm.\\nThe underlying task-parallel model provides a theoretically clean\\nway to quantify parallelism based on the notions of “work” and\\n“span.”\\nSpawning allows many divide-and-conquer algorithms to be\\nparallelized naturally. Moreover, just as serial divide-and-conquer\\nalgorithms lend themselves to analysis using recurrences, so do\\nparallel algorithms in the fork-join model.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 970}),\n",
              " Document(page_content='The fork-join programming model is faithful to how multicore\\nprogramming has been evolving in practice. A growing number of\\nmulticore environments support one variant or another of fork-\\njoin parallel programming, including Cilk [290, 291, 383, 396],\\nHabanero-Java [466], the Java Fork-Join Framework [279],\\nOpenMP [81], Task Parallel Library [289], Threading Building\\nBlocks [376], and X10 [82].\\nSection 26.1 introduces parallel pseudocode, shows how the\\nexecution of a task-parallel computation can be modeled as a directed\\nacyclic graph, and presents the metrics of work, span, and parallelism,\\nwhich you can use to analyze parallel algorithms. Section 26.2\\ninvestigates how to multiply matrices in parallel, and Section 26.3\\ntackles the tougher problem of designing an efﬁcient parallel merge sort.\\n26.1\\xa0\\xa0\\xa0\\xa0The basics of fork-join parallelism\\nOur exploration of parallel programming begins with the problem of\\ncomputing Fibonacci numbers recursively in parallel. We’ll look at a\\nstraightforward serial Fibonacci calculation, which, although inefﬁcient,\\nserves as a good illustration of how to express parallelism in\\npseudocode.\\nRecall that the Fibonacci numbers are deﬁned by equation (3.31) on\\npage 69:\\nTo calculate the nth Fibonacci number recursively, you could use the\\nordinary serial algorithm in the procedure FIB on the facing page. You\\nwould not really want to compute large Fibonacci numbers this way,\\nbecause this computation does needless repeated work, but parallelizing\\nit can be instructive.\\nFIB (n)', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 971}),\n",
              " Document(page_content='1if\\xa0n ≤ 1\\n2return\\xa0n\\n3else\\xa0x = FIB (n − 1)\\n4y = FIB (n − 2)\\n5return\\xa0x + y\\nTo analyze this algorithm, let T (n) denote the running time of FIB\\n(n). Since FIB (n) contains two recursive calls plus a constant amount of\\nextra work, we obtain the recurrence\\nT (n) = T (n − 1) + T (n − 2) + Θ (1).\\nThis recurrence has solution T (n) = Θ (Fn), which we can establish by\\nusing the substitution method (see Section 4.3). To show that T (n) =\\nO(Fn), we’ll adopt the inductive hypothesis that T (n) ≤ aFn − b, where a\\n> 1 and b > 0 are constants. Substituting, we obtain\\nT (n)≤(aFn−1 − b) + (aFn−2 − b) + Θ (1)\\n=a(Fn−1 + Fn−2) − 2b + Θ (1)\\n≤aFn − b,\\nif we choose b large enough to dominate the upper-bound constant in\\nthe Θ (1) term. We can then choose a large enough to upper-bound the\\nΘ(1) base case for small n. To show that T (n) = Ω(Fn), we use the\\ninductive hypothesis T (n) ≥ aFn − b. Substituting and following\\nreasoning similar to the asymptotic upper-bound argument, we\\nestablish this hypothesis by choosing b smaller than the lower-bound\\nconstant in the Θ (1) term and a small enough to lower-bound the Θ (1)\\nbase case for small n. Theorem 3.1 on page 56 then establishes that T (n)\\n= Θ(Fn), as desired. Since Fn = Θ ( ϕn), where \\n  is the\\ngolden ratio, by equation (3.34) on page 69, it follows that\\nThus this procedure is a particularly slow way to compute Fibonacci\\nnumbers, since it runs in exponential time. (See Problem 31-3 on page', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 972}),\n",
              " Document(page_content='954 for faster ways.)\\nLet’s see why the algorithm is inefﬁcient. Figure 26.1 shows the tree\\nof recursive procedure instances created when computing F6 with the\\nFIB procedure. The call to FIB(6) recursively calls FIB(5) and then\\nFIB(4). But, the call to FIB(5) also results in a call to FIB(4). Both\\ninstances of FIB(4) return the same result (F4 = 3). Since the FIB\\nprocedure does not memoize (recall the deﬁnition of “memoize” from\\npage 368), the second call to FIB(4) replicates the work that the ﬁrst call\\nperforms, which is wasteful.\\nFigure 26.1 The invocation tree for FIB(6). Each node in the tree represents a procedure instance\\nwhose children are the procedure instances it calls during its execution. Since each instance of\\nFIB with the same argument does the same work to produce the same result, the inefﬁciency of\\nthis algorithm for computing the Fibonacci numbers can be seen by the vast number of repeated\\ncalls to compute the same thing. The portion of the tree shaded blue appears in task-parallel\\nform in Figure 26.2.\\nAlthough the FIB procedure is a poor way to compute Fibonacci\\nnumbers, it can help us warm up to parallelism concepts. Perhaps the\\nmost basic concept is to understand is that if two parallel tasks operate\\non entirely different data, then—absent other interference—they each\\nproduce the same outcomes when executed at the same time as when\\nthey run serially one after the other. Within FIB (n), for example, the\\ntwo recursive calls in line 3 to FIB (n − 1) and in line 4 to FIB (n − 2)', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 973}),\n",
              " Document(page_content='can safely execute in parallel because the computation performed by\\none in no way affects the other.\\nParallel keywords\\nThe P-FIB procedure on the next page computes Fibonacci numbers,\\nbut using the parallel keywords spawn and sync to indicate parallelism in\\nthe pseudocode.\\nIf the keywords spawn and sync are deleted from P-FIB, the resulting\\npseudocode text is identical to FIB (other than renaming the procedure\\nin the header and in the two recursive calls). We deﬁne the serial\\nprojection1 of a parallel algorithm to be the serial algorithm that results\\nfrom ignoring the parallel directives, which in this case can be done by\\nomitting the keywords spawn and sync. For parallel for loops, which\\nwe’ll see later on, we omit the keyword parallel. Indeed, our parallel\\npseudocode possesses the elegant property that its serial projection is\\nalways ordinary serial pseudocode to solve the same problem.\\nP-FIB (n)\\n1if\\xa0n ≤ 1\\n2 return\\xa0n\\n3else\\xa0x = spawn P-FIB (n − 1)// don’t wait for subroutine to return\\n4 y = P-FIB (n − 2) // in parallel with spawned subroutine\\n5 sync // wait for spawned subroutine to ﬁnish\\n6 return\\xa0x + y\\nSemantics of parallel keywords\\nSpawning occurs when the keyword spawn precedes a procedure call, as\\nin line 3 of P-FIB. The semantics of a spawn differs from an ordinary\\nprocedure call in that the procedure instance that executes the spawn—\\nthe parent—may continue to execute in parallel with the spawned\\nsubroutine—its child—instead of waiting for the child to ﬁnish, as\\nwould happen in a serial execution. In this case, while the spawned child\\nis computing P-FIB (n − 1), the parent may go on to compute P-FIB\\n(n−2) in line 4 in parallel with the spawned child. Since the P-FIB', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 974}),\n",
              " Document(page_content='procedure is recursive, these two subroutine calls themselves create\\nnested parallelism, as do their children, thereby creating a potentially\\nvast tree of subcomputations, all executing in parallel.\\nThe keyword spawn does not say, however, that a procedure must\\nexecute in parallel with its spawned children, only that it may. The\\nparallel keywords express the logical parallelism of the computation,\\nindicating which parts of the computation may proceed in parallel. At\\nruntime, it is up to a scheduler to determine which subcomputations\\nactually run in parallel by assigning them to available processors as the\\ncomputation unfolds. We’ll discuss the theory behind task-parallel\\nschedulers shortly (on page 759).\\nA procedure cannot safely use the values returned by its spawned\\nchildren until after it executes a sync statement, as in line 5. The\\nkeyword sync indicates that the procedure must wait as necessary for all\\nits spawned children to ﬁnish before proceeding to the statement after\\nthe sync—the “join” of a fork-join parallel computation. The P-FIB\\nprocedure requires a sync before the return statement in line 6 to avoid\\nthe anomaly that would occur if x and y were summed before P-FIB (n\\n− 1) had ﬁnished and its return value had been assigned to x. In\\naddition to explicit join synchronization provided by the sync statement,\\nit is convenient to assume that every procedure executes a sync implicitly\\nbefore it returns, thus ensuring that all children ﬁnish before their\\nparent ﬁnishes.\\nA graph model for parallel execution\\nIt helps to view the execution of a parallel computation—the dynamic\\nstream of runtime instructions executed by processors under the\\ndirection of a parallel program—as a directed acyclic graph G = (V, E),\\ncalled a (parallel) trace.2 Conceptually, the vertices in V are executed\\ninstructions, and the edges in E represent dependencies between\\ninstructions, where (u, v) ∈ E means that the parallel program required\\ninstruction u to execute before instruction v.\\nIt’s sometimes inconvenient, especially if we want to focus on the\\nparallel structure of a computation, for a vertex of a trace to represent\\nonly one executed instruction. Consequently, if a chain of instructions', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 975}),\n",
              " Document(page_content='contains no parallel or procedural control (no spawn, sync, procedure\\ncall, or return—via either an explicit return statement or the return that\\nhappens implicitly upon reaching the end of a procedure), we group the\\nentire chain into a single strand. As an example, Figure 26.2 shows the\\ntrace that results from computing P-FIB(4) in the portion of Figure 26.1\\nshaded blue. Strands do not include instructions that involve parallel or\\nprocedural control. These control dependencies must be represented as\\nedges in the trace.\\nWhen a parent procedure calls a child, the trace contains an edge (u,\\nv) from the strand u in the parent that executes the call to the ﬁrst\\nstrand v of the spawned child, as illustrated in Figure 26.2 by the edge\\nfrom the orange strand in P-FIB(4) to the blue strand in P-FIB(2).\\nWhen the last strand v′ in the child returns, the trace contains an edge\\n(v′, u′) to the strand u′, where u′ is the successor strand of u in the\\nparent, as with the edge from the white strand in P-FIB(2) to the white\\nstrand in P-FIB(4).', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 976}),\n",
              " Document(page_content='Figure 26.2 The trace of P-FIB(4) corresponding to the shaded portion of Figure 26.1. Each\\ncircle represents one strand, with blue circles representing any instructions executed in the part\\nof the procedure (instance) up to the spawn of P-FIB (n − 1) in line 3; orange circles\\nrepresenting the instructions executed in the part of the procedure that calls P-FIB (n − 2) in\\nline 4 up to the sync in line 5, where it suspends until the spawn of P-FIB (n − 1) returns; and\\nwhite circles representing the instructions executed in the part of the procedure after the sync,\\nwhere it sums x and y, up to the point where it returns the result. Strands belonging to the same\\nprocedure are grouped into a rounded rectangle, blue for spawned procedures and tan for called\\nprocedures. Assuming that each strand takes unit time, the work is 17 time units, since there are\\n17 strands, and the span is 8 time units, since the critical path—shown with blue edges—\\ncontains 8 strands.\\nWhen the parent spawns a child, however, the trace is a little\\ndifferent. The edge (u, v) goes from parent to child as with a call, such\\nas the edge from the blue strand in P-FIB(4) to the blue strand in P-\\nFIB(3), but the trace contains another edge (u, u′) as well, indicating\\nthat u’s successor strand u′ can continue to execute while v is executing.\\nThe edge from the blue strand in P-FIB(4) to the orange strand in P-\\nFIB(4) illustrates one such edge. As with a call, there is an edge from the\\nlast strand v′ in the child, but with a spawn, it no longer goes to u’s\\nsuccessor. Instead, the edge is (v′, x), where x is the strand immediately\\nfollowing the sync in the parent that ensures that the child has ﬁnished,\\nas with the edge from the white strand in P-FIB(3) to the white strand\\nin P-FIB(4).', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 977}),\n",
              " Document(page_content='You can ﬁgure out what parallel control created a particular trace. If\\na strand has two successors, one of them must have been spawned, and\\nif a strand has multiple predecessors, the predecessors joined because of\\na sync statement. Thus, in the general case, the set V forms the set of\\nstrands, and the set E of directed edges represents dependencies between\\nstrands induced by parallel and procedural control. If G contains a\\ndirected path from strand u to strand v, we say that the two strands are\\n(logically) in series. If there is no path in G either from u to v or from v\\nto u, the strands are (logically) in parallel.\\nA fork-join parallel trace can be pictured as a dag of strands\\nembedded in an invocation tree of procedure instances. For example,\\nFigure 26.1 shows the invocation tree for FIB(6), which also serves as\\nthe invocation tree for P-FIB(6), the edges between procedure instances\\nnow representing either calls or spawns. Figure 26.2 zooms in on the\\nsubtree that is shaded blue, showing the strands that constitute each\\nprocedure instance in P-FIB(4). All directed edges connecting strands\\nrun either within a procedure or along undirected edges of the\\ninvocation tree in Figure 26.1. (More general task-parallel traces that\\nare not fork-join traces may contain some directed edges that do not\\nrun along the undirected tree edges.)\\nOur analyses generally assume that parallel algorithms execute on an\\nideal parallel computer, which consists of a set of processors and a\\nsequentially consistent shared memory. To understand sequential\\nconsistency, you ﬁrst need to know that memory is accessed by load\\ninstructions, which copy data from a location in the memory to a\\nregister within a processor, and by store instructions, which copy data\\nfrom a processor register to a location in the memory. A single line of\\npseudocode can entail several such instructions. For example, the line x\\n= y + z could result in load instructions to fetch each of y and z from\\nmemory into a processor, an instruction to add them together inside the\\nprocessor, and a store instruction to place the result x back into\\nmemory. In a parallel computer, several processors might need to load\\nor store at the same time. Sequential consistency means that even if\\nmultiple processors attempt to access the memory simultaneously, the\\nshared memory behaves as if exactly one instruction from one of the\\nprocessors is executed at a time, even though the actual transfer of data', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 978}),\n",
              " Document(page_content='may happen at the same time. It is as if the instructions were executed\\none at a time sequentially according to some global linear order among\\nall the processors that preserves the individual orders in which each\\nprocessor executes its own instructions.\\nFor task-parallel computations, which are scheduled onto processors\\nautomatically by a runtime system, the sequentially consistent shared\\nmemory behaves as if a parallel computation’s executed instructions\\nwere executed one by one in the order of a topological sort (see Section\\n20.4) of its trace. That is, you can reason about the execution by\\nimagining that the individual instructions (not generally the strands,\\nwhich may aggregate many instructions) are interleaved in some linear\\norder that preserves the partial order of the trace. Depending on\\nscheduling, the linear order could vary from one run of the program to\\nthe next, but the behavior of any execution is always as if the\\ninstructions executed serially in a linear order consistent with the\\ndependencies within the trace.\\nIn addition to making assumptions about semantics, the ideal\\nparallel-computer model makes some performance assumptions.\\nSpeciﬁcally, it assumes that each processor in the machine has equal\\ncomputing power, and it ignores the cost of scheduling. Although this\\nlast assumption may sound optimistic, it turns out that for algorithms\\nwith sufﬁcient “parallelism” (a term we’ll deﬁne precisely a little later),\\nthe overhead of scheduling is generally minimal in practice.\\nPerformance measures\\nWe can gauge the theoretical efﬁciency of a task-parallel algorithm\\nusing work/span analysis, which is based on two metrics: “work” and\\n“span.” The work of a task-parallel computation is the total time to\\nexecute the entire computation on one processor. In other words, the\\nwork is the sum of the times taken by each of the strands. If each strand\\ntakes unit time, the work is just the number of vertices in the trace. The\\nspan is the fastest possible time to execute the computation on an\\nunlimited number of processors, which corresponds to the sum of the\\ntimes taken by the strands along a longest path in the trace, where\\n“longest” means that each strand is weighted by its execution time. Such', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 979}),\n",
              " Document(page_content='a longest path is called the critical path of the trace, and thus the span is\\nthe weight of the longest (weighted) path in the trace. (Section 22.2,\\npages 617–619 shows how to ﬁnd a critical path in a dag G = (V, E) in\\nΘ(V + E) time.) For a trace in which each strand takes unit time, the\\nspan equals the number of strands on the critical path. For example, the\\ntrace of Figure 26.2 has 17 vertices in all and 8 vertices on its critical\\npath, so that if each strand takes unit time, its work is 17 time units and\\nits span is 8 time units.\\nThe actual running time of a task-parallel computation depends not\\nonly on its work and its span, but also on how many processors are\\navailable and how the scheduler allocates strands to processors. To\\ndenote the running time of a task-parallel computation on P processors,\\nwe subscript by P. For example, we might denote the running time of an\\nalgorithm on P processors by TP. The work is the running time on a\\nsingle processor, or T1. The span is the running time if we could run\\neach strand on its own processor—in other words, if we had an\\nunlimited number of processors—an d so we denote the span by T∞.\\nThe work and span provide lower bounds on the running time TP of\\na task-parallel computation on P processors:\\nIn one step, an ideal parallel computer with P processors can do\\nat most P units of work, and thus in TP time, it can perform at\\nmost P TP work. Since the total work to do is T1, we have P TP ≥\\nT1. Dividing by P yields the work law:\\nA P-processor ideal parallel computer cannot run any faster than\\na machine with an unlimited number of processors. Looked at\\nanother way, a machine with an unlimited number of processors\\ncan emulate a P-processor machine by using just P of its\\nprocessors. Thus, the span law follows:\\n', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 980}),\n",
              " Document(page_content='We deﬁne the speedup of a computation on P processors by the ratio\\nT1/TP, which says how many times faster the computation runs on P\\nprocessors than on one processor. By the work law, we have TP ≥ T1/P,\\nwhich implies that T1/TP ≤ P. Thus, the speedup on a P-processor ideal\\nparallel computer can be at most P. When the speedup is linear in the\\nnumber of processors, that is, when T1/TP = Θ (P), the computation\\nexhibits linear speedup. Perfect linear speedup occurs when T1/TP = P.\\nThe ratio T1/T∞ of the work to the span gives the parallelism of the\\nparallel computation. We can view the parallelism from three\\nperspectives. As a ratio, the parallelism denotes the average amount of\\nwork that can be performed in parallel for each step along the critical\\npath. As an upper bound, the parallelism gives the maximum possible\\nspeedup that can be achieved on any number of processors. Perhaps\\nmost important, the parallelism provides a limit on the possibility of\\nattaining perfect linear speedup. Speciﬁcally, once the number of\\nprocessors exceeds the parallelism, the computation cannot possibly\\nachieve perfect linear speedup. To see this last point, suppose that P >\\nT1/T∞, in which case the span law implies that the speedup satisﬁes\\nT1/TP ≤ T1/T∞ < P. Moreover, if the number P of processors in the\\nideal parallel computer greatly exceeds the parallelism—that is, if P ≫\\nT1/T∞—then T1/TP ≪ P, so that the speedup is much less than the\\nnumber of processors. In other words, if the number of processors\\nexceeds the parallelism, adding even more processors makes the\\nspeedup less perfect.\\nAs an example, consider the computation P-FIB(4) in Figure 26.2,\\nand assume that each strand takes unit time. Since the work is T1 = 17\\nand the span is T∞ = 8, the parallelism is T1/T∞ = 17/8 = 2.125.\\nConsequently, achieving much more than double the performance is\\nimpossible, no matter how many processors execute the computation.\\nFor larger input sizes, however, we’ll see that P-FIB (n) exhibits\\nsubstantial parallelism.\\nWe deﬁne the (parallel) slackness of a task-parallel computation\\nexecuted on an ideal parallel computer with P processors to be the ratio', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 981}),\n",
              " Document(page_content='(T1/T∞)/P = T1/(P T∞), which is the factor by which the parallelism of\\nthe computation exceeds the number of processors in the machine.\\nRestating the bounds on speedup, if the slackness is less than 1, perfect\\nlinear speedup is impossible, because T1/(P T∞) < 1 and the span law\\nimply that T1/TP ≤ T1/T∞ < P. Indeed, as the slackness decreases from\\n1 and approaches 0, the speedup of the computation diverges further\\nand further from perfect linear speedup. If the slackness is less than 1,\\nadditional parallelism in an algorithm can have a great impact on its\\nexecution efﬁciency. If the slackness is greater than 1, however, the work\\nper processor is the limiting constraint. We’ll see that as the slackness\\nincreases from 1, a good scheduler can achieve closer and closer to\\nperfect linear speedup. But once the slackness is much greater than 1,\\nthe advantage of additional parallelism shows diminishing returns.\\nScheduling\\nGood performance depends on more than just minimizing the work and\\nspan. The strands must also be scheduled efﬁciently onto the processors\\nof the parallel machine. Our fork-join parallel-programming model\\nprovides no way for a programmer to specify which strands to execute\\non which processors. Instead, we rely on the runtime system’s scheduler\\nto map the dynamically unfolding computation to individual processors.\\nIn practice, the scheduler maps the strands to static threads, and the\\noperating system schedules the threads on the processors themselves.\\nBut this extra level of indirection is unnecessary for our understanding\\nof scheduling. We can just imagine that the scheduler maps strands to\\nprocessors directly.\\nA task-parallel scheduler must schedule the computation without\\nknowing in advance when procedures will be spawned or when they will\\nﬁnish—that is, it must operate online. Moreover, a good scheduler\\noperates in a distributed fashion, where the threads implementing the\\nscheduler cooperate to load-balance the computation. Provably good\\nonline, distributed schedulers exist, but analyzing them is complicated.\\nInstead, to keep our analysis simple, we’ll consider an online centralized\\nscheduler that knows the global state of the computation at any\\nmoment.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 982}),\n",
              " Document(page_content='In particular, we’ll analyze greedy schedulers, which assign as many\\nstrands to processors as possible in each time step, never leaving a\\nprocessor idle if there is work that can be done. We’ll classify each step\\nof a greedy scheduler as follows:\\nComplete step: At least P strands are ready to execute, meaning\\nthat all strands on which they depend have ﬁnished execution. A\\ngreedy scheduler assigns any P of the ready strands to the\\nprocessors, completely utilizing all the processor resources.\\nIncomplete step: Fewer than P strands are ready to execute. A\\ngreedy scheduler assigns each ready strand to its own processor,\\nleaving some processors idle for the step, but executing all the\\nready strands.\\nThe work law tells us that the fastest running time TP that we can\\nhope for on P processors must be at least T1/P. The span law tells us\\nthat the fastest possible running time must be at least T∞. The following\\ntheorem shows that greedy scheduling is provably good in that it\\nachieves the sum of these two lower bounds as an upper bound.\\nTheorem 26.1\\nOn an ideal parallel computer with P processors, a greedy scheduler\\nexecutes a task-parallel computation with work T1 and span T∞ in time\\nProof\\xa0\\xa0\\xa0Without loss of generality, assume that each strand takes unit\\ntime. (If necessary, replace each longer strand by a chain of unit-time\\nstrands.) We’ll consider complete and incomplete steps separately.\\nIn each complete step, the P processors together perform a total of P\\nwork. Thus, if the number of complete steps is k, the total work\\nexecuting all the complete steps is kP. Since the greedy scheduler\\ndoesn’t execute any strand more than once and only T1 work needs to\\nbe performed, it follows that kP ≤ T1, from which we can conclude that\\nthe number k of complete steps is at most T1/P.', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 983}),\n",
              " Document(page_content='Now, let’s consider an incomplete step. Let G be the trace for the\\nentire computation, let G′ be the subtrace of G that has yet to be\\nexecuted at the start of the incomplete step, and let G″ be the subtrace\\nremaining to be executed after the incomplete step. Consider the set R\\nof strands that are ready at the beginning of the incomplete step, where\\n|R| < P. By deﬁnition, if a strand is ready, all its predecessors in trace G\\nhave executed. Thus the predecessors of strands in R do not belong to\\nG′. A longest path in G′ must necessarily start at a strand in R, since\\nevery other strand in G′ has a predecessor and thus could not start a\\nlongest path. Because the greedy scheduler executes all ready strands\\nduring the incomplete step, the strands of G″ are exactly those in G′\\nminus the strands in R. Consequently, the length of a longest path in G″\\nmust be 1 less than the length of a longest path in G′. In other words,\\nevery incomplete step decreases the span of the trace remaining to be\\nexecuted by 1. Hence, the number of incomplete steps can be at most\\nT∞.\\nSince each step is either complete or incomplete, the theorem follows.\\n▪\\nThe following corollary shows that a greedy scheduler always\\nperforms well.\\nCorollary 26.2\\nThe running time TP of any task-parallel computation scheduled by a\\ngreedy scheduler on a P-processor ideal parallel computer is within a\\nfactor of 2 of optimal.\\nProof\\xa0\\xa0\\xa0Let T*P be the running time produced by an optimal scheduler\\non a machine with P processors, and let T1 and T∞ be the work and\\nspan of the computation, respectively. Since the work and span laws—\\ninequalities (26.2) and (26.3)—give \\n , Theorem 26.1\\nimplies that\\n', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 984}),\n",
              " Document(page_content='▪\\nThe next corollary shows that, in fact, a greedy scheduler achieves\\nnear-perfect linear speedup on any task-parallel computation as the\\nslackness grows.\\nCorollary 26.3\\nLet TP be the running time of a task-parallel computation produced by\\na greedy scheduler on an ideal parallel computer with P processors, and\\nlet T1 and T∞be the work and span of the computation, respectively.\\nThen, if P ≪ T1/T∞, or equivalently, the parallel slackness is much\\ngreater than 1, we have TP ≈ T1/P, a speedup of approximately P.\\nProof\\xa0\\xa0\\xa0If we suppose that P ≪ T1/T∞, then it follows that T∞ ≪ T1/P,\\nand hence Theorem 26.1 gives TP ≤ T1/P + T∞ ≈ T1/P. Since the work\\nlaw (26.2) dictates that TP ≥ T1/P, we conclude that TP ≈ T1/P, which is\\na speedup of T1/TP ≈ P.\\n▪\\nThe ≪ symbol denotes “much less,” but how much is “much less”?\\nAs a rule of thumb, a slackness of at least 10—t hat is, 10 times more\\nparallelism than processors—generally sufﬁces to achieve good speedup.\\nThen, the span term in the greedy bound, inequality (26.4), is less than\\n10% of the work-per-processor term, which is good enough for most\\nengineering situations. For example, if a computation runs on only 10 o r\\n100 processors, it doesn’t make sense to value parallelism of, say\\n1,000,000, over parallelism of 10,000, even with the factor of 100\\ndifference. As Problem 26-2 shows, sometimes reducing extreme\\nparallelism yields algorithms that are better with respect to other\\nconcerns and which still scale up well on reasonable numbers of\\nprocessors.\\nAnalyzing parallel algorithms\\nWe now have all the tools we need to analyze parallel algorithms using\\nwork/span analysis, allowing us to bound an algorithm’s running time', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 985}),\n",
              " Document(page_content='on any number of processors. Analyzing the work is relatively\\nstraightforward, since it amounts to nothing more than analyzing the\\nrunning time of an ordinary serial algorithm, namely, the serial\\nprojection of the parallel algorithm. You should already be familiar\\nwith analyzing work, since that is what most of this textbook is about!\\nAnalyzing the span is the new thing that parallelism engenders, but it’s\\ngenerally no harder once you get the hang of it. Let’s investigate the\\nbasic ideas using the P-FIB program.\\nAnalyzing the work T1(n) of P-FIB (n) poses no hurdles, because\\nwe’ve already done it. The serial projection of P-FIB is effectively the\\noriginal FIB procedure, and hence, we have T1(n) = T (n) = Θ ( ϕn) from\\nequation (26.1).\\nFigure 26.3 illustrates how to analyze the span. If two traces are\\njoined in series, their spans add to form the span of their composition,\\nwhereas if they are joined in parallel, the span of their composition is\\nthe maximum of the spans of the two traces. As it turns out, the trace of\\nany fork-join parallel computation can be built up from single strands\\nby series-parallel composition.\\nFigure 26.3 Series-parallel composition of parallel traces. (a) When two traces are joined in\\nseries, the work of the composition is the sum of their work, and the span of the composition is\\nthe sum of their spans. (b) When two traces are joined in parallel, the work of the composition\\nremains the sum of their work, but the span of the composition is only the maximum of their\\nspans.\\nArmed with an understanding of series-parallel composition, we can\\nanalyze the span of P-FIB (n). The spawned call to P-FIB (n − 1) in line', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 986}),\n",
              " Document(page_content='3 runs in parallel with the call to P-FIB (n − 2) in line 4. Hence, we can\\nexpress the span of P-FIB (n) as the recurrence\\nT∞(n)=max {T∞(n − 1), T∞(n − 2)} + Θ (1)\\n=T∞(n − 1) + Θ (1),\\nwhich has solution T∞(n) = Θ (n). (The second equality above follows\\nfrom the ﬁrst because P-FIB (n − 1) uses P-FIB (n − 2) in its\\ncomputation, so that the span of P-FIB (n − 1) must be at least as large\\nas the span of P-FIB (n − 2).)\\nThe parallelism of P-FIB (n) is T1(n)/T∞(n) = Θ ( ϕn/n), which grows\\ndramatically as n gets large. Thus, Corollary 26.3 tells us that on even\\nthe largest parallel computers, a modest value for n sufﬁces to achieve\\nnear perfect linear speedup for P-FIB (n), because this procedure\\nexhibits considerable parallel slackness.\\nParallel loops\\nMany algorithms contain loops for which all the iterations can operate\\nin parallel. Although the spawn and sync keywords can be used to\\nparallelize such loops, it is more convenient to specify directly that the\\niterations of such loops can run in parallel. Our pseudocode provides\\nthis functionality via the parallel keyword, which precedes the for\\nkeyword in a for loop statement.\\nAs an example, consider the problem of multiplying a square n × n\\nmatrix A = (aij) by an n-vector x = (xj). The resulting n-vector y = (yi) is\\ngiven by the equation\\nfor i = 1, 2, … , n. The P-MAT-VEC procedure performs matrix-vector\\nmultiplication (actually, y = y + Ax) by computing all the entries of y in\\nparallel. The parallel for keywords in line 1 of P-MAT-VEC indicate\\nthat the n iterations of the loop body, which includes a serial for loop,\\nmay be run in parallel. The initialization y = 0, if desired, should be', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 987}),\n",
              " Document(page_content='performed before calling the procedure (and can be done with a parallel\\nfor loop).\\nP-MAT-VEC (A, x, y, n)\\n1parallel for\\xa0i = 1 to\\xa0n // parallel loop\\n2 for\\xa0j = 1 to\\xa0n // serial loop\\n3 yi = yi + aij\\xa0xj\\nCompilers for fork-join parallel programs can implement parallel for\\nloops in terms of spawn and sync by using recursive spawning. For\\nexample, for the parallel for loop in lines 1–3, a compiler can generate\\nthe auxiliary subroutine P-MAT-VEC-RECURSIVE and call P-MAT-\\nVEC-RECURSIVE (A, x, y, n, 1, n) in the place where the loop would\\nbe in the compiled code. As Figure 26.4 illustrates, this procedure\\nrecursively spawns the ﬁrst half of the iterations of the loop to execute\\nin parallel (line 5) with the second half of the iterations (line 6) and then\\nexecutes a sync (line 7), thereby creating a binary tree of parallel\\nexecution. Each leaf represents a base case, which is the serial for loop\\nof lines 2–3.\\nP-MAT-VEC-RECURSIVE (A, x, y, n, i, i ′)\\n1if\\xa0i == i′ // just one iteration to do?\\n2 for\\xa0j = 1 to\\xa0n // mimic P-MAT-VEC serial loop\\n3 yi = yi + aij\\xa0xj\\n4else\\xa0mid = ⌊(i + i′)/2 ⌋ // parallel divide-and-conquer\\n5 spawn P-MAT-VEC-RECURSIVE (A, x, y, n, i, mid)\\n6 P-MAT-VEC-RECURSIVE (A, x, y, n, mid + 1, i ′)\\n7 sync\\nTo calculate the work T1(n) of P-MAT-VEC on an n×n matrix,\\nsimply compute the running time of its serial projection, which comes\\nfrom replacing the parallel for loop in line 1 with an ordinary for loop.\\nThe running time of the resulting serial pseudocode is Θ (n2), which', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 988}),\n",
              " Document(page_content='means that T1(n) = Θ (n2). This analysis seems to ignore the overhead\\nfor recursive spawning in implementing the parallel loops, however.\\nIndeed, the overhead of recursive spawning does increase the work of a\\nparallel loop compared with that of its serial projection, but not\\nasymptotically. To see why, observe that since the tree of recursive\\nprocedure instances is a full binary tree, the number of internal nodes is\\none less than the number of leaves (see Exercise B.5-3 on page 1175) .\\nEach internal node performs constant work to divide the iteration\\nrange, and each leaf corresponds to a base case, which takes at least\\nconstant time ( Θ (n) time in this case). Thus, by amortizing the overhead\\nof recursive spawning over the work of the iterations in the leaves, we\\nsee that the overall work increases by at most a constant factor.\\nFigure 26.4 A trace for the computation of P-MAT-VEC-RECURSIVE (A, x, y, 8, 1, 8). The\\ntwo numbers within each rounded rectangle give the values of the last two parameters (i and i′ in\\nthe procedure header) in the invocation (spawn, in blue, or call, in tan) of the procedure. The\\nblue circles represent strands corresponding to the part of the procedure up to the spawn of P-\\nMAT-VEC-RECURSIVE in line 5. The orange circles represent strands corresponding to the\\npart of the procedure that calls P-MAT-VEC-RECURSIVE in line 6 up to the sync in line 7,\\nwhere it suspends until the spawned subroutine in line 5 returns. The white circles represent\\nstrands corresponding to the (negligible) part of the procedure after the sync up to the point\\nwhere it returns.\\nTo reduce the overhead of recursive spawning, task-parallel\\nplatforms sometimes coarsen the leaves of the recursion by executing\\nseveral iterations in a single leaf, either automatically or under', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 989}),\n",
              " Document(page_content='programmer control. This optimization comes at the expense of\\nreducing the parallelism. If the computation has sufﬁcient parallel\\nslackness, however, near-perfect linear speedup won’t be sacriﬁced.\\nAlthough recursive spawning doesn’t affect the work of a parallel\\nloop asymptotically, we must take it into account when analyzing the\\nspan. Consider a parallel loop with n iterations in which the ith iteration\\nhas span iter∞(i). Since the depth of recursion is logarithmic in the\\nnumber of iterations, the parallel loop’s span is\\nT∞(n) = Θ (lg n) + max {iter∞(i) : 1 ≤ i ≤ n}.\\nFor example, let’s compute the span of the doubly nested loops in\\nlines 1–3 of P-MAT-VEC. The span for the parallel for loop control is\\nΘ(lg n). For each iteration of the outer parallel loop, the inner serial for\\nloop contains n iterations of line 3. Since each iteration takes constant\\ntime, the total span for the inner serial for loop is Θ (n), no matter which\\niteration of the outer parallel for loop it’s in. Thus, taking the maximum\\nover all iterations of the outer loop and adding in the Θ (lg n) for loop\\ncontrol yields an overall span of T∞n = Θ (n) + Θ (lg n) = Θ (n) for the\\nprocedure. Since the work is Θ (n2), the parallelism is Θ (n2)/Θ(n) = Θ (n).\\n(Exercise 26.1-7 asks you to provide an implementation with even more\\nparallelism.)\\nRace conditions\\nA parallel algorithm is deterministic if it always does the same thing on\\nthe same input, no matter how the instructions are scheduled on the\\nmulticore computer. It is nondeterministic if its behavior might vary\\nfrom run to run when the input is the same. A parallel algorithm that is\\nintended to be deterministic may nevertheless act nondeterministically,\\nhowever, if it contains a difﬁcult-to-diagnose bug called a “determinacy\\nrace.”\\nFamous race bugs include the Therac-25 radiation therapy machine,\\nwhich killed three people and injured several others, and the Northeast\\nBlackout of 2003, which left over 50 million people in the United States\\nwithout power. These pernicious bugs are notoriously hard to ﬁnd. You', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 990}),\n",
              " Document(page_content='can run tests in the lab for days without a failure, only to discover that\\nyour software sporadically crashes in the ﬁeld, sometimes with dire\\nconsequences.\\nA determinacy race occurs when two logically parallel instructions\\naccess the same memory location and at least one of the instructions\\nmodiﬁes the value stored in the location. The toy procedure RACE-\\nEXAMPLE on the following page illustrates a determinacy race. After\\ninitializing x to 0 in line 1, RACE-EXAMPLE creates two parallel\\nstrands, each of which increments x in line 3. Although it might seem\\nthat a call of RACE-EXAMPLE should always print the value 2 (its\\nserial projection certainly does), it could instead print the value 1. Let’s\\nsee how this anomaly might occur.\\nWhen a processor increments x, the operation is not indivisible, but\\nis composed of a sequence of instructions:\\nFigure 26.5 Illustration of the determinacy race in RACE-EXAMPLE. (a) A trace showing the\\ndependencies among individual instructions. The processor registers are r1 and r2. Instructions\\nunrelated to the race, such as the implementation of loop control, are omitted. (b) An execution\\nsequence that elicits the bug, showing the values of x in memory and registers r1 and r2 for each\\nstep in the execution sequence.\\nRACE-EXAMPLE ( )\\n1x = 0\\n2parallel for\\xa0i = 1 to 2\\n3 x = x + 1 // determinacy race', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 991}),\n",
              " Document(page_content='4print x\\nLoad x from memory into one of the processor’s registers.\\nIncrement the value in the register.\\nStore the value in the register back into x in memory.\\nFigure 26.5(a) illustrates a trace representing the execution of RACE-\\nEXAMPLE, with the strands broken down to individual instructions.\\nRecall that since an ideal parallel computer supports sequential\\nconsistency, you can view the parallel execution of a parallel algorithm\\nas an interleaving of instructions that respects the dependencies in the\\ntrace. Part (b) of the ﬁgure shows the values in an execution of the\\ncomputation that elicits the anomaly. The value x is kept in memory,\\nand r1 and r2 are processor registers. In step 1, one of the processors\\nsets x to 0. In steps 2 and 3, processor 1 loads x from memory into its\\nregister r1 and increments it, producing the value 1 in r1. At that point,\\nprocessor 2 comes into the picture, executing instructions 4–6.  Processor\\n2 loads x from memory into register r2; increments it, producing the\\nvalue 1 in r2; and then stores this value into x, setting x to 1. Now,\\nprocessor 1 resumes with step 7, storing the value 1 in r1 into x, which\\nleaves the value of x unchanged. Therefore, step 8 prints the value 1,\\nrather than the value 2 that the serial projection would print.\\nLet’s recap what happened. By sequential consistency, the effect of\\nthe parallel execution is as if the executed instructions of the two\\nprocessors are interleaved. If processor 1 executes all its instructions\\nbefore processor 2, a trivial interleaving, the value 2 is printed.\\nConversely, if processor 2 executes all its instructions before processor 1,\\nthe value 2 is still printed. When the instructions of the two processors\\ninterleave nontrivially, however, it is possible, as in this example\\nexecution, that one of the updates to x is lost, resulting in the value 1\\nbeing printed.\\nOf course, many executions do not elicit the bug. That’s the problem\\nwith determinacy races. Generally, most instruction orderings produce\\ncorrect results, such as any where the instructions on the left branch', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 992}),\n",
              " Document(page_content='execute before the instructions on the right branch, or vice versa. But\\nsome orderings generate improper results when the instructions\\ninterleave. Consequently, races can be extremely hard to test for. Your\\nprogram may fail, but you may be unable to reliably reproduce the\\nfailure in subsequent tests, confounding your attempts to locate the bug\\nin your code and ﬁx it. Task-parallel programming environments often\\nprovide race-detection productivity tools to help you isolate race bugs.\\nMany parallel programs in the real world are intentionally\\nnondeterministic. They contain determinacy races, but they mitigate the\\ndangers of nondeterminism through the use of mutual-exclusion locks\\nand other methods of synchronization. For our purposes, however, we’ll\\ninsist on an absence of determinacy races in the algorithms we develop.\\nNondeterministic programs are indeed interesting, but nondeterministic\\nprogramming is a more advanced topic and unnecessary for a wide\\nswath of interesting parallel algorithms.\\nTo ensure that algorithms are deterministic, any two strands that\\noperate in parallel should be mutually noninterfering: they only read,\\nand do not modify, any memory locations accessed by both of them.\\nConsequently, in a parallel for construct, such as the outer loop of P-\\nMAT-VEC, we want all the iterations of the body, including any code\\nan iteration executes in subroutines, to be mutually noninterfering. And\\nbetween a spawn and its corresponding sync, we want the code executed\\nby the spawned child and the code executed by the parent to be\\nmutually noninterfering, once again including invoked subroutines.\\nAs an example of how easy it is to write code with unintentional\\nraces, the P-MAT-VEC-WRONG procedure on the next page is a faulty\\nparallel implementation of matrix-vector multiplication that achieves a\\nspan of Θ (lg n) by parallelizing the inner for loop. This procedure is\\nincorrect, unfortunately, due to determinacy races when updating yi in\\nline 3, which executes in parallel for all n values of j.\\nIndex variables of parallel for loops, such as i in line 1 and j in line 2,\\ndo not cause races between iterations. Conceptually, each iteration of\\nthe loop creates an independent variable to hold the index of that\\niteration during that iteration’s execution of the loop body. Even if two\\nparallel iterations both access the same index variable, they really are', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 993}),\n",
              " Document(page_content='accessing different variable instances—hence different memory\\nlocations—and no race occurs.\\nP-MAT-VEC-WRONG (A, x, y, n)\\n1parallel for\\xa0i = 1 to\\xa0n\\n2 parallel for\\xa0j = 1 to\\xa0n\\n3 yi = yi + aijxj // determinacy race\\nA parallel algorithm with races can sometimes be deterministic. As\\nan example, two parallel threads might store the same value into a\\nshared variable, and it wouldn’t matter which stored the value ﬁrst. For\\nsimplicity, however, we generally prefer code without determinacy races,\\neven if the races are benign. And good parallel programmers frown on\\ncode with determinacy races that cause nondeterministic behavior, if\\ndeterministic code that performs comparably is an option.\\nBut nondeterministic code does have its place. For example, you\\ncan’t implement a parallel hash table, a highly practical data structure,\\nwithout writing code containing determinacy races. Much research has\\ncentered around how to extend the fork-join model to incorporate\\nlimited “structured” nondeterminism while avoiding the full measure of\\ncomplications that arise when nondeterminism is completely\\nunrestricted.\\nA chess lesson\\nTo illustrate the power of work/span analysis, this section closes with a\\ntrue story that occurred during the development of one of the ﬁrst\\nworld-class parallel chess-playing programs [106] many years ago. The\\ntimings below have been simpliﬁed for exposition.\\nThe chess program was developed and tested on a 32-processor\\ncomputer, but it was designed to run on a supercomputer with 512\\nprocessors. Since the supercomputer availability was limited and\\nexpensive, the developers ran benchmarks on the small computer and\\nextrapolated performance to the large computer.\\nAt one point, the developers incorporated an optimization into the\\nprogram that reduced its running time on an important benchmark on', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 994}),\n",
              " Document(page_content='the small machine from T32 = 65 seconds to \\n  seconds. Yet, the\\ndevelopers used the work and span performance measures to conclude\\nthat the optimized version, which was faster on 32 processors, would\\nactually be slower than the original version on the 512 processors of the\\nlarge machine. As a result, they abandoned the “optimization.”\\nHere is their work/span analysis. The original version of the program\\nhad work T1 = 2048 seconds and span T∞= 1 second. Let’s treat\\ninequality (26.4) on page 760 as the equation TP = T1/P + T∞, which\\nwe can use as an approximation to the running time on P processors.\\nThen indeed we have T32 = 2048/32 + 1 = 65. With the optimization,\\nthe work becomes T′1 = 1024 seconds, and the span becomes T′∞ = 8\\nseconds. Our approximation gives T′32 = 1024/32 + 8 = 40.\\nThe relative speeds of the two versions switch when we estimate their\\nrunning times on 512 processors, however. The ﬁrst version has a\\nrunning time of T512 = 2048/512+1 = 5 seconds, and the second version\\nruns in \\n  seconds. The optimization that speeds up\\nthe program on 32 processors makes the program run for twice as long\\non 512 processors! The optimized version’s span of 8, which is not the\\ndominant term in the running time on 32 processors, becomes the\\ndominant term on 512 processors, nullifying the advantage from using\\nmore processors. The optimization does not scale up.\\nThe moral of the story is that work/span analysis, and measurements\\nof work and span, can be superior to measured running times alone in\\nextrapolating an algorithm’s scalability.\\nExercises\\n26.1-1\\nWhat does a trace for the execution of a serial algorithm look like?\\n26.1-2\\nSuppose that line 4 of P-FIB spawns P-FIB (n − 2), rather than calling\\nit as is done in the pseudocode. How would the trace of P-FIB(4) in\\nFigure 26.2 change? What is the impact on the asymptotic work, span,\\nand parallelism?', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 995}),\n",
              " Document(page_content='26.1-3\\nDraw the trace that results from executing P-FIB(5). Assuming that\\neach strand in the computation takes unit time, what are the work,\\nspan, and parallelism of the computation? Show how to schedule the\\ntrace on 3 processors using greedy scheduling by labeling each strand\\nwith the time step in which it is executed.\\n26.1-4\\nProve that a greedy scheduler achieves the following time bound, which\\nis slightly stronger than the bound proved in Theorem 26.1:\\n26.1-5\\nConstruct a trace for which one execution by a greedy scheduler can\\ntake nearly twice the time of another execution by a greedy scheduler on\\nthe same number of processors. Describe how the two executions would\\nproceed.\\n26.1-6\\nProfessor Karan measures her deterministic task-parallel algorithm on\\n4, 10, and 64 processors of an ideal parallel computer using a greedy\\nscheduler. She claims that the three runs yielded T4 = 80 seconds, T10 =\\n42 seconds, and T64 = 10 seconds. Argue that the professor is either\\nlying or incompetent. (Hint: Use the work law (26.2), the span law\\n(26.3), and inequality (26.5) from Exercise 26.1-4.)\\n26.1-7\\nGive a parallel algorithm to multiply an n × n matrix by an n-vector that\\nachieves Θ (n2/lg n) parallelism while maintaining Θ (n2) work.\\n26.1-8\\nAnalyze the work, span, and parallelism of the procedure P-\\nTRANSPOSE, which transposes an n × n matrix A in place.\\nP-TRANSPOSE (A, n)', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 996}),\n",
              " Document(page_content='1parallel for\\xa0j = 2 to\\xa0n\\n2 parallel for\\xa0i = 1 to\\xa0j − 1\\n3 exchange aij with aji\\n26.1-9\\nSuppose that instead of a parallel for loop in line 2, the P-TRANSPOSE\\nprocedure in Exercise 26.1-8 had an ordinary for loop. Analyze the\\nwork, span, and parallelism of the resulting algorithm.\\n26.1-10\\nFor what number of processors do the two versions of the chess\\nprogram run equally fast, assuming that TP = T1/P + T∞?\\n26.2\\xa0\\xa0\\xa0\\xa0Parallel matrix multiplication\\nIn this section, we’ll explore how to parallelize the three matrix-\\nmultiplication algorithms from Sections 4.1 and 4.2. We’ll see that each\\nalgorithm can be parallelized in a straightforward fashion using either\\nparallel loops or recursive spawning. We’ll analyze them using\\nwork/span analysis, and we’ll see that each parallel algorithm attains the\\nsame performance on one processor as its corresponding serial\\nalgorithm, while scaling up to large numbers of processors.\\nA parallel algorithm for matrix multiplication us ing par allel loops\\nThe ﬁrst algorithm we’ll study is P-MATRIX-MULTIPLY, which\\nsimply parallelizes the two outer loops in the procedure MATRIX-\\nMULTIPLY on page 81.\\nP-MATRIX-MULTIPLY (A, B, C, n)\\n1parallel for\\xa0i = 1 to\\xa0n // compute entries in each of n rows\\n2 parallel for\\xa0j = 1 to\\xa0n // compute n entries in row i\\n3 for\\xa0k = 1 to\\xa0n\\n4 cij = cij + aik · bkj// add in another term of equation\\n(4.1)', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 997}),\n",
              " Document(page_content='Let’s analyze P-MATRIX-MULTIPLY. Since the serial projection of\\nthe algorithm is just MATRIX-MULTIPLY, the work is the same as the\\nrunning time of MATRIX-MULTIPLY: T1(n) = Θ (n3). The span is\\nT∞(n) = Θ (n), because it follows a path down the tree of recursion for\\nthe parallel for loop starting in line 1, then down the tree of recursion\\nfor the parallel for loop starting in line 2, and then executes all n\\niterations of the ordinary for loop starting in line 3, resulting in a total\\nspan of Θ (lg n) + Θ (lg n) + Θ (n) = Θ (n). Thus the parallelism is Θ (n3)/\\nΘ(n) = Θ (n2). (Exercise 26.2-3 asks you to parallelize the inner loop to\\nobtain a parallelism of Θ (n3/lg n), which you cannot do\\nstraightforwardly using parallel for, because you would create races.)\\nA parallel divide-and-conquer algorithm for matrix multiplication\\nSection 4.1 shows how to multiply n × n matrices serially in Θ (n3) time\\nusing a divide-and-conquer strategy. Let’s see how to parallelize that\\nalgorithm using recursive spawning instead of calls.\\nThe serial MATRIX-MULTIPLY-RECURSIVE procedure on page\\n83 takes as input three n × n matrices A, B, and C and performs the\\nmatrix calculation C = C + A · B by recursively performing eight\\nmultiplications of n/2 × n/2 submatrices of A and B. The P-MATRIX-\\nMULTIPLY-RECURSIVE procedure on the following page\\nimplements the same divide-and-conquer strategy, but it uses spawning\\nto perform the eight multiplications in parallel. To avoid determinacy\\nraces in updating the elements of C, it creates a temporary matrix D to\\nstore four of the submatrix products. At the end, it adds C and D\\ntogether to produce the ﬁnal result. (Problem 26-2 asks you to eliminate\\nthe temporary matrix D at the expense of some parallelism.)\\nLines 2–3 of P-MATRIX-MULTIPLY-RECURSIVE handle the\\nbase case of multiplying 1 × 1 matrices. The remainder of the procedure\\ndeals with the recursive case. Line 4 allocates a temporary matrix D,\\nand lines 5–7 zero it. Line 8 partitions each of the four matrices A, B, C,\\nand D into n/2 × n/2 submatrices. (As with MATRIX-MULTIPLY-\\nRECURSIVE on page 83, we’re glossing over the subtle issue of how to', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 998}),\n",
              " Document(page_content='use index calculations to represent submatrix sections of a matrix.) The\\nspawned recursive call in line 9 sets C11 = C11 + A11 · B11, so that C11\\naccumulates the ﬁrst of the two terms in equation (4.5) on page 82.\\nSimilarly, lines 10–12 cause each of C12, C21, and C22 in parallel to\\naccumulate the ﬁrst of the two terms in equations (4.6)–(4.8),\\nrespectively. Line 13 sets the submatrix D11 to the submatrix product\\nA12 · B21, so that D11 equals the second of the two terms in equation\\n(4.5). Lines 14–16 set each of D12, D21, and D22 in parallel to the\\nsecond of the two terms in equations (4.6)–(4.8), respectively. The sync\\nstatement in line 17 ensures that all the spawned submatrix products in\\nlines 9–16 have been computed, after which the doubly nested parallel\\nfor loops in lines 18–20 add the elements of D to the corresponding\\nelements of C.\\nP-MATRIX-MULTIPLY-RECURSIVE (A, B, C, n)\\n\\xa0\\xa01if\\xa0n == 1 // just one element in each matrix?\\n\\xa0\\xa02 c11 = c11 + a11 · b11\\n\\xa0\\xa03 return\\n\\xa0\\xa04let D be a new n × n matrix// temporary matrix\\n\\xa0\\xa05parallel for\\xa0i = 1 to\\xa0n // set D = 0\\n\\xa0\\xa06 parallel for\\xa0j = 1 to\\xa0n\\n\\xa0\\xa07 dij = 0\\n\\xa0\\xa08partition A, B, C, and D into n/2 × n/2 submatrices A11, A12, A21,\\nA22; B11, B12, B21, B22; C11, C12, C21, C22; and D11, D12,\\nD21, D22; respectively\\n\\xa0\\xa09spawn P-MATRIX-MULTIPLY-RECURSIVE (A11, B11, C11,\\nn/2)\\n10spawn P-MATRIX-MULTIPLY-RECURSIVE (A11, B12, C12,\\nn/2)\\n11spawn P-MATRIX-MULTIPLY-RECURSIVE (A21, B11, C21,\\nn/2)\\n12spawn P-MATRIX-MULTIPLY-RECURSIVE (A21, B12, C22,', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 999}),\n",
              " ...]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Step 4: Split the Text into Chunks**"
      ],
      "metadata": {
        "id": "68n73kgaYcl1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter=RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)"
      ],
      "metadata": {
        "id": "Tz3q1oCgYPye",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1702083592199,
          "user_tz": 360,
          "elapsed": 5,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs=text_splitter.split_documents(data)"
      ],
      "metadata": {
        "id": "9kqT_CZVYr1Q",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1702083592859,
          "user_tz": 360,
          "elapsed": 664,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ejpwUSmYx27",
        "outputId": "a2af4fda-33c7-4560-e088-60243bfd6933",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1702083592859,
          "user_tz": 360,
          "elapsed": 6,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6112"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "docs[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dp9fEipegFuZ",
        "outputId": "d7b7f751-5fcf-4782-d43e-051a4f03c8e2",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1702083592859,
          "user_tz": 360,
          "elapsed": 5,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(page_content='Introduction to Algorithms\\nFourth Edition', metadata={'source': '/content/Introduction.to.Algorithms.4th.Edition.pdf', 'page': 1})"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Step 5: Setup the Environment**"
      ],
      "metadata": {
        "id": "K24-STW9ZGnG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"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\"\n",
        "PINECONE_API_KEY = os.environ.get('PINECONE_API_KEY', '657b2891-44ea-4b25-a99c-dd9d86f9c068')\n",
        "PINECONE_API_ENV = os.environ.get('PINECONE_API_ENV', 'gcp-starter')"
      ],
      "metadata": {
        "id": "aZMYGbDlY1pL",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1702083592859,
          "user_tz": 360,
          "elapsed": 3,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Step 6: Downlaod the Embeddings**"
      ],
      "metadata": {
        "id": "waQtomfxZhM4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings=HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')"
      ],
      "metadata": {
        "id": "_937693LZpoY",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1702083599254,
          "user_tz": 360,
          "elapsed": 6398,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Step 7: Initializing the Pinecone**"
      ],
      "metadata": {
        "id": "a9pCsYXaaL6E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize pinecone\n",
        "pinecone.init(\n",
        "    api_key=PINECONE_API_KEY,  # find at app.pinecone.io\n",
        "    environment=PINECONE_API_ENV  # next to api key in console\n",
        ")\n",
        "index_name = \"langchainpinecone\" # put in the name of your pinecone index here"
      ],
      "metadata": {
        "id": "7vYf3XVZZ0eY",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1702083599782,
          "user_tz": 360,
          "elapsed": 540,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Step 8: Create Embeddings for Each of the Text Chunk**"
      ],
      "metadata": {
        "id": "BAAiFNyoawX1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "docsearch=Pinecone.from_texts([t.page_content for t in docs], embeddings, index_name=index_name)"
      ],
      "metadata": {
        "id": "aPTbec-ea1r8",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1702083948994,
          "user_tz": 360,
          "elapsed": 349215,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# If you already have an index, you can load it like this\n"
      ],
      "metadata": {
        "id": "amB5Se8cs3zo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#docsearch = Pinecone.from_existing_index(index_name, embeddings)\n"
      ],
      "metadata": {
        "id": "ewMdzQFGs16I",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1702083948995,
          "user_tz": 360,
          "elapsed": 19,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Step 9: Similarity Search**"
      ],
      "metadata": {
        "id": "mQbKwQZubUI3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#query=\"What are examples of good data science teams?\"\n",
        "query=\"who is the author of the introduction to algorithms 4th edition\""
      ],
      "metadata": {
        "id": "SrXpO0ecbSWb",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1702083948995,
          "user_tz": 360,
          "elapsed": 6,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs=docsearch.similarity_search(query)"
      ],
      "metadata": {
        "id": "BDxlZIn-bbQY",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1702083949597,
          "user_tz": 360,
          "elapsed": 607,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7lXRzM-XbqHU",
        "outputId": "a4809a16-cd53-4ca8-c317-97457d477a70",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1702083949598,
          "user_tz": 360,
          "elapsed": 5,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content='Introduction to Algorithms\\nFourth Edition'),\n",
              " Document(page_content='Introduction to Algorithms\\nFourth Edition'),\n",
              " Document(page_content='Thomas H. Cormen\\nCharles E. Leiserson\\nRonald L. Rivest\\nClifford Stein\\nIntroduction to Algorithms\\nFourth Edition\\nThe MIT Press\\nCambridge, Massachusetts London, England'),\n",
              " Document(page_content='Thomas H. Cormen\\nCharles E. Leiserson\\nRonald L. Rivest\\nClifford Stein\\nIntroduction to Algorithms\\nFourth Edition\\nThe MIT Press\\nCambridge, Massachusetts London, England')]"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Step 9: Query the Docs to get the Answer Back (Llama 2 Model)**"
      ],
      "metadata": {
        "id": "SlcdYpG2rlRc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install llama-cpp-python --force-reinstall --upgrade --no-cache-dir --verbose"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5qLilW73vIby",
        "outputId": "a3e77d73-6282-4d53-c279-07bfc3b68a1a",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1702084084485,
          "user_tz": 360,
          "elapsed": 134891,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using pip 23.1.2 from /usr/local/lib/python3.10/dist-packages/pip (python 3.10)\n",
            "Collecting llama-cpp-python\n",
            "  Downloading llama_cpp_python-0.2.20.tar.gz (8.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Running command pip subprocess to install build dependencies\n",
            "  Using pip 23.1.2 from /usr/local/lib/python3.10/dist-packages/pip (python 3.10)\n",
            "  Collecting scikit-build-core[pyproject]>=0.5.1\n",
            "    Using cached scikit_build_core-0.7.0-py3-none-any.whl (136 kB)\n",
            "  Collecting exceptiongroup (from scikit-build-core[pyproject]>=0.5.1)\n",
            "    Using cached exceptiongroup-1.2.0-py3-none-any.whl (16 kB)\n",
            "  Collecting packaging>=20.9 (from scikit-build-core[pyproject]>=0.5.1)\n",
            "    Using cached packaging-23.2-py3-none-any.whl (53 kB)\n",
            "  Collecting tomli>=1.1 (from scikit-build-core[pyproject]>=0.5.1)\n",
            "    Using cached tomli-2.0.1-py3-none-any.whl (12 kB)\n",
            "  Collecting pathspec>=0.10.1 (from scikit-build-core[pyproject]>=0.5.1)\n",
            "    Using cached pathspec-0.11.2-py3-none-any.whl (29 kB)\n",
            "  Collecting pyproject-metadata>=0.5 (from scikit-build-core[pyproject]>=0.5.1)\n",
            "    Using cached pyproject_metadata-0.7.1-py3-none-any.whl (7.4 kB)\n",
            "  Installing collected packages: tomli, pathspec, packaging, exceptiongroup, scikit-build-core, pyproject-metadata\n",
            "  ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "  lida 0.0.10 requires fastapi, which is not installed.\n",
            "  lida 0.0.10 requires kaleido, which is not installed.\n",
            "  lida 0.0.10 requires python-multipart, which is not installed.\n",
            "  lida 0.0.10 requires uvicorn, which is not installed.\n",
            "  tensorflow 2.14.0 requires numpy>=1.23.5, but you have numpy 1.23.4 which is incompatible.\n",
            "  Successfully installed exceptiongroup-1.2.0 packaging-23.2 pathspec-0.11.2 pyproject-metadata-0.7.1 scikit-build-core-0.7.0 tomli-2.0.1\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Running command Getting requirements to build wheel\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Running command pip subprocess to install backend dependencies\n",
            "  Using pip 23.1.2 from /usr/local/lib/python3.10/dist-packages/pip (python 3.10)\n",
            "  Collecting ninja>=1.5\n",
            "    Using cached ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl (307 kB)\n",
            "  Collecting cmake>=3.21\n",
            "    Using cached cmake-3.27.9-py2.py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (26.1 MB)\n",
            "  Installing collected packages: ninja, cmake\n",
            "    Creating /tmp/pip-build-env-nh14dz_l/normal/local/bin\n",
            "    changing mode of /tmp/pip-build-env-nh14dz_l/normal/local/bin/ninja to 755\n",
            "    changing mode of /tmp/pip-build-env-nh14dz_l/normal/local/bin/cmake to 755\n",
            "    changing mode of /tmp/pip-build-env-nh14dz_l/normal/local/bin/cpack to 755\n",
            "    changing mode of /tmp/pip-build-env-nh14dz_l/normal/local/bin/ctest to 755\n",
            "  Successfully installed cmake-3.27.9 ninja-1.11.1.1\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Running command Preparing metadata (pyproject.toml)\n",
            "  *** scikit-build-core 0.7.0 using CMake 3.27.9 (metadata_wheel)\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting typing-extensions>=4.5.0 (from llama-cpp-python)\n",
            "  Downloading typing_extensions-4.8.0-py3-none-any.whl (31 kB)\n",
            "Collecting numpy>=1.20.0 (from llama-cpp-python)\n",
            "  Downloading numpy-1.26.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m87.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting diskcache>=5.6.1 (from llama-cpp-python)\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m121.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: llama-cpp-python\n",
            "  Running command Building wheel for llama-cpp-python (pyproject.toml)\n",
            "  *** scikit-build-core 0.7.0 using CMake 3.27.9 (wheel)\n",
            "  *** Configuring CMake...\n",
            "  loading initial cache file /tmp/tmpigdzttt1/build/CMakeInit.txt\n",
            "  -- The C compiler identification is GNU 11.4.0\n",
            "  -- The CXX compiler identification is GNU 11.4.0\n",
            "  -- Detecting C compiler ABI info\n",
            "  -- Detecting C compiler ABI info - done\n",
            "  -- Check for working C compiler: /usr/bin/cc - skipped\n",
            "  -- Detecting C compile features\n",
            "  -- Detecting C compile features - done\n",
            "  -- Detecting CXX compiler ABI info\n",
            "  -- Detecting CXX compiler ABI info - done\n",
            "  -- Check for working CXX compiler: /usr/bin/c++ - skipped\n",
            "  -- Detecting CXX compile features\n",
            "  -- Detecting CXX compile features - done\n",
            "  -- Found Git: /usr/bin/git (found version \"2.34.1\")\n",
            "  -- Performing Test CMAKE_HAVE_LIBC_PTHREAD\n",
            "  -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success\n",
            "  -- Found Threads: TRUE\n",
            "  -- Found CUDAToolkit: /usr/local/cuda/include (found version \"11.8.89\")\n",
            "  -- cuBLAS found\n",
            "  -- The CUDA compiler identification is NVIDIA 11.8.89\n",
            "  -- Detecting CUDA compiler ABI info\n",
            "  -- Detecting CUDA compiler ABI info - done\n",
            "  -- Check for working CUDA compiler: /usr/local/cuda/bin/nvcc - skipped\n",
            "  -- Detecting CUDA compile features\n",
            "  -- Detecting CUDA compile features - done\n",
            "  -- Using CUDA architectures: 52;61;70\n",
            "  GNU ld (GNU Binutils for Ubuntu) 2.38\n",
            "  -- CMAKE_SYSTEM_PROCESSOR: x86_64\n",
            "  -- x86 detected\n",
            "  CMake Warning (dev) at CMakeLists.txt:20 (install):\n",
            "    Target llama has PUBLIC_HEADER files but no PUBLIC_HEADER DESTINATION.\n",
            "  This warning is for project developers.  Use -Wno-dev to suppress it.\n",
            "\n",
            "  CMake Warning (dev) at CMakeLists.txt:29 (install):\n",
            "    Target llama has PUBLIC_HEADER files but no PUBLIC_HEADER DESTINATION.\n",
            "  This warning is for project developers.  Use -Wno-dev to suppress it.\n",
            "\n",
            "  -- Configuring done (12.6s)\n",
            "  -- Generating done (0.2s)\n",
            "  -- Build files have been written to: /tmp/tmpigdzttt1/build\n",
            "  *** Building project with Ninja...\n",
            "  Change Dir: '/tmp/tmpigdzttt1/build'\n",
            "\n",
            "  Run Build Command(s): /tmp/pip-build-env-nh14dz_l/normal/local/lib/python3.10/dist-packages/ninja/data/bin/ninja -v\n",
            "  [1/23] /usr/bin/cc -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_USE_CUBLAS -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-nxel_s3t/llama-cpp-python_0be589b98c9b4637bdf9c51f61d7ba08/vendor/llama.cpp/. -isystem /usr/local/cuda/include -O3 -DNDEBUG -std=gnu11 -fPIC -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -Wdouble-promotion -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -march=native -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-alloc.c.o -MF vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-alloc.c.o.d -o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-alloc.c.o -c /tmp/pip-install-nxel_s3t/llama-cpp-python_0be589b98c9b4637bdf9c51f61d7ba08/vendor/llama.cpp/ggml-alloc.c\n",
            "  [2/23] cd /tmp/pip-install-nxel_s3t/llama-cpp-python_0be589b98c9b4637bdf9c51f61d7ba08/vendor/llama.cpp && /tmp/pip-build-env-nh14dz_l/normal/local/lib/python3.10/dist-packages/cmake/data/bin/cmake -DMSVC= -DCMAKE_C_COMPILER_VERSION=11.4.0 -DCMAKE_C_COMPILER_ID=GNU -DCMAKE_VS_PLATFORM_NAME= -DCMAKE_C_COMPILER=/usr/bin/cc -P /tmp/pip-install-nxel_s3t/llama-cpp-python_0be589b98c9b4637bdf9c51f61d7ba08/vendor/llama.cpp/common/../scripts/gen-build-info-cpp.cmake\n",
            "  -- Found Git: /usr/bin/git (found version \"2.34.1\")\n",
            "  [3/23] /usr/bin/c++ -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_USE_CUBLAS -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600  -O3 -DNDEBUG -std=gnu++11 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -march=native -MD -MT vendor/llama.cpp/common/CMakeFiles/build_info.dir/build-info.cpp.o -MF vendor/llama.cpp/common/CMakeFiles/build_info.dir/build-info.cpp.o.d -o vendor/llama.cpp/common/CMakeFiles/build_info.dir/build-info.cpp.o -c /tmp/pip-install-nxel_s3t/llama-cpp-python_0be589b98c9b4637bdf9c51f61d7ba08/vendor/llama.cpp/common/build-info.cpp\n",
            "  [4/23] /usr/bin/cc -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_USE_CUBLAS -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-nxel_s3t/llama-cpp-python_0be589b98c9b4637bdf9c51f61d7ba08/vendor/llama.cpp/. -isystem /usr/local/cuda/include -O3 -DNDEBUG -std=gnu11 -fPIC -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -Wdouble-promotion -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -march=native -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-backend.c.o -MF vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-backend.c.o.d -o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-backend.c.o -c /tmp/pip-install-nxel_s3t/llama-cpp-python_0be589b98c9b4637bdf9c51f61d7ba08/vendor/llama.cpp/ggml-backend.c\n",
            "  [5/23] /usr/bin/cc -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_USE_CUBLAS -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-nxel_s3t/llama-cpp-python_0be589b98c9b4637bdf9c51f61d7ba08/vendor/llama.cpp/. -isystem /usr/local/cuda/include -O3 -DNDEBUG -std=gnu11 -fPIC -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -Wdouble-promotion -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -march=native -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-quants.c.o -MF vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-quants.c.o.d -o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-quants.c.o -c /tmp/pip-install-nxel_s3t/llama-cpp-python_0be589b98c9b4637bdf9c51f61d7ba08/vendor/llama.cpp/ggml-quants.c\n",
            "  [6/23] /usr/bin/c++ -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_USE_CUBLAS -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-nxel_s3t/llama-cpp-python_0be589b98c9b4637bdf9c51f61d7ba08/vendor/llama.cpp/common/. -I/tmp/pip-install-nxel_s3t/llama-cpp-python_0be589b98c9b4637bdf9c51f61d7ba08/vendor/llama.cpp/. -O3 -DNDEBUG -std=gnu++11 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -march=native -MD -MT vendor/llama.cpp/common/CMakeFiles/common.dir/sampling.cpp.o -MF vendor/llama.cpp/common/CMakeFiles/common.dir/sampling.cpp.o.d -o vendor/llama.cpp/common/CMakeFiles/common.dir/sampling.cpp.o -c /tmp/pip-install-nxel_s3t/llama-cpp-python_0be589b98c9b4637bdf9c51f61d7ba08/vendor/llama.cpp/common/sampling.cpp\n",
            "  [7/23] /usr/bin/c++ -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_USE_CUBLAS -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-nxel_s3t/llama-cpp-python_0be589b98c9b4637bdf9c51f61d7ba08/vendor/llama.cpp/common/. -I/tmp/pip-install-nxel_s3t/llama-cpp-python_0be589b98c9b4637bdf9c51f61d7ba08/vendor/llama.cpp/. -O3 -DNDEBUG -std=gnu++11 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -march=native -MD -MT vendor/llama.cpp/common/CMakeFiles/common.dir/console.cpp.o -MF vendor/llama.cpp/common/CMakeFiles/common.dir/console.cpp.o.d -o vendor/llama.cpp/common/CMakeFiles/common.dir/console.cpp.o -c /tmp/pip-install-nxel_s3t/llama-cpp-python_0be589b98c9b4637bdf9c51f61d7ba08/vendor/llama.cpp/common/console.cpp\n",
            "  [8/23] /usr/bin/c++ -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_USE_CUBLAS -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-nxel_s3t/llama-cpp-python_0be589b98c9b4637bdf9c51f61d7ba08/vendor/llama.cpp/common/. -I/tmp/pip-install-nxel_s3t/llama-cpp-python_0be589b98c9b4637bdf9c51f61d7ba08/vendor/llama.cpp/. -O3 -DNDEBUG -std=gnu++11 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -march=native -MD -MT vendor/llama.cpp/common/CMakeFiles/common.dir/grammar-parser.cpp.o -MF vendor/llama.cpp/common/CMakeFiles/common.dir/grammar-parser.cpp.o.d -o vendor/llama.cpp/common/CMakeFiles/common.dir/grammar-parser.cpp.o -c /tmp/pip-install-nxel_s3t/llama-cpp-python_0be589b98c9b4637bdf9c51f61d7ba08/vendor/llama.cpp/common/grammar-parser.cpp\n",
            "  [9/23] /usr/bin/c++ -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_USE_CUBLAS -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-nxel_s3t/llama-cpp-python_0be589b98c9b4637bdf9c51f61d7ba08/vendor/llama.cpp/common/. -I/tmp/pip-install-nxel_s3t/llama-cpp-python_0be589b98c9b4637bdf9c51f61d7ba08/vendor/llama.cpp/. -O3 -DNDEBUG -std=gnu++11 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -march=native -MD -MT vendor/llama.cpp/common/CMakeFiles/common.dir/train.cpp.o -MF vendor/llama.cpp/common/CMakeFiles/common.dir/train.cpp.o.d -o vendor/llama.cpp/common/CMakeFiles/common.dir/train.cpp.o -c /tmp/pip-install-nxel_s3t/llama-cpp-python_0be589b98c9b4637bdf9c51f61d7ba08/vendor/llama.cpp/common/train.cpp\n",
            "  [10/23] /usr/bin/c++ -DLLAMA_BUILD -DLLAMA_SHARED -I/tmp/pip-install-nxel_s3t/llama-cpp-python_0be589b98c9b4637bdf9c51f61d7ba08/vendor/llama.cpp/examples/llava/. -I/tmp/pip-install-nxel_s3t/llama-cpp-python_0be589b98c9b4637bdf9c51f61d7ba08/vendor/llama.cpp/examples/llava/../.. -I/tmp/pip-install-nxel_s3t/llama-cpp-python_0be589b98c9b4637bdf9c51f61d7ba08/vendor/llama.cpp/examples/llava/../../common -I/tmp/pip-install-nxel_s3t/llama-cpp-python_0be589b98c9b4637bdf9c51f61d7ba08/vendor/llama.cpp/. -isystem /usr/local/cuda/include -O3 -DNDEBUG -fPIC -Wno-cast-qual -MD -MT vendor/llama.cpp/examples/llava/CMakeFiles/llava.dir/llava.cpp.o -MF vendor/llama.cpp/examples/llava/CMakeFiles/llava.dir/llava.cpp.o.d -o vendor/llama.cpp/examples/llava/CMakeFiles/llava.dir/llava.cpp.o -c /tmp/pip-install-nxel_s3t/llama-cpp-python_0be589b98c9b4637bdf9c51f61d7ba08/vendor/llama.cpp/examples/llava/llava.cpp\n",
            "  [11/23] /usr/bin/c++  -I/tmp/pip-install-nxel_s3t/llama-cpp-python_0be589b98c9b4637bdf9c51f61d7ba08/vendor/llama.cpp/common/. -I/tmp/pip-install-nxel_s3t/llama-cpp-python_0be589b98c9b4637bdf9c51f61d7ba08/vendor/llama.cpp/. -I/tmp/pip-install-nxel_s3t/llama-cpp-python_0be589b98c9b4637bdf9c51f61d7ba08/vendor/llama.cpp/examples/llava/. -I/tmp/pip-install-nxel_s3t/llama-cpp-python_0be589b98c9b4637bdf9c51f61d7ba08/vendor/llama.cpp/examples/llava/../.. -I/tmp/pip-install-nxel_s3t/llama-cpp-python_0be589b98c9b4637bdf9c51f61d7ba08/vendor/llama.cpp/examples/llava/../../common -O3 -DNDEBUG -MD -MT vendor/llama.cpp/examples/llava/CMakeFiles/llava-cli.dir/llava-cli.cpp.o -MF vendor/llama.cpp/examples/llava/CMakeFiles/llava-cli.dir/llava-cli.cpp.o.d -o vendor/llama.cpp/examples/llava/CMakeFiles/llava-cli.dir/llava-cli.cpp.o -c /tmp/pip-install-nxel_s3t/llama-cpp-python_0be589b98c9b4637bdf9c51f61d7ba08/vendor/llama.cpp/examples/llava/llava-cli.cpp\n",
            "  [12/23] /usr/bin/cc -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_USE_CUBLAS -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-nxel_s3t/llama-cpp-python_0be589b98c9b4637bdf9c51f61d7ba08/vendor/llama.cpp/. -isystem /usr/local/cuda/include -O3 -DNDEBUG -std=gnu11 -fPIC -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -Wdouble-promotion -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -march=native -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml.c.o -MF vendor/llama.cpp/CMakeFiles/ggml.dir/ggml.c.o.d -o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml.c.o -c /tmp/pip-install-nxel_s3t/llama-cpp-python_0be589b98c9b4637bdf9c51f61d7ba08/vendor/llama.cpp/ggml.c\n",
            "  [13/23] /usr/bin/c++ -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_USE_CUBLAS -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-nxel_s3t/llama-cpp-python_0be589b98c9b4637bdf9c51f61d7ba08/vendor/llama.cpp/common/. -I/tmp/pip-install-nxel_s3t/llama-cpp-python_0be589b98c9b4637bdf9c51f61d7ba08/vendor/llama.cpp/. -O3 -DNDEBUG -std=gnu++11 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -march=native -MD -MT vendor/llama.cpp/common/CMakeFiles/common.dir/common.cpp.o -MF vendor/llama.cpp/common/CMakeFiles/common.dir/common.cpp.o.d -o vendor/llama.cpp/common/CMakeFiles/common.dir/common.cpp.o -c /tmp/pip-install-nxel_s3t/llama-cpp-python_0be589b98c9b4637bdf9c51f61d7ba08/vendor/llama.cpp/common/common.cpp\n",
            "  [14/23] /usr/bin/c++ -DLLAMA_BUILD -DLLAMA_SHARED -I/tmp/pip-install-nxel_s3t/llama-cpp-python_0be589b98c9b4637bdf9c51f61d7ba08/vendor/llama.cpp/examples/llava/. -I/tmp/pip-install-nxel_s3t/llama-cpp-python_0be589b98c9b4637bdf9c51f61d7ba08/vendor/llama.cpp/examples/llava/../.. -I/tmp/pip-install-nxel_s3t/llama-cpp-python_0be589b98c9b4637bdf9c51f61d7ba08/vendor/llama.cpp/examples/llava/../../common -I/tmp/pip-install-nxel_s3t/llama-cpp-python_0be589b98c9b4637bdf9c51f61d7ba08/vendor/llama.cpp/. -isystem /usr/local/cuda/include -O3 -DNDEBUG -fPIC -Wno-cast-qual -MD -MT vendor/llama.cpp/examples/llava/CMakeFiles/llava.dir/clip.cpp.o -MF vendor/llama.cpp/examples/llava/CMakeFiles/llava.dir/clip.cpp.o.d -o vendor/llama.cpp/examples/llava/CMakeFiles/llava.dir/clip.cpp.o -c /tmp/pip-install-nxel_s3t/llama-cpp-python_0be589b98c9b4637bdf9c51f61d7ba08/vendor/llama.cpp/examples/llava/clip.cpp\n",
            "  [15/23] : && /tmp/pip-build-env-nh14dz_l/normal/local/lib/python3.10/dist-packages/cmake/data/bin/cmake -E rm -f vendor/llama.cpp/examples/llava/libllava_static.a && /usr/bin/ar qc vendor/llama.cpp/examples/llava/libllava_static.a  vendor/llama.cpp/examples/llava/CMakeFiles/llava.dir/llava.cpp.o vendor/llama.cpp/examples/llava/CMakeFiles/llava.dir/clip.cpp.o && /usr/bin/ranlib vendor/llama.cpp/examples/llava/libllava_static.a && :\n",
            "  [16/23] /usr/bin/c++ -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_USE_CUBLAS -DK_QUANTS_PER_ITERATION=2 -DLLAMA_BUILD -DLLAMA_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dllama_EXPORTS -I/tmp/pip-install-nxel_s3t/llama-cpp-python_0be589b98c9b4637bdf9c51f61d7ba08/vendor/llama.cpp/. -isystem /usr/local/cuda/include -O3 -DNDEBUG -std=gnu++11 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wno-format-truncation -Wextra-semi -march=native -MD -MT vendor/llama.cpp/CMakeFiles/llama.dir/llama.cpp.o -MF vendor/llama.cpp/CMakeFiles/llama.dir/llama.cpp.o.d -o vendor/llama.cpp/CMakeFiles/llama.dir/llama.cpp.o -c /tmp/pip-install-nxel_s3t/llama-cpp-python_0be589b98c9b4637bdf9c51f61d7ba08/vendor/llama.cpp/llama.cpp\n",
            "  [17/23] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_USE_CUBLAS -DK_QUANTS_PER_ITERATION=2 -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -I/tmp/pip-install-nxel_s3t/llama-cpp-python_0be589b98c9b4637bdf9c51f61d7ba08/vendor/llama.cpp/. -isystem /usr/local/cuda/include -O3 -DNDEBUG -std=c++11 \"--generate-code=arch=compute_52,code=[compute_52,sm_52]\" \"--generate-code=arch=compute_61,code=[compute_61,sm_61]\" \"--generate-code=arch=compute_70,code=[compute_70,sm_70]\" -Xcompiler=-fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -use_fast_math -Wno-pedantic -Xcompiler \"-Wno-array-bounds -Wno-format-truncation -Wextra-semi\" -march=native -MD -MT vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda.cu.o -MF vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda.cu.o.d -x cu -c /tmp/pip-install-nxel_s3t/llama-cpp-python_0be589b98c9b4637bdf9c51f61d7ba08/vendor/llama.cpp/ggml-cuda.cu -o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda.cu.o\n",
            "  [18/23] : && /tmp/pip-build-env-nh14dz_l/normal/local/lib/python3.10/dist-packages/cmake/data/bin/cmake -E rm -f vendor/llama.cpp/libggml_static.a && /usr/bin/ar qc vendor/llama.cpp/libggml_static.a  vendor/llama.cpp/CMakeFiles/ggml.dir/ggml.c.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-alloc.c.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-backend.c.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-quants.c.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda.cu.o && /usr/bin/ranlib vendor/llama.cpp/libggml_static.a && :\n",
            "  [19/23] : && /usr/bin/g++ -fPIC  -shared -Wl,-soname,libggml_shared.so -o vendor/llama.cpp/libggml_shared.so vendor/llama.cpp/CMakeFiles/ggml.dir/ggml.c.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-alloc.c.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-backend.c.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-quants.c.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda.cu.o  /usr/local/cuda-11.8/targets/x86_64-linux/lib/libcudart.so  /usr/local/cuda-11.8/targets/x86_64-linux/lib/libcublas.so  /usr/local/cuda-11.8/targets/x86_64-linux/lib/libcublasLt.so  /usr/local/cuda-11.8/targets/x86_64-linux/lib/libculibos.a  -lcudadevrt  -lcudart_static  -lrt  -lpthread  -ldl -L\"/usr/local/cuda/targets/x86_64-linux/lib/stubs\" -L\"/usr/local/cuda/targets/x86_64-linux/lib\" && :\n",
            "  [20/23] : && /usr/bin/c++ -fPIC -O3 -DNDEBUG   -shared -Wl,-soname,libllama.so -o vendor/llama.cpp/libllama.so vendor/llama.cpp/CMakeFiles/ggml.dir/ggml.c.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-alloc.c.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-backend.c.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-quants.c.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda.cu.o vendor/llama.cpp/CMakeFiles/llama.dir/llama.cpp.o -L/usr/local/cuda/targets/x86_64-linux/lib -Wl,-rpath,/usr/local/cuda-11.8/targets/x86_64-linux/lib:  /usr/local/cuda-11.8/targets/x86_64-linux/lib/libcudart.so  /usr/local/cuda-11.8/targets/x86_64-linux/lib/libcublas.so  /usr/local/cuda-11.8/targets/x86_64-linux/lib/libcublasLt.so  /usr/local/cuda-11.8/targets/x86_64-linux/lib/libculibos.a  -lcudadevrt  -lcudart_static  -lrt  -lpthread  -ldl && :\n",
            "  [21/23] : && /tmp/pip-build-env-nh14dz_l/normal/local/lib/python3.10/dist-packages/cmake/data/bin/cmake -E rm -f vendor/llama.cpp/common/libcommon.a && /usr/bin/ar qc vendor/llama.cpp/common/libcommon.a  vendor/llama.cpp/common/CMakeFiles/build_info.dir/build-info.cpp.o vendor/llama.cpp/common/CMakeFiles/common.dir/common.cpp.o vendor/llama.cpp/common/CMakeFiles/common.dir/sampling.cpp.o vendor/llama.cpp/common/CMakeFiles/common.dir/console.cpp.o vendor/llama.cpp/common/CMakeFiles/common.dir/grammar-parser.cpp.o vendor/llama.cpp/common/CMakeFiles/common.dir/train.cpp.o && /usr/bin/ranlib vendor/llama.cpp/common/libcommon.a && :\n",
            "  [22/23] : && /usr/bin/c++ -fPIC -O3 -DNDEBUG   -shared -Wl,-soname,libllava.so -o vendor/llama.cpp/examples/llava/libllava.so vendor/llama.cpp/examples/llava/CMakeFiles/llava.dir/llava.cpp.o vendor/llama.cpp/examples/llava/CMakeFiles/llava.dir/clip.cpp.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml.c.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-alloc.c.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-backend.c.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-quants.c.o vendor/llama.cpp/CMakeFiles/ggml.dir/ggml-cuda.cu.o  -Wl,-rpath,/tmp/tmpigdzttt1/build/vendor/llama.cpp:/usr/local/cuda-11.8/targets/x86_64-linux/lib:  vendor/llama.cpp/libllama.so  /usr/local/cuda-11.8/targets/x86_64-linux/lib/libcudart.so  /usr/local/cuda-11.8/targets/x86_64-linux/lib/libcublas.so  /usr/local/cuda-11.8/targets/x86_64-linux/lib/libculibos.a  /usr/local/cuda-11.8/targets/x86_64-linux/lib/libcublasLt.so && :\n",
            "  [23/23] : && /usr/bin/c++ -O3 -DNDEBUG  vendor/llama.cpp/examples/llava/CMakeFiles/llava.dir/llava.cpp.o vendor/llama.cpp/examples/llava/CMakeFiles/llava.dir/clip.cpp.o vendor/llama.cpp/examples/llava/CMakeFiles/llava-cli.dir/llava-cli.cpp.o -o vendor/llama.cpp/examples/llava/llava-cli  -Wl,-rpath,/tmp/tmpigdzttt1/build/vendor/llama.cpp:/usr/local/cuda-11.8/targets/x86_64-linux/lib:  vendor/llama.cpp/common/libcommon.a  vendor/llama.cpp/libllama.so  /usr/local/cuda-11.8/targets/x86_64-linux/lib/libcudart.so  /usr/local/cuda-11.8/targets/x86_64-linux/lib/libcublas.so  /usr/local/cuda-11.8/targets/x86_64-linux/lib/libculibos.a  /usr/local/cuda-11.8/targets/x86_64-linux/lib/libcublasLt.so && :\n",
            "\n",
            "  *** Installing project into wheel...\n",
            "  -- Install configuration: \"Release\"\n",
            "  -- Installing: /tmp/tmpigdzttt1/wheel/platlib/lib/libggml_shared.so\n",
            "  -- Installing: /tmp/tmpigdzttt1/wheel/platlib/lib/cmake/Llama/LlamaConfig.cmake\n",
            "  -- Installing: /tmp/tmpigdzttt1/wheel/platlib/lib/cmake/Llama/LlamaConfigVersion.cmake\n",
            "  -- Installing: /tmp/tmpigdzttt1/wheel/platlib/include/ggml.h\n",
            "  -- Installing: /tmp/tmpigdzttt1/wheel/platlib/include/ggml-cuda.h\n",
            "  -- Installing: /tmp/tmpigdzttt1/wheel/platlib/lib/libllama.so\n",
            "  -- Set runtime path of \"/tmp/tmpigdzttt1/wheel/platlib/lib/libllama.so\" to \"\"\n",
            "  -- Installing: /tmp/tmpigdzttt1/wheel/platlib/include/llama.h\n",
            "  -- Installing: /tmp/tmpigdzttt1/wheel/platlib/bin/convert.py\n",
            "  -- Installing: /tmp/tmpigdzttt1/wheel/platlib/bin/convert-lora-to-ggml.py\n",
            "  -- Installing: /tmp/tmpigdzttt1/wheel/platlib/llama_cpp/libllama.so\n",
            "  -- Set runtime path of \"/tmp/tmpigdzttt1/wheel/platlib/llama_cpp/libllama.so\" to \"\"\n",
            "  -- Installing: /tmp/pip-install-nxel_s3t/llama-cpp-python_0be589b98c9b4637bdf9c51f61d7ba08/llama_cpp/libllama.so\n",
            "  -- Set runtime path of \"/tmp/pip-install-nxel_s3t/llama-cpp-python_0be589b98c9b4637bdf9c51f61d7ba08/llama_cpp/libllama.so\" to \"\"\n",
            "  -- Installing: /tmp/tmpigdzttt1/wheel/platlib/lib/libllava.so\n",
            "  -- Set runtime path of \"/tmp/tmpigdzttt1/wheel/platlib/lib/libllava.so\" to \"\"\n",
            "  -- Installing: /tmp/tmpigdzttt1/wheel/platlib/bin/llava-cli\n",
            "  -- Set runtime path of \"/tmp/tmpigdzttt1/wheel/platlib/bin/llava-cli\" to \"\"\n",
            "  -- Installing: /tmp/tmpigdzttt1/wheel/platlib/llama_cpp/libllava.so\n",
            "  -- Set runtime path of \"/tmp/tmpigdzttt1/wheel/platlib/llama_cpp/libllava.so\" to \"\"\n",
            "  -- Installing: /tmp/pip-install-nxel_s3t/llama-cpp-python_0be589b98c9b4637bdf9c51f61d7ba08/llama_cpp/libllava.so\n",
            "  -- Set runtime path of \"/tmp/pip-install-nxel_s3t/llama-cpp-python_0be589b98c9b4637bdf9c51f61d7ba08/llama_cpp/libllava.so\" to \"\"\n",
            "  *** Making wheel...\n",
            "  *** Created llama_cpp_python-0.2.20-cp310-cp310-manylinux_2_35_x86_64.whl...\n",
            "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.2.20-cp310-cp310-manylinux_2_35_x86_64.whl size=7138569 sha256=c298c508c05435f417afc9804be530d89290580f469c6f869833d198ffbf2257\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-uvxmz_8e/wheels/ef/f2/d2/0becb03047a348d7bd9a5b91ec88f4654d6fa7d67ea4e84d43\n",
            "Successfully built llama-cpp-python\n",
            "Installing collected packages: typing-extensions, numpy, diskcache, llama-cpp-python\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.8.0\n",
            "    Uninstalling typing_extensions-4.8.0:\n",
            "      Removing file or directory /usr/local/lib/python3.10/dist-packages/__pycache__/typing_extensions.cpython-310.pyc\n",
            "      Removing file or directory /usr/local/lib/python3.10/dist-packages/typing_extensions-4.8.0.dist-info/\n",
            "      Removing file or directory /usr/local/lib/python3.10/dist-packages/typing_extensions.py\n",
            "      Successfully uninstalled typing_extensions-4.8.0\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.23.4\n",
            "    Uninstalling numpy-1.23.4:\n",
            "      Removing file or directory /usr/local/bin/f2py\n",
            "      Removing file or directory /usr/local/bin/f2py3\n",
            "      Removing file or directory /usr/local/bin/f2py3.10\n",
            "      Removing file or directory /usr/local/lib/python3.10/dist-packages/numpy-1.23.4.dist-info/\n",
            "      Removing file or directory /usr/local/lib/python3.10/dist-packages/numpy.libs/\n",
            "      Removing file or directory /usr/local/lib/python3.10/dist-packages/numpy/\n",
            "      Successfully uninstalled numpy-1.23.4\n",
            "  changing mode of /usr/local/bin/f2py to 755\n",
            "  Attempting uninstall: diskcache\n",
            "    Found existing installation: diskcache 5.6.3\n",
            "    Uninstalling diskcache-5.6.3:\n",
            "      Removing file or directory /usr/local/lib/python3.10/dist-packages/diskcache-5.6.3.dist-info/\n",
            "      Removing file or directory /usr/local/lib/python3.10/dist-packages/diskcache/\n",
            "      Successfully uninstalled diskcache-5.6.3\n",
            "  Attempting uninstall: llama-cpp-python\n",
            "    Found existing installation: llama-cpp-python 0.1.78\n",
            "    Uninstalling llama-cpp-python-0.1.78:\n",
            "      Removing file or directory /usr/local/lib/python3.10/dist-packages/llama_cpp/\n",
            "      Removing file or directory /usr/local/lib/python3.10/dist-packages/llama_cpp_python-0.1.78.dist-info/\n",
            "      Successfully uninstalled llama-cpp-python-0.1.78\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires fastapi, which is not installed.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "lida 0.0.10 requires python-multipart, which is not installed.\n",
            "lida 0.0.10 requires uvicorn, which is not installed.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires openai, which is not installed.\n",
            "llmx 0.0.15a0 requires tiktoken, which is not installed.\n",
            "cupy-cuda11x 11.0.0 requires numpy<1.26,>=1.20, but you have numpy 1.26.2 which is incompatible.\n",
            "tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.8.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed diskcache-5.6.3 llama-cpp-python-0.2.20 numpy-1.26.2 typing-extensions-4.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Import All the Required Libraries"
      ],
      "metadata": {
        "id": "d8Obx-wbsPbR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import LlamaCpp\n",
        "from langchain.callbacks.manager import CallbackManager\n",
        "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
        "from huggingface_hub import hf_hub_download\n",
        "from langchain.chains.question_answering import load_qa_chain"
      ],
      "metadata": {
        "id": "eWDsgqwFfjLl",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1702084084485,
          "user_tz": 360,
          "elapsed": 6,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Callbacks support token-wise streaming\n",
        "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
        "# Verbose is required to pass to the callback manager"
      ],
      "metadata": {
        "id": "6IGdw0HUfjRB",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1702084084485,
          "user_tz": 360,
          "elapsed": 5,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Quantized Models from the Hugging Face Community"
      ],
      "metadata": {
        "id": "GASSH4KiuVWd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Hugging Face community provides quantized models, which allow us to efficiently and effectively utilize the model on the T4 GPU. It is important to consult reliable sources before using any model.\n",
        "\n",
        "There are several variations available, but the ones that interest us are based on the GGLM library.\n",
        "\n",
        "We can see the different variations that Llama-2-13B-GGML has [here](https://huggingface.co/models?search=llama%202%20ggml).\n",
        "\n",
        "\n",
        "\n",
        "In this case, we will use the model called [Llama-2-13B-chat-GGML](https://huggingface.co/TheBloke/Llama-2-13B-chat-GGML)."
      ],
      "metadata": {
        "id": "yB3z0JmNuZqo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Quantization reduces precision to optimize resource usage."
      ],
      "metadata": {
        "id": "_Byr1f3m7NMu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Quantization is a technique to reduce the computational and memory costs of running inference by representing the weights and activations with low-precision data types like 8-bit integer ( int8 ) instead of the usual 32-bit floating point ( float32 )."
      ],
      "metadata": {
        "id": "M-SRSFdz72S6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name_or_path = \"TheBloke/Llama-2-13B-chat-GGML\"\n",
        "model_basename = \"llama-2-13b-chat.ggmlv3.q5_1.bin\" # the model is in bin format\n",
        "\n",
        "# model_name_or_path = \"TheBloke/CodeLlama-13B-Python-GGUF\"\n",
        "# model_basename = \"codellama-13b-python.Q5_K_M.gguf\""
      ],
      "metadata": {
        "id": "nVK36nelfwL-",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1702084388794,
          "user_tz": 360,
          "elapsed": 197,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = hf_hub_download(repo_id=model_name_or_path, filename=model_basename)"
      ],
      "metadata": {
        "id": "uAYwhA9Zf61F",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1702084390115,
          "user_tz": 360,
          "elapsed": 209,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  pip install llama-cpp-python==0.1.78 and numpy==1.23.4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 550
        },
        "id": "W58xnBk2Dc7h",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1702084100191,
          "user_tz": 360,
          "elapsed": 15476,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "e0e8a4fa-c0d2-43d8-a062-76414f977546"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting llama-cpp-python==0.1.78\n",
            "  Using cached llama_cpp_python-0.1.78-cp310-cp310-linux_x86_64.whl\n",
            "Requirement already satisfied: and in /usr/local/lib/python3.10/dist-packages (66.0.2)\n",
            "Collecting numpy==1.23.4\n",
            "  Using cached numpy-1.23.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python==0.1.78) (4.8.0)\n",
            "Requirement already satisfied: diskcache>=5.6.1 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python==0.1.78) (5.6.3)\n",
            "Installing collected packages: numpy, llama-cpp-python\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.26.2\n",
            "    Uninstalling numpy-1.26.2:\n",
            "      Successfully uninstalled numpy-1.26.2\n",
            "  Attempting uninstall: llama-cpp-python\n",
            "    Found existing installation: llama_cpp_python 0.2.20\n",
            "    Uninstalling llama_cpp_python-0.2.20:\n",
            "      Successfully uninstalled llama_cpp_python-0.2.20\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires fastapi, which is not installed.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "lida 0.0.10 requires python-multipart, which is not installed.\n",
            "lida 0.0.10 requires uvicorn, which is not installed.\n",
            "tensorflow 2.14.0 requires numpy>=1.23.5, but you have numpy 1.23.4 which is incompatible.\n",
            "tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.8.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed llama-cpp-python-0.1.78 numpy-1.23.4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_gpu_layers=40  # Change this value based on your model and your GPU VRAM pool.\n",
        "n_batch=256  # Should be between 1 and n_ctx, consider the amount of VRAM in your GPU.\n",
        "\n",
        "# Loading model,\n",
        "llm = LlamaCpp(\n",
        "    model_path=model_path,\n",
        "    max_tokens=256,\n",
        "    n_gpu_layers=n_gpu_layers,\n",
        "    n_batch=n_batch,\n",
        "    callback_manager=callback_manager,\n",
        "    n_ctx=1024,\n",
        "    verbose=True,\n",
        ")\n",
        "\n",
        "# llm = LlamaCpp(\n",
        "#   model_path=model_path,\n",
        "#   max_tokens=256,\n",
        "#   n_gpu_layers=n_gpu_layers,\n",
        "#   n_batch=n_batch,\n",
        "#   callback_manager=callback_manager,\n",
        "#   n_ctx=1024,\n",
        "#   verbose=True,\n",
        "# )"
      ],
      "metadata": {
        "id": "tM3kTMKUfjT5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1702084498775,
          "user_tz": 360,
          "elapsed": 103703,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "4853661c-83ed-4549-e0f5-0ed2dba1164a"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain=load_qa_chain(llm, chain_type=\"stuff\")"
      ],
      "metadata": {
        "id": "npgS9EoYhA8m",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1702084514216,
          "user_tz": 360,
          "elapsed": 192,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query=\"who is the author of the introduction to algorithms 4th edition\"\n",
        "docs=docsearch.similarity_search(query)"
      ],
      "metadata": {
        "id": "ISwGOqhrhCfG",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1702084516314,
          "user_tz": 360,
          "elapsed": 695,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IEhQ8Qs_kNDa",
        "outputId": "9798c7aa-5b00-49ef-ecc0-6399ca4b3eb4",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1702084517534,
          "user_tz": 360,
          "elapsed": 206,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content='Introduction to Algorithms\\nFourth Edition'),\n",
              " Document(page_content='Introduction to Algorithms\\nFourth Edition'),\n",
              " Document(page_content='Thomas H. Cormen\\nCharles E. Leiserson\\nRonald L. Rivest\\nClifford Stein\\nIntroduction to Algorithms\\nFourth Edition\\nThe MIT Press\\nCambridge, Massachusetts London, England'),\n",
              " Document(page_content='Thomas H. Cormen\\nCharles E. Leiserson\\nRonald L. Rivest\\nClifford Stein\\nIntroduction to Algorithms\\nFourth Edition\\nThe MIT Press\\nCambridge, Massachusetts London, England')]"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain.run(input_documents=docs, question=query)"
      ],
      "metadata": {
        "id": "7d1OLGzzhDvw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "e803174e-a25b-453b-86aa-7d7876b0a5f4",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1702084704953,
          "user_tz": 360,
          "elapsed": 184880,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " The authors are Thomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest, and Clifford Stein."
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' The authors are Thomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest, and Clifford Stein.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query=\"who is the author of the introduction to algorithms 4th edition\"\n",
        "docs=docsearch.similarity_search(query)"
      ],
      "metadata": {
        "id": "XoVfiG-Yjcyv",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1702084820487,
          "user_tz": 360,
          "elapsed": 784,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain.run(input_documents=docs, question=query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "id": "za0WuBbkjc7A",
        "outputId": "d1ebcf72-40d9-44fa-9f50-182c17233d07",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1702084871186,
          "user_tz": 360,
          "elapsed": 39836,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Thomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest, and Clifford Stein are the authors of Introduction to Algorithms, 4th Edition."
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' Thomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest, and Clifford Stein are the authors of Introduction to Algorithms, 4th Edition.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Step 10: Query the Docs to get the Answer Back (Hugging Face Model)**"
      ],
      "metadata": {
        "id": "McjkimvGb9Me"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import HuggingFaceHub"
      ],
      "metadata": {
        "id": "N1ZKmRDZbsIc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm=HuggingFaceHub(repo_id=\"google/flan-t5-xxl\", model_kwargs={\"temperature\":0.5, \"max_length\":512})"
      ],
      "metadata": {
        "id": "7aMLkSG_cEfM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain=load_qa_chain(llm, chain_type=\"stuff\")"
      ],
      "metadata": {
        "id": "F4e33CLGcKml"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query=\"What are examples of good data science teams?\"\n",
        "docs=docsearch.similarity_search(query)"
      ],
      "metadata": {
        "id": "lNgpns06cb0I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain.run(input_documents=docs, question=query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "7sP_7PwlcfwV",
        "outputId": "7dd68009-776d-49e8-d6c6-ca18e8c74e3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Data Science teams need a broad view of the organization. Leaders must be key advocates who meet'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GoA5fiENfZjm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}